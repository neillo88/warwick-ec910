[
  {
    "objectID": "material-cef.html",
    "href": "material-cef.html",
    "title": "Conditional Expectation Function",
    "section": "",
    "text": "Consider the random variable \\(Y_i\\in\\mathbb{R}\\) and the random vector \\(X_i\\in\\mathbb{R}^k\\), \\(k\\geq1\\).1\n\n\nThe Conditional Expecation Function (CEF) - denoted \\(E[Y_i|X_i]\\) - is a random function. It is a function that returns the expected value of \\(Y_i\\) for each realized value of \\(X_i\\). Since \\(X_i\\) is a random vector the resulting function is random itself.\nIf we fix \\(X_i=x\\), then the value at which we are evaluating the function is no longer random. The result is a constant: the expected value of \\(Y_i\\) at the given \\(x\\).\n\\[\nE[Y_i|X_i=x] = \\int y \\cdot f_{Y}(y|X_i=x)dy = \\int y dF_{Y}(y|X_i=x)\n\\]\nThis follows the same logic that the expectation of a random variable is, \\(E[Y_i]\\), is not random.\nDiscrete case. The book devotes a lot time to the discussion of cases were \\(X_i\\) is a discrete random variable; using the notation \\(W_i\\in\\{0,1\\}\\) or \\(D_i\\in\\{0,1\\}\\). In this unique case, we can write the CEF as,\n\\[\nE[Y_i|D_i] = E[Y_i|D_i=0] + D_i\\cdot\\big(E[Y_i|D_i=1]-E[Y_i|D_i=0]\\big)\n\\]\nThe above function returns \\(E[Y_i|D_i=0]\\) when \\(D_i=0\\) and \\(E[Y_i|D_i=1]\\) when \\(D_i=1\\). This expression for the CEF will be useful in latter chapters of the book.\n\n\n\nThe Law of Iterated Expectations says that given two random variables2 \\([Y_i,X_i]\\), we can express the unconditional expected value of \\(Y_i\\) as the expected value of the conditional expectation of \\(Y_i\\) on \\(X_i\\).\n\\[\n        E[Y_i] = E\\big[E[Y_i|X_i]\\big]\n\\]\nWhere the outside expectation is with respect to \\(X_i\\),3 since the CEF is a random function of \\(X_i\\). We can expand this as follows,\n\\[\nE[Y_i] = \\int t \\cdot f_{Y_i}(t)dt = \\int\\int y \\cdot f_{Y_i|X}(y|x)dyf_X(x)dx = E\\big[E[Y_i|X_i]\\big]\n\\]\n\nExample 1 Suppose \\(Y_i\\) and \\(X_i\\) are both discrete, \\(Y_i\\in\\{1,2\\}\\) and \\(X_i\\in\\{3,4\\}\\), with the joint distribution:\n\n\\(f_{Y,X}\\)\n\n\n\n\\(X_i=3\\)\n\\(X_i=2\\)\n\n\n\\(Y_i=1\\)\n1/10\n3/10\n\n\n\\(Y_i=2\\)\n2/10\n4/10\n\n\n\nWe can then define the two marginal distributions,\n\n\\(f_Y\\)\n\n\n\\(Y_i=1\\)\n\\(Y_i=2\\)\n\n\n4/10\n6/10\n\n\n\nand,\n\n\\(f_X\\)\n\n\n\\(X_i=3\\)\n\\(X_i=4\\)\n\n\n3/10\n7/10\n\n\n\nLikewise, we know the conditional distribution \\(f_{Y|X}\\); which we get by dividing the joint distribution by the marginal distribution of \\(X_i\\). Each column of the conditional distribution should add up to 1.\n\n\\(f_{Y|X}\\)\n\n\n\n\\(X_i=3\\)\n\\(X_i=4\\)\n\n\n\\(Y_i=1\\)\n1/3\n3/7\n\n\n\\(Y_i=2\\)\n2/3\n4/7\n\n\n\nNow we can calculate the following objects:\n\n\\(E[Y_i]\\)\n\n\\[\n\\begin{aligned}\n        E[Y_i] =& 1\\cdot Pr(Y_i=1)+2\\cdot Pr(Y_i=2) \\\\\n        =&1\\cdot 4/10+2\\cdot 6/10 \\\\\n        =&16/10\n    \\end{aligned}\n\\]\n\n\\(E[Y_i|X_i=3]\\)\n\n\\[\n\\begin{aligned}\n        E[Y_i|X_i=3] =& 1\\cdot Pr(Y_i=1|X_i=3)+2\\cdot Pr(Y_i=2|X_i=3) \\\\\n        =&1\\cdot 1/3+2\\cdot 2/3 \\\\\n        =&5/3\n\\end{aligned}\n\\]\n\n\\(E[Y_i|X_i=4]\\)\n\n\\[\n\\begin{aligned}\n        E[Y_i|X_i=4] =& 1\\cdot Pr(Y_i=1|X_i=4)+2\\cdot Pr(Y_i=2|X_i=4) \\\\\n        =&1\\cdot 3/7+2\\cdot 4/7 \\\\\n        =&11/7\n    \\end{aligned}\n\\]\n\n\\(E\\big[E[Y_i|X_i]\\big]\\)\n\n\\[\n\\begin{aligned}\n        E\\big[E[Y_i|X_i]\\big] =& E[Y_i|X_i=3]\\cdot Pr(X_i=3)+ E[Y_i|X_i=4]\\cdot Pr(X_i=4) \\\\\n        =&5/3\\cdot3/10+11/7\\cdot 7/10 \\\\\n        =&16/10\n    \\end{aligned}\n\\]\nWe have therefore demonstrated the law of iterated expectations.\n\nWe can extend this principle to conditional expectations. Suppose you have three random variables/vectors \\(\\{Y_i,X_i,Z_i\\}\\), we can express the conditional expected value of \\(Y_i\\) on \\(X_i\\) as the (conditional) expected value of the conditional expectation of \\(Y_i\\) on \\(X_i\\) and \\(Z_i\\).\n\\[\n        E[Y_i|X_i] = E\\big[E[Y_i|X_i,Z_i]|X_i\\big]\n\\]\nHere the outside expectation is with respect \\(Z_i\\) conditional on \\(X_i\\). It utilizes the conditional distribution \\(f_{Z|X}\\) to form the outside expectation,\n\\[\nE[Y_i|X_i] = \\int y \\cdot f_{Y|X}(y|X_i)dt = \\int\\int y \\cdot f_{Y|X,Z}(y|X_i,z)dyf_{Z|X}(z|X_i)dz = E\\big[E[Y_i|X_i,Z_i]|X_i\\big]\n\\]\n\n\n\nThe following three theorems can be found in a range of Econometrics textbooks and Microeconometrics texts, including MM & MHE\n\nTheorem 1 We can express the observed outcome \\(Y_i\\) as a sum of \\(E[Y_i|X_i]+\\varepsilon_i\\) where \\(E[\\varepsilon_i|X_i]=0\\) (i.e., mean independent).\n\n\nProof. \n\n\\(E[\\varepsilon_i | X_i] = E[Y_i - E[Y_i | X_i] | X_i] = E[Y_i | X_i] - E[Y_i | X_i] = 0\\)\n\\(E[h(X_i)\\varepsilon_i] = E[h(X_i)E[\\varepsilon_i | X_i]] = E[h(X_i) \\times 0] = 0\\)\n\n\n\nTheorem 2 \\(E[Y_i|X_i]\\) is the best predictor of \\(Y_i\\).\n\n\nProof. \\[\n\\begin{aligned}\n(Y_i - m(X_i))^2 =& \\left((Y_i - E[Y_i | X_i]) + (E[Y_i | X_i] - m(X_i))\\right)^2 \\\\\n=& (Y_i - E[Y_i \\| X_i])^2 + (E[Y_i | X_i] - m(X_i))^2 \\\\&+ 2(Y_i - E[Y_i | X_i]) \\times (E[Y_i | X_i] - m(X_i))\n\\end{aligned}\n\\]\nThe last term (cross product) is mean zero. Thus, the function is minimized by setting \\(m(X_i) = E[Y_i | X_i]\\).\n\n\nTheorem 3 [ANOVA Theorem] The variance of \\(Y_i\\) can be decomposed as \\(V(E[Y_i|X_i])+E(V(Y_i|X_i))\\)\n\n\nProof. \\[\n\\begin{aligned}\n        V(Y_i)=&V(E[Y_i|X_i] + \\varepsilon_i) \\\\\n        =&V(E[Y_i|X_i])+V(\\varepsilon_i) \\\\\n        =&V(E[Y_i|X_i])+E[\\varepsilon_i^2]\n    \\end{aligned}\n\\] The second line follows from Theorem 1.1 (independence) and\n\\[\n        E[\\varepsilon_i^2]=E\\left[E[\\varepsilon_i^2|X_i]\\right]=E\\left[V(Y_i|X_i)\\right]\n\\]"
  },
  {
    "objectID": "material-cef.html#definition",
    "href": "material-cef.html#definition",
    "title": "Conditional Expectation Function",
    "section": "",
    "text": "The Conditional Expecation Function (CEF) - denoted \\(E[Y_i|X_i]\\) - is a random function. It is a function that returns the expected value of \\(Y_i\\) for each realized value of \\(X_i\\). Since \\(X_i\\) is a random vector the resulting function is random itself.\nIf we fix \\(X_i=x\\), then the value at which we are evaluating the function is no longer random. The result is a constant: the expected value of \\(Y_i\\) at the given \\(x\\).\n\\[\nE[Y_i|X_i=x] = \\int y \\cdot f_{Y}(y|X_i=x)dy = \\int y dF_{Y}(y|X_i=x)\n\\]\nThis follows the same logic that the expectation of a random variable is, \\(E[Y_i]\\), is not random.\nDiscrete case. The book devotes a lot time to the discussion of cases were \\(X_i\\) is a discrete random variable; using the notation \\(W_i\\in\\{0,1\\}\\) or \\(D_i\\in\\{0,1\\}\\). In this unique case, we can write the CEF as,\n\\[\nE[Y_i|D_i] = E[Y_i|D_i=0] + D_i\\cdot\\big(E[Y_i|D_i=1]-E[Y_i|D_i=0]\\big)\n\\]\nThe above function returns \\(E[Y_i|D_i=0]\\) when \\(D_i=0\\) and \\(E[Y_i|D_i=1]\\) when \\(D_i=1\\). This expression for the CEF will be useful in latter chapters of the book."
  },
  {
    "objectID": "material-cef.html#law-of-iterated-expectations",
    "href": "material-cef.html#law-of-iterated-expectations",
    "title": "Conditional Expectation Function",
    "section": "",
    "text": "The Law of Iterated Expectations says that given two random variables2 \\([Y_i,X_i]\\), we can express the unconditional expected value of \\(Y_i\\) as the expected value of the conditional expectation of \\(Y_i\\) on \\(X_i\\).\n\\[\n        E[Y_i] = E\\big[E[Y_i|X_i]\\big]\n\\]\nWhere the outside expectation is with respect to \\(X_i\\),3 since the CEF is a random function of \\(X_i\\). We can expand this as follows,\n\\[\nE[Y_i] = \\int t \\cdot f_{Y_i}(t)dt = \\int\\int y \\cdot f_{Y_i|X}(y|x)dyf_X(x)dx = E\\big[E[Y_i|X_i]\\big]\n\\]\n\nExample 1 Suppose \\(Y_i\\) and \\(X_i\\) are both discrete, \\(Y_i\\in\\{1,2\\}\\) and \\(X_i\\in\\{3,4\\}\\), with the joint distribution:\n\n\\(f_{Y,X}\\)\n\n\n\n\\(X_i=3\\)\n\\(X_i=2\\)\n\n\n\\(Y_i=1\\)\n1/10\n3/10\n\n\n\\(Y_i=2\\)\n2/10\n4/10\n\n\n\nWe can then define the two marginal distributions,\n\n\\(f_Y\\)\n\n\n\\(Y_i=1\\)\n\\(Y_i=2\\)\n\n\n4/10\n6/10\n\n\n\nand,\n\n\\(f_X\\)\n\n\n\\(X_i=3\\)\n\\(X_i=4\\)\n\n\n3/10\n7/10\n\n\n\nLikewise, we know the conditional distribution \\(f_{Y|X}\\); which we get by dividing the joint distribution by the marginal distribution of \\(X_i\\). Each column of the conditional distribution should add up to 1.\n\n\\(f_{Y|X}\\)\n\n\n\n\\(X_i=3\\)\n\\(X_i=4\\)\n\n\n\\(Y_i=1\\)\n1/3\n3/7\n\n\n\\(Y_i=2\\)\n2/3\n4/7\n\n\n\nNow we can calculate the following objects:\n\n\\(E[Y_i]\\)\n\n\\[\n\\begin{aligned}\n        E[Y_i] =& 1\\cdot Pr(Y_i=1)+2\\cdot Pr(Y_i=2) \\\\\n        =&1\\cdot 4/10+2\\cdot 6/10 \\\\\n        =&16/10\n    \\end{aligned}\n\\]\n\n\\(E[Y_i|X_i=3]\\)\n\n\\[\n\\begin{aligned}\n        E[Y_i|X_i=3] =& 1\\cdot Pr(Y_i=1|X_i=3)+2\\cdot Pr(Y_i=2|X_i=3) \\\\\n        =&1\\cdot 1/3+2\\cdot 2/3 \\\\\n        =&5/3\n\\end{aligned}\n\\]\n\n\\(E[Y_i|X_i=4]\\)\n\n\\[\n\\begin{aligned}\n        E[Y_i|X_i=4] =& 1\\cdot Pr(Y_i=1|X_i=4)+2\\cdot Pr(Y_i=2|X_i=4) \\\\\n        =&1\\cdot 3/7+2\\cdot 4/7 \\\\\n        =&11/7\n    \\end{aligned}\n\\]\n\n\\(E\\big[E[Y_i|X_i]\\big]\\)\n\n\\[\n\\begin{aligned}\n        E\\big[E[Y_i|X_i]\\big] =& E[Y_i|X_i=3]\\cdot Pr(X_i=3)+ E[Y_i|X_i=4]\\cdot Pr(X_i=4) \\\\\n        =&5/3\\cdot3/10+11/7\\cdot 7/10 \\\\\n        =&16/10\n    \\end{aligned}\n\\]\nWe have therefore demonstrated the law of iterated expectations.\n\nWe can extend this principle to conditional expectations. Suppose you have three random variables/vectors \\(\\{Y_i,X_i,Z_i\\}\\), we can express the conditional expected value of \\(Y_i\\) on \\(X_i\\) as the (conditional) expected value of the conditional expectation of \\(Y_i\\) on \\(X_i\\) and \\(Z_i\\).\n\\[\n        E[Y_i|X_i] = E\\big[E[Y_i|X_i,Z_i]|X_i\\big]\n\\]\nHere the outside expectation is with respect \\(Z_i\\) conditional on \\(X_i\\). It utilizes the conditional distribution \\(f_{Z|X}\\) to form the outside expectation,\n\\[\nE[Y_i|X_i] = \\int y \\cdot f_{Y|X}(y|X_i)dt = \\int\\int y \\cdot f_{Y|X,Z}(y|X_i,z)dyf_{Z|X}(z|X_i)dz = E\\big[E[Y_i|X_i,Z_i]|X_i\\big]\n\\]"
  },
  {
    "objectID": "material-cef.html#properties-of-the-cef",
    "href": "material-cef.html#properties-of-the-cef",
    "title": "Conditional Expectation Function",
    "section": "",
    "text": "The following three theorems can be found in a range of Econometrics textbooks and Microeconometrics texts, including MM & MHE\n\nTheorem 1 We can express the observed outcome \\(Y_i\\) as a sum of \\(E[Y_i|X_i]+\\varepsilon_i\\) where \\(E[\\varepsilon_i|X_i]=0\\) (i.e., mean independent).\n\n\nProof. \n\n\\(E[\\varepsilon_i | X_i] = E[Y_i - E[Y_i | X_i] | X_i] = E[Y_i | X_i] - E[Y_i | X_i] = 0\\)\n\\(E[h(X_i)\\varepsilon_i] = E[h(X_i)E[\\varepsilon_i | X_i]] = E[h(X_i) \\times 0] = 0\\)\n\n\n\nTheorem 2 \\(E[Y_i|X_i]\\) is the best predictor of \\(Y_i\\).\n\n\nProof. \\[\n\\begin{aligned}\n(Y_i - m(X_i))^2 =& \\left((Y_i - E[Y_i | X_i]) + (E[Y_i | X_i] - m(X_i))\\right)^2 \\\\\n=& (Y_i - E[Y_i \\| X_i])^2 + (E[Y_i | X_i] - m(X_i))^2 \\\\&+ 2(Y_i - E[Y_i | X_i]) \\times (E[Y_i | X_i] - m(X_i))\n\\end{aligned}\n\\]\nThe last term (cross product) is mean zero. Thus, the function is minimized by setting \\(m(X_i) = E[Y_i | X_i]\\).\n\n\nTheorem 3 [ANOVA Theorem] The variance of \\(Y_i\\) can be decomposed as \\(V(E[Y_i|X_i])+E(V(Y_i|X_i))\\)\n\n\nProof. \\[\n\\begin{aligned}\n        V(Y_i)=&V(E[Y_i|X_i] + \\varepsilon_i) \\\\\n        =&V(E[Y_i|X_i])+V(\\varepsilon_i) \\\\\n        =&V(E[Y_i|X_i])+E[\\varepsilon_i^2]\n    \\end{aligned}\n\\] The second line follows from Theorem 1.1 (independence) and\n\\[\n        E[\\varepsilon_i^2]=E\\left[E[\\varepsilon_i^2|X_i]\\right]=E\\left[V(Y_i|X_i)\\right]\n\\]"
  },
  {
    "objectID": "material-cef.html#footnotes",
    "href": "material-cef.html#footnotes",
    "title": "Conditional Expectation Function",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe subscript \\(i\\) is not necessary here. However, this notation is consistent with the rest of the book. In this book, \\(Y_i\\) denotes a random variable, \\(\\in \\mathbb{R}\\), and \\(Y\\) a random vector, \\(\\in \\mathbb{R}^n\\). Likewise, \\(X_i\\) is a random vector, \\(\\in \\mathbb{R}^k\\), while \\(X\\) will represent a random matrix, \\(\\in \\mathbb{R}^n \\times \\mathbb{R}^k\\).↩︎\nThis can be extended to random vectors.↩︎\nSome texts use the notation \\(E_X\\big[E[Y_i|X_i]\\big]\\) to demonstrate that the outside expectation is with respect to \\(X_i\\).↩︎"
  },
  {
    "objectID": "lecture-1.html",
    "href": "lecture-1.html",
    "title": "Classical Linear Regression Model",
    "section": "",
    "text": "In this handout we will revisit the Classical Linear Regression Model (CLRM) (see Wooldridge 2010, chaps. 1–2). The goal of this week’s lecture is to:"
  },
  {
    "objectID": "lecture-1.html#model-specification",
    "href": "lecture-1.html#model-specification",
    "title": "Classical Linear Regression Model",
    "section": "Model Specification",
    "text": "Model Specification\nThe linear population regression model is given by,\n\\[\n\\begin{aligned}\n  Y_i =& X_i'\\beta+\\varepsilon_i \\\\\n  =& \\beta_1\\mathbf{1}+\\beta_2X_{i2}+\\beta_3X_{i3}+...+\\beta_kX_{ik}+\\varepsilon_i\n\\end{aligned}\n\\]\nfor \\(i = 1,2,...,n\\). Where,\n\n\\(i\\): unit of observation; e.g. individual, firm, union, political party, etc.\n\\(Y_i \\in \\mathbb{R}\\): scalar random variable.\n\\(X_i \\in \\mathbb{R}^k\\): \\(k\\)-dimensional (column1) vector of regressors.2\n\\(\\beta\\): \\(k\\)-dimensional, non-random vector of unknown population parameters.\n\\(\\varepsilon_i\\): unobserved, random error term.3\n\nThe linear population regression equation is linear in parameters. This is an important assumption that does NOT restrict the model from being non-linear in regressors. For example, the equation\n\\[Y_i = \\beta_1 + \\beta_2X_{i2} + \\beta_3X_{i2}^{\\color{red}{2}} + \\varepsilon_i\\] non-linear in \\(X_{i2}\\), but still linear in parameters. In contrast, the equation\n\\[Y_i = \\beta_1 + \\beta_2X_{i2} + ({\\color{red}{\\beta_2\\beta_3}})X_{i3} + \\varepsilon_i\\] is non-linear in parameters.\n\nIntercept\nThe constant (intercept) in the equation serves an important purpose. While there is no a priori reason for the model to have a constant term, it does ensure that the error term is mean zero.\n\nProof. Suppose \\(E[\\varepsilon_i] = \\gamma\\).\nWe can then define a new error term, \\(\\upsilon_i = \\varepsilon_i - \\gamma\\), such \\(E[\\upsilon_i] = \\gamma\\). The population regression model can be rewritten as, \\[ \\begin{aligned} Y_i =& X_ i'\\beta + v_i + \\gamma  \\\\\n=& \\underbrace{(\\beta_1+\\gamma)}_{\\tilde{\\beta}_1}\\mathbf{1} + \\beta_2X_{i2} + \\beta_3X_{i3} + ... + \\beta_kX_{ik} + v_i\n\\end{aligned}\n\\] The model has a new intercept \\(\\tilde{\\beta}_1=\\beta_1 + \\gamma\\), but the other parameters remain unchanged.\n\n\n\nMatrix notation\nFor a sample of \\(n\\) observations, we can stack the unit-level linear regression equation into a vector,\n\\[\nY =\\underbrace{\\begin{bmatrix}Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix}}_{n\\times 1} = \\underbrace{\\begin{bmatrix}X_1'\\beta \\\\ X_2'\\beta \\\\ \\vdots \\\\ X_n'\\beta\\end{bmatrix}}_{n\\times 1}  + \\underbrace{\\begin{bmatrix}\\varepsilon_1 \\\\ \\varepsilon_2 \\\\ \\vdots \\\\ \\varepsilon_n\\end{bmatrix}}_{n\\times 1}  \n= \\underbrace{\\begin{bmatrix}X_{11} & X_{12} & \\dots & X_{1k}\\\\ X_{21} & X_{22} && \\\\ \\vdots & & \\ddots & \\\\ X_{n1} & & & X_{nk} \\end{bmatrix}}_{n\\times k} \\underbrace{\\begin{bmatrix}\\beta_1 \\\\ \\beta_2 \\\\ \\vdots \\\\ \\beta_k\\end{bmatrix}}_{k\\times 1} + \\begin{bmatrix}\\varepsilon_1 \\\\ \\varepsilon_2 \\\\ \\vdots \\\\ \\varepsilon_n\\end{bmatrix} = X\\beta + \\varepsilon\n\\] Notice, in matrix notation, you lose the transpose from \\(X_i'\\beta\\). Apart from the absence of the \\(i\\) subscript, this is a useful way of knowing the dimension of the equation (in my notes). You MUST always write \\(X\\beta\\) and not \\(\\beta X\\). For the scalar case, \\(X_i'\\beta = \\beta'X_i\\), but for the vector case \\(\\beta X\\) is not defined since \\(\\beta\\) is \\(k\\times 1\\) and \\(X\\) is \\(n\\times k\\)."
  },
  {
    "objectID": "lecture-1.html#clrm-assumptions",
    "href": "lecture-1.html#clrm-assumptions",
    "title": "Classical Linear Regression Model",
    "section": "CLRM Assumptions",
    "text": "CLRM Assumptions\nAssumption CLRM 1. Population regression equation is linear in parameters: \\[Y = X\\beta+\\varepsilon\\]\nAssumption CLRM 2. Conditional mean independence of the error term: \\[E[\\varepsilon|X]=0\\]\nTogether, CLRM 1. and CLRM 2. imply that\n\\[ E[Y|X] = X\\beta  \\] This means that the Conditional Expectation Function is known and linear in parameters.\nConditional mean independence implies - by the Law of Iterated Expectations - mean independence of the error term,\n\\[E[\\varepsilon|X]=0 \\Rightarrow E\\big[E[\\varepsilon|X]\\big]=E[\\varepsilon]=0\\]\nand uncorrelatedness,\n\\[E[\\varepsilon|X]=0 \\Rightarrow E[\\varepsilon X]=0\\] Note, neither of the above statements hold the other way around. Mean independence does not imply conditional mean independence and uncorrelatedness (zero correlation/covariance) does not imply conditional mean independence.\nUncorrelatedness rules out linear relationships between the regressors and error term while conditional mean independence rules out non-linear relationships too.\nIn general, distributional independence implies mean independence which then implies uncorrelatedness.\n\nIn the case joint-normally distributed random variables, uncorrelatedness implies independence. That is, if \\[ \\begin{bmatrix}Y_1 \\\\ Y_2\\end{bmatrix}\\sim N\\bigg(\\begin{bmatrix}\\mu_1 \\\\ \\mu_2\\end{bmatrix},\\begin{bmatrix}\\sigma_1^2 & \\sigma_{12}\\\\ \\sigma_{21} & \\sigma_2^2\\end{bmatrix}\\bigg)\\] Then \\(\\sigma_{12}=\\sigma_{21}=0 \\iff f_1*f_2 = f_{12}\\).\n\nWe will later show that uncorrelatedness is sufficient for consistency of the Ordinary Least Squares estimater, while conditional mean independence is required for unbiasedness of OLS.\nAssumption CLRM 3. Homoskedasticity: \\(Var(\\varepsilon|X) = E[\\varepsilon\\varepsilon'|X] =  \\sigma^2I_n\\)\nCLRM 3. states that the variance of the error term is independent of \\(X\\) and constant across units. The diagonal nature of the covariance matrix also implies that the error terms are uncorrelated across units in the data. Note, this does not imply independence of the error terms across units.\nModels with heteroskedasticity relax the assumption of constant variance, allowing for a richer variance-covariance matrix that typically depends on \\(X\\).\nThis assumption is unlikely to hold in time-series models where units represent repeated observations across time. Such violations are referred to as serial correlation or autocorrelation.\nEven in cross-sectional data settings, you can have non-zero correlations across units in the data. A common instance of this is the case of clustering. Clustering can occur when units experience common/correlated ‘shocks’; for example, the data contains groups of students from the same classroom who have a the same teacher. This can also be the result of clustered sampling, a common practice in multi-stage survey design.\nAssumption CLRM 4. Full rank: \\(rank(X)=k\\)4\nCLRM 4. is some time referred to as the absence of perfect (or exact) collinearity. Do not confuse this with multicollinearity. Multicollinearity occurs when regressors are highly (linearly) correlated with one another, yielding imprecise estimates.\nAssumption CLRM 5. Normality of the error term: \\(U|X \\sim N(0,\\sigma^2 I_n)\\)\nAssumption CLRM 6. Observations \\(\\{(Y_i,X_i): i=1,...,n\\}\\) are independently and identically distributed (iid).\nCLRM 5 & 6 are not part of the Classical assumptions, but do simplify the problem of inference. Note, CLRM 5 implies independence across error terms, not implied by CLRM 4.\n\nNon-random \\(X\\)\nThere is an alternative version of the CLRM in which \\(X\\) is a non-random, matrix of regressors/predictors. With \\(X\\) fixed, the error term is the only only random variable in the model. CLRM assumptions 1 and 4 remain the same, while CLRM 2, 3, 5, and 6 become:\nAssumption CLRM 2a. Mean independence of the error term: \\[E[\\varepsilon]=0\\]\nAssumption CLRM 3a. Homoskedasticity: \\(Var(\\varepsilon) = \\sigma^2I_n\\)\nAssumption CLRM 5a. Normality of the error term: \\(U \\sim N(0,\\sigma^2 I_n)\\)\nAssumption CLRM 6a. Observations \\(\\{\\varepsilon_i: i=1,...,n\\}\\) are independently and identically distributed (iid).\n\n\nIdentification\nCLRM 1,2 and 4. are the identifying assumptions of the model. These assumptions allow us to write the parameter of interest as a set of ‘observable’ moments in the data. We can demonstrate this as follows.\n\nProof. Start with CLRM 2.\n\\[\n    E[\\varepsilon_i|X_i]=0\n\\]\nPre-multiply by the vector \\(X_i\\), \\[\n        X_iE[\\varepsilon_i|X_i]=0\n\\] Since the expectation is conditional on \\(X_i\\), we can bring \\(X_i\\) inside the expectation function,\n\\[\n        E[X_i\\varepsilon_i|X_i]=0\n    \\] This conditional expectation is a random-function of \\(X_i\\). If we take the expectation of this function w.r.t. \\(X\\), we achieve the aforementioned result that conditional mean independence implies zero covariance, \\[\n        E\\left[E[X_i\\varepsilon_i|X_i]\\right]=E[X_i\\varepsilon_i]=0\n    \\]\nNow substitute in for \\(\\varepsilon_i\\) using the linear regression model from CLRM 1 and separate the resulting two terms,\n\\[\n\\begin{aligned}\n    &E[X_i(Y_i-X_i'\\beta)]=0 \\\\\n    \\Rightarrow &E[X_iX_i']\\beta=E[X_iY_i]\n\\end{aligned}\n\\]\nSince \\(\\beta\\) is a non-random vector, we can remove it from the expectation function.\nNow we have a system of linear equations (of the form \\(Av = b\\)) with a unique solution if and only if the matrix \\(E[X_iX_i']\\) is invertible. For the inverse of \\(E[X_iX_i']\\) to exist, we require CLRM 3.\n\\[\n    \\beta = E[X_iX_i']^{-1}E[X_iY_i]\n\\]\n\nWe cannot compute \\(\\beta\\) because we do not know the joint distribution of \\((Y_i,X_i)\\) needed to solve for the variance-covariance matrices. However, \\(\\beta\\) is (point) identified because both \\(Y\\) and \\(X\\) are observed in the data and the parameters are “pinned down” by a unique set of ‘observable’ moments in the data.\n\\(\\beta\\) is not identified if the above system of linear equations does not have a unique solution. This will occur if two or more of the regressors are perfectly colinear.5 \\(\\beta\\) is also not be identified is the resulting expression for \\(\\beta\\) includes ‘objects’ (moments, distribution/scale parameters) that are not ‘observed’ in the data. For example, if the error is not mean independent, the above expression will include a bias term that depends on \\(E[X_i'\\varepsilon_i]\\).\nIn this instance, the identification of \\(\\beta\\) is scale dependent. That is, if we multiply \\(Y_i\\) by a scalar, \\(\\beta\\) is multiplied by the same scalar. Consider cases where a researcher is modelling standardized test-scores."
  },
  {
    "objectID": "lecture-1.html#interpretation",
    "href": "lecture-1.html#interpretation",
    "title": "Classical Linear Regression Model",
    "section": "Interpretation",
    "text": "Interpretation\nIn this linear regression model each slope coefficient has a partial derivative interpretation,\n\\[\n\\beta_j = \\frac{\\partial E[Y_i|X_i]}{\\partial X_{ij}}\n\\] or, as a vector, \\[\n\\beta = \\frac{\\partial E[Y_i|X_i]}{\\partial X_{i}} = \\begin{bmatrix}\\frac{\\partial E[Y_i|X_i]}{\\partial X_{i1}}\\\\ \\vdots \\\\ \\frac{\\partial E[Y_i|X_i]}{\\partial X_{ik}}\\end{bmatrix} = \\begin{bmatrix}\\beta_1\\\\ \\vdots \\\\ \\beta_k\\end{bmatrix}\n\\]\nNote, the derivative is expressed in terms of changes in the expected value of \\(Y_i\\) (conditional on \\(X_i\\)), not \\(Y_i\\) itself. This is because \\(Y_i\\) is a random variable, but under CLRM 1 & 2\n\\[\nE[Y_i|X_i] = X_i'\\beta\n\\]\nFor a given value of \\(X_i\\), the above expression is non-random.\nAs \\(\\beta_j\\) is a partial derivative, its interpretation is one that “holds fixed” the value of other regressors (i.e. ceteris paribus). Because of this, many researchers apply the experimental language of control variables when interpretting regression coefficients.\nConsider this simplified version of the linear regression model popularized by Mincer.\n\\[WAGE_i = \\gamma_1 + \\gamma_2EDU_i + \\gamma_3EXP_ i + \\upsilon_i\\] where,\n\n\\(WAGE_i\\): individual wage (£’s)\n\\(EDU_i\\): years of schooling/education\n\\(EXP_i\\): years of experience\n\nAssuming \\(E[\\upsilon_i|S_i,EXP_i]=0\\), \\(\\beta_3\\) is the expected change in wages from an additional year of experience, holding fixed years of schooling. The model implies that if we were to consider two individuals with the same years of schooling, but a 1-year difference in work experience, then we would expect the more experienced worker to earn \\(\\beta_3\\) £’s more.\nWe would get the same interpretation from an experiment where we control individual schooling, but (randomly) vary years of experience by one year across units in the population.\nRemember, for the above linear regression model, this interpretation is based on the assumption that the conditional expectation function is correctly specified. If no, then this interpretation is incorrect. Moreover, there are other ways to think about “controlling” for covariates that we will address towards the end of this module.\n\nSemi-elasticities and elasticities\nThe original Mincer equation has the outcome as the log of wages,\n\\[\\ln(WAGE_i) = \\gamma_1 + \\gamma_2EDU_i + \\gamma_3EXP_ i + \\gamma_4EXP^2_ i + \\upsilon_i\\] The interpretation of \\(\\beta_3\\) is now in terms of expected log-points of wages.\n\\[\n\\beta_3 = \\frac{\\partial E[ln(WAGE_i)|EDU_i,EXP_i]}{\\partial EXP_{i}}\n\\]\nThis can be converted into a percentage change in (expected) wages,\n\\[\\%\\Delta E[WAGE_i|EDU_i,EXP_i] = (\\exp^{\\beta_3}-1)\\times 100\\] For values of \\(\\beta_3 \\in [-0.1,0.1]\\) this value is closely approximated by \\(\\beta_3\\times 100\\).\nNext, consider a model where the regressor is in logs, while the outcome remains in levels. For example, a model of commuting cost as a function of distance to work,\n\\[COST_i = \\gamma_1 + \\gamma_2\\ln(DIST_i) + + \\nu_i\\] Here the interpretation of \\(\\beta_2\\) is, \\[\n\\beta_2 = \\frac{\\partial E[COST_i|\\ln(DIST_i)]}{\\partial \\ln(DIST_i)}\n\\] We can convert this to \\(\\%\\Delta\\) in \\(DIST_i\\), using the fact that a 1% change in distance implies a change in log points of \\(\\ln(1.01)\\approxeq 0.01\\). Thus, we can approximate the expected change in cost by \\(\\beta_2/100\\).\nFinally, when both the outcome and regressor are logged, the coefficient as an elasticity interpretation. For example, in the taxation literature, it is common to see taxable income modeled as a function of the (marginal) tax rate,\n\\[\n  \\ln(INC_i) = \\beta_1 + \\beta_2 \\ln(RATE_i) + \\xi_i\n\\] Here, \\(\\beta_2\\) has an tax elasticity interpretation,\n\\[\n  \\beta_2 = \\frac{\\partial E[\\ln(INC_i)|\\ln(RATE_i)]}{\\partial \\ln(RATE_i)} = \\frac{\\%\\Delta E[INC_i|RATE_i]}{\\%\\Delta RATE_i}\n\\]"
  },
  {
    "objectID": "lecture-1.html#footnotes",
    "href": "lecture-1.html#footnotes",
    "title": "Classical Linear Regression Model",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMy notation assumes that \\(X_i\\) is a column vector, which makes \\(X_i'\\beta\\) a scalar. Wooldridge (2010) uses the notation \\(X_i\\beta\\), implying that \\(X_i\\) is a row vector. This is a matter of preference.↩︎\nYou might also refer to the vector of regressors as covariates or explanatory variables. Some texts will use the term independent variables, but this name implies a specific relationship between \\(Y\\) and \\(X\\) that need not hold.↩︎\nThis is NOT the residual.↩︎\nSee extra material on Linear Algebra. Since \\(X\\) is a random variable we should add to the assumption: \\(rank(X) = k\\) almost surely (abbreviated a.s.). This means that the set of events in which \\(X\\) is not full rank occur with probability 0. The reason for this addition is that such a set of events may not be empty.↩︎\nThere are many such failures of (parametric) identification in models that include dummy variables (or fixed effects). Earlier we saw that the intercept is not separately identified from the mean of the error term. Mean independence of the error term, \\(E[\\varepsilon_i]=0\\), is required for us to separately ‘identify’ \\(\\beta_1\\).↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Quantitative Methods: Econometrics B",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "material-linearalgebra.html",
    "href": "material-linearalgebra.html",
    "title": "Linear Algebra",
    "section": "",
    "text": "Consider a set of \\(k\\) \\(n\\)-dimensional vectors \\(\\{X_{1},X_{2},...,X_{k}\\}\\). These vectors are,\n\nDefinition 1 linearly dependent if there exists a set of scalars \\(\\{a_{1},a_{2},\\dots,a_{k}\\}\\) such that\n\\[\na_1X_1 + a_2X_2+\\ldots+a_kX_k=0\n\\]\nwhere at least one \\(a_i\\neq0\\).\n\nAlternatively, they are,\n\nDefinition 2 linearly independent if the only set of scalars \\(\\{a_{1},a_{2},\\dots,a_{k}\\}\\) that satisfies the above condition is \\(a_1,a_2,\\dots,a_k=0\\).\n\nIf we collect these \\(k\\) column-vectors in a matrix, \\(X=[X_1\\;X_2 \\dots X_k]\\), then the linear dependence condition can be written as,\n\\[\na_1X_1 + a_2X_2+\\ldots+a_kX_k=\\begin{bmatrix} X_1\\;X_2 \\dots X_k\\end{bmatrix}\\begin{bmatrix}a_1\\\\a_2\\\\\\vdots\\\\a_k\\end{bmatrix}=Xa = 0\n\\]\nGiven any \\(n\\times k\\) matrix \\(X\\), its columns are,\n\nDefinition 3 linearly dependent if there exists a vector \\(a\\in\\mathbb{R}^k\\) such that \\(a\\neq0\\) and \\(Xa=0\\);\n\nor,\n\nDefinition 4 linearly independent if the only vector \\(a\\in\\mathbb{R}^k\\) such that \\(Xa=0\\) is \\(a=0\\).\n\nFor any matrix there may be more than one vector \\(a\\in\\mathbb{R}^{k}\\) such that \\(Xa=0\\). Indeed, if both \\(a_{1},a_{2}\\in\\mathbb{R}^{k}\\)\nsatisfy this condition and \\(a_{1}\\neq a_{2}\\) then you can show that any linear combination of \\(\\{a_{1},a_{2}\\}\\) satisfies the\ncondition \\(X(a_{1}b_{1}+a_{2}b_{2})=0\\) for \\(b_{1},b_{2}\\in\\mathbb{R}\\). Thus, there exists an entire set of vectors which satisfy this condition. This set is referred to as the,\n\nDefinition 5 null space of \\(X\\), \\[\n\\mathcal{N}(X) = \\{a\\in\\mathbb{R}^k:\\;Xa=0\\}\n\\]\n\nIt should be evident from the definition that if the columns of \\(X\\) are linearly independent then \\(\\mathcal{N}(X)=\\{0\\}\\), a singleton. That is, it just includes the 0-vector.\n\n\n\nHere, we concern ourselves only with real vectors from \\(\\mathbb{R}^n\\).\n\nDefinition 6 A vector space, denoted \\(\\mathcal{V}\\), refers to a set of vectors which is closed under finite addition and scalar multiplication.\n\n\nDefinition 7 A set of \\(k\\) linearly independent vectors, \\(\\{X_1,X_2,\\dots,X_k\\}\\), forms a basis for vector space \\(\\mathcal{V}\\) if \\(\\forall\\;y\\in\\mathcal{V}\\) there exists a set of \\(k\\) scalars such that, \\[\ny=X_1b_1+X_2b_2+\\ldots+X_kb_k\n\\]\n\nBased on these definitions, it is evident that for the Euclidean space, \\(\\mathbb{E}^n\\), any \\(n\\) linearly independent vectors from \\(\\mathbb{R}^n\\) is a basis. For example, any point in \\(\\mathbb{E}^2\\) can be defined as a multiple of,\n\\[\n\\begin{bmatrix}1\\\\0\\end{bmatrix}\\quad \\text{and} \\quad\\begin{bmatrix}0\\\\1\\end{bmatrix}\n\\]\nConsider again the \\(n\\times k\\) matrix \\(X\\), where \\(k&lt;n\\). Then we define the,\n\nDefinition 8 column space (or span) of \\(X\\), denoted \\(\\mathcal{S}(X)\\), as the vector space generate by the \\(k\\) columns of \\(X\\). Formally, \\[\n\\mathcal{S}(X) = \\{y\\in\\mathbb{R}^n:\\;y=Xb\\quad\\text{for some }b\\in \\mathbb{R}^k\\}\n\\]\n\nA property to note about the span or column space \\(X\\) is,\nResult: \\(\\mathcal{S}(X)=\\mathcal{S}(XX')\\) :::\nwhere \\(XX'\\) is a \\(n\\times n\\) matrix.\nFinally, we can define the,\n\nDefinition 9 orthogonal column space (or orthogonal span) of \\(X\\) as, \\[\n\\mathcal{S}^{\\perp}(X) = \\{y\\in \\mathbb{R}^k:\\;y'x=0\\quad \\forall x\\in\\mathcal{S}(X)\\}\n\\]\n\n\n\n\nConsider a \\(n\\times k\\) matrix \\(X\\), the\n\nDefinition 10 row rank of \\(X\\) is the maximum number of linearly independent rows: \\[\nrowrank(X) \\leq n\n\\]\n\nWe say that matrix \\(X\\) has full row rank if \\(rowrank(X)=n\\).\nThe,\n\nDefinition 11 column rank of \\(X\\) is the maximum number of linearly independent columns:\n\\[\ncolrank(X) \\leq k\n\\]\n\nWe say that matrix \\(X\\) has full column rank if \\(colrank(X)=k\\).\nAn important result is,\n\nResult: the rank of \\(X\\): \\[\nr(X) = rowrank(X)=colrank(X) \\\\\n\\Rightarrow r(X)\\leq min\\{n,k\\}\n\\]\n\nIn addition, since the \\(r(X)\\) depends on the number of linearly independent columns, we can say that,\n\nResult: the dimension of \\(\\mathcal{S}(X)\\), \\(dim(\\mathcal{S}(X))\\), is given by the \\(r(X)\\).\n\nHere are a few additional results,\n\nResult: \\(r(X)=r(X')\\)\nResult: \\(r(XY)\\leq min\\{r(X),r(Y)\\}\\)\nResult: \\(r(XY)=r(X)\\) if \\(Y\\) is square and full rank\nResult: \\(r(X+Y)\\leq r(X) + r(Y)\\)\n\n\n\n\nConsider the case of a square, \\(n\\times n\\), matrix \\(A\\). We say that,\n\nDefinition 12 \\(A\\) is singular if the \\(r(A)&lt;n\\),\n\nor that,\n\nDefinition 13 \\(A\\) is non-singular if the \\(r(A)=n\\).\n\nThe singularity of a square matrix is important as it determines the invertibility of a matrix, which typically relates the existence of a unique solution in systems of linear equations. Here are a few key results,\n\nResult: There exists a matrix \\(B=A^{-1}\\), such that \\(AB=I_n\\) (where \\(I_n\\) is the identity matrix), if and only if \\(A\\) is non-singular.\nResult: \\(A\\) is non-singular if and only if the determinant of \\(A\\) is non-zero: \\(det(A)\\neq0\\).1\nResult: Likewise, \\(A\\) is singular if and only if \\(det(A)=0\\).\nResult: \\(AA^{-1}=A^{-1}A=I\\)\nResult: \\((A')^{-1}=(A^{-1})'\\)\nResult: If their respective inverses exist, then \\((AB)^{-1}=B^{-1}A^{-1}\\).\nResult: \\(det(AB)=det(A)det(B)\\)\nResult: \\(det(A^{-1})=det(A)^{-1}\\)\n\nFor any square matrix \\(A\\),\n\nDefinition 14 the trace of \\(A\\) is the sum of all diagonal elements: \\[\ntr(A) = \\sum_{i=1}^na_{ii}\n\\]\n\nRegarding the trace of a square matrix, here are a few important results:\n\nResult: \\(tr(A+B) = tr(A) + tr(B)\\)\nResult: \\(tr(\\lambda A) = \\lambda tr(A)\\) where \\(\\lambda\\) is a scalar\nResult: \\(tr(A) = tr(A')\\)\nResult: \\(tr(AB) = tr(BA)\\) where \\(AB\\) and \\(BA\\) are both square, but need not be of the same order.\nResult: \\(||A|| = (tr(A'A))^{1/2}\\)\n\n\n\n\nA symmetric matrix has the property that \\(A=A'\\). Therefore, \\(A\\) must be square.\nHere are a few important results concerning symmetric matrices.\n\nResult: \\(A^{-1}\\) exists if \\(det(A)\\neq 0\\) and \\(r(A)=n\\)\nResult: A is diagonalizable.2\nResult: The eigenvector decomposition of a square matrix gives you \\(A=C\\Lambda C^{-1}\\) where \\(\\Lambda\\) is a diagonal matrix of eigenvalues and $C$ a matrix of the corresponding eigenvectors. The symmetry of \\(A\\) gives you that \\(C^{-1}=C'\\Rightarrow A=C\\Lambda C'\\) with \\(C'C=CC'=I_{n}\\).3\n\nA key definition concerning symmetric matrices is their positive definiteness:\n\nDefinition 15 \\(A\\) is positive semi-definite if for any \\(x\\in\\mathbb{R}^n,\\;x'Ax\\geq0\\).\n\nGiven the eigenvector decomposition of a symmetric matrix, positive semi-definiteness implies \\(\\Lambda\\) is positive semi-definite: \\(\\lambda_i\\geq0\\quad\\forall i\\). Likewise,\n\nDefinition 16 \\(A\\) is positive definite if for any \\(x\\in\\mathbb{R}^n,\\;x'Ax&gt;0\\).\n\nAgain, based on the egeinvector decomposition, positive semi-definiteness implies \\(\\Lambda\\) is positive definite: \\(\\lambda_i&gt;0\\quad\\forall i\\).\nA few more results are:\n\nResult: \\(tr(A) = \\sum_{i=1}^n\\lambda_i\\)\nResult: \\(r(A) = r(\\Lambda)\\)\nResult: \\(det(A) = \\prod_{i=1}^n \\lambda_i\\)\n\nThis last result can be used to prove that any positive definite matrix is non-singular and therefore has an inverse.\nAny full-rank, positive semi-definite, symmetric matrix \\(B\\) has the additional properties:\n\nResult: \\(B=C\\Lambda C'\\) and \\(B^{-1} = C\\Lambda^{-1}C'\\)\nResult: We can define the square-root of \\(B\\) as \\(B^{1/2} = C\\Lambda^{1/2}C'\\). Similarly, \\(B^{-1/2} = C\\Lambda^{-1/2}C'\\).\n\n\n\n\nAn idempotent matrix has the property that \\(D=DD\\). Therefore, \\(D\\) must be square.\nHere are a few important results concerning idempotent matrices.\n\nResult: \\(D\\) is positive definite\nResult: \\(D\\) is diagonalizable\nResult: \\((I_n-D)\\) is also an idempotent matrix\nResult: With the exception of \\(I_n\\), all idempotent matrices are singular.\nResult: \\(r(D) = tr(D) = \\sum_{i=1}^n\\lambda_i\\)\nResult: \\(\\lambda_i\\in\\{0,1\\}\\quad \\forall\\;i\\)\n\nProjection matrices are idempotent, but need not be symmetric. However, for the purposes of this module we will deal exclusively with symmetric idempotent projection matrices.\n\n\n\nHere we will look at the derivatives of scalar with respect to (W.r.t.) a vector. You can also define other derivatives, such as the derivative of a vector w.r.t. a vector and the derivative of a scalar with respect to a matrix. However, these are not needed for these notes.\n\n\nSuppose \\(f(x)\\in R\\) (i.e. a scalar) and \\(x\\in R^n\\) (i.e. a \\(n\\times 1\\) vector). Then we can define the partial derivative of \\(f(x)\\) w.r.t. \\(x\\) as,\n\\[\n  \\frac{\\partial f(x)}{\\partial x}  = \\begin{bmatrix}\\frac{\\partial f(x)}{\\partial x_1} \\\\ \\frac{\\partial f(x)}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial f(x)}{\\partial x_n} \\end{bmatrix}\n\\]\n\n\n\nA special case is when \\(f(x)\\) is linear in \\(x\\),\n\\[\nf(x) = a'x = \\sum_{i=1}^n a_ix_i\n\\] for \\(a\\in R^n\\). The derivative of \\(a'x\\) with respect to the vector \\(x\\) can be defined as,\n\\[\n  \\begin{aligned}\n  \\frac{\\partial a'x}{\\partial x} & = \\begin{bmatrix}\\frac{\\partial a'x}{\\partial x_1} \\\\ \\frac{\\partial a'x}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial a'x}{\\partial x_n} \\end{bmatrix} \\\\\n  & = \\begin{bmatrix}a_1 \\\\ a_2 \\\\ \\vdots \\\\ a_n \\end{bmatrix} \\\\\n  & = a\n  \\end{aligned}\n\\] since the the partial derivate of \\(a'x = \\sum_{i=1}^n a_ix_i\\) w.r.t. \\(x_i\\) is just the scalar \\(a_i\\).\nThe derivative of a scalar w.r.t. to a vector yields a vector of partial derivatives.\nSince \\(a'x\\) is a scalar, it is by definition symmetric: \\(a'x = x'a\\). Thus,\n\\[\n\\frac{\\partial x'a}{\\partial x} = \\frac{\\partial a'x}{\\partial x} = a\n\\]\n\n\n\nSuppose \\(f(x)\\) is a linear transformation of \\(x\\),\n\\[\nf(x) = A'x\n\\] for any \\(m\\times n\\) matrix A,\n\\[\n  A = \\begin{bmatrix}a_1' \\\\ a_2' \\\\ \\vdots \\\\ a_m'\\end{bmatrix}\n\\] where \\(a_i\\in R^n\\;\\forall i=1,\\dots,m\\) and,\n\\[\n  Ax = \\begin{bmatrix}a_1'x \\\\ a_2'x \\\\ \\vdots \\\\ a_m'x\\end{bmatrix}\n\\] Note, \\(f(x)=Ax\\in R^m\\), a \\(m\\times 1\\) vector. We can then define,\n\\[\n  \\begin{aligned}\n  \\frac{\\partial Ax}{\\partial x'} & = \\begin{bmatrix}\\frac{\\partial a_1'x}{\\partial x_1} & \\frac{\\partial a_1'x}{\\partial x_2} & \\dots & \\frac{\\partial a_1'x}{\\partial x_n}\\\\ \\frac{\\partial a_2'x}{\\partial x_1} & \\frac{\\partial a_2'x}{\\partial x_2} & \\dots & \\frac{\\partial a_2'x}{\\partial x_n}\\\\ \\vdots & \\vdots & \\ddots & \\vdots\\\\ \\frac{\\partial a_m'x}{\\partial x_1} & \\frac{\\partial a_m'x}{\\partial x_2} & \\dots & \\frac{\\partial a_m'x}{\\partial x_n}\\\\ \\end{bmatrix} \\\\\n  & = \\begin{bmatrix} a_{11} & a_{12} & \\dots & a_{1n} \\\\ a_{21} & a_{22} & \\dots & a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots\\\\ a_{m1} & a_{m2} & \\dots & a_{mn} \\\\ \\end{bmatrix} \\\\\n  & = A\n  \\end{aligned}\n\\]\nSince Ax is \\(m\\times 1\\) column vector, we take the derivative w.r.t. \\(x'\\) a row vector and not the column vector \\(x\\). This results in a matrix of partial derivatives.\n\n\n\nA second special case is where the function takes on the quadaratic form,\n\\[\nf(x) = x'Ax = \\sum_{i=1}^N\\sum_{j=1}^n a_{ij}x_ix_j\n\\]\nfor \\(n\\times n\\) (square) matrix A. As in the first linear case, \\(f(x)\\) is scalar.\nDefine \\(c = Ax\\), the \\(x'Ax = x'c\\). From the linear case, we know that,\n\\[\n\\frac{\\partial x'c}{\\partial x} = c\n\\]\nSimilarly, if we define \\(d = A'x\\) then \\(x'Ax = d'x\\). From the linear case, we know that,\n\\[\n\\frac{\\partial d'x}{\\partial x} = d\n\\]\nWe can define the total derivative as the sum of the partial derivatives w.r.t. to the first and second \\(x\\). Combining these two results, we have that,\n\\[\n\\frac{\\partial x'Ax}{\\partial x} = Ax + A'x\n\\]\nAnd if \\(A\\) is symmetric, this result simplifies to \\(2Ax\\)."
  },
  {
    "objectID": "material-linearalgebra.html#linear-dependence",
    "href": "material-linearalgebra.html#linear-dependence",
    "title": "Linear Algebra",
    "section": "",
    "text": "Consider a set of \\(k\\) \\(n\\)-dimensional vectors \\(\\{X_{1},X_{2},...,X_{k}\\}\\). These vectors are,\n\nDefinition 1 linearly dependent if there exists a set of scalars \\(\\{a_{1},a_{2},\\dots,a_{k}\\}\\) such that\n\\[\na_1X_1 + a_2X_2+\\ldots+a_kX_k=0\n\\]\nwhere at least one \\(a_i\\neq0\\).\n\nAlternatively, they are,\n\nDefinition 2 linearly independent if the only set of scalars \\(\\{a_{1},a_{2},\\dots,a_{k}\\}\\) that satisfies the above condition is \\(a_1,a_2,\\dots,a_k=0\\).\n\nIf we collect these \\(k\\) column-vectors in a matrix, \\(X=[X_1\\;X_2 \\dots X_k]\\), then the linear dependence condition can be written as,\n\\[\na_1X_1 + a_2X_2+\\ldots+a_kX_k=\\begin{bmatrix} X_1\\;X_2 \\dots X_k\\end{bmatrix}\\begin{bmatrix}a_1\\\\a_2\\\\\\vdots\\\\a_k\\end{bmatrix}=Xa = 0\n\\]\nGiven any \\(n\\times k\\) matrix \\(X\\), its columns are,\n\nDefinition 3 linearly dependent if there exists a vector \\(a\\in\\mathbb{R}^k\\) such that \\(a\\neq0\\) and \\(Xa=0\\);\n\nor,\n\nDefinition 4 linearly independent if the only vector \\(a\\in\\mathbb{R}^k\\) such that \\(Xa=0\\) is \\(a=0\\).\n\nFor any matrix there may be more than one vector \\(a\\in\\mathbb{R}^{k}\\) such that \\(Xa=0\\). Indeed, if both \\(a_{1},a_{2}\\in\\mathbb{R}^{k}\\)\nsatisfy this condition and \\(a_{1}\\neq a_{2}\\) then you can show that any linear combination of \\(\\{a_{1},a_{2}\\}\\) satisfies the\ncondition \\(X(a_{1}b_{1}+a_{2}b_{2})=0\\) for \\(b_{1},b_{2}\\in\\mathbb{R}\\). Thus, there exists an entire set of vectors which satisfy this condition. This set is referred to as the,\n\nDefinition 5 null space of \\(X\\), \\[\n\\mathcal{N}(X) = \\{a\\in\\mathbb{R}^k:\\;Xa=0\\}\n\\]\n\nIt should be evident from the definition that if the columns of \\(X\\) are linearly independent then \\(\\mathcal{N}(X)=\\{0\\}\\), a singleton. That is, it just includes the 0-vector."
  },
  {
    "objectID": "material-linearalgebra.html#vector-spaces-bases-and-spans",
    "href": "material-linearalgebra.html#vector-spaces-bases-and-spans",
    "title": "Linear Algebra",
    "section": "",
    "text": "Here, we concern ourselves only with real vectors from \\(\\mathbb{R}^n\\).\n\nDefinition 6 A vector space, denoted \\(\\mathcal{V}\\), refers to a set of vectors which is closed under finite addition and scalar multiplication.\n\n\nDefinition 7 A set of \\(k\\) linearly independent vectors, \\(\\{X_1,X_2,\\dots,X_k\\}\\), forms a basis for vector space \\(\\mathcal{V}\\) if \\(\\forall\\;y\\in\\mathcal{V}\\) there exists a set of \\(k\\) scalars such that, \\[\ny=X_1b_1+X_2b_2+\\ldots+X_kb_k\n\\]\n\nBased on these definitions, it is evident that for the Euclidean space, \\(\\mathbb{E}^n\\), any \\(n\\) linearly independent vectors from \\(\\mathbb{R}^n\\) is a basis. For example, any point in \\(\\mathbb{E}^2\\) can be defined as a multiple of,\n\\[\n\\begin{bmatrix}1\\\\0\\end{bmatrix}\\quad \\text{and} \\quad\\begin{bmatrix}0\\\\1\\end{bmatrix}\n\\]\nConsider again the \\(n\\times k\\) matrix \\(X\\), where \\(k&lt;n\\). Then we define the,\n\nDefinition 8 column space (or span) of \\(X\\), denoted \\(\\mathcal{S}(X)\\), as the vector space generate by the \\(k\\) columns of \\(X\\). Formally, \\[\n\\mathcal{S}(X) = \\{y\\in\\mathbb{R}^n:\\;y=Xb\\quad\\text{for some }b\\in \\mathbb{R}^k\\}\n\\]\n\nA property to note about the span or column space \\(X\\) is,\nResult: \\(\\mathcal{S}(X)=\\mathcal{S}(XX')\\) :::\nwhere \\(XX'\\) is a \\(n\\times n\\) matrix.\nFinally, we can define the,\n\nDefinition 9 orthogonal column space (or orthogonal span) of \\(X\\) as, \\[\n\\mathcal{S}^{\\perp}(X) = \\{y\\in \\mathbb{R}^k:\\;y'x=0\\quad \\forall x\\in\\mathcal{S}(X)\\}\n\\]"
  },
  {
    "objectID": "material-linearalgebra.html#rank",
    "href": "material-linearalgebra.html#rank",
    "title": "Linear Algebra",
    "section": "",
    "text": "Consider a \\(n\\times k\\) matrix \\(X\\), the\n\nDefinition 10 row rank of \\(X\\) is the maximum number of linearly independent rows: \\[\nrowrank(X) \\leq n\n\\]\n\nWe say that matrix \\(X\\) has full row rank if \\(rowrank(X)=n\\).\nThe,\n\nDefinition 11 column rank of \\(X\\) is the maximum number of linearly independent columns:\n\\[\ncolrank(X) \\leq k\n\\]\n\nWe say that matrix \\(X\\) has full column rank if \\(colrank(X)=k\\).\nAn important result is,\n\nResult: the rank of \\(X\\): \\[\nr(X) = rowrank(X)=colrank(X) \\\\\n\\Rightarrow r(X)\\leq min\\{n,k\\}\n\\]\n\nIn addition, since the \\(r(X)\\) depends on the number of linearly independent columns, we can say that,\n\nResult: the dimension of \\(\\mathcal{S}(X)\\), \\(dim(\\mathcal{S}(X))\\), is given by the \\(r(X)\\).\n\nHere are a few additional results,\n\nResult: \\(r(X)=r(X')\\)\nResult: \\(r(XY)\\leq min\\{r(X),r(Y)\\}\\)\nResult: \\(r(XY)=r(X)\\) if \\(Y\\) is square and full rank\nResult: \\(r(X+Y)\\leq r(X) + r(Y)\\)"
  },
  {
    "objectID": "material-linearalgebra.html#properties-of-square-matrices",
    "href": "material-linearalgebra.html#properties-of-square-matrices",
    "title": "Linear Algebra",
    "section": "",
    "text": "Consider the case of a square, \\(n\\times n\\), matrix \\(A\\). We say that,\n\nDefinition 12 \\(A\\) is singular if the \\(r(A)&lt;n\\),\n\nor that,\n\nDefinition 13 \\(A\\) is non-singular if the \\(r(A)=n\\).\n\nThe singularity of a square matrix is important as it determines the invertibility of a matrix, which typically relates the existence of a unique solution in systems of linear equations. Here are a few key results,\n\nResult: There exists a matrix \\(B=A^{-1}\\), such that \\(AB=I_n\\) (where \\(I_n\\) is the identity matrix), if and only if \\(A\\) is non-singular.\nResult: \\(A\\) is non-singular if and only if the determinant of \\(A\\) is non-zero: \\(det(A)\\neq0\\).1\nResult: Likewise, \\(A\\) is singular if and only if \\(det(A)=0\\).\nResult: \\(AA^{-1}=A^{-1}A=I\\)\nResult: \\((A')^{-1}=(A^{-1})'\\)\nResult: If their respective inverses exist, then \\((AB)^{-1}=B^{-1}A^{-1}\\).\nResult: \\(det(AB)=det(A)det(B)\\)\nResult: \\(det(A^{-1})=det(A)^{-1}\\)\n\nFor any square matrix \\(A\\),\n\nDefinition 14 the trace of \\(A\\) is the sum of all diagonal elements: \\[\ntr(A) = \\sum_{i=1}^na_{ii}\n\\]\n\nRegarding the trace of a square matrix, here are a few important results:\n\nResult: \\(tr(A+B) = tr(A) + tr(B)\\)\nResult: \\(tr(\\lambda A) = \\lambda tr(A)\\) where \\(\\lambda\\) is a scalar\nResult: \\(tr(A) = tr(A')\\)\nResult: \\(tr(AB) = tr(BA)\\) where \\(AB\\) and \\(BA\\) are both square, but need not be of the same order.\nResult: \\(||A|| = (tr(A'A))^{1/2}\\)"
  },
  {
    "objectID": "material-linearalgebra.html#properties-of-symmetric-matrices",
    "href": "material-linearalgebra.html#properties-of-symmetric-matrices",
    "title": "Linear Algebra",
    "section": "",
    "text": "A symmetric matrix has the property that \\(A=A'\\). Therefore, \\(A\\) must be square.\nHere are a few important results concerning symmetric matrices.\n\nResult: \\(A^{-1}\\) exists if \\(det(A)\\neq 0\\) and \\(r(A)=n\\)\nResult: A is diagonalizable.2\nResult: The eigenvector decomposition of a square matrix gives you \\(A=C\\Lambda C^{-1}\\) where \\(\\Lambda\\) is a diagonal matrix of eigenvalues and $C$ a matrix of the corresponding eigenvectors. The symmetry of \\(A\\) gives you that \\(C^{-1}=C'\\Rightarrow A=C\\Lambda C'\\) with \\(C'C=CC'=I_{n}\\).3\n\nA key definition concerning symmetric matrices is their positive definiteness:\n\nDefinition 15 \\(A\\) is positive semi-definite if for any \\(x\\in\\mathbb{R}^n,\\;x'Ax\\geq0\\).\n\nGiven the eigenvector decomposition of a symmetric matrix, positive semi-definiteness implies \\(\\Lambda\\) is positive semi-definite: \\(\\lambda_i\\geq0\\quad\\forall i\\). Likewise,\n\nDefinition 16 \\(A\\) is positive definite if for any \\(x\\in\\mathbb{R}^n,\\;x'Ax&gt;0\\).\n\nAgain, based on the egeinvector decomposition, positive semi-definiteness implies \\(\\Lambda\\) is positive definite: \\(\\lambda_i&gt;0\\quad\\forall i\\).\nA few more results are:\n\nResult: \\(tr(A) = \\sum_{i=1}^n\\lambda_i\\)\nResult: \\(r(A) = r(\\Lambda)\\)\nResult: \\(det(A) = \\prod_{i=1}^n \\lambda_i\\)\n\nThis last result can be used to prove that any positive definite matrix is non-singular and therefore has an inverse.\nAny full-rank, positive semi-definite, symmetric matrix \\(B\\) has the additional properties:\n\nResult: \\(B=C\\Lambda C'\\) and \\(B^{-1} = C\\Lambda^{-1}C'\\)\nResult: We can define the square-root of \\(B\\) as \\(B^{1/2} = C\\Lambda^{1/2}C'\\). Similarly, \\(B^{-1/2} = C\\Lambda^{-1/2}C'\\)."
  },
  {
    "objectID": "material-linearalgebra.html#properties-of-idempotent-matrices",
    "href": "material-linearalgebra.html#properties-of-idempotent-matrices",
    "title": "Linear Algebra",
    "section": "",
    "text": "An idempotent matrix has the property that \\(D=DD\\). Therefore, \\(D\\) must be square.\nHere are a few important results concerning idempotent matrices.\n\nResult: \\(D\\) is positive definite\nResult: \\(D\\) is diagonalizable\nResult: \\((I_n-D)\\) is also an idempotent matrix\nResult: With the exception of \\(I_n\\), all idempotent matrices are singular.\nResult: \\(r(D) = tr(D) = \\sum_{i=1}^n\\lambda_i\\)\nResult: \\(\\lambda_i\\in\\{0,1\\}\\quad \\forall\\;i\\)\n\nProjection matrices are idempotent, but need not be symmetric. However, for the purposes of this module we will deal exclusively with symmetric idempotent projection matrices."
  },
  {
    "objectID": "material-linearalgebra.html#vector-differentiation",
    "href": "material-linearalgebra.html#vector-differentiation",
    "title": "Linear Algebra",
    "section": "",
    "text": "Here we will look at the derivatives of scalar with respect to (W.r.t.) a vector. You can also define other derivatives, such as the derivative of a vector w.r.t. a vector and the derivative of a scalar with respect to a matrix. However, these are not needed for these notes.\n\n\nSuppose \\(f(x)\\in R\\) (i.e. a scalar) and \\(x\\in R^n\\) (i.e. a \\(n\\times 1\\) vector). Then we can define the partial derivative of \\(f(x)\\) w.r.t. \\(x\\) as,\n\\[\n  \\frac{\\partial f(x)}{\\partial x}  = \\begin{bmatrix}\\frac{\\partial f(x)}{\\partial x_1} \\\\ \\frac{\\partial f(x)}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial f(x)}{\\partial x_n} \\end{bmatrix}\n\\]\n\n\n\nA special case is when \\(f(x)\\) is linear in \\(x\\),\n\\[\nf(x) = a'x = \\sum_{i=1}^n a_ix_i\n\\] for \\(a\\in R^n\\). The derivative of \\(a'x\\) with respect to the vector \\(x\\) can be defined as,\n\\[\n  \\begin{aligned}\n  \\frac{\\partial a'x}{\\partial x} & = \\begin{bmatrix}\\frac{\\partial a'x}{\\partial x_1} \\\\ \\frac{\\partial a'x}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial a'x}{\\partial x_n} \\end{bmatrix} \\\\\n  & = \\begin{bmatrix}a_1 \\\\ a_2 \\\\ \\vdots \\\\ a_n \\end{bmatrix} \\\\\n  & = a\n  \\end{aligned}\n\\] since the the partial derivate of \\(a'x = \\sum_{i=1}^n a_ix_i\\) w.r.t. \\(x_i\\) is just the scalar \\(a_i\\).\nThe derivative of a scalar w.r.t. to a vector yields a vector of partial derivatives.\nSince \\(a'x\\) is a scalar, it is by definition symmetric: \\(a'x = x'a\\). Thus,\n\\[\n\\frac{\\partial x'a}{\\partial x} = \\frac{\\partial a'x}{\\partial x} = a\n\\]\n\n\n\nSuppose \\(f(x)\\) is a linear transformation of \\(x\\),\n\\[\nf(x) = A'x\n\\] for any \\(m\\times n\\) matrix A,\n\\[\n  A = \\begin{bmatrix}a_1' \\\\ a_2' \\\\ \\vdots \\\\ a_m'\\end{bmatrix}\n\\] where \\(a_i\\in R^n\\;\\forall i=1,\\dots,m\\) and,\n\\[\n  Ax = \\begin{bmatrix}a_1'x \\\\ a_2'x \\\\ \\vdots \\\\ a_m'x\\end{bmatrix}\n\\] Note, \\(f(x)=Ax\\in R^m\\), a \\(m\\times 1\\) vector. We can then define,\n\\[\n  \\begin{aligned}\n  \\frac{\\partial Ax}{\\partial x'} & = \\begin{bmatrix}\\frac{\\partial a_1'x}{\\partial x_1} & \\frac{\\partial a_1'x}{\\partial x_2} & \\dots & \\frac{\\partial a_1'x}{\\partial x_n}\\\\ \\frac{\\partial a_2'x}{\\partial x_1} & \\frac{\\partial a_2'x}{\\partial x_2} & \\dots & \\frac{\\partial a_2'x}{\\partial x_n}\\\\ \\vdots & \\vdots & \\ddots & \\vdots\\\\ \\frac{\\partial a_m'x}{\\partial x_1} & \\frac{\\partial a_m'x}{\\partial x_2} & \\dots & \\frac{\\partial a_m'x}{\\partial x_n}\\\\ \\end{bmatrix} \\\\\n  & = \\begin{bmatrix} a_{11} & a_{12} & \\dots & a_{1n} \\\\ a_{21} & a_{22} & \\dots & a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots\\\\ a_{m1} & a_{m2} & \\dots & a_{mn} \\\\ \\end{bmatrix} \\\\\n  & = A\n  \\end{aligned}\n\\]\nSince Ax is \\(m\\times 1\\) column vector, we take the derivative w.r.t. \\(x'\\) a row vector and not the column vector \\(x\\). This results in a matrix of partial derivatives.\n\n\n\nA second special case is where the function takes on the quadaratic form,\n\\[\nf(x) = x'Ax = \\sum_{i=1}^N\\sum_{j=1}^n a_{ij}x_ix_j\n\\]\nfor \\(n\\times n\\) (square) matrix A. As in the first linear case, \\(f(x)\\) is scalar.\nDefine \\(c = Ax\\), the \\(x'Ax = x'c\\). From the linear case, we know that,\n\\[\n\\frac{\\partial x'c}{\\partial x} = c\n\\]\nSimilarly, if we define \\(d = A'x\\) then \\(x'Ax = d'x\\). From the linear case, we know that,\n\\[\n\\frac{\\partial d'x}{\\partial x} = d\n\\]\nWe can define the total derivative as the sum of the partial derivatives w.r.t. to the first and second \\(x\\). Combining these two results, we have that,\n\\[\n\\frac{\\partial x'Ax}{\\partial x} = Ax + A'x\n\\]\nAnd if \\(A\\) is symmetric, this result simplifies to \\(2Ax\\)."
  },
  {
    "objectID": "material-linearalgebra.html#footnotes",
    "href": "material-linearalgebra.html#footnotes",
    "title": "Linear Algebra",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThese notes do not cover how to calculate the determinant of a square matrix. You should be able to find a definition easily online.↩︎\nA matrix is diagonalizable if it is similar to some other diagonal matrix. Matrices \\(B\\) and \\(C\\) are similar if \\(C=PBP^{-1}\\). A square matrix which is not diagonalizable is defective. This property relates closely to eigenvector decomposition.↩︎\nRecall, an eigenvalue and eigenvector pair, \\((\\lambda,c)\\), of matrix \\(A\\) satisfy:\n\\[\nAc = \\lambda c\\Rightarrow (A-\\lambda I_n)c=0\n\\]↩︎"
  }
]