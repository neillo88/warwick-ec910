[
  {
    "objectID": "overview.html#outline",
    "href": "overview.html#outline",
    "title": "EC910",
    "section": "Outline",
    "text": "Outline\nThe general structure of EC910 (or EC9871); including,\n\nwho it’s for;\n’metrics A or B;\na tentative syllabus.\n\nMSc BES Economics-track students only take term 1 of Econometrics B"
  },
  {
    "objectID": "overview.html#whos-it-for",
    "href": "overview.html#whos-it-for",
    "title": "EC910",
    "section": "Who’s it for?",
    "text": "Who’s it for?\nTarget audience: someone who wants to understand the language and concepts needed to comprehend more technical Economic research.\nBecause,\n\n\nyou are considering a PhD;\nyou have a strong background in Econometrics, Statistics, or Mathematics (incl., Physics and Engineering);\nyour favourite question is “Why?” and proofs help you sleep;\nyou don’t like sunshine and prefer Greek letters."
  },
  {
    "objectID": "overview.html#litmus-test",
    "href": "overview.html#litmus-test",
    "title": "EC910",
    "section": "Litmus Test",
    "text": "Litmus Test\nWhat is the first thing you think, when you see this?\n\n\\[\n\\bigg[\\underbrace{n^{-1}\\sum_{i=1}^n\\frac{\\partial g(W_i,\\hat{\\theta}^{GMM})}{\\partial \\theta'}}_{Q_n\\big(\\hat{\\theta}^{GMM}\\big)}\\bigg]'A_n'A_n n^{-1}\\sum_{i=1}^{n}g(W_i,\\hat{\\theta}^{GMM}) = 0\n\\]\n\n\n\n“please, NO MORE”\n“interesting, tell me more”"
  },
  {
    "objectID": "overview.html#individual-learning-outcomes",
    "href": "overview.html#individual-learning-outcomes",
    "title": "EC910",
    "section": "Individual Learning Outcomes",
    "text": "Individual Learning Outcomes\nBy the end of this year, you should be able to:\n\n\ndiscuss and prove the properties of common estimators, and demonstrate these properties using simulations;\ncompute various estimators using experimental and observational data;\nuse statistical languages/software (R/Stata);\naccurately interpret and discuss the results of empirical research output;\nengage more confidently with technical Economics research."
  },
  {
    "objectID": "overview.html#metrics-a-or-b",
    "href": "overview.html#metrics-a-or-b",
    "title": "EC910",
    "section": "Metrics A or B",
    "text": "Metrics A or B\n\n\nMetrics B:\n\nassumes familiarity with basic material\nmore methodological, incl. proofs\ntheory focused\nuse vector notation\napplications in seminars"
  },
  {
    "objectID": "overview.html#foundation",
    "href": "overview.html#foundation",
    "title": "EC910",
    "section": "Foundation",
    "text": "Foundation\nMetrics B assumes you have a good knowledge of the following:\n\nt and F tests for linear restrictions\nHeteroskedasticity and Serial Correlation\nEstimation - in the presence of above problems\nDummy variables.\nExogeneity and Endogeneity"
  },
  {
    "objectID": "overview.html#module-structure",
    "href": "overview.html#module-structure",
    "title": "EC910",
    "section": "Module Structure",
    "text": "Module Structure\n\nTerm 1: Neil Lloyd (Module Leader)\n\nCross-section and panel data models\nSelection models\nCausal inference\n\nTerm 2: Han Zhang\n\nInstrumental Variables\nTime Series"
  },
  {
    "objectID": "overview.html#weekly-seminars",
    "href": "overview.html#weekly-seminars",
    "title": "EC910",
    "section": "Weekly Seminars",
    "text": "Weekly Seminars\n\nStart Week 3\nSign-up for class via Tabula\nTutors\n\nNeil Lloyd (Term 1)\nCarolina Kansikas (Term 1)\nHan Zhang (Term 2)\n\n\n\n\n\n\nEC910: Econometrics B"
  },
  {
    "objectID": "material-interpretation.html",
    "href": "material-interpretation.html",
    "title": "Interpretation of Linear Models",
    "section": "",
    "text": "In this short handout we will consider the interpretation of linear regression model coefficients in models with different combinations of outcome and regressor variables:\n\ncontinuous level-level\ncontinuous-discrete\ndiscrete-continuous\ndiscrete-discrete\nlog-level\nlevel-log\nlog-log\n\nIn all instances, we will work on the CLRM model assumptions 1 & 2, which tell us that the conditional expectation function is linear in parameters:\n\\[\nE[Y_i|X_i] = X_i'\\beta\n\\]\n\n\nIf \\(Y_i\\) and \\(X_i\\) are both continuously distributed random variables then,\n\\[\n\\beta_j = \\frac{\\partial E[Y_i|X_i]}{\\partial X_{ij}}\n\\] or, as a vector,\n\\[\n\\beta = \\frac{\\partial E[Y_i|X_i]}{\\partial X_{i}} = \\begin{bmatrix}\\frac{\\partial E[Y_i|X_i]}{\\partial X_{i1}}\\\\ \\vdots \\\\ \\frac{\\partial E[Y_i|X_i]}{\\partial X_{ik}}\\end{bmatrix} = \\begin{bmatrix}\\beta_1\\\\ \\vdots \\\\ \\beta_k\\end{bmatrix}\n\\]\nThe regression parameter has a partial derivative interpretation with respect to the CEF. As discussed in Handout 1, this is often used to motivate the experimental language of ceteris paribus: “holding all else fixed.\n\n\n\nConsider a case where there is a single discrete regressor: \\(D_i \\in \\{0,1\\}\\). For example,\n\\[\n  Y_i = \\beta_1 + \\beta_2 D_i + \\varepsilon_i\n\\] We cannot apply the partial derivative interpretation since \\(D\\) is not continuous. Instead, we will look at differences:\n\\[\n\\begin{aligned}\n    E[Y_i|D_i=1] =& \\beta_1 + \\beta_2 \\\\\n    E[Y_i|D_i=0] =& \\beta_1 \\\\   \n    \\Rightarrow \\beta_2 =& E[Y_i|D_i=1] - E[Y_i|D_i=0]\n\\end{aligned}\n\\]\nWe can easily extend this the case where the model includes additional (discrete or continuous) covariates, as well as case where the variable takes on multiple discrete values.\n\n\n\nIf the outcome is discrete (\\(Y_i\\in\\{0,1\\}\\)) while the regressors are continuous, the resulting linear model is referred to as a linear probability model.\n\\[\nE[Y_i|X_i] = Pr(Y_i = 1|X_i) = X_i'\\beta\n\\] This is differentiable, since \\(X\\) is continuous and the same partial derivative interpretation follows.\n\\[\n\\beta_j = \\frac{\\partial Pr(Y_i=1|X_i)}{\\partial X_{ij}}\n\\]\nNote, the unit of \\(Y\\) is probability-points (\\(\\in[0,1]\\)), not %-points (\\(\\in[0,100]\\)). Of course, the conversion of units can be made by \\(\\times 100\\) to measure in %-points.\n\n\n\nIf both the outcome and regressor(s) are discrete, then the parameter identifies a difference in conditional probabilities, \\[\n\\beta_2 = Pr(Y_i|D_i=1) - Pr(Y_i=1|D_i=0)\n\\] Note, the unit of \\(Y\\) is probability-points (\\(\\in[0,1]\\)), not %-points (\\(\\in[0,100]\\)).\n\n\n\nConsider the model,\n\\[\n  \\ln(Y_i) = X_i'\\beta + \\varepsilon_i\n\\] Then,\n\\[\n  X_i'\\beta = E[\\ln(Y_i)|X_i]\n\\]\n\\[\n\\beta_j = \\frac{\\partial E[\\ln(Y_i)|X_i]}{\\partial X_{ij}}\n\\]\nThe coefficient is therefore measured in log-units of \\(Y\\). The relation to a change in the (expected) level of \\(Y\\) is given by,\n\\[\n\\%\\Delta E[Y_i|X_i] = (exp(\\beta)-1)\\times 100\n\\] For reasonably small values of \\(\\beta\\) (i.e. within the range \\([-0.1,0.1]\\)) this can be approximated by,\n\\[\n\\%\\Delta E[Y_i|X_i] = \\beta\\times 100\n\\] A 1-unit change in \\(X_{i1}\\) is associated with a \\(\\beta_1\\) percent change in the expected value of \\(Y\\).\nThis referred to as a semi-elasticity.\n\n\n\nIf the regressor(s) is measure in log-units; for example,\n\\[\n  Y_i = \\beta_1 + \\beta_2 \\ln(X_i)_i + \\varepsilon_i\n\\] Then,\n\\[\n\\beta_2 = \\frac{\\partial E[Y_i|X_i]}{\\partial \\ln(X_{i})}\n\\]\nA 1 percent increase in \\(X\\) is given by \\(X\\times1.01\\). This is equivalent to a change in \\(\\ln(X)\\) of,\n\\[\n\\ln(X_i\\times1.01) - \\ln(X_i) = \\ln(1.01) \\approx 0.01\n\\] Thus, a 1 percent increase in the level of \\(X\\) is associated with a \\(\\beta_2/100\\) increase in the expected value of \\(Y\\). Or, more accurate\n\\[\n  \\Delta E[Y_i|X_i] = \\beta_2\\times \\ln(1.01)\n\\] This is also a semi-elasticity.\n\n\n\nIn models where both the outcome and regressor are log-transformed with an elasticity interpretation.\n\\[\n    \\ln(Y_i) = \\beta_1 + \\beta_2 \\ln(X_i)_i + \\varepsilon_i\n\\] \\[\n\\beta_2 = \\frac{\\partial E[\\ln(Y_i)|X_i]}{\\partial \\ln(X_{i})}\n\\] \\(\\beta_2\\) is a the % change in the expected value of \\(Y\\) from a 1 % change in \\(X\\)."
  },
  {
    "objectID": "material-interpretation.html#continuous-level-level-models",
    "href": "material-interpretation.html#continuous-level-level-models",
    "title": "Interpretation of Linear Models",
    "section": "",
    "text": "If \\(Y_i\\) and \\(X_i\\) are both continuously distributed random variables then,\n\\[\n\\beta_j = \\frac{\\partial E[Y_i|X_i]}{\\partial X_{ij}}\n\\] or, as a vector,\n\\[\n\\beta = \\frac{\\partial E[Y_i|X_i]}{\\partial X_{i}} = \\begin{bmatrix}\\frac{\\partial E[Y_i|X_i]}{\\partial X_{i1}}\\\\ \\vdots \\\\ \\frac{\\partial E[Y_i|X_i]}{\\partial X_{ik}}\\end{bmatrix} = \\begin{bmatrix}\\beta_1\\\\ \\vdots \\\\ \\beta_k\\end{bmatrix}\n\\]\nThe regression parameter has a partial derivative interpretation with respect to the CEF. As discussed in Handout 1, this is often used to motivate the experimental language of ceteris paribus: “holding all else fixed."
  },
  {
    "objectID": "material-interpretation.html#continuous-discrete-models",
    "href": "material-interpretation.html#continuous-discrete-models",
    "title": "Interpretation of Linear Models",
    "section": "",
    "text": "Consider a case where there is a single discrete regressor: \\(D_i \\in \\{0,1\\}\\). For example,\n\\[\n  Y_i = \\beta_1 + \\beta_2 D_i + \\varepsilon_i\n\\] We cannot apply the partial derivative interpretation since \\(D\\) is not continuous. Instead, we will look at differences:\n\\[\n\\begin{aligned}\n    E[Y_i|D_i=1] =& \\beta_1 + \\beta_2 \\\\\n    E[Y_i|D_i=0] =& \\beta_1 \\\\   \n    \\Rightarrow \\beta_2 =& E[Y_i|D_i=1] - E[Y_i|D_i=0]\n\\end{aligned}\n\\]\nWe can easily extend this the case where the model includes additional (discrete or continuous) covariates, as well as case where the variable takes on multiple discrete values."
  },
  {
    "objectID": "material-interpretation.html#discrete-continuous-models",
    "href": "material-interpretation.html#discrete-continuous-models",
    "title": "Interpretation of Linear Models",
    "section": "",
    "text": "If the outcome is discrete (\\(Y_i\\in\\{0,1\\}\\)) while the regressors are continuous, the resulting linear model is referred to as a linear probability model.\n\\[\nE[Y_i|X_i] = Pr(Y_i = 1|X_i) = X_i'\\beta\n\\] This is differentiable, since \\(X\\) is continuous and the same partial derivative interpretation follows.\n\\[\n\\beta_j = \\frac{\\partial Pr(Y_i=1|X_i)}{\\partial X_{ij}}\n\\]\nNote, the unit of \\(Y\\) is probability-points (\\(\\in[0,1]\\)), not %-points (\\(\\in[0,100]\\)). Of course, the conversion of units can be made by \\(\\times 100\\) to measure in %-points."
  },
  {
    "objectID": "material-interpretation.html#discrete-discrete-models",
    "href": "material-interpretation.html#discrete-discrete-models",
    "title": "Interpretation of Linear Models",
    "section": "",
    "text": "If both the outcome and regressor(s) are discrete, then the parameter identifies a difference in conditional probabilities, \\[\n\\beta_2 = Pr(Y_i|D_i=1) - Pr(Y_i=1|D_i=0)\n\\] Note, the unit of \\(Y\\) is probability-points (\\(\\in[0,1]\\)), not %-points (\\(\\in[0,100]\\))."
  },
  {
    "objectID": "material-interpretation.html#log-level-models",
    "href": "material-interpretation.html#log-level-models",
    "title": "Interpretation of Linear Models",
    "section": "",
    "text": "Consider the model,\n\\[\n  \\ln(Y_i) = X_i'\\beta + \\varepsilon_i\n\\] Then,\n\\[\n  X_i'\\beta = E[\\ln(Y_i)|X_i]\n\\]\n\\[\n\\beta_j = \\frac{\\partial E[\\ln(Y_i)|X_i]}{\\partial X_{ij}}\n\\]\nThe coefficient is therefore measured in log-units of \\(Y\\). The relation to a change in the (expected) level of \\(Y\\) is given by,\n\\[\n\\%\\Delta E[Y_i|X_i] = (exp(\\beta)-1)\\times 100\n\\] For reasonably small values of \\(\\beta\\) (i.e. within the range \\([-0.1,0.1]\\)) this can be approximated by,\n\\[\n\\%\\Delta E[Y_i|X_i] = \\beta\\times 100\n\\] A 1-unit change in \\(X_{i1}\\) is associated with a \\(\\beta_1\\) percent change in the expected value of \\(Y\\).\nThis referred to as a semi-elasticity."
  },
  {
    "objectID": "material-interpretation.html#level-log-models",
    "href": "material-interpretation.html#level-log-models",
    "title": "Interpretation of Linear Models",
    "section": "",
    "text": "If the regressor(s) is measure in log-units; for example,\n\\[\n  Y_i = \\beta_1 + \\beta_2 \\ln(X_i)_i + \\varepsilon_i\n\\] Then,\n\\[\n\\beta_2 = \\frac{\\partial E[Y_i|X_i]}{\\partial \\ln(X_{i})}\n\\]\nA 1 percent increase in \\(X\\) is given by \\(X\\times1.01\\). This is equivalent to a change in \\(\\ln(X)\\) of,\n\\[\n\\ln(X_i\\times1.01) - \\ln(X_i) = \\ln(1.01) \\approx 0.01\n\\] Thus, a 1 percent increase in the level of \\(X\\) is associated with a \\(\\beta_2/100\\) increase in the expected value of \\(Y\\). Or, more accurate\n\\[\n  \\Delta E[Y_i|X_i] = \\beta_2\\times \\ln(1.01)\n\\] This is also a semi-elasticity."
  },
  {
    "objectID": "material-interpretation.html#log-log-models",
    "href": "material-interpretation.html#log-log-models",
    "title": "Interpretation of Linear Models",
    "section": "",
    "text": "In models where both the outcome and regressor are log-transformed with an elasticity interpretation.\n\\[\n    \\ln(Y_i) = \\beta_1 + \\beta_2 \\ln(X_i)_i + \\varepsilon_i\n\\] \\[\n\\beta_2 = \\frac{\\partial E[\\ln(Y_i)|X_i]}{\\partial \\ln(X_{i})}\n\\] \\(\\beta_2\\) is a the % change in the expected value of \\(Y\\) from a 1 % change in \\(X\\)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Quantitative Methods: Econometrics B",
    "section": "",
    "text": "This is not the official module website. The EC910 (or EC987, 2024/25) Moodle-page is the primary source of communication and resources for this module. All material available on this website can be found there.\nI designed this website as a means of testing out the Quarto package, developed by RStudio.1 This was partly as a way to develop my own skills and better incorporate the R language into the module. However, there are also accessibility benefits to this approach. Mathematical notation published in pdf’s using LaTeX cannot be read by a screen reader, while LaTeX published in html can. In addition, the website has built-in dark-mode option (see top-right toggle). For those who prefer pdf (for printing and notetaking), each page contains a unique downloadable link.\nPlease let me know if you find any mistakes in my notes or code. In addition, I welcome suggestions on how to improve the R (or Stata) code provided in this module. I will endeavor to do my best to acknowledge the many people who have contributed to this material and my wider knowledge of this subject."
  },
  {
    "objectID": "index.html#welcome-to-metrics-b",
    "href": "index.html#welcome-to-metrics-b",
    "title": "Quantitative Methods: Econometrics B",
    "section": "",
    "text": "This is not the official module website. The EC910 (or EC987, 2024/25) Moodle-page is the primary source of communication and resources for this module. All material available on this website can be found there.\nI designed this website as a means of testing out the Quarto package, developed by RStudio.1 This was partly as a way to develop my own skills and better incorporate the R language into the module. However, there are also accessibility benefits to this approach. Mathematical notation published in pdf’s using LaTeX cannot be read by a screen reader, while LaTeX published in html can. In addition, the website has built-in dark-mode option (see top-right toggle). For those who prefer pdf (for printing and notetaking), each page contains a unique downloadable link.\nPlease let me know if you find any mistakes in my notes or code. In addition, I welcome suggestions on how to improve the R (or Stata) code provided in this module. I will endeavor to do my best to acknowledge the many people who have contributed to this material and my wider knowledge of this subject."
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Quantitative Methods: Econometrics B",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nI am building off material developed by Prof. Wiji Arulampalam, who taught this module from 2011-2024. Since arriving at Warwick in 2021, I have learned so much from Prof. Arulampalam’s expertise and years of research and professional experience. More importantly, I have so enjoyed working alongside her as both colleague and friend. I wish her well in her retirement.\nI have also borrowed from material developed by Prof. Vadim Marmer (Vancouver School of Economics, UBC) who taught the Econometrics module I attended during my own Master’s degree. I was the teaching assistant for this module from 2017-2019 and developed some additional notes found here. I am hugely indebted to Prof. Marmer, whose teaching and professional guidance have been foundational to my academic career.\nI hope you enjoy this module!\nNeil Lloyd"
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Quantitative Methods: Econometrics B",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.↩︎"
  },
  {
    "objectID": "handout-2.html",
    "href": "handout-2.html",
    "title": "Estimators and their Properties",
    "section": "",
    "text": "Code\nlibrary(ggplot2)\nlibrary(latex2exp)"
  },
  {
    "objectID": "handout-2.html#overview",
    "href": "handout-2.html#overview",
    "title": "Estimators and their Properties",
    "section": "1 Overview",
    "text": "1 Overview\nIn this handout we will investigate the desirable properties of an estimator. Further reading can be found in:\n\nAppendix A of Cameron and Trivedi (2005)\nSection 2.6 of Verbeek (2017)"
  },
  {
    "objectID": "handout-2.html#an-estimator",
    "href": "handout-2.html#an-estimator",
    "title": "Estimators and their Properties",
    "section": "2 An estimator",
    "text": "2 An estimator\nAn estimator is a rule that tells you how to get your estimate given the observed data. As it is a that it is a function of random variables, an estimator is a random variable itself.\nWithout further information, we do not know anything about the distribution of this random variable and, in practice, will typically have to estimate the parameters of this distribution: mean, variance, skewness.\nThe assumed model (or data generating process) may pin down certain parameters or distributions. For example, under assumpions CLRM 5 & 6 the ordinary least squares estimator for \\(\\beta\\) will be normally distributed."
  },
  {
    "objectID": "handout-2.html#properties-of-estimators",
    "href": "handout-2.html#properties-of-estimators",
    "title": "Estimators and their Properties",
    "section": "3 Properties of estimators",
    "text": "3 Properties of estimators\nWhen evaluating an estimator you want to consider:\n\nbias: does the estimator yield estimates that ‘hit the right target’ on average?\nefficiency: do the estimates generated by the estimator have a limited dispersion/variance?\ndistribution: do you know the exact/approximate distribution of the estimator with which you can construct a valid hypothesis test?\n\nIt is important to consider both the small sample and large sample (asymptotic) properties of an estimator.\nUnbiasedness (property 1) is typically emphasized over properties 2 and 3. Afterall, a precise estimator of the wrong target is not particularly useful. That said, the expected value of an estimator is not always well defined.\n\n\n\n\n\n\nImportant\n\n\n\nThese are properties of the estimator, a random variable, and not the estimate. The estimate is a just a constant: a non-random realization of the estimator.\n\n\n\n3.1 Small vs large sample properties\nSometimes it is easier to study the large-sample, or asymptotic, properties of an estimator. That is, the properties of the estimator as the sample size gets large: \\(n\\rightarrow \\infty\\).\nSmall-sample properties include:\n\nBiasedness\nEfficiency/variance\nFinite-sample distribution\n\nLarge-sample properties include:\n\nConsistency\nAsymptotic distribution\n\nAn estimator can have an asymptotic variance (efficiency); although, in EC910 will mostly discuss estimators with variances that shrink to zero as \\(n\\rightarrow \\infty\\).\n\nThe phrase “finite-sample” is also used in other contexts. For example, in the Causal Inference (or Treatment Effects) literature “finite-sample” is used to describe estimands. This literature distinguishes between finite-sample and super-population estimands. The former relate to settings where the sample is treated as fixed, while the latter to settings where the sample is take as a draw from an unknown super population.\nThe estimator for a super-population estimand will have both finite and asymptotic properities.\n\n\n\n3.2 Bias\nThe estimator \\(\\hat{\\theta}\\) is unbiased for \\(\\theta\\) if,\n\\[\nE(\\hat{\\theta}) = \\theta\n\\] \\(\\hat{\\theta}\\) is a function of sample size, \\(n\\), while \\(\\theta\\) is not. Some texts will use the notation \\(\\hat{\\theta}_n\\) to emphasize this point.\n\n\nCode\nx_values &lt;- seq(-2, 3, length.out = 100)\nggplot(data.frame(x = x_values), aes(x = x)) +\n  stat_function(fun = dnorm, args = list(mean = 1, sd = 1), color = \"red\") + \n  stat_function(fun = dnorm, args = list(mean = 0, sd = .5), color = \"blue\") + \n  geom_vline(xintercept = 1, linetype = \"dashed\", color = \"red\", linewidth = 1) +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"blue\", linewidth = 1) + \n  labs(\n    title = NULL,\n    x = NULL,\n    y = \"Density\"\n  ) +\n  scale_x_continuous(\n    breaks = c(0, 1),            \n    labels = c(TeX(\"$\\\\theta$\"), TeX(\"$E[\\\\hat{\\\\theta}]$\"))  \n  ) +\n  theme_minimal() +\n  annotate(\"text\", x = 1.75, y = 0.35, label = \"Biased\", vjust = -1, hjust = 0.5, size = 5, color = \"red\") +\n  annotate(\"text\", x = -0.75, y = 0.6, label = \"Unbiased\", vjust = -1, hjust = 0.5, size = 5, color = \"blue\")\n\n\n\n\n\n\n\n\n\n\n\n3.3 Efficiency\nEfficiency is a relative concept. The estimator \\(\\hat{\\theta}\\) is more efficient the estimator \\(\\tilde{\\theta}\\), if both are unbiased for \\(\\theta\\) and\n\\[\nVar(\\hat{\\theta})&lt;Var(\\tilde{\\theta})\n\\] What if the estimator is biased? We can use Mean Square Error:\n\nDefinition 1 (MSE) \\(MSE = E\\big[(\\hat{\\theta}-\\theta)^2\\big] = \\text{bias}^2+\\text{variance}\\)\n\nNote, the above definition assumes that a finite variance.\n\nProof. \\[\n\\begin{aligned}\nE\\big[(\\hat{\\theta}-\\theta)^2\\big] &= E\\big[\\big((\\hat{\\theta}-E[\\hat{\\theta}])-(E[\\hat{\\theta}]-\\theta)\\big)^2\\big] \\\\\n&=E\\big[(\\hat{\\theta}-E[\\hat{\\theta}])^2\\big]+ E\\big[(\\theta-E[\\hat{\\theta}])\\big] - 2E\\big[(\\hat{\\theta}-E[\\hat{\\theta}])(\\theta-E[\\hat{\\theta}])\\big] \\\\\n&=Var(\\hat{\\theta}) + \\text{bias}^2\n\\end{aligned}\n\\]\n\nThe first line adds and subtracts the mean of the estimator, which need not be \\(\\theta\\), the targetted population parameter. The second expands the outside exponent over the two bracketted terms.\nThe cross-product term in line 2 is 0. Can you show this?\n\n\n3.4 Consistency\nConsistency is an asumptotic property of an estimator.\n\nDefinition 2 The estimator \\(\\hat{\\theta}_n\\) is a consistent estimator of \\(\\theta\\) if it converges in probability to \\(\\theta\\) as \\(n\\rightarrow\\infty\\),\n\\[\nPr(|\\hat{\\theta}_n-\\theta|\\geq\\varepsilon)\\rightarrow 0\\qquad n\\rightarrow 0\n\\] for \\(\\varepsilon\\) very small.1\n\nIntuitively, this means that tail area probabilities (i.e. probability of an estimator very far from the true value) goes to zero as the sample size gets large.\nWe use the notations, \\(p\\lim\\),\n\\[\np\\lim\\hat{\\theta}_n = \\theta\n\\]\nor convergence in probability,\n\\[\\hat{\\theta}_n \\rightarrow_p \\theta\\qquad\\text{as}\\qquad n\\rightarrow \\infty\\]\nAn important theorem regarding the consistency of estimators is Slutzky’s theorem concerning continuous functions of estimators. For example, we will use this theorem to prove the consistency of the OLS estimator.\n\nTheorem 1 (Slutzky’s Theorem) If \\(p\\lim\\hat{\\theta}_n = \\theta\\) and \\(h(\\cdot)\\) is a continuous function, then\n\\[\np\\lim\\;h(\\hat{\\theta}_n) = h\\big(p\\lim\\hat{\\theta}_n\\big)=h(\\theta)\n\\]\n\nHere are some useful examples,\n\nExample 1 Given two consistent estimators \\(\\big[\\hat{\\theta}_n,\\hat{\\beta}_n\\big]\\),\n\n\\(p\\lim\\;(a\\hat{\\theta}_n+ b\\hat{\\beta}_n) = a(p\\lim\\;\\hat{\\theta}_n)+b(p\\lim\\;\\hat{\\beta}_n) =a\\theta +b\\beta\\) for constants \\(a\\) and \\(b\\)\n\\(p\\lim\\;(\\hat{\\theta}_n\\times\\hat{\\beta}_n) = (p\\lim\\;\\hat{\\theta}_n)\\times (p\\lim\\;\\hat{\\beta}_n)=\\theta\\beta\\)\n\\(p\\lim\\;(\\hat{\\theta}_n^2) = (p\\lim\\;\\hat{\\theta}_n)^2 = \\theta^2\\)\n\\(p\\lim\\;\\bigg(\\frac{\\hat{\\theta}_n}{\\hat{\\beta}_n}\\bigg) = \\frac{\\theta}{\\beta}\\)\n\\(p\\lim\\;\\exp(\\hat{\\theta}_n) = \\exp(p\\lim\\;\\hat{\\theta}_n) = \\exp(\\theta)\\)\n\n\nMSE convergence (to zero) is a sufficient condition for consistency. However, it is not a necessary.\nFor an unbiased estimator,\n\\[\nVar(\\hat{\\theta}_n)\\rightarrow 0 \\quad \\text{as}\\quad n\\rightarrow 0 \\implies p\\lim\\hat{\\theta}_n = \\theta\n\\]\nFor a biased estimator,\n\\[\nMSE(\\hat{\\theta}_n)\\rightarrow 0 \\quad \\text{as}\\quad n\\rightarrow 0 \\implies p\\lim\\hat{\\theta}_n = \\theta\n\\]\n\nExercise 1 Given a sample of independently and identically distributed (iid) random variables, \\[\nX_1,...,X_n \\sim N(\\mu,\\sigma^2)\n\\] Show that the mean estimator - \\(\\overline{X} = \\frac{1}{n}\\sum_{i=1}{n}X_i\\) is a consistent estimator of \\(\\mu\\); i.e. \\(p \\lim\\;\\overline{X} = \\mu\\).\n\nThe above exercise relates to one of the most important examples of convergence in probability:\n\nTheorem 2 (Weak Law of Large Numbers) Let \\(X_1,...,X_n\\) be a sample of iid random variables, such that \\(E|X_1|&lt;\\infty\\). Then, \\[\nn^{-1}\\sum_{i=1}^{n}X_i \\rightarrow_p E[X_1]\\qquad \\text{as}\\qquad n\\rightarrow \\infty\n\\]\n\nHere we use the notation \\(E[X_1]\\), since the data is iid and \\(E[X_i]=E[X_1]\\) for \\(i=1,...,n\\). We prove a modified version of the WLLN theorem, assuming \\(E[X_1^2]&lt;\\infty\\). Since \\(E[X_1^2]&lt;\\infty\\implies\\) both \\(E|X_1|&lt;\\infty\\) and \\(Var(X_1)&lt;\\infty\\), we will have proven WLLN theorem.\n\nTheorem 3 Let \\(X_1,...,X_n\\) be a sample of iid random variables, such that \\(Var(X_1)&lt;\\infty\\). Then, \\[\nn^{-1}\\sum_{i=1}^{n}X_i \\rightarrow_p E[X_1]\\qquad \\text{as}\\qquad n\\rightarrow \\infty\n\\]\n\nTo complete the proof of WLLN, we will need to use Markov’s Inequality. We will not prove this lemma, but versions of the proof are readily available online.\n\nLemma 1 (Markov’s Inequality) Let \\(X\\) be a random variable. For \\(\\varepsilon&gt;0\\) and \\(r&gt;0\\), then\n\\[\nPr(|X|\\geq \\varepsilon)\\leq \\frac{E\\big[|X|^2\\big]}{\\varepsilon^2}\n\\]\n\nNow, we can complete the proof of WLLN, assuming a finite second moment.\n\nProof. \\[\n\\begin{aligned}\nPr\\bigg(\\bigg|n^{-1}\\sum_{i=1}^{n}X_i - E[X_1]\\bigg|\\geq \\varepsilon\\bigg) &= Pr\\bigg(\\bigg|n^{-1}\\sum_{i=1}^{n}(X_i - E[X_1])\\bigg|\\geq \\varepsilon\\bigg) \\\\\n&\\leq \\frac{E\\big[|\\sum_{i=1}^{n}(X_i - E[X_1])|^2\\big]}{n^2\\varepsilon^2} \\\\\n&=\\frac{\\sum_{i=1}^{n}\\sum_{j=1}^{n}E\\big[(X_i - E[X_1])(X_j - E[X_1])\\big]}{n^2\\varepsilon^2} \\\\\n&=\\frac{\\sum_{i=1}^{n}E\\big[(X_i - E[X_1])^2\\big]}{n^2\\varepsilon^2} \\\\\n&=\\frac{nVar(X_1)}{n^2\\varepsilon^2} \\\\\n&\\rightarrow 0 \\qquad \\text{as}\\qquad n\\rightarrow \\infty\n\\end{aligned}\n\\]\n\nNote, the WLLN holds under a weaker condition than iid. Between lines 3 and 4 we used the independence of observations to set correlations between units to 0. Thus, we required a weaker assumption of uncorrelateness: \\(Cov(X_i,X_j)=0\\;\\forall\\;i\\neq j\\).\nUnder the WLLN, you can show that the sample variance converges in probability to the population variance.\n\\[\np\\lim \\big(n^{-1}\\sum_{i=1}^{n}(X_i - \\overline{X})^2\\big) \\rightarrow_p Var(X_i)\n\\]\n\n\n3.5 Distribution\nIn this section we will focus on the asymptotic distribution of an estimator. If we know the joint distribution of the data, then we can potentially work out the distribution of the estimator in a finite sample. This is especially true when the random variables in question are drawn from known families of distributions.\nFor example, we know that the sum of Normal distributed random variables is Normally distributed itself. And the sum of the square of standard-Normally distributed random variables is Chi-squared distributed. We used these results do determine the distribution of test-statistics corresponding to the Classical Linear Regression Model.2\nHowever, what do you do when you do not know the underlying distribution of the random variables? Here we will rely on a the Central Limit Theorem, which tells us about the approximate distribution of an estimator when the sample is large.\nBefore discussing CLT, we must define a new convergence concept: convergence in distribution.\n\nDefinition 3 Let \\(X_1,...,X_n\\) be a sequence of random variables and let \\(F_n(x)\\) denote the marginal CDF of \\(X_n\\), \\[\nF_n(x) = Pr(X_n\\leq x)\n\\] Then, \\(X_n\\) converges in distribution if \\(F_n(x)\\rightarrow F(x)\\) as \\(n\\rightarrow \\infty\\), where F(x) is continuous.\n\nConvergence in distribution can be denoted,\n\\[\nX_n\\rightarrow_d X\n\\]\nwhere X is a random variable with distribution function \\(F(x)\\). Note, it is not the random variables that are converging, but rather the distributions of said random variables.\nAs with convergence in probability, there are some basic rules for manipulation.\n\nThe first is Cramer Convergence Theorem: Suppose \\(X_n\\rightarrow_dX\\) and \\(Y_n\\rightarrow_p c\\). then,\n\n\\(X_n+Y_n\\rightarrow_d X+c\\)\n\\(Y_nX_n\\rightarrow_d cX\\)\n\\(X_n/Y_n\\rightarrow_d X/c\\), for \\(c\\neq 0\\)\n\nIf \\(X_n\\rightarrow_p X\\), then \\(X_n\\rightarrow_d X\\). The converse is not true, with the exception:\nIf \\(X_n\\rightarrow_d C\\), a constant, then \\(X_n\\rightarrow_p C\\).\nIf \\(X_n-Y_n\\rightarrow_p 0\\), and \\(Y_n\\rightarrow_d Y\\), then \\(X_n\\rightarrow_d Y\\).\n\nUnlike with convergence in probability, \\(X_n\\rightarrow_d X\\) and \\(Y_n\\rightarrow_d Y\\) does NOT imply \\(X_n + Y_n \\rightarrow_d X+Y\\), unless joint convergences holds too. This is because the former are statements concerning the marginal distributions of \\(X_n\\) and \\(Y_n\\), while the distribution of \\(X_n + Y_n\\) depends on the joint distribution.\nIf \\((X_n,Y_n)\\rightarrow_d (X,Y)\\) (joint convergence), then \\(X_n+Y_n\\rightarrow_dX+Y\\). This result follows from the Central Mapping Theorem.\n\nTheorem 4 (Continuous Mapping Theorem) Suppose \\(X_n\\rightarrow_d X\\) and let \\(h(\\cdot)\\) be a continuous function. Then, \\(h(X_n)\\rightarrow_d h(X)\\).\n\nCMT holds for a vector of random variables as well as single random variable. Thus, if\n\\[\n\\begin{bmatrix}X_n \\\\ Y_n\\end{bmatrix}\\rightarrow_d \\begin{bmatrix}X \\\\ Y\\end{bmatrix}\n\\] then by CMT, \\[\nX_n+Y_n=\\begin{bmatrix}1 & 1\\end{bmatrix}\\begin{bmatrix}X_n \\\\ Y_n\\end{bmatrix} \\rightarrow_d\\begin{bmatrix}1 & 1\\end{bmatrix}\\begin{bmatrix}X \\\\ Y\\end{bmatrix} = X+Y\n\\]\nCan you show:\n\n\\((X_n,Y_n)\\rightarrow_d (X,Y)\\) (i.e. joint convergence) implies \\(X_n\\rightarrow_d X\\) (i.e. marginal convergence)?\n\\(\\sum_{j=1}^k X_{jn}^2 \\sim \\chi^2_k\\) for \\(X_n\\sim N(0,I_k)\\), \\(X_n\\in \\mathbf{R}^k\\).\n\nHaving discussed the CMT, we are now ready to discuss the Central Limit Theorem. Both are used extensively in Econometrics. We will not prove CLT as the proof requires a more detailed discussion of Moment Generating Functions.\nRecall, for an iid random sample, the sample converges in probability to the population mean:\n\\[\np \\lim\\;\\overline{X} = E[X_1]\n\\]\nThe rate of convergence for this estimator is \\(\\sqrt{n}\\). The estimator is said to be root-\\(n\\)-consistent.\n\nTheorem 5 (Central Limit Theorem) Let \\(X_1,...,X_n\\) be a sample of iid random variables such that \\(E[X_1]=0\\) and \\(0&lt;E[X_1^2]&lt;\\infty\\). Then,\n\\[\nn^{-1/2}\\sum_{i=1}^{n}X_i\\rightarrow_d N(0,E[X_1^2])\n\\]\n\nConsider a random sample drawn independently from a distribution with mean \\(\\mu\\) and variance \\(\\sigma\\). Note, this distribution need not be Normal. It holds that, $X_1-,…,X_n-$ are iid and \\(E[X_1-\\mu]\\). In addition, \\(E[(X_1-\\mu)^2]=\\sigma^2&lt;\\infty\\). Therefore, by CLT\n\\[\nn^{1/2}(\\overline{X}-\\mu) = n^{-1/2}\\sum_{i=1}^{n}(X_i-\\mu)\\rightarrow_d N(0,\\sigma^2)\n\\]\nIn practice, we use CLT to determine the approximate distribution of an estimator in large samples. Based on the above result, we can say\n\\[\nn^{1/2}(\\overline{X}-\\mu)\\overset{a}{\\sim} N(0,\\sigma^2)\n\\] or,\n\\[\n\\overline{X}\\overset{a}{\\sim}N(\\mu,\\sigma^2/n)\n\\]\nwhere \\(\\sigma^2/n\\) is referred to as the asymptotic variance of \\(\\overline{X}\\). Here, the symbol \\(\\overset{a}{\\sim}\\) can be interpreted as “approximately in large samples”.\nWith CMT and CLT, we need one more theorem before we continue. We know from the WLLN that \\(\\overline{X}\\rightarrow_p E[X_1]=\\mu\\). Moreover, by Slutzky’s theorem we know that \\(h(\\overline{X})\\rightarrow_p h(\\mu)\\), for \\(h(\\cdot)\\) continuous. However, we do not know the approximate distribution of \\(h(\\overline{X})\\) in a large sample.\nConsider, \\(h(\\mu)\\) is a non-random constant and CLT applies to \\(n^{1/2}(\\overline{X}-\\mu)\\) and not \\(\\overline{X}\\). The latter implying that we cannot use CMT.\n\nTheorem 6 (Delta Method) Let \\(\\hat{\\theta}_n\\) be a random k-dimensional vector. Suppose that \\(n^{1/2}(\\hat{\\theta}_n-\\theta)\\rightarrow_d Y\\), where \\(theta\\) is a non-random k-dimensional vector and \\(Y\\) a random k-dimensional vector.\nLet \\(h: \\mathbf{R}^k\\rightarrow\\mathbf{R}^m\\) be a function (mapping) that is continuously differentiable on some open neighbourhood of \\(\\theta\\). Then,\n\\[\nn^{1/2}\\big(h(\\hat{\\theta}_n)-h(\\theta)\\big)\\rightarrow_d \\frac{\\partial h(\\theta)}{\\partial\\theta'}Y\n\\]\n\nThe proof involves Cramer’s Convergence Theorem, Slutzky’s Theorem, as well as the Mean Value Theorem (which we have not discussed). This result is used to derive the limiting distribution of non-linear models and their marginal effects (e.g. probit/logit), as well as non-linear tests of regression coefficients."
  },
  {
    "objectID": "handout-2.html#properties-of-the-expectation-operator",
    "href": "handout-2.html#properties-of-the-expectation-operator",
    "title": "Estimators and their Properties",
    "section": "4 Properties of the E[xpectation] operator",
    "text": "4 Properties of the E[xpectation] operator\n\nThe expectation of a continuously-distributed, random variable \\(X\\) can be defined as:\n\\[\nE[X] = \\int_{-\\infty}^{\\infty}tf_Xdt = \\int_{-\\infty}^{\\infty}tdF_X(t)\n\\]\nIf \\(X\\) takes on discrete values, \\(X \\in \\mathbf{X} = \\{x_1,x_2,...,x_m\\}\\), we can replace the integral with a summation and the probability density function (pdf: \\(f_X\\)) with a probability mass function (pmf: \\(p_X\\)).\n\\[\nE[X] = \\sum_{t\\in \\mathbf{X}}tp_X(t)\n\\]\nAn important property of the Exptation operator is that it is linear. Let \\(\\{a,b\\}\\) be two constants (non-random scalars), then\n\\[\nE[aX+b] = aE[X]+b\n\\]\nCan you show this, using the above definition of the expectation operator?\nSimilarly, consider two random varibales \\(X\\) and \\(Y\\). Then,\n\\[\nE[aX + bY] = E[aX] + E[bY] = aE[X] + bE[Y]\n\\]\nHowever, note\n\n\\(E[XY] = E[X] \\times E[Y]\\) if \\(X\\) and \\(Y\\) are independent\n\nThis follows from the fact that \\(f_{X,Y} = f_X\\cdot f_Y\\) if \\(X\\) and \\(Y\\) are independent. Note, this is not an iff (if-and-only-if) statement.\n\n\\(E\\left[\\frac{X}{Y}\\right]\\neq\\frac{E[X]}{E[Y]}\\) for \\(E[Y]\\neq0\\)\n\\(E[\\ln(X)]\\neq \\ln(E[Y])\\)\n\nIn general, \\(E[h(X)]\\neq h(E[Y])\\) with the exception of \\(h(\\cdot)\\) linear function."
  },
  {
    "objectID": "handout-2.html#footnotes",
    "href": "handout-2.html#footnotes",
    "title": "Estimators and their Properties",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n\\(|x-a|&gt;b\\; \\implies \\;-b&gt;x-a\\quad\\text{and}\\quad b&lt;x-a\\)↩︎\nIn this setting, we make an assumption around the distribution of the error term in the model.↩︎"
  },
  {
    "objectID": "handout-1.html",
    "href": "handout-1.html",
    "title": "Classical Linear Regression Model & Ordinary Least Squares",
    "section": "",
    "text": "In this handout we will revisit the Classical Linear Regression Model (CLRM) (see Wooldridge 2010, chaps. 1–2). The goal of this week’s lecture is to:\n\nunderstand the model specification;\nit’s underlying assumptions;\nand the appropriate interpretation;\nthe OLS estimator, using linear algebra;\nthe geometry of OLS and partitioned regression result."
  },
  {
    "objectID": "handout-1.html#overview",
    "href": "handout-1.html#overview",
    "title": "Classical Linear Regression Model & Ordinary Least Squares",
    "section": "",
    "text": "In this handout we will revisit the Classical Linear Regression Model (CLRM) (see Wooldridge 2010, chaps. 1–2). The goal of this week’s lecture is to:\n\nunderstand the model specification;\nit’s underlying assumptions;\nand the appropriate interpretation;\nthe OLS estimator, using linear algebra;\nthe geometry of OLS and partitioned regression result."
  },
  {
    "objectID": "handout-1.html#model-specification",
    "href": "handout-1.html#model-specification",
    "title": "Classical Linear Regression Model & Ordinary Least Squares",
    "section": "2 Model Specification",
    "text": "2 Model Specification\nThe linear population regression model is given by,\n\\[\n\\begin{aligned}\n  Y_i =& X_i'\\beta+\\varepsilon_i \\\\\n  =& \\beta_1\\mathbf{1}+\\beta_2X_{i2}+\\beta_3X_{i3}+...+\\beta_kX_{ik}+\\varepsilon_i\n\\end{aligned}\n\\]\nfor \\(i = 1,2,...,n\\). Where,\n\n\\(i\\): unit of observation; e.g. individual, firm, union, political party, etc.\n\\(Y_i \\in \\mathbb{R}\\): scalar random variable.\n\\(X_i \\in \\mathbb{R}^k\\): \\(k\\)-dimensional (column1) vector of regressors, with \\(k&lt;n\\).2\n\\(\\beta\\): \\(k\\)-dimensional, non-random vector of unknown population parameters.\n\\(\\varepsilon_i\\): unobserved, random error term.3\n\nThe linear population regression equation is linear in parameters. This is an important assumption that does NOT restrict the model from being non-linear in regressors. For example, the equation\n\\[Y_i = \\beta_1 + \\beta_2X_{i2} + \\beta_3X_{i2}^{\\color{red}{2}} + \\varepsilon_i\\] non-linear in \\(X_{i2}\\), but still linear in parameters. In contrast, the equation\n\\[Y_i = \\beta_1 + \\beta_2X_{i2} + ({\\color{red}{\\beta_2\\beta_3}})X_{i3} + \\varepsilon_i\\] is non-linear in parameters.\n\n2.1 Intercept\nThe constant (intercept) in the equation serves an important purpose. While there is no a priori reason for the model to have a constant term, it does ensure that the error term is mean zero.\n\nProof. Suppose \\(E[\\varepsilon_i] = \\gamma\\).\nWe can then define a new error term, \\(\\upsilon_i = \\varepsilon_i - \\gamma\\), such \\(E[\\upsilon_i] = \\gamma\\). The population regression model can be rewritten as, \\[ \\begin{aligned} Y_i =& X_ i'\\beta + v_i + \\gamma  \\\\\n=& \\underbrace{(\\beta_1+\\gamma)}_{\\tilde{\\beta}_1}\\mathbf{1} + \\beta_2X_{i2} + \\beta_3X_{i3} + ... + \\beta_kX_{ik} + v_i\n\\end{aligned}\n\\] The model has a new intercept \\(\\tilde{\\beta}_1=\\beta_1 + \\gamma\\), but the other parameters remain unchanged.\n\n\n\n2.2 Matrix notation\nFor a sample of \\(n\\) observations, we can stack the unit-level linear regression equation into a vector,\n\\[\nY =\\underbrace{\\begin{bmatrix}Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix}}_{n\\times 1} = \\underbrace{\\begin{bmatrix}X_1'\\beta \\\\ X_2'\\beta \\\\ \\vdots \\\\ X_n'\\beta\\end{bmatrix}}_{n\\times 1}  + \\underbrace{\\begin{bmatrix}\\varepsilon_1 \\\\ \\varepsilon_2 \\\\ \\vdots \\\\ \\varepsilon_n\\end{bmatrix}}_{n\\times 1}  \n= \\underbrace{\\begin{bmatrix}X_{11} & X_{12} & \\dots & X_{1k}\\\\ X_{21} & X_{22} && \\\\ \\vdots & & \\ddots & \\\\ X_{n1} & & & X_{nk} \\end{bmatrix}}_{n\\times k} \\underbrace{\\begin{bmatrix}\\beta_1 \\\\ \\beta_2 \\\\ \\vdots \\\\ \\beta_k\\end{bmatrix}}_{k\\times 1} + \\begin{bmatrix}\\varepsilon_1 \\\\ \\varepsilon_2 \\\\ \\vdots \\\\ \\varepsilon_n\\end{bmatrix} = X\\beta + \\varepsilon\n\\] Notice, in matrix notation, you lose the transpose from \\(X_i'\\beta\\). Apart from the absence of the \\(i\\) subscript, this is a useful way of knowing the dimension of the equation (in my notes). You MUST always write \\(X\\beta\\) and not \\(\\beta X\\). For the scalar case, \\(X_i'\\beta = \\beta'X_i\\), but for the vector case \\(\\beta X\\) is not defined since \\(\\beta\\) is \\(k\\times 1\\) and \\(X\\) is \\(n\\times k\\)."
  },
  {
    "objectID": "handout-1.html#clrm-assumptions",
    "href": "handout-1.html#clrm-assumptions",
    "title": "Classical Linear Regression Model & Ordinary Least Squares",
    "section": "3 CLRM Assumptions",
    "text": "3 CLRM Assumptions\nAssumption CLRM 1. Population regression equation is linear in parameters: \\[Y = X\\beta+\\varepsilon\\]\nAssumption CLRM 2. Conditional mean independence of the error term: \\[E[\\varepsilon|X]=0\\]\nAssumption CLRM 2. is stronger than \\(E[\\varepsilon_i|X_i]\\) (mean independence for unit \\(i\\)). If all units were independent, then \\(E[\\varepsilon_i|X_i]\\) would imply \\(E[\\varepsilon|X]=0\\). However, since we have not (yet) assumed this, we need this stronger exogeneity assumption. Consider, if \\(i\\) represented units of time (\\(t\\)), as in time-series models, independence across \\(i\\) will not hold.\nTogether, CLRM 1. and CLRM 2. imply that\n\\[ E[Y|X] = X\\beta  \\] This means that the Conditional Expectation Function is known and linear in parameters.\nConditional mean independence implies - by the Law of Iterated Expectations - mean independence of the error term,\n\\[E[\\varepsilon|X]=0 \\Rightarrow E\\big[E[\\varepsilon|X]\\big]=E[\\varepsilon]=0\\]\nand uncorrelatedness,\n\\[E[\\varepsilon|X]=0 \\Rightarrow E[\\varepsilon X]=0\\] Note, neither of the above statements hold the other way around. Mean independence does not imply conditional mean independence and uncorrelatedness (zero correlation/covariance) does not imply conditional mean independence.\nUncorrelatedness rules out linear relationships between the regressors and error term while conditional mean independence rules out non-linear relationships too.\nIn general, distributional independence implies mean independence which then implies uncorrelatedness.\n\nIn the case joint-normally distributed random variables, uncorrelatedness implies independence. That is, if \\[ \\begin{bmatrix}Y_1 \\\\ Y_2\\end{bmatrix}\\sim N\\bigg(\\begin{bmatrix}\\mu_1 \\\\ \\mu_2\\end{bmatrix},\\begin{bmatrix}\\sigma_1^2 & \\sigma_{12}\\\\ \\sigma_{21} & \\sigma_2^2\\end{bmatrix}\\bigg)\\] Then \\(\\sigma_{12}=\\sigma_{21}=0 \\iff f_1*f_2 = f_{12}\\).\n\nWe will later show that uncorrelatedness is sufficient for consistency of the Ordinary Least Squares estimater, while conditional mean independence is required for unbiasedness of OLS.\nAssumption CLRM 3. Homoskedasticity: \\(Var(\\varepsilon|X) = E[\\varepsilon\\varepsilon'|X] =  \\sigma^2I_n\\)\nCLRM 3. states that the variance of the error term is independent of \\(X\\) and constant across units. The diagonal nature of the covariance matrix also implies that the error terms are uncorrelated across units in the data. Note, this does not imply independence of the error terms across units.\nModels with heteroskedasticity relax the assumption of constant variance, allowing for a richer variance-covariance matrix that typically depends on \\(X\\).\nThis assumption is unlikely to hold in time-series models where units represent repeated observations across time. Such violations are referred to as serial correlation or autocorrelation.\nEven in cross-sectional data settings, you can have non-zero correlations across units in the data. A common instance of this is the case of clustering. Clustering can occur when units experience common/correlated ‘shocks’; for example, the data contains groups of students from the same classroom who have a the same teacher. This can also be the result of clustered sampling, a common practice in multi-stage survey design.\nAssumption CLRM 4. Full rank: \\(rank(X)=k\\quad a.s.\\) a.s.4\nSince \\(X\\) is a random variable we should add to the assumption: \\(rank(X) = k\\) almost surely (abbreviated a.s.). This means that the set of events in which \\(X\\) is not full rank occur with probability 0. The reason for this addition is that such a set of events (in the sample space) may not be empty.\nCLRM 4. is some time referred to as the absence of perfect (or exact) collinearity. Do not confuse this with multicollinearity. Multicollinearity occurs when regressors are highly (linearly) correlated with one another, yielding imprecise estimates.\nAssumption CLRM 5. Normality of the error term: \\(\\varepsilon|X \\sim N(0,\\sigma^2 I_n)\\)\nAssumption CLRM 6. Observations \\(\\{(Y_i,X_i): i=1,...,n\\}\\) are independently and identically distributed (iid).\nCLRM 5 & 6 are not part of the Classical assumptions, but do simplify the problem of inference. Note, CLRM 5 implies independence across error terms, not implied by CLRM 3.\n\n3.1 Non-random \\(X\\)\nThere is an alternative version of the CLRM in which \\(X\\) is a non-random, matrix of regressors/predictors. With \\(X\\) fixed, the error term is the only only random variable in the model. CLRM assumptions 1 and 4 remain the same, while CLRM 2, 3, 5, and 6 become:\nAssumption CLRM 2a. Mean independence of the error term: \\[E[\\varepsilon]=0\\]\nAssumption CLRM 3a. Homoskedasticity: \\(Var(\\varepsilon) = \\sigma^2I_n\\)\nAssumption CLRM 5a. Normality of the error term: \\(\\varepsilon \\sim N(0,\\sigma^2 I_n)\\)\nAssumption CLRM 6a. Observations \\(\\{\\varepsilon_i: i=1,...,n\\}\\) are independently and identically distributed (iid).\n\n\n3.2 Identification\nCLRM 1,2 and 4. are the identifying assumptions of the model. These assumptions allow us to write the parameter of interest as a set of ‘observable’ moments in the data. We can demonstrate this as follows.\n\nProof. Start with CLRM 2.\n\\[\n    E[\\varepsilon_i|X_i]=0\n\\]\nPre-multiply by the vector \\(X_i\\), \\[\n        X_iE[\\varepsilon_i|X_i]=0\n\\] Since the expectation is conditional on \\(X_i\\), we can bring \\(X_i\\) inside the expectation function,\n\\[\n        E[X_i\\varepsilon_i|X_i]=0\n    \\] This conditional expectation is a random-function of \\(X_i\\). If we take the expectation of this function w.r.t. \\(X\\), we achieve the aforementioned result that conditional mean independence implies zero covariance, \\[\n        E\\left[E[X_i\\varepsilon_i|X_i]\\right]=E[X_i\\varepsilon_i]=0\n    \\]\nNow substitute in for \\(\\varepsilon_i\\) using the linear regression model from CLRM 1 and separate the resulting two terms,\n\\[\n\\begin{aligned}\n    &E[X_i(Y_i-X_i'\\beta)]=0 \\\\\n    \\Rightarrow &E[X_iX_i']\\beta=E[X_iY_i]\n\\end{aligned}\n\\]\nSince \\(\\beta\\) is a non-random vector, we can remove it from the expectation function.\nNow we have a system of linear equations (of the form \\(Av = b\\)) with a unique solution if and only if the matrix \\(E[X_iX_i']\\) is invertible. For the inverse of \\(E[X_iX_i']\\) to exist, we require CLRM 4, since \\(rank(X)=k\\quad a.s.\\Rightarrow rank(E[X_iX_i'])=k\\).5\n\\[\n    \\beta = E[X_iX_i']^{-1}E[X_iY_i]\n\\]\n\nWe cannot compute \\(\\beta\\) because we do not know the joint distribution of \\((Y_i,X_i)\\) needed to solve for the variance-covariance matrices. However, \\(\\beta\\) is (point) identified because both \\(Y\\) and \\(X\\) are observed in the data and the parameters are “pinned down” by a unique set of ‘observable’ moments in the data.\n\\(\\beta\\) is not identified if the above system of linear equations does not have a unique solution. This will occur if two or more of the regressors are perfectly colinear.6 \\(\\beta\\) is also not be identified if the resulting expression for \\(\\beta\\) includes ‘objects’ (moments, distribution/scale parameters) that are not ‘observed’ in the data. For example, if the error is not mean independent, the above expression will include a bias term that depends on \\(E[X_i'\\varepsilon_i]\\).\nIn this instance, the identification of \\(\\beta\\) is scale dependent. That is, if we multiply \\(Y_i\\) by a scalar, \\(\\beta\\) is multiplied by the same scalar. For example, in cases where a researcher is modelling standardized test-scores."
  },
  {
    "objectID": "handout-1.html#interpretation",
    "href": "handout-1.html#interpretation",
    "title": "Classical Linear Regression Model & Ordinary Least Squares",
    "section": "4 Interpretation",
    "text": "4 Interpretation\nIn this linear regression model each slope coefficient has a partial derivative interpretation,\n\\[\n\\beta_j = \\frac{\\partial E[Y_i|X_i]}{\\partial X_{ij}}\n\\] or, as a vector, \\[\n\\beta = \\frac{\\partial E[Y_i|X_i]}{\\partial X_{i}} = \\begin{bmatrix}\\frac{\\partial E[Y_i|X_i]}{\\partial X_{i1}}\\\\ \\vdots \\\\ \\frac{\\partial E[Y_i|X_i]}{\\partial X_{ik}}\\end{bmatrix} = \\begin{bmatrix}\\beta_1\\\\ \\vdots \\\\ \\beta_k\\end{bmatrix}\n\\]\nNote, the derivative is expressed in terms of changes in the expected value of \\(Y_i\\) (conditional on \\(X_i\\)), not \\(Y_i\\) itself. This is because \\(Y_i\\) is a random variable, but under CLRM 1 & 2\n\\[\nE[Y_i|X_i] = X_i'\\beta\n\\]\nFor a given value of \\(X_i\\), the above expression is non-random.\nAs \\(\\beta_j\\) is a partial derivative, its interpretation is one that “holds fixed” the value of other regressors (i.e. ceteris paribus). Because of this, many researchers apply the experimental language of control variables when interpretting regression coefficients. However, this is dependent on the assumed linearity of the CEF."
  },
  {
    "objectID": "handout-1.html#ordinary-least-squares",
    "href": "handout-1.html#ordinary-least-squares",
    "title": "Classical Linear Regression Model & Ordinary Least Squares",
    "section": "5 Ordinary Least Squares",
    "text": "5 Ordinary Least Squares\nOLS is an estimator for \\(\\beta\\). As will become evident in Lecture 3, it is not the only estimator for \\(\\beta\\).\nThe OLS estimator is the solution to,\n\\[\n\\min_b\\;\\sum_{i=1}^n(Y_i-X_i'b)^2\n\\]\nUsing vector notation, we can rewrite this as\n\\[\n\\begin{aligned}\n&\\min_b\\;(Y-Xb)'(Y-Xb)\\\\\n=&\\min_b\\;Y'Y-Y'Xb-b'X'Y+b'X'Xb \\\\\n=&\\min_b\\;Y'Y-2b'X'Y+b'X'Xb\n\\end{aligned}\n\\] From line 2 to 3 we use the fact that \\(Y'Xb\\) is a scalar and therefore symmetric: \\(Y'Xb=b'X'Y\\).7\nDifferentiating the above expression w.r.t. the vector \\(b\\) and setting the first-order conditions to \\(0\\), we find that the following condition must hold for \\(\\hat{\\beta}\\), the solution.\n\\[\n  \\begin{aligned}\n  &0=-2X'Y+2X'X\\hat{\\beta}\n  \\\\ \\Rightarrow& X'X\\hat{\\beta} = X'Y\n  \\end{aligned}\n\\]\n\nHow did we get this result? Deriving the first order conditions requires knowledge of how to solve for the derivative of a scalar respect to a column vector (in this case \\(b\\in R^k\\)). The extra material on Linear Algebra has some notes on vector differentiation.\nWe can ignore the first term \\(Y'Y\\) as it does not depend on \\(b\\). The second term is \\(-2b'X'Y\\). Here we can use the rule that, \\[\n  \\frac{\\partial z'a}{\\partial z} = \\frac{\\partial a'z}{\\partial z} = a\n\\] In this instance, \\(a = X'Y \\in R^k\\). Thus, \\[\n  \\frac{\\partial -2b'X'Y}{\\partial b} = -2\\frac{\\partial b'X'Y}{\\partial b} = -2X'Y\n\\] The third term is \\(b'X'Xb\\). This is what is commonly referred to as a quadratic form: \\(z'Az\\). We know that the derivative of this form is, \\[\n  \\frac{\\partial z'Az}{\\partial z} = Az + A'z\n\\] and if \\(A\\) is symmetric, the result simplies to \\(2Az\\). In this instance, \\(A = X'X\\) is symmetric and the derivative is given by, \\[\n  \\frac{\\partial b'X'Xb}{\\partial b} = 2X'X\n\\]\n\nIn order to solve for \\(\\hat{\\beta}\\) we need to move the \\(X'X\\) term to the right-hand side. If these were scalars we would simply divide both sides by the same constant. However, as \\(X'X\\) is a matrix, division is not possible. Instead, we need to pre-multiply both sides by the inverse of \\(X'X\\): \\((X'X)^{-1}\\). Here’s the issue: the inverse of a matrix need not exist.\nGiven a square \\(k\\times k\\) matrix \\(A\\), its inverse exists if and only if \\(A\\) is non-singular. For \\(A\\) to be non-singular its rank must have full rank: \\(r(A)=k\\), the number of rows/columns. This means that all \\(k\\) columns/rows must be linearly independent. (See Material on Linear Algebra for a more detailed discussion of all these terms.)\nIn our application, \\(A=X'X\\) and\n\\[ r(X'X) = r(X) = colrank(X)\\leq k \\]\nTo insure that the inverse of \\(X'X\\) exists, \\(X\\) must have full column rank: all column vectors must be linearly independent. In practice, this means that no regressor can be a perfect linear combination of others. However, we have this from\nCLRM 4: \\(rank(X)=k\\)\nYou may know this assumption by another name: the absence of perfect colinearity between regressors.\n\nThe rank condition is the reason we exclude a base category when working with categorical variables.\nRecall, most linear regression models are specified with a constant. Thus, the first column of \\(X\\) is\n\\[ X_1 = \\begin{bmatrix}1 \\\\ 1 \\\\ \\vdots \\\\ 1\\end{bmatrix} \\] a \\(n\\times 1\\) vector vector of \\(1\\)’s, denoted here as \\(\\ell\\). Suppose you have a categorical - for example, gender in an individual level dataset - that splits the same in two. The categories are assumed to be exhaustive and mutually exclusive. If you create two dummy variables, one for each category,\n\\[ X_2 = \\begin{bmatrix}1 \\\\ \\vdots \\\\1\\\\0\\\\ \\vdots \\\\ 0\\end{bmatrix}\\qquad\\text{and}\\qquad X_3 = \\begin{bmatrix}0 \\\\ \\vdots \\\\0\\\\1\\\\ \\vdots \\\\ 1\\end{bmatrix} \\]\nit is evident that \\(X_2+X_3 = \\ell\\). (Here I have depicted the sample as sorted along these two categories.) If \\(X=[X_1\\;X_2\\;X_3]\\), then it is rank-deficient: \\(r(X) = 2&lt;3\\), since \\(X_3=X_1-X_2\\). Thus, we can only include two of these three regressors. We can even exclude the constant and have \\(X=[X_2\\;X_3]\\).\n\nIf \\(X\\) is full rank, then \\((X'X)^{-1}\\) exists and,\n\\[\n\\hat{\\beta} = (X'X)^{-1}X'Y\n\\]\nThis relatively simple expression is the solution to least squares minimization problem. Just think, it would take less than three lines of code to programme this. That is the power of knowing a little linear algebra.\nWe can write the same expression in terms of summations over unit-level observations,\n\\[\n\\hat{\\beta} = \\bigg(\\sum_{i=1}^nX_iX_i'\\bigg)^{-1}\\sum_{i=1}^nX_iY_i\n\\]\nNote, the change in position of the transpose: \\(X_i\\) is a column vector \\(\\Rightarrow\\) \\(X_i'X_i\\) is a scalar while \\(X_iX_i'\\) is a \\(k\\times k\\) matrix. To match the first expression, the term inside the parenthesis must be a \\(k\\times k\\) matrix. Similarly, \\(X'Y\\) is a \\(k\\times 1\\) vector, as is \\(X_iY_i\\).\n\n5.1 Univariate case\nUndergraduate textbooks all teach a very similar expression for the OLS estimator of a uni-variate regression model (with a constant); typically, something like,8\n\\[\nY_i = \\beta_1+\\beta_2X_{i2}+\\varepsilon_i\n\\] We know that the OLS estimators are give by,\n\\[\n\\begin{aligned}\n\\tilde{\\beta}_2 =& \\frac{\\sum(Y_i-\\bar{Y})(X_{i2}-\\bar{X}_2)}{\\sum_{i=1}^n(X_{i2}-\\bar{X}_2)^2} \\\\\n\\text{and}\\qquad \\tilde{\\beta}_1 =& \\bar{Y}-\\tilde{\\beta_2}\\bar{X}_2\n\\end{aligned}\n\\]\nI am deliberately using the notation \\(\\tilde{\\beta}\\) to distinguish these two estimators from the expression below. Let us see if we can replicate this result, using vector notation. The the model is,\n\\[\n\\begin{aligned}\nY =& X\\beta+\\varepsilon \\\\\n=& \\begin{bmatrix}1&X_{12} \\\\ 1 & X_{22} \\\\ \\vdots & \\vdots \\\\ 1 & X_{n2}\\end{bmatrix}\\begin{bmatrix}\\beta_1 \\\\ \\beta_2 \\end{bmatrix} + \\varepsilon \\\\\n=& \\begin{bmatrix}\\ell &X_{2} \\end{bmatrix}\\begin{bmatrix}\\beta_1 \\\\ \\beta_2 \\end{bmatrix} + \\varepsilon\n\\end{aligned}\n\\]\nTherefore,\n\\[\n\\begin{aligned}\n\\hat{\\beta} = \\begin{bmatrix}\\hat{\\beta}_1 \\\\ \\hat{\\beta}_2\\end{bmatrix}=&(X'X)^{-1}X'Y \\\\\n=&\\bigg(\\begin{bmatrix}\\ell' \\\\ X_{2}' \\end{bmatrix}\\begin{bmatrix}\\ell &X_{2}\\end{bmatrix}\\bigg)^{-1}\\begin{bmatrix}\\ell'  \\\\ X_{2}' \\end{bmatrix}Y \\\\\n=&\\begin{bmatrix}\\ell'\\ell & \\ell'X_2 \\\\ X_{2}'\\ell & X_2'X_2 \\end{bmatrix}^{-1}\\begin{bmatrix}\\ell'Y  \\\\ X_{2}'Y \\end{bmatrix}\n\\end{aligned}\n\\]\nI went through this rather quickly, using a number of linear algebra rules that you may not be familiar with. Do not worry, the point of the exercise is not become a linear algebra master, but instead to focus on the element of each of each matrix/vector. Each element is a scalar (size \\(1\\times 1\\)).\nIf we right them each down as sums you they might be a little more familiar. First consider the \\(2\\times 2\\) matrix:\n\nelement [1,1]: \\(\\ell'\\ell = \\sum_{i=1}^n 1 = n\\)\nelement [1,2]: \\(\\ell'X_2 = \\sum_{i=1}^nX_{i2} = n\\bar{X}_2\\)\nelement [2,1]: \\(X_2'\\ell = \\sum_{i=1}^nX_{i2} = n\\bar{X}_2\\) (as above, since scalars are symmetric)\nelement [2,2]: \\(X_2'X_2=\\sum_{i=1}^nX_{i2}^2\\)\n\nNext, consider the final \\(2\\times 1\\) vector,\n\nelement [1,1]: \\(\\ell'Y = \\sum_{i=1}^n Y_i = n\\bar{Y}\\)\nelement [2,1]: \\(X_2'Y = \\sum_{i=1}^nY_iX_{i2}\\)\n\nOur OLS estimator is therefore,\n\\[\n\\hat{\\beta} = \\begin{bmatrix} n & n\\bar{X}_2 \\\\ n\\bar{X}_2 & \\sum_{i=1}^nX_{i2}^2 \\end{bmatrix}^{-1}\\begin{bmatrix}n\\bar{Y}  \\\\ \\sum_{i=1}^nY_iX_{i2} \\end{bmatrix}\n\\]\nWe now need to solve for the inverse of the \\(2\\times 2\\) matrix. You can easily find notes on how to do this online. Here, I will just provide the solution.\n\\[\n\\hat{\\beta} = \\frac{1}{n\\sum_{i=1}^nX_{i2}^2-n^2\\bar{X}_2^2}\\begin{bmatrix} \\sum_{i=1}^nX_{i2}^2 & -n\\bar{X}_2 \\\\ -n\\bar{X}_2 &  n\\end{bmatrix}\\begin{bmatrix}n\\bar{Y}  \\\\ \\sum_{i=1}^nY_iX_{i2} \\end{bmatrix}\n\\]\nRemember, this is still a \\(2\\times 1\\) vector. We can now solve for the final solution:\n\\[\n\\begin{aligned}\n\\hat{\\beta} =& \\frac{1}{n\\sum_{i=1}^nX_{i2}^2-n^2\\bar{X}_2^2}\\begin{bmatrix} n\\bar{Y}\\sum_{i=1}^nX_{i2}^2 -n\\bar{X}_2\\sum_{i=1}^nY_iX_{i2} \\\\ n\\sum_{i=1}^nY_iX_{i2}-n^2\\bar{X}_2\\bar{Y}\\end{bmatrix} \\\\\n=& \\frac{1}{n\\sum_{i=1}^n(X_{i2}-\\bar{X}_2)^2}\\begin{bmatrix} n\\bar{Y}\\sum_{i=1}^nX_{i2}^2 + n^2\\bar{Y}\\bar{X}^2 - n^2\\bar{Y}\\bar{X}^2 -n\\bar{X}_2\\sum_{i=1}^nY_iX_{i2} \\\\ n\\sum_{i=1}^n(Y_i-\\bar{Y})(X_{i2}-\\bar{X}_2) \\end{bmatrix} \\\\\n=& \\frac{1}{n\\sum_{i=1}^n(X_{i2}-\\bar{X}_2)^2}\\begin{bmatrix} n\\bar{Y}\\sum_{i=1}^n(X_{i2}-\\bar{X})^2 -n\\bar{X}_2\\sum_{i=1}^n(Y_i-\\bar{Y})(X_{i2}-\\bar{X}_2) \\\\ n\\sum_{i=1}^n(Y_i-\\bar{Y})(X_{i2}-\\bar{X}_2) \\end{bmatrix} \\\\\n=& \\begin{bmatrix} \\bar{Y}  -\\frac{n\\sum_{i=1}^n(Y_i-\\bar{Y})(X_{i2}-\\bar{X}_2)}{n\\sum_{i=1}^n(X_{i2}-\\bar{X}_2)^2}\\bar{X}_2 \\\\ \\frac{n\\sum_{i=1}^n(Y_i-\\bar{Y})(X_{i2}-\\bar{X}_2)}{n\\sum_{i=1}^n(X_{i2}-\\bar{X}_2)^2} \\end{bmatrix} \\\\\n=& \\begin{bmatrix} \\bar{Y}  -\\tilde{\\beta}_2\\bar{X}_2 \\\\ \\tilde{\\beta}_2 \\end{bmatrix} \\\\\n=& \\begin{bmatrix}\\tilde{\\beta}_1 \\\\ \\tilde{\\beta}_2 \\end{bmatrix}\n\\end{aligned}\n\\]\nThe math is a little involved, but it shows you these solutions are are the same. Unfortunately, the working gets even more arduous in a multivariate context. However, there are useful tools to help us with that we will discuss next.\n\n\n5.2 Geometry of OLS\nIn the last section we saw how the OLS estimator can, more generally, be described as a linear transformation of the \\(Y\\) vector.\n\\[\n\\hat{\\beta} = (X'X)^{-1}X'Y\n\\]\nWe also saw that in order for there to be a (unique) solution to the least squared problem, the \\(X\\) matrix must be full rank. This rules out any perfect colinearity between columns (i.e. regressors) in the \\(X\\) matrix, including the constant.\nGiven the vector of OLS coefficients, we can also estimate the residual,\n\\[\n\\begin{aligned}\n\\hat{\\varepsilon} =& Y - X\\hat{\\beta} \\\\\n=&Y-X(X'X)^{-1}X'Y \\\\\n=&(I_n-X(X'X)X^{-1})Y\n\\end{aligned}\n\\]\nby plugging the definition of \\(\\hat{\\beta}\\). Thus, the OLS estimator separates the vector \\(Y\\) into two components:\n\\[\n\\begin{aligned}\nY =& X\\hat{\\beta} + \\hat{\\varepsilon} \\\\\n=&\\underbrace{X(X'X)^{-1}X'}_{P_X}Y + (\\underbrace{I_n-X(X'X)X^{-1}}_{I_n-P_X = M_X})Y \\\\\n=&P_XY + M_XY\n\\end{aligned}\n\\]\nThe matrix \\(P_X = X(X'X)^{-1}X'\\) is a \\(n\\times n\\) projection matrix. It is a linear transformation that projects any vector into the span of \\(X\\): \\(S(X)\\subset\\mathbb{R}^n\\). (See for more information on these terms.) \\(S(X)\\) is the vector space spanned by the columns of \\(X\\). The dimensions of this vector space depends on the rank of \\(P_X\\),\n\\[\ndim(S(X)) = r(P_X) = r(X) = k\n\\]\nThe matrix \\(M_X = I_n-X(X'X)^{-1}X'\\) is also a \\(n\\times n\\) projection matrix. It projects any vector into \\(X\\)’s orthogonal span: \\(S^{\\perp}(X)\\). Any vector \\(z\\in S^{\\perp}(X)\\) is orthogonal to \\(X\\). This includes the estimated residual, which is by definition orthogonal to the predicted values and, indeed, any column of \\(X\\) (i.e. any regressor). The dimension of this orthogonal vector space depends on the rank of \\(M_X\\),\n\\[\ndim(S^{\\perp}(X)) = r(M_X) = r(I_n)-r(X) = n-k\n\\]\nThe orthogonality of these two projections can be easily shown, since projection matrices are idempotent (\\(P_XP_X = P_X\\)) and symmetric (\\(P_X' = P_X\\)). Consider the inner product of these two projections,\n\\[\nP_X'M_X = P_X(I_n-P_X) = P_X-P_XP_X = P_X-P_X = 0\n\\]\nThe least squares estimator is a projection of Y into two vector spaces: one the span of the columns of \\(X\\) and the other a space orthogonal to \\(X\\).\nWhy is this useful? Well, it helps us understand the “mechanics” (technically geometry) of OLS. When working with linear regression models, we typically assume either strict exogeneity - \\(E[\\varepsilon|X]=0\\) - or uncorrelatedness - \\(E[X'\\varepsilon]=0\\) - where the former implies the latter (but not the other way around).\nWhen we use OLS, we estimate the vector \\(\\hat{\\beta}\\) such that,\n\\[\nX'(Y-X\\hat{\\beta})=X'\\hat{\\varepsilon}=0 \\quad always\n\\]\nThis is true, not just in expectation, but by definition. The relationship is “mechanical”: the regressors and estimated residual are perfectly uncorrelated. This can be easily shown:\n\\[\n\\begin{aligned}\nX'\\hat{\\varepsilon} =& X'M_XY \\\\\n=& X'(I_n-P_X)Y \\\\\n=&X'I_nY-X'X(X'X)^{-1}X'Y \\\\\n=&X'Y-X'Y \\\\\n=&0\n\\end{aligned}\n\\]\nYou are essentially imposing the assumption of uncorrelatedness between the explained and unexplained components of Y on the data. This means that if the assumption is wrong, so is the projection.\n\n\n5.3 Partitioned regression\nThe tools of linear algebra can help us better understand partitioned regression. Indeed, I would go as far to to say that it is quite difficult to understand partitioned regression without an understanding of projection matrices. Moreover, we need to understand partitioned regression to really understand multivariate regression. The partitioned regression result is referred to as Frisch-Waugh-Lovell Theorem (FWL).\n\nTheorem 1 FWL says that if you have two sets of regressors, \\([X_1,X_2]\\), then \\(\\hat{\\beta}_1\\), the OLS estimator for \\(\\beta_1\\), from the regression,\n\\[\n        Y = X_1\\beta_1 + X_2\\beta_2 + \\varepsilon\n\\]\nis also given by the regression,\n\\[\n        M_2Y = M_2X_1\\beta_1 + \\xi\n\\]\n\nWe can demonstrate the FWL theorem result using projection matrices. Two simplify matters, we will divide the set of regressors into two groups: \\(X_1\\) a single regressor and \\(X_2\\) a \\(n\\times (k-1)\\) matrix. We can rewrite the linear model as,\n\\[\nY = X\\beta+ \\varepsilon =\\beta_1X_1+X_2\\beta_2+\\varepsilon\n\\]\nLet us begin by applying our existing knowledge. From above, we know that the residual from the regression of \\(X_1\\) on \\(X_2\\) is,\n\\[\n\\hat{\\upsilon} = M_2X_1\n\\]\nwhere \\(M_2 = I_n-X_2(X_2'X_2)^{-1}X_2'\\). It turns out, it does not matter if we residualize \\(Y\\) too. Can you see why? Thus, the model we estimate in the second step, is\n\\[\nY = \\gamma_1\\underbrace{M_2X_1}_{\\hat{\\upsilon}}+\\xi\n\\]\nWe know that \\(\\hat{\\gamma}_1 = (\\hat{\\upsilon}'\\hat{\\upsilon})^{-1}\\hat{\\upsilon}'Y\\). Replacing the value of the residual, we get,\n\\[\n\\begin{aligned}\n\\hat{\\gamma}_1 =& (\\hat{\\upsilon}'\\hat{\\upsilon})^{-1}\\hat{\\upsilon}'Y \\\\\n=&(X_1'M_2M_2X_1)^{-1}X_1'M_2Y \\\\\n=&(X_1'M_2X_1)^{-1}X_1'M_2Y\n\\end{aligned}\n\\] Here, we use both the symmetric and idempotent qualities of \\(M_2\\). Next we want to show that \\(\\hat{\\beta}_1\\) is given by the same value. This part is more complicated. Let’s start with by reminding ourselves of the following:\n\\[\n\\begin{aligned}\nX'X\\hat{\\beta} &= X'Y \\\\\n\\begin{bmatrix}X_1 & X_2\\end{bmatrix}'\\begin{bmatrix}X_1 & X_2\\end{bmatrix}\\begin{bmatrix}\\hat{\\beta}_1 \\\\ \\hat{\\beta}_2\\end{bmatrix} &= \\begin{bmatrix}X_1 & X_2\\end{bmatrix}'Y \\\\\n\\begin{bmatrix}X_1'X_1 & X_1'X_2 \\\\ X_2'X_1 & X_2'X_2 \\end{bmatrix}\\begin{bmatrix}\\hat{\\beta}_1 \\\\ \\hat{\\beta}_2\\end{bmatrix} &= \\begin{bmatrix}X_1'Y \\\\ X_2'Y\\end{bmatrix}\n\\end{aligned}\n\\]\nWe could solve for \\(\\hat{\\beta}_1\\) by solving for the inverse of \\(X'X\\); however, this will take a long time. An easier approach is to simply verify that \\(\\hat{\\beta}_1=(X_1'M_2X_1)^{-1}X_1'M_2Y\\). Recall, \\(\\hat{\\beta}\\) splits \\(Y\\) into two components:\n\\[\nY = \\hat{\\beta}_1X_1+X_2\\hat{\\beta}_2 + \\hat{\\varepsilon}\n\\]\nIf we plug this definition of \\(Y\\) into the above expression we get,\n\\[\n\\begin{aligned}\n&(X_1'M_2X_1)^{-1}X_1'M_2(\\hat{\\beta}_1X_1+X_2\\hat{\\beta}_2 + \\hat{\\varepsilon}) \\\\\n=&\\hat{\\beta}_1\\underbrace{(X_1'M_2X_1)^{-1}X_1'M_2X_1}_{=I_n} \\\\\n&+\\underbrace{(X_1'M_2X_1)^{-1}X_1'M_2X_2\\hat{\\beta_2}}_{=0} \\\\\n&+\\underbrace{(X_1'M_2X_1)^{-1}X_1'M_2\\hat{\\varepsilon}}_{=0} \\\\\n=&\\hat{\\beta}_1\n\\end{aligned}\n\\]\nIn line 2, I use the fact that \\(\\hat{\\beta}_1\\) is a scalar and can be moved to the front (since the order of multiplication does not matter with a scalar). In line 3, I use the fact that \\(M_2X_2=0\\) by definition. Line 4 uses the fact that \\(M_2\\hat{\\varepsilon}=\\hat{\\varepsilon}\\) which means that \\(X_1'M_2\\hat{\\varepsilon}=X_1'\\hat{\\varepsilon}=0\\).\nThe OLS estimator solves for \\(\\beta_1\\) using the variance in \\(X_1\\) that is orthogonal to \\(X_2\\). This is the manner in which we “hold \\(X_2\\) constant”: the variation in \\(M_2X_1\\) is orthogonal to \\(X_2\\). Changes in \\(M_2X_1\\) are uncorrelated with changes in \\(X_2\\); as if the variation in \\(M_2X_1\\) arose independently of \\(X_2\\). However, uncorrelatedness does NOT imply independence."
  },
  {
    "objectID": "handout-1.html#footnotes",
    "href": "handout-1.html#footnotes",
    "title": "Classical Linear Regression Model & Ordinary Least Squares",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMy notation assumes that \\(X_i\\) is a column vector, which makes \\(X_i'\\beta\\) a scalar. Wooldridge (2010) uses the notation \\(X_i\\beta\\), implying that \\(X_i\\) is a row vector. This is a matter of preference.↩︎\nYou might also refer to the vector of regressors or explanatory variables. The terms covariates or control variables are more common in Microeconometrics literature, where regressors are typically included for identification of causal effects. Some texts will use the term independent variables, but this name implies a specific relationship between \\(Y\\) and \\(X\\) that need not hold. Note, we will asssume in this term that \\(n&gt;k\\); i.e. this is “small” data.↩︎\nThis is NOT the residual.↩︎\nSee extra material on Linear Algebra to read more on rank.↩︎\nFor \\(n\\) large, \\(rank(E[X_iX_i'])=k\\Rightarrow rank(X)=k\\). This follows from Law of Large Numbers, since \\(plim(n^{-1}X'X) = E[X_iX_i']\\).↩︎\nThere are many such failures of (parametric) identification in models that include dummy variables (or fixed effects). Earlier we saw that the intercept is not separately identified from the mean of the error term. Mean independence of the error term, \\(E[\\varepsilon_i]=0\\), is required for us to separately ‘identify’ \\(\\beta_1\\).↩︎\nWhen working with vectors and matrices it is important to keep track of their size. You can only multiply two matrices/vectors if their column and row dimensions match. For example, if \\(A\\) and \\(B\\) are both \\(n\\times k\\) matrices (\\(n\\neq k\\)), then \\(AB\\) is not defined since \\(A\\) has \\(k\\) columns and \\(B\\) \\(n\\) rows. For the same reason \\(BA\\) is also not defined. However, you can pre-multiply \\(B\\) with \\(A'\\) as \\(A'\\) is a \\(k\\times n\\) matrix: \\(A'B\\) is therefore a \\((k\\times n)\\cdot (n\\times k)=k\\times k\\) matrix. Similarly, \\(B'A\\) is defined, but is a \\(n\\times n\\) matrix.\nOrder matters when working with matrices and vectors. Pre-multiplication and post-multiplication are not the same thing.\nKeep track of the size of each term to ensure they correspond to one another. In this instance, each term should be a scalar. For example, \\(-2b'X'Y\\) is the multiplication of a scalar (\\(-2\\): size \\(1\\times 1\\)), row vector (\\(b'\\): size \\(1\\times k\\)), matrix (\\(X'\\): size \\(k\\times n\\)), and column vector (\\(Y\\): size \\(n\\times 1\\)). Thus we have a \\((1\\times 1)\\cdot (1\\times k)\\cdot (k\\times n)\\cdot (n\\times 1)=1\\times 1\\).↩︎\nOnce you are familiar with vector notation, it is relatively easy to tell whether a model is uni- or multi-variate. This is because the notation \\(\\beta_2 X_{i2}\\) is not consistent with \\(X_{2i}\\) being a vector (row or column).\nIf \\(X_{i2}\\) is a \\(k\\times 1\\) vector then so is \\(\\beta_2\\). Thus, \\(\\beta_2 X_{i2}\\) is \\((k\\times 1)\\cdot (k\\times1)\\), which is not defined.\nIf \\(X_{i2}\\) is a row vector (as in Wooldridge, 2011), \\(\\beta_2 X_{i2}\\) will then be \\((k\\times 1)\\cdot (1\\times k)\\), a \\(k\\times k\\) matrix. This cannot be correct since the model is defined at the unit level.\nThus, if you see a model written with the parameter in front of the regressor, you know that this must be a single regressor. This is subtle, yet imporant, distinction that researchers often use to convey the structure of their model. Whenever \\(X_{i2}\\) is a vector, researchers will almost always use the notation \\(X_{i2}'\\beta\\) or \\(X_{i2}\\beta\\), depending on whether \\(X_{i2}\\) is assumed to be a column or row vector.↩︎"
  },
  {
    "objectID": "handout-3.html",
    "href": "handout-3.html",
    "title": "Estimation Methods",
    "section": "",
    "text": "In this handout we will look at several approaches to generate estimators:\n\nLeast Squares\nMethod of Moments\nMaximum Likelihood\n\nWe will discuss each approach in the context of the Classical Linear Regression Model discussed in Lecture 1. You may also wish to revise the notes on Linear Algrebra.\nFurther reading can be found in:\n\nSection 5.6 of Cameron and Trivedi (2005)\nSection 6.1 of Verbeek (2017)"
  },
  {
    "objectID": "handout-3.html#overview",
    "href": "handout-3.html#overview",
    "title": "Estimation Methods",
    "section": "",
    "text": "In this handout we will look at several approaches to generate estimators:\n\nLeast Squares\nMethod of Moments\nMaximum Likelihood\n\nWe will discuss each approach in the context of the Classical Linear Regression Model discussed in Lecture 1. You may also wish to revise the notes on Linear Algrebra.\nFurther reading can be found in:\n\nSection 5.6 of Cameron and Trivedi (2005)\nSection 6.1 of Verbeek (2017)"
  },
  {
    "objectID": "handout-3.html#review-of-clrm",
    "href": "handout-3.html#review-of-clrm",
    "title": "Estimation Methods",
    "section": "2 Review of CLRM",
    "text": "2 Review of CLRM\nThe classical linear regression model is states that the conditional expectation function \\(E[Y_i|X_i]\\) is linear in parameters.\nFor the random sample \\(i=1,...,n\\),\n\\[\nY_i = X_i'\\beta + u_i\n\\] where \\(X_i\\) is a random k-dimensional vector (k-vector) and \\(\\beta\\) a non-random k-vector of population parameters. Both \\(Y_i\\) and \\(u_i\\) are random scalars.\nAs we saw, we can stack each observation into a column vector:\n\\[\nY =\\underbrace{\\begin{bmatrix}Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix}}_{n\\times 1} = \\underbrace{\\begin{bmatrix}X_1'\\beta \\\\ X_2'\\beta \\\\ \\vdots \\\\ X_n'\\beta\\end{bmatrix}}_{n\\times 1}  + \\underbrace{\\begin{bmatrix}u_1 \\\\ u_2 \\\\ \\vdots \\\\ u_n\\end{bmatrix}}_{n\\times 1}  \n= \\underbrace{\\begin{bmatrix}X_{11} & X_{12} & \\dots & X_{1k}\\\\ X_{21} & X_{22} && \\\\ \\vdots & & \\ddots & \\\\ X_{n1} & & & X_{nk} \\end{bmatrix}}_{n\\times k} \\underbrace{\\begin{bmatrix}\\beta_1 \\\\ \\beta_2 \\\\ \\vdots \\\\ \\beta_k\\end{bmatrix}}_{k\\times 1} + \\begin{bmatrix}u_1 \\\\ u_2 \\\\ \\vdots \\\\ u_n\\end{bmatrix} = X\\beta + u\n\\]\nwhere \\(X\\) is now a \\(n\\times k\\) random matrix, but \\(\\beta\\) remains a non-random k-vector of population parameters.\n\n\n\n\n\n\nData as a Matrix\n\n\n\nIf you have any experience working datasets in Stata/R, you will know that they tend to have a rectangular structure: each row typically represents an observation and each column a variable. This is the structure depicted above in matrix notation: each row of the \\(X\\) matrix depicts an observation and each column a regressor. The dataset you are using contains is a matrix of observations for both the outcome variable and regressors: \\([Y,X,]\\). Of course, we do not observe the error term.\n\n\nIn this section, we will employ the CLRM assumptions as we discuss the three approaches to estimation."
  },
  {
    "objectID": "handout-3.html#ordinary-least-squares",
    "href": "handout-3.html#ordinary-least-squares",
    "title": "Estimation Methods",
    "section": "3 (Ordinary) Least Squares",
    "text": "3 (Ordinary) Least Squares\nThe Ordinary Least Squares (OLS) estimator is the ‘work-horse’ of applied economics research.1 It is not the only Least Squares estimator, but as the simplest case is the most useful place to start. Other Least Squares estimators include Weighted Least Squares (WLS), (Feasible) Generalized Least Squares (GLS), Non-Linear Least Squares and Two-Stage Least Squares (2SLS).\nOLS is “ordinary” in the sense that there is no additional features to the method. For example, WLS applies a unique weight to each observation while OLS weights each observation equally. While OLS is arguably ‘vanilla’ in this way, it is efficient (as we shall see).\nIn general, LS estimators minimize a measure of ‘distance’ between the observed outcomes and the fitted values of the model. The measure of distance is sum of squared deviations (squared Euclidean or \\(\\ell_2\\) norm).2\n\n3.1 Application to CLRM\nIn the case of OLS estimator to the CLRM, the goal is to find the \\(b\\)-vector that minimizes,\n\\[\n\\sum_{i=1}^n(Y_i-\\underbrace{\\tilde{Y}_i}_{X_i'b})^2 = \\sum_{i=1}^n\\tilde{u}_i^2\n\\] This sum-of-squares can be written as the inner product of two-vectors:3\n\\[\n\\sum_{i=1}^n\\tilde{u}_i^2 = \\tilde{u}'\\tilde{u} = (Y-Xb)'(Y-Xb)\n\\]\nApplying the rules of matrix transposition, the inner product of these two matrices is given by,\n\\[\n(Y'-b'X')(Y-Xb) = Y'Y -b'X'Y-Y'Xb+b'X'Xb\n\\] Since all terms are scalars, \\(b'X'Y=Y'Xb\\); which then gives us, \\[\nY'Y -2b'X'Y+b'X'Xb\n\\] Thus, the (ordinary) least-squares estimator the vector that solves this linear expression. \\[\n\\hat{\\beta}^{OLS} = \\underset{b}{\\text{arg min}}\\quad Y'Y -2b'X'Y+b'X'Xb\n\\]\nUsing the rules of vector differentiation (see Linear Algrebra) we can find the first order conditions:\n\\[\n-2X'Y +2X'X\\hat{\\beta}^{OLS}= 0\n\\] If the \\(X\\) matrix is full rank (=\\(k\\)), then \\(X'X\\) is non-singular and its inverse exists. Recall, this was one of the CLRM assumptions. Then, \\[\n\\hat{\\beta}^{OLS} = (X'X)^{-1}X'Y\n\\]\n\n\n3.2 Bias\nIs the OLS estimator unbiased? The answer will depend on the assumption of the model. Here, we have assumed that the model being estimated is a CLRM. This means that we have assumed conditional mean independence of the error term:\n\\[\nE[u|X] = 0\n\\] The OLS-estimator is given by,\n\\[\n\\hat{\\beta}^{OLS} = (X'X)^{-1}X'Y = (X'X)^{-1}X'(X\\beta+u) = \\underbrace{(X'X)^{-1}X'X}_{I_n}\\beta+(X'X)^{-1}X'u=\\beta+(X'X)^{-1}X'u\n\\] plugging in the definition of \\(Y\\) from the model.\nHence,\n\\[\nE[\\hat{\\beta}^{OLS}|X] = E[\\beta+(X'X)^{-1}X'u|X] = \\beta+E[(X'X)^{-1}X'u|X]\n\\] Here we apply the linearity of the expectation operator and the factor that \\(\\beta\\) is a non-random vector. Next, we exploit the fact that conditional on \\(X\\), any function of \\(X\\) is non-random and can come out of the expectation operator.\n\\[\nE[\\hat{\\beta}^{OLS}|X] = \\beta+(X'X)^{-1}X'\\underbrace{E[u|X]}_{=0} = \\beta\n\\]\nNotice, we require the stronger assumption of conditional mean independence, \\(E[u|X]=0\\). Uncorrelateness, \\(E[X'u]=0\\), is insufficient for unbiasedness.\n\n\n\n\n\n\nImportant\n\n\n\nNotice, unbiasedness depends on the assumptions of the model and not any properties of the estimator. The estimator is simply a calculation using observed data. The properties and interpretation of this computation depend on the assumptions we make regarding the underlying model.\n\n\nIt is also worth noting that the unbiasedness of the OLS estimator does NOT depend on any assumptions regarding the variance or distribution of the error term.\n\n\n3.3 Efficiency\nThe OLS estimator is a \\(k\\)-dimensional random vector. The variance of this vector is a \\(k\\times k\\) variance-covariance matrix.\n\\[\nVar(\\hat{\\beta}) = E\\big[\\underbrace{(\\hat{\\beta}-E[\\hat{\\beta}])}_{k\\times 1}\\underbrace{(\\hat{\\beta}-E[\\hat{\\beta}])'}_{1\\times k}\\big]\n\\] The off-diagonals of the matrix are the covariances: \\(Cov(\\hat{\\beta}_j,\\hat{\\beta}_k)\\) for \\(j\\neq k\\).\nWe have just shown that \\(E[\\hat{\\beta}]=\\beta\\) and\n\\[\n\\hat{\\beta}^{OLS} -\\beta=(X'X)^{-1}X'u\n\\]\nThus, the (conditional) variance of this estimator is then given by,\n\\[\n\\begin{aligned}\nVar(\\hat{\\beta}^{OLS}|X) =& E\\big[(X'X)^{-1}X'uu'X(X'X)^{-1}|X\\big] \\\\\n=& (X'X)^{-1}X'E[uu'|X]X(X'X)^{-1} \\\\\n=& (X'X)^{-1}X'Var(u|X)X(X'X)^{-1}\n\\end{aligned}\n\\]\nThe variance of the estimator depends on the variance of the error term, the unexplained part of the model. In order to any further expressions for this variance calculation, we need to go back to the model. What assumptions did we make concerning the variance in the CLRM?\nUnder the assumption CLRM 3 of homoskedasticity,\n\\[\nVar(u|X) = \\sigma^2 I_n = \\begin{bmatrix}\\sigma^2& 0 & \\dots & 0 \\\\\n0 & \\sigma^2 & & \\\\\n\\vdots & & \\ddots & \\\\\n0 & & & \\sigma^2\\end{bmatrix}\n\\]\nthe above expression simplies to\n\\[\n\\begin{aligned}\nVar(\\hat{\\beta}^{OLS}|X) =& (X'X)^{-1}\\sigma^2I_nX(X'X)^{-1} \\\\\n=&\\sigma^2(X'X)^{-1}X'X(X'X)^{-1} \\\\\n=&\\sigma^2(X'X)^{-1}\n\\end{aligned}\n\\]\nIf we made a different assumption of heteroskedasticty (CLRM 3), then\n\\[\nVar(u|X) = \\begin{bmatrix}\\sigma^2_1& 0 & \\dots & 0 \\\\\n0 & \\sigma^2_2 & & \\\\\n\\vdots & & \\ddots & \\\\\n0 & & & \\sigma^2_n\\end{bmatrix} = \\Omega\n\\]\nthe variance matrix does not reduce to a scalar multiplied by the identity matrix. And,\n\\[\nVar(\\hat{\\beta}^{OLS}|X) = (X'X)^{-1}\\Omega X(X'X)^{-1}\n\\]\nThis is commonly referred to as a ‘sandwich’ formula, given the way \\(Var(u|X)=\\Omega\\) is sandwiched between two linear transformations. The Eicker-Huber-White estimator for heteroskedastic standard errors of \\(\\hat{\\beta}^{OLS}\\) replaces \\(Var(u|X)=E[uu'|X]\\) with \\(\\hat{u}\\hat{u}'\\), the OLS residuals.\n\n\n3.4 Finite-sample distribution\nThe finite sample distribution of the OLS estimator depends on the assumptions of the model. Under CLRM 5,\n\\[\nu|X \\sim N(0,\\sigma^2 I_n)\n\\] And we have already shown that the OLS estimator is simply a linear transformation of the error term, \\[\n\\hat{\\beta}^{OLS}=\\beta+(X'X)^{-1}X'u\n\\]\nThen, using the properties of the Normal distribution4\n\\[\n\\hat{\\beta}^{OLS}|X=N\\big(\\beta,\\sigma^2(X'X)^{-1}\\big)\n\\] assuming homoskedasticity. With heteroskedastic variance, you simply change the variance, as the assumption has no implications for biasedness.\n\n\n3.5 Consistency\nRecall from Lecture 2 that an estimator is consistent if it converges in probability to the parameter. In this case, we want to show that\n\\[\n\\hat{\\beta}^{OLS}\\rightarrow_p \\beta\\qquad\\text{as}\\qquad n\\rightarrow \\infty\n\\] Using the derivation \\(\\hat{\\beta}^{OLS} -\\beta=(X'X)^{-1}X'u\\), we need to show that \\((X'X)^{-1}X'u \\rightarrow_p 0\\). To emphasize the fact that \\(\\hat{\\beta}\\) is a function of the sample size, I am going to switch to the notation \\(\\hat{\\beta}_n\\) for this section.\nFor the consistency of the OLS estimator we require a few assumptions,\n\nCLRM 1: linear in parameters\nCLRM 2b: uncorrelatedness, \\(E[X_iu_i]=0\\)\nCLRM 6: data is iid\n(NEW) CLRM 7: \\(E[X_iX_i']\\) is a finite, positive-definite matrix.\n\nWe begin by re-writing the expression, \\(\\hat{\\beta}^{OLS}=\\beta+(X'X)^{-1}X'u\\) in summation notation and then scaling by \\(n\\),\n\\[\n\\hat{\\beta}_n=\\beta+\\bigg(\\sum_{i=1}^nX_iX_i'\\bigg)^{-1}\\sum_{i=1}^nX_iu_i = \\beta+\\bigg(n^{-1}\\sum_{i=1}^nX_iX_i'\\bigg)^{-1}n^{-1}\\sum_{i=1}^nX_iu_i\n\\]\nBy the WLLN,5\n\\[\nn^{-1}\\sum_{i=1}^nX_iu_i\\rightarrow_p E[X_1u_1] = 0\n\\]\nSimilarly, by WLLN, underpinned by finiteness of \\(E[X_1X_1']\\) (CLRM 7), \\[\nn^{-1}\\sum_{i=1}^nX_iX_i'\\rightarrow_p E[X_1X_1']\n\\]\nSince \\(E[X_1X_1']\\) is also positive definite (CLRM 7), then by Slutzky’s Theorem\n\\[\n\\bigg(n^{-1}\\sum_{i=1}^nX_iX_i'\\bigg)^{-1}\\rightarrow_p \\big(E[X_1X_1']\\big)^{-1}\n\\]\nHence, by Slutzky’s Theorem, which says that the product of two consistent estimators converges in probability to the product of their targets,\n\\[\n(X'X)^{-1}X'u \\rightarrow_p \\big(E[X_1X_1']\\big)^{-1}E[X_1u_1] = 0\n\\]\nThus,\n\\[\np \\lim(\\hat{\\beta}_n) = \\beta\n\\]\n\n\n\n\n\n\nImportant\n\n\n\nScaling each term by \\(n\\) is very important, as without it, both terms do not have a finite mean. Consider, under iid,\n\\[\nE\\bigg[\\sum_{i=1}^nX_iX_i'\\bigg] = nE[X_1X_1']\n\\] while, \\[\nE\\bigg[n^{-1}\\sum_{i=1}^nX_iX_i'\\bigg] = E[X_1X_1']\n\\]\n\n\n\n\n3.6 Asymptotic Distribution\nTo derive the asymptotic distribution of the OLS estimator we will need to apply the Central Limit Theorem. We will need to scale by \\(\\sqrt{n}\\), to derive the distribution of,\n\\[\n\\sqrt{n}(\\hat{\\beta}_n-\\beta)\n\\]\nRecall from Lecture 2 that by Cramer’s Convergence Theorem, \\(Y_nX_n\\rightarrow_d cX\\) where \\(X_n\\rightarrow_d\\) and \\(Y_n\\rightarrow_p c\\). This result holds for the case where \\(Y_n\\) is a random matrix. \\[\n\\sqrt{n}(\\hat{\\beta}_n-\\beta) = \\big(n^{-1}X'X\\big)^{-1}n^{-1/2}Xu\n\\] We have already established that, \\[\n\\big(n^{-1}X'X\\big)^{-1}\\rightarrow_p \\big(E[X_1X_1']\\big)^{-1}\n\\] under assumptions CLRM 1, 2b, 6, and 7.\nWe therefore need to consider the asymptotic distribution of \\(n^{-1/2}Xu\\). By CLRM 2b \\(E[X_1u_1]=0\\), fulfilling one of the CLT conditions. We then need the second moment to be finite: \\(Var(X_1u1) = E[u_1^2X_1X_1']\\). This is a \\(k\\times k\\) matrix.\nWe will need to make some additional assumptions:\n\n(NEW) CLRM 8: \\(E[u_1^2X_1X_1']\\) is a finite positive-definite matrix.6\n\nUnder assumptions CLRM 1, 2, 6, and 8, by CLT,\n\\[\nn^{-1/2}Xu\\rightarrow N(0,E[u_1^2 X_1 X_1'])\n\\] There, \\[\n\\sqrt{n}(\\hat{\\beta}_n-\\beta) \\rightarrow_d N\\bigg(0,\\big(E[X_1X_1']\\big)^{-1}E[u_1^2X_1X_1']\\big(E[X_1X_1']\\big)^{-1}\\bigg)\n\\] Under the homoskedasticity, \\(E[u_1^2X_1X_1']=\\sigma^2 E[X_1,X_1']\\), giving us, \\[\n\\sqrt{n}(\\hat{\\beta}_n-\\beta) \\rightarrow_d N\\bigg(0,\\sigma^2\\big(E[X_1X_1']\\big)^{-1}\\bigg)\n\\] We can approximate the asymptotic distribution \\(\\hat{\\beta}_n\\) by multiplying by \\(\\sqrt{n}\\) and replacing \\(\\big(E[X_1X_1']\\big)^{-1}\\) with the approximation \\(\\big(n^{-1}X'X\\big)^{-1}\\). \\[\n\\hat{\\beta}_n \\overset{a}{\\sim}N\\big(\\beta,\\sigma^2(X'X)^{-1}\\big)\n\\] The \\(n\\) in the variance formula is cancelled out by the pre-multiply of \\(\\sqrt{n}\\).\n\n\n3.7 Other properties\n\nAmong the class of unbiased linear estimators of the CLRM, the OLS is the Best Linear Unbiased Estimator (BLUE). “Best” here means lowest variance. Can you show this?"
  },
  {
    "objectID": "handout-3.html#method-of-moments",
    "href": "handout-3.html#method-of-moments",
    "title": "Estimation Methods",
    "section": "4 Method of Moments",
    "text": "4 Method of Moments\nThe method of moments (MM) approach is to match assumed ‘moments’ given by the model with their sample analogue. This is a very general approach and is used extensively in applied macroeconomics, where a structural model gives rise to moments between economic variables that can be matched in the data.\nA general principle of MM is that the number of moments, \\(m\\), must be \\(\\geq k\\), the number of parameters being estimated. This akin to saying, the number of equations must be greater or equal to the number of variables being solved for. If the number of moments exceeds the number of parameters, we say that the model is overidentified. In the case of instrumental variables, overidentification allows you two test certain model assumptions.\nIn term 2, you will study instrumental variables which adopts a GMM approach to estimation. MM approaches are also used extensively in time series. For now, we will apply the MM approach to the CLRM.\n\n4.1 General setup\nThe observed data is given by, \\(W_1,...,W_n\\), read \\(W_i\\) is a \\(p\\)-dimension random vector. Let \\(g(W_i,\\theta)\\) be a \\(l\\)-dimension function (i.e. \\(\\in \\mathbf{R}^l\\)) and \\(\\theta\\in\\mathbf{R}^k\\):\n\\[\ng(W_i,\\theta) = \\begin{bmatrix}g_1(W_i,\\theta) \\\\ \\vdots \\\\g_l(W_i,\\theta)\\end{bmatrix}\n\\] We assume that the true value of the parameter \\(\\theta_0\\in\\Theta\\subset \\mathbf{R}^{k}\\) satisifies the condition, \\[\nE\\big[g(W_i,\\theta_0)\\big] = 0\n\\] We say that the model is identified if there is a unique solution to the above equations. That is, \\(E\\big[g(W_i,\\theta)\\big] = E\\big[g(W_i,\\tilde{\\theta})\\big] = 0\\;\\Rightarrow\\;\\theta=\\tilde{\\theta}\\). A necessary condition for identifiction is \\(l\\geq k\\); i.e. the number of equations is at least as large as the number of unknown parameters. A model can be underidentified, which typically means that there is not a unique solution for some of the parameters.\n\n\n4.2 Estimator\nThe basic principle of MM estimation is to replacing the expectation operator with the average function and solve for \\(\\hat{\\theta}\\). \\[\nn^{-1}\\sum_{i=1}^n g(W_i,\\hat{\\theta}^{MM}) = 0\n\\] However, this only works when \\(l=k\\) (exactly identified cases). For \\(l&gt;k\\) (overidentified cases) there is no unique vector that solves all \\(l\\) equations.\nThe Generalized Method of Moments (GMM) approach applies a set of weights to the minimization problem. Let \\(A_n\\) be a \\(l\\times l\\) weight matrix, such that \\(A_n\\rightarrow_p A\\). Then,\n\\[\n\\hat{\\theta}^{GMM} = \\underset{\\theta\\in\\Theta}{\\text{arg min}}\\;\\bigg|\\bigg|A_n n^{-1}\\sum_{i=1}^n g(W_i,\\hat{\\theta}^{MM})\\bigg|\\bigg|^2\n\\]\n\n\n4.3 Application to CLRM\nAssumption CLRM 2b tells us that the regressors are uncorrelated with the error term. \\[\nE[X_iu_i] = 0\n\\] This is the moment that gives rise to identification in the CLRM. Given CLRM 1, we can replace the error term in the above moment with \\(Y_i-X_i'\\beta\\). \\[\nE[X_i(Y_i-X_i'\\beta)] = 0\n\\] Thus, the \\(g(W_i,\\beta)=X_i(Y_i-X_i'\\beta)\\) for \\(W_i = [Y_i,X_i']'\\).\nHow many equations are there? Recall, \\(X_i\\) is a \\(k\\)-dimension random vector. So,\n\\[\nE[X_i(Y_i-X_i'\\beta)] = \\begin{bmatrix}E[X_{i1}(Y_i-X_i'\\beta)]\\\\E[X_{i2}(Y_i-X_i'\\beta)]\\\\ \\vdots \\\\ E[X_{ik}(Y_i-X_i'\\beta)]\\end{bmatrix}=0\n\\] The are \\(k\\)-moments (or equations), meaning that we can estimate up to \\(k\\) parameters. In this instance, we have a failure of identification if \\(rank(E[X_iX_i'])&lt;k\\); which is required for the invertibility of \\(E[X_iX_i']\\). This condition is met by assumption CLRM 4; ensuring exact identification.\nThe MM estimator for the CLRM is then given by the solution to,\n\\[\nn^{-1}\\sum_{i=1}^n X_i(Y_i-X_i'\\hat{\\beta}^{MM}) = 0\n\\]\nThe solution is equivalent to the OLS estimator: \\[\n\\hat{\\beta}^{MM} = \\bigg(n^{-1}\\sum_{i=1}^n X_iX_i'\\bigg)n^{-1}\\sum_{i=1}^nX_iY_i = \\big(X'X\\big)^{-1}X'Y = \\hat{\\beta}^{OLS}\n\\]\n\n\n4.4 Consistency\nAs we have already established the consistency of the OLS estimator, we will briefly review the case of the GMM estimator here. A more detailed discussion will be provided in term 2.\nRecall, the assumption \\(A_n\\rightarrow_p A\\). Then,\n\\[\n\\bigg|\\bigg|A_n n^{-1}\\sum_{i=1}^n g(W_i,\\theta)\\bigg|\\bigg|^2\\rightarrow_p \\big|\\big|A E\\big[ g(W_i,\\theta)\\big]\\big|\\big|^2\n\\] In instance where identification is exact/unique, \\(E\\big[ g(W_i,\\theta)\\big]=0\\iff\\theta=\\theta_0\\). Which is to say that the true value of \\(\\theta\\) is the unique minizer.\nThen,\n\\[\n\\begin{aligned}\n\\hat{\\theta}^{GMM} =&\\underset{\\theta\\in\\Theta}{\\text{arg min}}\\; \\bigg|\\bigg|A_n n^{-1}\\sum_{i=1}^n g(W_i,\\theta)\\bigg|\\bigg|^2 \\\\\n\\rightarrow_p&\\underset{\\theta\\in\\Theta}{\\text{arg min}}\\;\\big|\\big|A E\\big[ g(W_i,\\theta)\\big]\\big|\\big|^2 \\\\\n=&\\theta_0\n\\end{aligned}\n\\] The formal proof requires a number of additional regulrity assumptions; including, the compactness of \\(\\Theta\\).\n\n\n4.5 Asymptotic Normality\nThe GMM estimator is asymptotically normal.\n\\[\n\\sqrt{n}\\big(\\hat{\\theta}^{GMM}-\\theta_0\\big)\\rightarrow_d N(0,V)\n\\] where, \\[\n\\begin{aligned}\nV =& (Q'A'AQ)^{-1}QA'A\\Omega A'AQ (Q'A'AQ)^{-1} \\\\\nQ =& E\\bigg[\\frac{\\partial g(W_i,\\theta_0)}{\\partial \\theta'}\\bigg] \\\\\n\\Omega =& E\\big[ g(W_i,\\theta)g(W_i,\\theta)'\\big]\n\\end{aligned}\n\\] Where does this result come from? We won’t go through the proof in details. However, it starts from the FOCs. The GMM estimator solves,\n\\[\n\\bigg[\\underbrace{n^{-1}\\sum_{i=1}^n\\frac{\\partial g(W_i,\\hat{\\theta}^{GMM})}{\\partial \\theta'}}_{Q_n\\big(\\hat{\\theta}^{GMM}\\big)}\\bigg]'A_n'A_n n^{-1}\\sum_{i=1}^{n}g(W_i,\\hat{\\theta}^{GMM}) = 0\n\\] In the above expression, the matrix of derivatives will converge (under some regularity conditions) in probability: \\(Q_n\\big(\\hat{\\theta}^{GMM}\\big)\\rightarrow_p Q\\). Second, since \\(E\\big[ g(W_i,\\theta)\\big]=0\\), by CLT we know,\n\\[\nn^{-1/2}\\sum_{i=1}^{n}g(W_i,\\theta_0)\\rightarrow_d N(0,\\underbrace{E\\big[ g(W_i,\\theta)g(W_i,\\theta)'\\big]}_\\Omega)\n\\] We can therefore see where the components of the Variance come from. The proof requires a bit more work. First, we need the distribution of \\(\\sqrt{n}\\big(\\hat{\\theta}^{GMM}-\\theta_0\\big)\\), not \\(n^{-1/2}\\sum_{i=1}^{n}g(W_i,\\theta_0)\\). Second, the FOCs contain \\(n^{-1}\\sum_{i=1}^{n}g(W_i,\\hat{\\theta}^{GMM})\\neq\\) and not \\(n^{-1}\\sum_{i=1}^{n}g(W_i,\\theta_0)\\).\nThis is resolve using a mean value expansion:\n\\[\ng(W_i,,\\hat{\\theta}^{GMM}) = g(W_i,\\theta_0) + \\frac{\\partial g(W_i,\\hat{\\theta}^*)}{\\partial \\theta'}\\big(\\hat{\\theta}^{GMM}-\\theta_0\\big)\n\\] Plugging this expansion into the FOCs, you can rearrange to solve,\n\\[\n\\sqrt{n}\\big(\\hat{\\theta}^{GMM}-\\theta_0\\big) = -[Q_n\\big(\\hat{\\theta}^{GMM}\\big)'A_n'A_nQ_n\\big(\\hat{\\theta}^*\\big)]^{-1}Q_n\\big(\\hat{\\theta}^{GMM}\\big)'A_n'A_nn^{-1/2}\\sum_{i=1}^{n}g(W_i,\\theta_0)\n\\] Since \\(\\hat{\\theta}^*\\) is a mean value, it is a also consistent and \\(Q_n\\big(\\hat{\\theta}^*\\big)\\rightarrow_p Q\\).\n\n\n4.6 Additional comments\n\nThe targetted moments may be highly non-linear. For example, the Lucas Model pins down the rate of return on a risky asset \\(R_{j,t}\\) using the relative utility of consumption today and tomorrow. The equilibrium condition for assets \\(j=1,...,m\\) is given by,\n\\[\nE\\bigg[\\underbrace{\\delta\\bigg(\\frac{C_{t+1}}{C_t}\\bigg)^{-\\alpha}(1+R_{j,t})-1}_{g(W_{j,t},\\theta)}\\bigg]=0\n\\]\nwhere \\(W_{j,t}=[C_t,C_{t+1},R_{j,t}]\\) and \\(\\theta = [\\alpha,\\delta]\\). As the moment must hold for each asset, \\(\\theta\\) is identified so long as \\(m\\geq 2\\).\nGiven the non-linearity of the \\(g\\)-function, there is no closed for solution. Instead, the GMM estimator must be solved for using numerical optimization.\nSome (macroeconomic) models are not identified (or underidentified). For example, a simple RBC model (with a random government component) yields the following moment condition from Euler equation,7\n\\[\nE\\bigg[\\underbrace{\\beta\\frac{C_{t+1}}{C_t}\\big(f_K + (1-\\delta)\\big)-1}_{g(W_i,\\theta)}\\bigg]=0\n\\] where \\(W_{j,t}=[C_t,C_{t+1},f_K]\\) and \\(\\theta = [\\beta,\\delta]\\). In this application, there is only a single moment but two unknown parameters. For this reason, you will need to find an additional instrument that introduces an additional moment to identify both parameters."
  },
  {
    "objectID": "handout-3.html#maximum-likelihood",
    "href": "handout-3.html#maximum-likelihood",
    "title": "Estimation Methods",
    "section": "5 Maximum Likelihood",
    "text": "5 Maximum Likelihood\nMaximum Likelihood (ML or MLE) are a general class of estimators that exploit a knowledge of the underlying distribution of unobservables in the model. As the name suggests, the goal will be to maximize the likelihood (i.e. probability) of observing the a given sample of data, given the assumed distribution of the data, governed by a fixed set of parameters.\n\n5.1 General setup\nConsider an iid random sample of data: \\(W_i,...,W_n\\). We will assume that the data is drawn from a known distribution, \\(f(w_i;\\theta)\\), where \\(\\theta\\in\\Theta\\subset \\mathbf{R}^k\\) is an unknown vector of population parameters.8\n\n\n\n\n\n\nNotation\n\n\n\nThe notation used to describe ML estimation varies quite a bit across texts. One key difference appears to be how to denote a parameterized distribution. The density function, \\(f(w_i;\\theta)\\), is the density at \\(w_i\\) (the realized value for observation \\(i\\)), where the distribution is parameterized by \\(\\theta\\). Some texts use the conditional notation, \\(f(w_i|\\theta)\\), as the distribution depends on \\(\\theta\\). However, probabilistic conditions tend to be based on random variables and not non-random parameters. I found this StackExchange discussion on the topic quite interesting. Needless to say, there is much agreement and notation appears to differ across Mathematics and Statistics, and among the Statisticians, between frequentists and Bayesians. Even the Wikipedia page on MLE uses a combination of the two notations. I will use ‘;’; which also happens to be the notation used by Wooldridge (2010).\n\n\nAs the sample is iid, the joint density (or pdf) of the realized observations is given by the product of marginals,\n\\[\nf(w;\\theta)=\\prod_{i=1}^n f(w_i;\\theta)\n\\] This is referred to as the likelihood function.\nSuppose \\(W_i = [Y_i,X_i']'\\), a vector contain a single outcome variable and a set of covariates. We can then define the joint conditional likelihood as,\n\\[\n\\begin{aligned}\n\\ell_i(\\theta) =& f(Y_i|X_i;\\theta) \\\\\nL_n(\\theta) =& \\prod_{i=1}^n f(Y_i|X_i;\\theta)\n\\end{aligned}\n\\] Here my notation differs from Wooldridge (2010), who uses \\(\\ell_i(\\theta)\\) to denote the conditional log-likelihood for observation \\(i\\) (see Wooldridge 2010, 471). Take note of the fact that the likelihood function is a random function of \\(\\theta\\), since it depends on the random variables \\(W_i = [Y_i,X_i']'\\).9\n\n\n5.2 Estimator\nThe goal of ML estimation is to solve the value of \\(\\hat{\\theta}\\) that maximizes the likelihood of observing the data.\n\\[\n\\hat{\\theta}^{ML} = \\underset{\\theta}{\\text{arg max}}\\;L_n(\\theta)\n\\] In practice, we apply a monotonic transformation to the likelihood function. By taking the log of the likelihood, the product of marginal distributions becomes a sum. As the transformation is monotonic, the solution to the above problem is equivalent to the solution to,\n\\[\n\\hat{\\theta}^{ML} = \\underset{\\theta}{\\text{arg max}}\\;n^{-1}\\log L_n(\\theta) = \\underset{\\theta}{\\text{arg max}}\\;n^{-1}\\sum_{i=1}^n\\log\\ell_i(\\theta)\n\\] In addition, the division by \\(n\\) makes this problem the sample analogue of, \\[\n      \\underset{\\theta\\in\\Theta}{\\text{max}}\\;E[\\log\\ell_i(\\theta)]\n\\] It turns out that the true value of the parameter, \\(\\theta_0\\), is the solution to the above problem [see Wooldridge (2010), pp. 473].10 We will prove this for the unconditional case when we discuss consistency of ML.\nAssuming a continuous, concave density function, we can solve for \\(\\hat{\\theta}^{ML}\\) using first-order conditions.\n\\[\n\\frac{1}{n}\\frac{\\partial \\log L_n(\\hat{\\theta})}{\\partial \\theta} =\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial \\log\\ell_i(\\hat{\\theta})}{\\partial \\theta}= n^{-1}S(\\hat{\\theta})=0\n\\]\nThe vector of partial derivatives is referred to as the score function: \\(S(\\theta)\\). When evaluated at the ML estimator, the score function is 0. This is a \\(k\\)-dimensional vector in which row is the partial derivative with respect to \\(\\theta_k\\).\n\n\n5.3 Application to CLRM\nUnder CLRM 5, \\(U|X \\sim N(0,\\sigma^2 I_n)\\). Together with CLRM 1 and 6, we know the conditional distribution of \\(Y\\). \\[\nY_i|X_i\\sim_{iid} N(X_i'\\beta,\\sigma^2)\n\\] where \\(X_i'\\beta\\) is the conditional mean of \\(Y_i\\). Therefore, the conditional likelihood of the data is given by, \\[\nL_n(\\beta,\\sigma^2) = \\prod_{i=1}^n \\bigg[\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\bigg(-\\frac{1}{2}\\bigg(\\frac{Y_i-X_i'\\beta}{\\sigma}\\bigg)^2\\bigg)\\bigg]\n\\] :::{.callout-important} We are working with the conditional likelihood. To define the likelihood of observing the entire sample, \\(W_1,...,W_n\\), we would also need to consider the distribution of \\(X_i\\). We would then define \\(f(W_i;\\theta) = f(Y_i|X_i;\\theta)\\cdot f(X_i)\\), where \\(f(X_i)\\) may be parameterized by its own set of parameters. \\(\\theta\\) is the set of parameters that parameterize the conditional distribution of \\(Y|X\\). ::: Taking the log transformation and divide by \\(n\\), we get, \\[\nn^{-1}\\log L_n(\\beta,\\sigma^2) =  -\\frac{1}{2}\\log(\\sigma^2)-\\frac{1}{2}\\log(2\\pi)-\\frac{1}{2n\\sigma^2} \\sum_{i=1}^n(Y_i-X_i'\\beta)^2\n\\] It should be immediately clear that maximizing this expression will be equivalent to minimizing the sum of squared errors.\nConsider the FOC’s set to 0 at the optimal point. First, w.r.t. \\(\\beta\\),\n\\[\n\\begin{aligned}\n\\frac{\\partial n^{-1}\\log L_n(\\hat{\\beta},\\hat{\\sigma}^2)}{\\partial\\beta} =& -\\frac{1}{n\\sigma^2} \\sum_{i=1}^nX_i(Y_i-X_i'\\hat{\\beta}) = 0 \\\\\n\\Rightarrow \\hat{\\beta}^{ML} =& \\bigg(\\sum_{i=1}^n X_iX_i'\\bigg)\\sum_{i=1}^nX_iY_i \\\\\n=& \\big(X'X\\big)^{-1}X'Y\n\\end{aligned}\n\\] In the case of the CLRM, \\(\\hat{\\beta}^{ML}=\\hat{\\beta}^{MM}=\\hat{\\beta}^{OLS}\\).\nSecond, w.r.t. \\(\\sigma^2\\), \\[\n\\begin{aligned}\n\\frac{\\partial n^{-1}\\log L_n(\\hat{\\beta},\\hat{\\sigma}^2)}{\\partial\\sigma^2} =& -\\frac{1}{2\\hat{\\sigma}^2}+ \\frac{1}{n2\\hat{\\sigma}^4}\\sum_{i=1}^n(Y_i-X_i'\\hat{\\beta})^2 = 0 \\\\\n\\Rightarrow \\hat{\\sigma}^{2}_{ML} =& n^{-1}\\sum_{i=1}^n(Y_i-X_i'\\hat{\\beta})^2\n\\end{aligned}\n\\] This estimator for the variance is consistent, but biased for small samples. This is because it scales by \\(n\\) and not \\(n-1\\), a distinction that is ignorable as \\(n\\rightarrow\\infty\\). For this reason, when conducting inference you should use the asymptotic distribution of the ML estimator.\n\n\n5.4 Consistency\nThe ML estimator is consistent. This can be shown in a couple of steps. To simplify notation we will examine the proof for the unconditional likelihood, but the same will hold for the conditional. The proof will require Jensen’s inequality:\n\nTheorem 1 For \\(h(\\cdot)\\) concave, then \\(E[h(X)]\\leq h(E[X])\\).\n\n\nProof. By the WLLN, for ALL values of \\(\\theta\\in\\Theta\\),\n\\[\n\\begin{aligned}\nn^{-1}\\sum_{i=1}^n\\log f(W_i;\\theta) \\rightarrow_p&\\;E\\big[\\log f(W_i;\\theta)\\big] \\\\\n=&\\int\\big(\\log f(w;\\theta)\\big)f(w;\\theta_0)dw\n\\end{aligned}\n\\] Note, an important distinction in the last line: the expectation is based on the density function parameterized by the true value, \\(\\theta_0\\). This is because the data is generated by the true density.\nWe have convergence for ALL values of \\(\\theta\\), but now need to establish convergence to the \\(\\theta_0\\). Consider the difference, \\[\n\\begin{aligned}\nE\\big[\\log f(W_i;\\theta)\\big]-E\\big[\\log f(W_i;\\theta_0)\\big]\n=&E\\bigg[\\log\\frac{f(W_i;\\theta)}{f(W_i;\\theta_0)}\\bigg] \\\\\n\\leq&\\log\\bigg[\\frac{f(W_i;\\theta)}{f(W_i;\\theta_0)}\\bigg] \\qquad \\text{by Jensen's} \\\\\n=&\\log \\int\\bigg(\\frac{f(w;\\theta)}{f(w,\\theta_0)}\\bigg)f(w;\\theta_0)dw \\\\\n=&\\log \\int f(w;\\theta)dw \\\\\n=&\\log 1 \\\\\n=&0\n\\end{aligned}\n\\] The inequality can be made strict if we assume that \\(Pr\\big(f(W_i;\\theta_0)\\neq f(W_i;\\theta)\\big)&gt;0\\) \\(\\forall \\theta\\neq\\theta_0\\). This ensures that \\(\\theta_0\\) is a unique solution. Since the difference is \\(\\leq 0\\), it follows that, \\[\n\\theta_0 = \\underset{\\theta\\in\\Theta}{\\text{arg max}} E[\\log f(W_i;\\theta)]\n\\] Which implies, \\[\n\\begin{aligned}\n\\hat{\\theta}^{ML}_n =& \\;\\underset{\\theta\\in\\Theta}{\\text{arg max}} \\;n^{-1}\\log L_n(\\theta) \\\\\n\\rightarrow_p& \\;\\underset{\\theta\\in\\Theta}{\\text{arg max}} E\\big[\\log f(W_i,\\theta)\\big]\\\\\n=& \\theta_0\n\\end{aligned}\n\\]\n\n\n\n5.5 Asymptotic Normality\nThe ML estimator is asymtotically normal. We will not prove this result, but rather focus on the form of the asymptotic variance and its estimator. The proof uses the Mean Value Theorem and CLT.\n\\[\n\\sqrt{n}\\big(\\hat{\\theta}^{ML}_n-\\theta_0\\big)\\rightarrow_d N(0,V)\n\\]\nwhere \\(V=[J(\\theta_0)]^{-1}\\). \\(J(\\theta)\\) is referred to as the information matrix, given by the expectation of the (Hessian) matrix of second-order derivatives:\n\\[\nJ(\\theta) = -E\\bigg[\\frac{\\partial^2}{\\partial\\theta\\partial\\theta'}\\log f(W_i,\\theta_0)\\bigg]\n\\] \\([nJ(\\theta_0)]^{-1}\\) is used to approximate the variance, but since \\(J\\) is not observed, it must be estimated. This is done by replacing the expectation in the information matrix with sample average:\n\\[\n\\hat{V}_H = \\bigg[\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial^2}{\\partial\\theta\\partial\\theta'}\\log f(W_i,\\hat{\\theta})\\bigg]^{-1}\n\\]\n\n\n5.6 Additional comments\n\nML estimators are invariant. If \\(\\hat{\\theta}\\) is the ML-estimator for \\(\\theta\\), then \\(\\ln(\\hat{\\theta})\\) is the ML-estimator for \\(\\ln(\\theta)\\).\nIn general, there are no closed form solutions for ML estimators; the CLRM being one exception. For this reason, ML estimation requires numerical optimization.\nThe ML estimator is efficient. That is, its variance is at least as small as any other consistent (and asymptotically normal) estimator.\nML estimators require as to know the true PDF, up to its parameters. For example, probit (logit) models assumes that the error term is normally (logistically) distributed.\nIn some cases, the estimator may be consistent even if the PDF is misspecified. As is the case for the OLS estimator of the linear model. These estimators are referred to as a quasi-ML estimators."
  },
  {
    "objectID": "handout-3.html#footnotes",
    "href": "handout-3.html#footnotes",
    "title": "Estimation Methods",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMachine Learning techniques, such as neural networks, use non-linear operators such as the Softmax function and Rectified Linear Unit (ReLU). Who knows, in a few years, we may no longer think of OLS as the ‘work-horse’ of applied statistics and research.↩︎\nA measure of distance must be positive. You can use the (sum of) absolute-value deviations, but the least squares has nice properties including ease of differentiation and overall efficiency (of the estimator).↩︎\nThe inner (or dot) product of two equal-length vectors, \\(a\\) and \\(b\\), is defined as: \\[\n\\langle a,b\\rangle=a\\cdot b = \\sum_{i=1}^k a_i\\times b_i = a'b\n\\]↩︎\nIf \\(Y\\sim N(\\mu,\\Sigma)\\), \\(Y\\in\\mathbf{R}^k\\), then \\(AY+b\\sim N(A\\mu+b,A\\Sigma A')\\) for any non-random \\(m\\times k\\) \\(A\\)-matrix and \\(m\\times 1\\) \\(b\\)-vector.↩︎\nThe assumptions are fulfilled by CLRM 2b and CLRM 6.↩︎\nThe finiteness of this matrix requires two assumptions:\n\n\\(E[X_{i,j}^4]&lt;\\infty\\) for all \\(j=1,...k\\) (i.e. each regressor has finite fourth moment)\n\\(E[U_j^4]&lt; \\infty\\)\n\nThese assumptions are sufficient for all elements of matrix \\(E[u_1^2 X_1 X_1']\\) to be finite. The proof is an application of the Cauchy-Schwartz Inequality, which we haven’t covered.↩︎\nThis example is taken from Canova (2007, ch. 5, p. 167).↩︎\n\\(\\Theta\\) is a parameter space and is typically assumed to be compact: a closed and bounded subset of Euclidean space.↩︎\nYou may also see the equivalent notation \\(L(W_i,\\theta)\\equiv L_n(\\theta)\\). The subscript-$n$ implies that the function depends on the sample.↩︎\nThis is actually a non-trivial issue and beyond the scope of this module. As noted by Wooldridge (2010), we can arrive at the ML estimator by picking the value of \\(\\theta\\) to maximize the joint likelihood. However, this approach assumes that the true value of \\(\\theta\\in\\Theta\\), \\$\\theta_0$, maximizes the joint likelihood. This is not immediately evident. Once established, we have a more robust basis of the ML estimator.↩︎"
  },
  {
    "objectID": "material-cef.html",
    "href": "material-cef.html",
    "title": "Conditional Expectation Function",
    "section": "",
    "text": "Consider the random variable \\(Y_i\\in\\mathbb{R}\\) and the random vector \\(X_i\\in\\mathbb{R}^k\\), \\(k\\geq1\\).1\n\n\nThe Conditional Expecation Function (CEF) - denoted \\(E[Y_i|X_i]\\) - is a random function. It is a function that returns the expected value of \\(Y_i\\) for each realized value of \\(X_i\\). Since \\(X_i\\) is a random vector the resulting function is random itself.\nIf we fix \\(X_i=x\\), then the value at which we are evaluating the function is no longer random. The result is a constant: the expected value of \\(Y_i\\) at the given \\(x\\).\n\\[\nE[Y_i|X_i=x] = \\int y \\cdot f_{Y}(y|X_i=x)dy = \\int y dF_{Y}(y|X_i=x)\n\\]\nThis follows the same logic that the expectation of a random variable is, \\(E[Y_i]\\), is not random.\nDiscrete case. The book devotes a lot time to the discussion of cases were \\(X_i\\) is a discrete random variable; using the notation \\(W_i\\in\\{0,1\\}\\) or \\(D_i\\in\\{0,1\\}\\). In this unique case, we can write the CEF as,\n\\[\nE[Y_i|D_i] = E[Y_i|D_i=0] + D_i\\cdot\\big(E[Y_i|D_i=1]-E[Y_i|D_i=0]\\big)\n\\]\nThe above function returns \\(E[Y_i|D_i=0]\\) when \\(D_i=0\\) and \\(E[Y_i|D_i=1]\\) when \\(D_i=1\\). This expression for the CEF will be useful in latter chapters of the book.\n\n\n\nThe Law of Iterated Expectations says that given two random variables2 \\([Y_i,X_i]\\), we can express the unconditional expected value of \\(Y_i\\) as the expected value of the conditional expectation of \\(Y_i\\) on \\(X_i\\).\n\\[\n        E[Y_i] = E\\big[E[Y_i|X_i]\\big]\n\\]\nWhere the outside expectation is with respect to \\(X_i\\),3 since the CEF is a random function of \\(X_i\\). We can expand this as follows,\n\\[\nE[Y_i] = \\int t \\cdot f_{Y_i}(t)dt = \\int\\int y \\cdot f_{Y_i|X}(y|x)dyf_X(x)dx = E\\big[E[Y_i|X_i]\\big]\n\\]\n\nExample 1 Suppose \\(Y_i\\) and \\(X_i\\) are both discrete, \\(Y_i\\in\\{1,2\\}\\) and \\(X_i\\in\\{3,4\\}\\), with the joint distribution:\n\n\\(f_{Y,X}\\)\n\n\n\n\\(X_i=3\\)\n\\(X_i=2\\)\n\n\n\\(Y_i=1\\)\n1/10\n3/10\n\n\n\\(Y_i=2\\)\n2/10\n4/10\n\n\n\nWe can then define the two marginal distributions,\n\n\\(f_Y\\)\n\n\n\\(Y_i=1\\)\n\\(Y_i=2\\)\n\n\n4/10\n6/10\n\n\n\nand,\n\n\\(f_X\\)\n\n\n\\(X_i=3\\)\n\\(X_i=4\\)\n\n\n3/10\n7/10\n\n\n\nLikewise, we know the conditional distribution \\(f_{Y|X}\\); which we get by dividing the joint distribution by the marginal distribution of \\(X_i\\). Each column of the conditional distribution should add up to 1.\n\n\\(f_{Y|X}\\)\n\n\n\n\\(X_i=3\\)\n\\(X_i=4\\)\n\n\n\\(Y_i=1\\)\n1/3\n3/7\n\n\n\\(Y_i=2\\)\n2/3\n4/7\n\n\n\nNow we can calculate the following objects:\n\n\\(E[Y_i]\\)\n\n\\[\n\\begin{aligned}\n        E[Y_i] =& 1\\cdot Pr(Y_i=1)+2\\cdot Pr(Y_i=2) \\\\\n        =&1\\cdot 4/10+2\\cdot 6/10 \\\\\n        =&16/10\n    \\end{aligned}\n\\]\n\n\\(E[Y_i|X_i=3]\\)\n\n\\[\n\\begin{aligned}\n        E[Y_i|X_i=3] =& 1\\cdot Pr(Y_i=1|X_i=3)+2\\cdot Pr(Y_i=2|X_i=3) \\\\\n        =&1\\cdot 1/3+2\\cdot 2/3 \\\\\n        =&5/3\n\\end{aligned}\n\\]\n\n\\(E[Y_i|X_i=4]\\)\n\n\\[\n\\begin{aligned}\n        E[Y_i|X_i=4] =& 1\\cdot Pr(Y_i=1|X_i=4)+2\\cdot Pr(Y_i=2|X_i=4) \\\\\n        =&1\\cdot 3/7+2\\cdot 4/7 \\\\\n        =&11/7\n    \\end{aligned}\n\\]\n\n\\(E\\big[E[Y_i|X_i]\\big]\\)\n\n\\[\n\\begin{aligned}\n        E\\big[E[Y_i|X_i]\\big] =& E[Y_i|X_i=3]\\cdot Pr(X_i=3)+ E[Y_i|X_i=4]\\cdot Pr(X_i=4) \\\\\n        =&5/3\\cdot3/10+11/7\\cdot 7/10 \\\\\n        =&16/10\n    \\end{aligned}\n\\]\nWe have therefore demonstrated the law of iterated expectations.\n\nWe can extend this principle to conditional expectations. Suppose you have three random variables/vectors \\(\\{Y_i,X_i,Z_i\\}\\), we can express the conditional expected value of \\(Y_i\\) on \\(X_i\\) as the (conditional) expected value of the conditional expectation of \\(Y_i\\) on \\(X_i\\) and \\(Z_i\\).\n\\[\n        E[Y_i|X_i] = E\\big[E[Y_i|X_i,Z_i]|X_i\\big]\n\\]\nHere the outside expectation is with respect \\(Z_i\\) conditional on \\(X_i\\). It utilizes the conditional distribution \\(f_{Z|X}\\) to form the outside expectation,\n\\[\nE[Y_i|X_i] = \\int y \\cdot f_{Y|X}(y|X_i)dt = \\int\\int y \\cdot f_{Y|X,Z}(y|X_i,z)dyf_{Z|X}(z|X_i)dz = E\\big[E[Y_i|X_i,Z_i]|X_i\\big]\n\\]\n\n\n\nThe following three theorems can be found in a range of Econometrics textbooks and Microeconometrics texts, including MM & MHE\n\nTheorem 1 We can express the observed outcome \\(Y_i\\) as a sum of \\(E[Y_i|X_i]+\\varepsilon_i\\) where \\(E[\\varepsilon_i|X_i]=0\\) (i.e., mean independent).\n\n\nProof. \n\n\\(E[\\varepsilon_i | X_i] = E[Y_i - E[Y_i | X_i] | X_i] = E[Y_i | X_i] - E[Y_i | X_i] = 0\\)\n\\(E[h(X_i)\\varepsilon_i] = E[h(X_i)E[\\varepsilon_i | X_i]] = E[h(X_i) \\times 0] = 0\\)\n\n\n\nTheorem 2 \\(E[Y_i|X_i]\\) is the best predictor of \\(Y_i\\).\n\n\nProof. \\[\n\\begin{aligned}\n(Y_i - m(X_i))^2 =& \\left((Y_i - E[Y_i | X_i]) + (E[Y_i | X_i] - m(X_i))\\right)^2 \\\\\n=& (Y_i - E[Y_i \\| X_i])^2 + (E[Y_i | X_i] - m(X_i))^2 \\\\&+ 2(Y_i - E[Y_i | X_i]) \\times (E[Y_i | X_i] - m(X_i))\n\\end{aligned}\n\\]\nThe last term (cross product) is mean zero. Thus, the function is minimized by setting \\(m(X_i) = E[Y_i | X_i]\\).\n\n\nTheorem 3 [ANOVA Theorem] The variance of \\(Y_i\\) can be decomposed as \\(V(E[Y_i|X_i])+E(V(Y_i|X_i))\\)\n\n\nProof. \\[\n\\begin{aligned}\n        V(Y_i)=&V(E[Y_i|X_i] + \\varepsilon_i) \\\\\n        =&V(E[Y_i|X_i])+V(\\varepsilon_i) \\\\\n        =&V(E[Y_i|X_i])+E[\\varepsilon_i^2]\n    \\end{aligned}\n\\] The second line follows from Theorem 1.1 (independence) and\n\\[\n        E[\\varepsilon_i^2]=E\\left[E[\\varepsilon_i^2|X_i]\\right]=E\\left[V(Y_i|X_i)\\right]\n\\]"
  },
  {
    "objectID": "material-cef.html#definition",
    "href": "material-cef.html#definition",
    "title": "Conditional Expectation Function",
    "section": "",
    "text": "The Conditional Expecation Function (CEF) - denoted \\(E[Y_i|X_i]\\) - is a random function. It is a function that returns the expected value of \\(Y_i\\) for each realized value of \\(X_i\\). Since \\(X_i\\) is a random vector the resulting function is random itself.\nIf we fix \\(X_i=x\\), then the value at which we are evaluating the function is no longer random. The result is a constant: the expected value of \\(Y_i\\) at the given \\(x\\).\n\\[\nE[Y_i|X_i=x] = \\int y \\cdot f_{Y}(y|X_i=x)dy = \\int y dF_{Y}(y|X_i=x)\n\\]\nThis follows the same logic that the expectation of a random variable is, \\(E[Y_i]\\), is not random.\nDiscrete case. The book devotes a lot time to the discussion of cases were \\(X_i\\) is a discrete random variable; using the notation \\(W_i\\in\\{0,1\\}\\) or \\(D_i\\in\\{0,1\\}\\). In this unique case, we can write the CEF as,\n\\[\nE[Y_i|D_i] = E[Y_i|D_i=0] + D_i\\cdot\\big(E[Y_i|D_i=1]-E[Y_i|D_i=0]\\big)\n\\]\nThe above function returns \\(E[Y_i|D_i=0]\\) when \\(D_i=0\\) and \\(E[Y_i|D_i=1]\\) when \\(D_i=1\\). This expression for the CEF will be useful in latter chapters of the book."
  },
  {
    "objectID": "material-cef.html#law-of-iterated-expectations",
    "href": "material-cef.html#law-of-iterated-expectations",
    "title": "Conditional Expectation Function",
    "section": "",
    "text": "The Law of Iterated Expectations says that given two random variables2 \\([Y_i,X_i]\\), we can express the unconditional expected value of \\(Y_i\\) as the expected value of the conditional expectation of \\(Y_i\\) on \\(X_i\\).\n\\[\n        E[Y_i] = E\\big[E[Y_i|X_i]\\big]\n\\]\nWhere the outside expectation is with respect to \\(X_i\\),3 since the CEF is a random function of \\(X_i\\). We can expand this as follows,\n\\[\nE[Y_i] = \\int t \\cdot f_{Y_i}(t)dt = \\int\\int y \\cdot f_{Y_i|X}(y|x)dyf_X(x)dx = E\\big[E[Y_i|X_i]\\big]\n\\]\n\nExample 1 Suppose \\(Y_i\\) and \\(X_i\\) are both discrete, \\(Y_i\\in\\{1,2\\}\\) and \\(X_i\\in\\{3,4\\}\\), with the joint distribution:\n\n\\(f_{Y,X}\\)\n\n\n\n\\(X_i=3\\)\n\\(X_i=2\\)\n\n\n\\(Y_i=1\\)\n1/10\n3/10\n\n\n\\(Y_i=2\\)\n2/10\n4/10\n\n\n\nWe can then define the two marginal distributions,\n\n\\(f_Y\\)\n\n\n\\(Y_i=1\\)\n\\(Y_i=2\\)\n\n\n4/10\n6/10\n\n\n\nand,\n\n\\(f_X\\)\n\n\n\\(X_i=3\\)\n\\(X_i=4\\)\n\n\n3/10\n7/10\n\n\n\nLikewise, we know the conditional distribution \\(f_{Y|X}\\); which we get by dividing the joint distribution by the marginal distribution of \\(X_i\\). Each column of the conditional distribution should add up to 1.\n\n\\(f_{Y|X}\\)\n\n\n\n\\(X_i=3\\)\n\\(X_i=4\\)\n\n\n\\(Y_i=1\\)\n1/3\n3/7\n\n\n\\(Y_i=2\\)\n2/3\n4/7\n\n\n\nNow we can calculate the following objects:\n\n\\(E[Y_i]\\)\n\n\\[\n\\begin{aligned}\n        E[Y_i] =& 1\\cdot Pr(Y_i=1)+2\\cdot Pr(Y_i=2) \\\\\n        =&1\\cdot 4/10+2\\cdot 6/10 \\\\\n        =&16/10\n    \\end{aligned}\n\\]\n\n\\(E[Y_i|X_i=3]\\)\n\n\\[\n\\begin{aligned}\n        E[Y_i|X_i=3] =& 1\\cdot Pr(Y_i=1|X_i=3)+2\\cdot Pr(Y_i=2|X_i=3) \\\\\n        =&1\\cdot 1/3+2\\cdot 2/3 \\\\\n        =&5/3\n\\end{aligned}\n\\]\n\n\\(E[Y_i|X_i=4]\\)\n\n\\[\n\\begin{aligned}\n        E[Y_i|X_i=4] =& 1\\cdot Pr(Y_i=1|X_i=4)+2\\cdot Pr(Y_i=2|X_i=4) \\\\\n        =&1\\cdot 3/7+2\\cdot 4/7 \\\\\n        =&11/7\n    \\end{aligned}\n\\]\n\n\\(E\\big[E[Y_i|X_i]\\big]\\)\n\n\\[\n\\begin{aligned}\n        E\\big[E[Y_i|X_i]\\big] =& E[Y_i|X_i=3]\\cdot Pr(X_i=3)+ E[Y_i|X_i=4]\\cdot Pr(X_i=4) \\\\\n        =&5/3\\cdot3/10+11/7\\cdot 7/10 \\\\\n        =&16/10\n    \\end{aligned}\n\\]\nWe have therefore demonstrated the law of iterated expectations.\n\nWe can extend this principle to conditional expectations. Suppose you have three random variables/vectors \\(\\{Y_i,X_i,Z_i\\}\\), we can express the conditional expected value of \\(Y_i\\) on \\(X_i\\) as the (conditional) expected value of the conditional expectation of \\(Y_i\\) on \\(X_i\\) and \\(Z_i\\).\n\\[\n        E[Y_i|X_i] = E\\big[E[Y_i|X_i,Z_i]|X_i\\big]\n\\]\nHere the outside expectation is with respect \\(Z_i\\) conditional on \\(X_i\\). It utilizes the conditional distribution \\(f_{Z|X}\\) to form the outside expectation,\n\\[\nE[Y_i|X_i] = \\int y \\cdot f_{Y|X}(y|X_i)dt = \\int\\int y \\cdot f_{Y|X,Z}(y|X_i,z)dyf_{Z|X}(z|X_i)dz = E\\big[E[Y_i|X_i,Z_i]|X_i\\big]\n\\]"
  },
  {
    "objectID": "material-cef.html#properties-of-the-cef",
    "href": "material-cef.html#properties-of-the-cef",
    "title": "Conditional Expectation Function",
    "section": "",
    "text": "The following three theorems can be found in a range of Econometrics textbooks and Microeconometrics texts, including MM & MHE\n\nTheorem 1 We can express the observed outcome \\(Y_i\\) as a sum of \\(E[Y_i|X_i]+\\varepsilon_i\\) where \\(E[\\varepsilon_i|X_i]=0\\) (i.e., mean independent).\n\n\nProof. \n\n\\(E[\\varepsilon_i | X_i] = E[Y_i - E[Y_i | X_i] | X_i] = E[Y_i | X_i] - E[Y_i | X_i] = 0\\)\n\\(E[h(X_i)\\varepsilon_i] = E[h(X_i)E[\\varepsilon_i | X_i]] = E[h(X_i) \\times 0] = 0\\)\n\n\n\nTheorem 2 \\(E[Y_i|X_i]\\) is the best predictor of \\(Y_i\\).\n\n\nProof. \\[\n\\begin{aligned}\n(Y_i - m(X_i))^2 =& \\left((Y_i - E[Y_i | X_i]) + (E[Y_i | X_i] - m(X_i))\\right)^2 \\\\\n=& (Y_i - E[Y_i \\| X_i])^2 + (E[Y_i | X_i] - m(X_i))^2 \\\\&+ 2(Y_i - E[Y_i | X_i]) \\times (E[Y_i | X_i] - m(X_i))\n\\end{aligned}\n\\]\nThe last term (cross product) is mean zero. Thus, the function is minimized by setting \\(m(X_i) = E[Y_i | X_i]\\).\n\n\nTheorem 3 [ANOVA Theorem] The variance of \\(Y_i\\) can be decomposed as \\(V(E[Y_i|X_i])+E(V(Y_i|X_i))\\)\n\n\nProof. \\[\n\\begin{aligned}\n        V(Y_i)=&V(E[Y_i|X_i] + \\varepsilon_i) \\\\\n        =&V(E[Y_i|X_i])+V(\\varepsilon_i) \\\\\n        =&V(E[Y_i|X_i])+E[\\varepsilon_i^2]\n    \\end{aligned}\n\\] The second line follows from Theorem 1.1 (independence) and\n\\[\n        E[\\varepsilon_i^2]=E\\left[E[\\varepsilon_i^2|X_i]\\right]=E\\left[V(Y_i|X_i)\\right]\n\\]"
  },
  {
    "objectID": "material-cef.html#footnotes",
    "href": "material-cef.html#footnotes",
    "title": "Conditional Expectation Function",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe subscript \\(i\\) is not necessary here. However, this notation is consistent with the rest of the book. In this book, \\(Y_i\\) denotes a random variable, \\(\\in \\mathbb{R}\\), and \\(Y\\) a random vector, \\(\\in \\mathbb{R}^n\\). Likewise, \\(X_i\\) is a random vector, \\(\\in \\mathbb{R}^k\\), while \\(X\\) will represent a random matrix, \\(\\in \\mathbb{R}^n \\times \\mathbb{R}^k\\).↩︎\nThis can be extended to random vectors.↩︎\nSome texts use the notation \\(E_X\\big[E[Y_i|X_i]\\big]\\) to demonstrate that the outside expectation is with respect to \\(X_i\\).↩︎"
  },
  {
    "objectID": "material-linearalgebra.html",
    "href": "material-linearalgebra.html",
    "title": "Linear Algebra",
    "section": "",
    "text": "Consider a set of \\(k\\) \\(n\\)-dimensional vectors \\(\\{X_{1},X_{2},...,X_{k}\\}\\). These vectors are,\n\nDefinition 1 linearly dependent if there exists a set of scalars \\(\\{a_{1},a_{2},\\dots,a_{k}\\}\\) such that\n\\[\na_1X_1 + a_2X_2+\\ldots+a_kX_k=0\n\\]\nwhere at least one \\(a_i\\neq0\\).\n\nAlternatively, they are,\n\nDefinition 2 linearly independent if the only set of scalars \\(\\{a_{1},a_{2},\\dots,a_{k}\\}\\) that satisfies the above condition is \\(a_1,a_2,\\dots,a_k=0\\).\n\nIf we collect these \\(k\\) column-vectors in a matrix, \\(X=[X_1\\;X_2 \\dots X_k]\\), then the linear dependence condition can be written as,\n\\[\na_1X_1 + a_2X_2+\\ldots+a_kX_k=\\begin{bmatrix} X_1\\;X_2 \\dots X_k\\end{bmatrix}\\begin{bmatrix}a_1\\\\a_2\\\\\\vdots\\\\a_k\\end{bmatrix}=Xa = 0\n\\]\nGiven any \\(n\\times k\\) matrix \\(X\\), its columns are,\n\nDefinition 3 linearly dependent if there exists a vector \\(a\\in\\mathbb{R}^k\\) such that \\(a\\neq0\\) and \\(Xa=0\\);\n\nor,\n\nDefinition 4 linearly independent if the only vector \\(a\\in\\mathbb{R}^k\\) such that \\(Xa=0\\) is \\(a=0\\).\n\nFor any matrix there may be more than one vector \\(a\\in\\mathbb{R}^{k}\\) such that \\(Xa=0\\). Indeed, if both \\(a_{1},a_{2}\\in\\mathbb{R}^{k}\\)\nsatisfy this condition and \\(a_{1}\\neq a_{2}\\) then you can show that any linear combination of \\(\\{a_{1},a_{2}\\}\\) satisfies the\ncondition \\(X(a_{1}b_{1}+a_{2}b_{2})=0\\) for \\(b_{1},b_{2}\\in\\mathbb{R}\\). Thus, there exists an entire set of vectors which satisfy this condition. This set is referred to as the,\n\nDefinition 5 null space of \\(X\\), \\[\n\\mathcal{N}(X) = \\{a\\in\\mathbb{R}^k:\\;Xa=0\\}\n\\]\n\nIt should be evident from the definition that if the columns of \\(X\\) are linearly independent then \\(\\mathcal{N}(X)=\\{0\\}\\), a singleton. That is, it just includes the 0-vector.\n\n\n\nHere, we concern ourselves only with real vectors from \\(\\mathbb{R}^n\\).\n\nDefinition 6 A vector space, denoted \\(\\mathcal{V}\\), refers to a set of vectors which is closed under finite addition and scalar multiplication.\n\n\nDefinition 7 A set of \\(k\\) linearly independent vectors, \\(\\{X_1,X_2,\\dots,X_k\\}\\), forms a basis for vector space \\(\\mathcal{V}\\) if \\(\\forall\\;y\\in\\mathcal{V}\\) there exists a set of \\(k\\) scalars such that, \\[\ny=X_1b_1+X_2b_2+\\ldots+X_kb_k\n\\]\n\nBased on these definitions, it is evident that for the Euclidean space, \\(\\mathbb{E}^n\\), any \\(n\\) linearly independent vectors from \\(\\mathbb{R}^n\\) is a basis. For example, any point in \\(\\mathbb{E}^2\\) can be defined as a multiple of,\n\\[\n\\begin{bmatrix}1\\\\0\\end{bmatrix}\\quad \\text{and} \\quad\\begin{bmatrix}0\\\\1\\end{bmatrix}\n\\]\nConsider again the \\(n\\times k\\) matrix \\(X\\), where \\(k&lt;n\\). Then we define the,\n\nDefinition 8 column space (or span) of \\(X\\), denoted \\(\\mathcal{S}(X)\\), as the vector space generate by the \\(k\\) columns of \\(X\\). Formally, \\[\n\\mathcal{S}(X) = \\{y\\in\\mathbb{R}^n:\\;y=Xb\\quad\\text{for some }b\\in \\mathbb{R}^k\\}\n\\]\n\nA property to note about the span or column space \\(X\\) is,\nResult: \\(\\mathcal{S}(X)=\\mathcal{S}(XX')\\) :::\nwhere \\(XX'\\) is a \\(n\\times n\\) matrix.\nFinally, we can define the,\n\nDefinition 9 orthogonal column space (or orthogonal span) of \\(X\\) as, \\[\n\\mathcal{S}^{\\perp}(X) = \\{y\\in \\mathbb{R}^k:\\;y'x=0\\quad \\forall x\\in\\mathcal{S}(X)\\}\n\\]\n\n\n\n\nConsider a \\(n\\times k\\) matrix \\(X\\), the\n\nDefinition 10 row rank of \\(X\\) is the maximum number of linearly independent rows: \\[\nrowrank(X) \\leq n\n\\]\n\nWe say that matrix \\(X\\) has full row rank if \\(rowrank(X)=n\\).\nThe,\n\nDefinition 11 column rank of \\(X\\) is the maximum number of linearly independent columns:\n\\[\ncolrank(X) \\leq k\n\\]\n\nWe say that matrix \\(X\\) has full column rank if \\(colrank(X)=k\\).\nAn important result is,\n\nResult: the rank of \\(X\\): \\[\nr(X) = rowrank(X)=colrank(X) \\\\\n\\Rightarrow r(X)\\leq min\\{n,k\\}\n\\]\n\nIn addition, since the \\(r(X)\\) depends on the number of linearly independent columns, we can say that,\n\nResult: the dimension of \\(\\mathcal{S}(X)\\), \\(dim(\\mathcal{S}(X))\\), is given by the \\(r(X)\\).\n\nHere are a few additional results,\n\nResult: \\(r(X)=r(X')\\)\nResult: \\(r(XY)\\leq min\\{r(X),r(Y)\\}\\)\nResult: \\(r(XY)=r(X)\\) if \\(Y\\) is square and full rank\nResult: \\(r(X+Y)\\leq r(X) + r(Y)\\)\n\n\n\n\nConsider the case of a square, \\(n\\times n\\), matrix \\(A\\). We say that,\n\nDefinition 12 \\(A\\) is singular if the \\(r(A)&lt;n\\),\n\nor that,\n\nDefinition 13 \\(A\\) is non-singular if the \\(r(A)=n\\).\n\nThe singularity of a square matrix is important as it determines the invertibility of a matrix, which typically relates the existence of a unique solution in systems of linear equations. Here are a few key results,\n\nResult: There exists a matrix \\(B=A^{-1}\\), such that \\(AB=I_n\\) (where \\(I_n\\) is the identity matrix), if and only if \\(A\\) is non-singular.\nResult: \\(A\\) is non-singular if and only if the determinant of \\(A\\) is non-zero: \\(det(A)\\neq0\\).1\nResult: Likewise, \\(A\\) is singular if and only if \\(det(A)=0\\).\nResult: \\(AA^{-1}=A^{-1}A=I\\)\nResult: \\((A')^{-1}=(A^{-1})'\\)\nResult: If their respective inverses exist, then \\((AB)^{-1}=B^{-1}A^{-1}\\).\nResult: \\(det(AB)=det(A)det(B)\\)\nResult: \\(det(A^{-1})=det(A)^{-1}\\)\n\nFor any square matrix \\(A\\),\n\nDefinition 14 the trace of \\(A\\) is the sum of all diagonal elements: \\[\ntr(A) = \\sum_{i=1}^na_{ii}\n\\]\n\nRegarding the trace of a square matrix, here are a few important results:\n\nResult: \\(tr(A+B) = tr(A) + tr(B)\\)\nResult: \\(tr(\\lambda A) = \\lambda tr(A)\\) where \\(\\lambda\\) is a scalar\nResult: \\(tr(A) = tr(A')\\)\nResult: \\(tr(AB) = tr(BA)\\) where \\(AB\\) and \\(BA\\) are both square, but need not be of the same order.\nResult: \\(||A|| = (tr(A'A))^{1/2}\\)\n\n\n\n\nA symmetric matrix has the property that \\(A=A'\\). Therefore, \\(A\\) must be square.\nHere are a few important results concerning symmetric matrices.\n\nResult: \\(A^{-1}\\) exists if \\(det(A)\\neq 0\\) and \\(r(A)=n\\)\nResult: A is diagonalizable.2\nResult: The eigenvector decomposition of a square matrix gives you \\(A=C\\Lambda C^{-1}\\) where \\(\\Lambda\\) is a diagonal matrix of eigenvalues and $C$ a matrix of the corresponding eigenvectors. The symmetry of \\(A\\) gives you that \\(C^{-1}=C'\\Rightarrow A=C\\Lambda C'\\) with \\(C'C=CC'=I_{n}\\).3\n\nA key definition concerning symmetric matrices is their positive definiteness:\n\nDefinition 15 \\(A\\) is positive semi-definite if for any \\(x\\in\\mathbb{R}^n,\\;x'Ax\\geq0\\).\n\nGiven the eigenvector decomposition of a symmetric matrix, positive semi-definiteness implies \\(\\Lambda\\) is positive semi-definite: \\(\\lambda_i\\geq0\\quad\\forall i\\). Likewise,\n\nDefinition 16 \\(A\\) is positive definite if for any \\(x\\in\\mathbb{R}^n,\\;x'Ax&gt;0\\).\n\nAgain, based on the egeinvector decomposition, positive semi-definiteness implies \\(\\Lambda\\) is positive definite: \\(\\lambda_i&gt;0\\quad\\forall i\\).\nA few more results are:\n\nResult: \\(tr(A) = \\sum_{i=1}^n\\lambda_i\\)\nResult: \\(r(A) = r(\\Lambda)\\)\nResult: \\(det(A) = \\prod_{i=1}^n \\lambda_i\\)\n\nThis last result can be used to prove that any positive definite matrix is non-singular and therefore has an inverse.\nAny full-rank, positive semi-definite, symmetric matrix \\(B\\) has the additional properties:\n\nResult: \\(B=C\\Lambda C'\\) and \\(B^{-1} = C\\Lambda^{-1}C'\\)\nResult: We can define the square-root of \\(B\\) as \\(B^{1/2} = C\\Lambda^{1/2}C'\\). Similarly, \\(B^{-1/2} = C\\Lambda^{-1/2}C'\\).\n\n\n\n\nAn idempotent matrix has the property that \\(D=DD\\). Therefore, \\(D\\) must be square.\nHere are a few important results concerning idempotent matrices.\n\nResult: \\(D\\) is positive definite\nResult: \\(D\\) is diagonalizable\nResult: \\((I_n-D)\\) is also an idempotent matrix\nResult: With the exception of \\(I_n\\), all idempotent matrices are singular.\nResult: \\(r(D) = tr(D) = \\sum_{i=1}^n\\lambda_i\\)\nResult: \\(\\lambda_i\\in\\{0,1\\}\\quad \\forall\\;i\\)\n\nProjection matrices are idempotent, but need not be symmetric. However, for the purposes of this module we will deal exclusively with symmetric idempotent projection matrices.\n\n\n\nHere we will look at the derivatives of scalar with respect to (W.r.t.) a vector. You can also define other derivatives, such as the derivative of a vector w.r.t. a vector and the derivative of a scalar with respect to a matrix. However, these are not needed for these notes.\n\n\nSuppose \\(f(x)\\in R\\) (i.e. a scalar) and \\(x\\in R^n\\) (i.e. a \\(n\\times 1\\) vector). Then we can define the partial derivative of \\(f(x)\\) w.r.t. \\(x\\) as,\n\\[\n  \\frac{\\partial f(x)}{\\partial x}  = \\begin{bmatrix}\\frac{\\partial f(x)}{\\partial x_1} \\\\ \\frac{\\partial f(x)}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial f(x)}{\\partial x_n} \\end{bmatrix}\n\\]\n\n\n\nA special case is when \\(f(x)\\) is linear in \\(x\\),\n\\[\nf(x) = a'x = \\sum_{i=1}^n a_ix_i\n\\] for \\(a\\in R^n\\). The derivative of \\(a'x\\) with respect to the vector \\(x\\) can be defined as,\n\\[\n  \\begin{aligned}\n  \\frac{\\partial a'x}{\\partial x} & = \\begin{bmatrix}\\frac{\\partial a'x}{\\partial x_1} \\\\ \\frac{\\partial a'x}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial a'x}{\\partial x_n} \\end{bmatrix} \\\\\n  & = \\begin{bmatrix}a_1 \\\\ a_2 \\\\ \\vdots \\\\ a_n \\end{bmatrix} \\\\\n  & = a\n  \\end{aligned}\n\\] since the the partial derivate of \\(a'x = \\sum_{i=1}^n a_ix_i\\) w.r.t. \\(x_i\\) is just the scalar \\(a_i\\).\nThe derivative of a scalar w.r.t. to a vector yields a vector of partial derivatives.\nSince \\(a'x\\) is a scalar, it is by definition symmetric: \\(a'x = x'a\\). Thus,\n\\[\n\\frac{\\partial x'a}{\\partial x} = \\frac{\\partial a'x}{\\partial x} = a\n\\]\n\n\n\nSuppose \\(f(x)\\) is a linear transformation of \\(x\\),\n\\[\nf(x) = A'x\n\\] for any \\(m\\times n\\) matrix A,\n\\[\n  A = \\begin{bmatrix}a_1' \\\\ a_2' \\\\ \\vdots \\\\ a_m'\\end{bmatrix}\n\\] where \\(a_i\\in R^n\\;\\forall i=1,\\dots,m\\) and,\n\\[\n  Ax = \\begin{bmatrix}a_1'x \\\\ a_2'x \\\\ \\vdots \\\\ a_m'x\\end{bmatrix}\n\\] Note, \\(f(x)=Ax\\in R^m\\), a \\(m\\times 1\\) vector. We can then define,\n\\[\n  \\begin{aligned}\n  \\frac{\\partial Ax}{\\partial x'} & = \\begin{bmatrix}\\frac{\\partial a_1'x}{\\partial x_1} & \\frac{\\partial a_1'x}{\\partial x_2} & \\dots & \\frac{\\partial a_1'x}{\\partial x_n}\\\\ \\frac{\\partial a_2'x}{\\partial x_1} & \\frac{\\partial a_2'x}{\\partial x_2} & \\dots & \\frac{\\partial a_2'x}{\\partial x_n}\\\\ \\vdots & \\vdots & \\ddots & \\vdots\\\\ \\frac{\\partial a_m'x}{\\partial x_1} & \\frac{\\partial a_m'x}{\\partial x_2} & \\dots & \\frac{\\partial a_m'x}{\\partial x_n}\\\\ \\end{bmatrix} \\\\\n  & = \\begin{bmatrix} a_{11} & a_{12} & \\dots & a_{1n} \\\\ a_{21} & a_{22} & \\dots & a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots\\\\ a_{m1} & a_{m2} & \\dots & a_{mn} \\\\ \\end{bmatrix} \\\\\n  & = A\n  \\end{aligned}\n\\]\nSince Ax is \\(m\\times 1\\) column vector, we take the derivative w.r.t. \\(x'\\) a row vector and not the column vector \\(x\\). This results in a matrix of partial derivatives.\n\n\n\nA second special case is where the function takes on the quadaratic form,\n\\[\nf(x) = x'Ax = \\sum_{i=1}^N\\sum_{j=1}^n a_{ij}x_ix_j\n\\]\nfor \\(n\\times n\\) (square) matrix A. As in the first linear case, \\(f(x)\\) is scalar.\nDefine \\(c = Ax\\), the \\(x'Ax = x'c\\). From the linear case, we know that,\n\\[\n\\frac{\\partial x'c}{\\partial x} = c\n\\]\nSimilarly, if we define \\(d = A'x\\) then \\(x'Ax = d'x\\). From the linear case, we know that,\n\\[\n\\frac{\\partial d'x}{\\partial x} = d\n\\]\nWe can define the total derivative as the sum of the partial derivatives w.r.t. to the first and second \\(x\\). Combining these two results, we have that,\n\\[\n\\frac{\\partial x'Ax}{\\partial x} = Ax + A'x\n\\]\nAnd if \\(A\\) is symmetric, this result simplifies to \\(2Ax\\)."
  },
  {
    "objectID": "material-linearalgebra.html#linear-dependence",
    "href": "material-linearalgebra.html#linear-dependence",
    "title": "Linear Algebra",
    "section": "",
    "text": "Consider a set of \\(k\\) \\(n\\)-dimensional vectors \\(\\{X_{1},X_{2},...,X_{k}\\}\\). These vectors are,\n\nDefinition 1 linearly dependent if there exists a set of scalars \\(\\{a_{1},a_{2},\\dots,a_{k}\\}\\) such that\n\\[\na_1X_1 + a_2X_2+\\ldots+a_kX_k=0\n\\]\nwhere at least one \\(a_i\\neq0\\).\n\nAlternatively, they are,\n\nDefinition 2 linearly independent if the only set of scalars \\(\\{a_{1},a_{2},\\dots,a_{k}\\}\\) that satisfies the above condition is \\(a_1,a_2,\\dots,a_k=0\\).\n\nIf we collect these \\(k\\) column-vectors in a matrix, \\(X=[X_1\\;X_2 \\dots X_k]\\), then the linear dependence condition can be written as,\n\\[\na_1X_1 + a_2X_2+\\ldots+a_kX_k=\\begin{bmatrix} X_1\\;X_2 \\dots X_k\\end{bmatrix}\\begin{bmatrix}a_1\\\\a_2\\\\\\vdots\\\\a_k\\end{bmatrix}=Xa = 0\n\\]\nGiven any \\(n\\times k\\) matrix \\(X\\), its columns are,\n\nDefinition 3 linearly dependent if there exists a vector \\(a\\in\\mathbb{R}^k\\) such that \\(a\\neq0\\) and \\(Xa=0\\);\n\nor,\n\nDefinition 4 linearly independent if the only vector \\(a\\in\\mathbb{R}^k\\) such that \\(Xa=0\\) is \\(a=0\\).\n\nFor any matrix there may be more than one vector \\(a\\in\\mathbb{R}^{k}\\) such that \\(Xa=0\\). Indeed, if both \\(a_{1},a_{2}\\in\\mathbb{R}^{k}\\)\nsatisfy this condition and \\(a_{1}\\neq a_{2}\\) then you can show that any linear combination of \\(\\{a_{1},a_{2}\\}\\) satisfies the\ncondition \\(X(a_{1}b_{1}+a_{2}b_{2})=0\\) for \\(b_{1},b_{2}\\in\\mathbb{R}\\). Thus, there exists an entire set of vectors which satisfy this condition. This set is referred to as the,\n\nDefinition 5 null space of \\(X\\), \\[\n\\mathcal{N}(X) = \\{a\\in\\mathbb{R}^k:\\;Xa=0\\}\n\\]\n\nIt should be evident from the definition that if the columns of \\(X\\) are linearly independent then \\(\\mathcal{N}(X)=\\{0\\}\\), a singleton. That is, it just includes the 0-vector."
  },
  {
    "objectID": "material-linearalgebra.html#vector-spaces-bases-and-spans",
    "href": "material-linearalgebra.html#vector-spaces-bases-and-spans",
    "title": "Linear Algebra",
    "section": "",
    "text": "Here, we concern ourselves only with real vectors from \\(\\mathbb{R}^n\\).\n\nDefinition 6 A vector space, denoted \\(\\mathcal{V}\\), refers to a set of vectors which is closed under finite addition and scalar multiplication.\n\n\nDefinition 7 A set of \\(k\\) linearly independent vectors, \\(\\{X_1,X_2,\\dots,X_k\\}\\), forms a basis for vector space \\(\\mathcal{V}\\) if \\(\\forall\\;y\\in\\mathcal{V}\\) there exists a set of \\(k\\) scalars such that, \\[\ny=X_1b_1+X_2b_2+\\ldots+X_kb_k\n\\]\n\nBased on these definitions, it is evident that for the Euclidean space, \\(\\mathbb{E}^n\\), any \\(n\\) linearly independent vectors from \\(\\mathbb{R}^n\\) is a basis. For example, any point in \\(\\mathbb{E}^2\\) can be defined as a multiple of,\n\\[\n\\begin{bmatrix}1\\\\0\\end{bmatrix}\\quad \\text{and} \\quad\\begin{bmatrix}0\\\\1\\end{bmatrix}\n\\]\nConsider again the \\(n\\times k\\) matrix \\(X\\), where \\(k&lt;n\\). Then we define the,\n\nDefinition 8 column space (or span) of \\(X\\), denoted \\(\\mathcal{S}(X)\\), as the vector space generate by the \\(k\\) columns of \\(X\\). Formally, \\[\n\\mathcal{S}(X) = \\{y\\in\\mathbb{R}^n:\\;y=Xb\\quad\\text{for some }b\\in \\mathbb{R}^k\\}\n\\]\n\nA property to note about the span or column space \\(X\\) is,\nResult: \\(\\mathcal{S}(X)=\\mathcal{S}(XX')\\) :::\nwhere \\(XX'\\) is a \\(n\\times n\\) matrix.\nFinally, we can define the,\n\nDefinition 9 orthogonal column space (or orthogonal span) of \\(X\\) as, \\[\n\\mathcal{S}^{\\perp}(X) = \\{y\\in \\mathbb{R}^k:\\;y'x=0\\quad \\forall x\\in\\mathcal{S}(X)\\}\n\\]"
  },
  {
    "objectID": "material-linearalgebra.html#rank",
    "href": "material-linearalgebra.html#rank",
    "title": "Linear Algebra",
    "section": "",
    "text": "Consider a \\(n\\times k\\) matrix \\(X\\), the\n\nDefinition 10 row rank of \\(X\\) is the maximum number of linearly independent rows: \\[\nrowrank(X) \\leq n\n\\]\n\nWe say that matrix \\(X\\) has full row rank if \\(rowrank(X)=n\\).\nThe,\n\nDefinition 11 column rank of \\(X\\) is the maximum number of linearly independent columns:\n\\[\ncolrank(X) \\leq k\n\\]\n\nWe say that matrix \\(X\\) has full column rank if \\(colrank(X)=k\\).\nAn important result is,\n\nResult: the rank of \\(X\\): \\[\nr(X) = rowrank(X)=colrank(X) \\\\\n\\Rightarrow r(X)\\leq min\\{n,k\\}\n\\]\n\nIn addition, since the \\(r(X)\\) depends on the number of linearly independent columns, we can say that,\n\nResult: the dimension of \\(\\mathcal{S}(X)\\), \\(dim(\\mathcal{S}(X))\\), is given by the \\(r(X)\\).\n\nHere are a few additional results,\n\nResult: \\(r(X)=r(X')\\)\nResult: \\(r(XY)\\leq min\\{r(X),r(Y)\\}\\)\nResult: \\(r(XY)=r(X)\\) if \\(Y\\) is square and full rank\nResult: \\(r(X+Y)\\leq r(X) + r(Y)\\)"
  },
  {
    "objectID": "material-linearalgebra.html#properties-of-square-matrices",
    "href": "material-linearalgebra.html#properties-of-square-matrices",
    "title": "Linear Algebra",
    "section": "",
    "text": "Consider the case of a square, \\(n\\times n\\), matrix \\(A\\). We say that,\n\nDefinition 12 \\(A\\) is singular if the \\(r(A)&lt;n\\),\n\nor that,\n\nDefinition 13 \\(A\\) is non-singular if the \\(r(A)=n\\).\n\nThe singularity of a square matrix is important as it determines the invertibility of a matrix, which typically relates the existence of a unique solution in systems of linear equations. Here are a few key results,\n\nResult: There exists a matrix \\(B=A^{-1}\\), such that \\(AB=I_n\\) (where \\(I_n\\) is the identity matrix), if and only if \\(A\\) is non-singular.\nResult: \\(A\\) is non-singular if and only if the determinant of \\(A\\) is non-zero: \\(det(A)\\neq0\\).1\nResult: Likewise, \\(A\\) is singular if and only if \\(det(A)=0\\).\nResult: \\(AA^{-1}=A^{-1}A=I\\)\nResult: \\((A')^{-1}=(A^{-1})'\\)\nResult: If their respective inverses exist, then \\((AB)^{-1}=B^{-1}A^{-1}\\).\nResult: \\(det(AB)=det(A)det(B)\\)\nResult: \\(det(A^{-1})=det(A)^{-1}\\)\n\nFor any square matrix \\(A\\),\n\nDefinition 14 the trace of \\(A\\) is the sum of all diagonal elements: \\[\ntr(A) = \\sum_{i=1}^na_{ii}\n\\]\n\nRegarding the trace of a square matrix, here are a few important results:\n\nResult: \\(tr(A+B) = tr(A) + tr(B)\\)\nResult: \\(tr(\\lambda A) = \\lambda tr(A)\\) where \\(\\lambda\\) is a scalar\nResult: \\(tr(A) = tr(A')\\)\nResult: \\(tr(AB) = tr(BA)\\) where \\(AB\\) and \\(BA\\) are both square, but need not be of the same order.\nResult: \\(||A|| = (tr(A'A))^{1/2}\\)"
  },
  {
    "objectID": "material-linearalgebra.html#properties-of-symmetric-matrices",
    "href": "material-linearalgebra.html#properties-of-symmetric-matrices",
    "title": "Linear Algebra",
    "section": "",
    "text": "A symmetric matrix has the property that \\(A=A'\\). Therefore, \\(A\\) must be square.\nHere are a few important results concerning symmetric matrices.\n\nResult: \\(A^{-1}\\) exists if \\(det(A)\\neq 0\\) and \\(r(A)=n\\)\nResult: A is diagonalizable.2\nResult: The eigenvector decomposition of a square matrix gives you \\(A=C\\Lambda C^{-1}\\) where \\(\\Lambda\\) is a diagonal matrix of eigenvalues and $C$ a matrix of the corresponding eigenvectors. The symmetry of \\(A\\) gives you that \\(C^{-1}=C'\\Rightarrow A=C\\Lambda C'\\) with \\(C'C=CC'=I_{n}\\).3\n\nA key definition concerning symmetric matrices is their positive definiteness:\n\nDefinition 15 \\(A\\) is positive semi-definite if for any \\(x\\in\\mathbb{R}^n,\\;x'Ax\\geq0\\).\n\nGiven the eigenvector decomposition of a symmetric matrix, positive semi-definiteness implies \\(\\Lambda\\) is positive semi-definite: \\(\\lambda_i\\geq0\\quad\\forall i\\). Likewise,\n\nDefinition 16 \\(A\\) is positive definite if for any \\(x\\in\\mathbb{R}^n,\\;x'Ax&gt;0\\).\n\nAgain, based on the egeinvector decomposition, positive semi-definiteness implies \\(\\Lambda\\) is positive definite: \\(\\lambda_i&gt;0\\quad\\forall i\\).\nA few more results are:\n\nResult: \\(tr(A) = \\sum_{i=1}^n\\lambda_i\\)\nResult: \\(r(A) = r(\\Lambda)\\)\nResult: \\(det(A) = \\prod_{i=1}^n \\lambda_i\\)\n\nThis last result can be used to prove that any positive definite matrix is non-singular and therefore has an inverse.\nAny full-rank, positive semi-definite, symmetric matrix \\(B\\) has the additional properties:\n\nResult: \\(B=C\\Lambda C'\\) and \\(B^{-1} = C\\Lambda^{-1}C'\\)\nResult: We can define the square-root of \\(B\\) as \\(B^{1/2} = C\\Lambda^{1/2}C'\\). Similarly, \\(B^{-1/2} = C\\Lambda^{-1/2}C'\\)."
  },
  {
    "objectID": "material-linearalgebra.html#properties-of-idempotent-matrices",
    "href": "material-linearalgebra.html#properties-of-idempotent-matrices",
    "title": "Linear Algebra",
    "section": "",
    "text": "An idempotent matrix has the property that \\(D=DD\\). Therefore, \\(D\\) must be square.\nHere are a few important results concerning idempotent matrices.\n\nResult: \\(D\\) is positive definite\nResult: \\(D\\) is diagonalizable\nResult: \\((I_n-D)\\) is also an idempotent matrix\nResult: With the exception of \\(I_n\\), all idempotent matrices are singular.\nResult: \\(r(D) = tr(D) = \\sum_{i=1}^n\\lambda_i\\)\nResult: \\(\\lambda_i\\in\\{0,1\\}\\quad \\forall\\;i\\)\n\nProjection matrices are idempotent, but need not be symmetric. However, for the purposes of this module we will deal exclusively with symmetric idempotent projection matrices."
  },
  {
    "objectID": "material-linearalgebra.html#vector-differentiation",
    "href": "material-linearalgebra.html#vector-differentiation",
    "title": "Linear Algebra",
    "section": "",
    "text": "Here we will look at the derivatives of scalar with respect to (W.r.t.) a vector. You can also define other derivatives, such as the derivative of a vector w.r.t. a vector and the derivative of a scalar with respect to a matrix. However, these are not needed for these notes.\n\n\nSuppose \\(f(x)\\in R\\) (i.e. a scalar) and \\(x\\in R^n\\) (i.e. a \\(n\\times 1\\) vector). Then we can define the partial derivative of \\(f(x)\\) w.r.t. \\(x\\) as,\n\\[\n  \\frac{\\partial f(x)}{\\partial x}  = \\begin{bmatrix}\\frac{\\partial f(x)}{\\partial x_1} \\\\ \\frac{\\partial f(x)}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial f(x)}{\\partial x_n} \\end{bmatrix}\n\\]\n\n\n\nA special case is when \\(f(x)\\) is linear in \\(x\\),\n\\[\nf(x) = a'x = \\sum_{i=1}^n a_ix_i\n\\] for \\(a\\in R^n\\). The derivative of \\(a'x\\) with respect to the vector \\(x\\) can be defined as,\n\\[\n  \\begin{aligned}\n  \\frac{\\partial a'x}{\\partial x} & = \\begin{bmatrix}\\frac{\\partial a'x}{\\partial x_1} \\\\ \\frac{\\partial a'x}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial a'x}{\\partial x_n} \\end{bmatrix} \\\\\n  & = \\begin{bmatrix}a_1 \\\\ a_2 \\\\ \\vdots \\\\ a_n \\end{bmatrix} \\\\\n  & = a\n  \\end{aligned}\n\\] since the the partial derivate of \\(a'x = \\sum_{i=1}^n a_ix_i\\) w.r.t. \\(x_i\\) is just the scalar \\(a_i\\).\nThe derivative of a scalar w.r.t. to a vector yields a vector of partial derivatives.\nSince \\(a'x\\) is a scalar, it is by definition symmetric: \\(a'x = x'a\\). Thus,\n\\[\n\\frac{\\partial x'a}{\\partial x} = \\frac{\\partial a'x}{\\partial x} = a\n\\]\n\n\n\nSuppose \\(f(x)\\) is a linear transformation of \\(x\\),\n\\[\nf(x) = A'x\n\\] for any \\(m\\times n\\) matrix A,\n\\[\n  A = \\begin{bmatrix}a_1' \\\\ a_2' \\\\ \\vdots \\\\ a_m'\\end{bmatrix}\n\\] where \\(a_i\\in R^n\\;\\forall i=1,\\dots,m\\) and,\n\\[\n  Ax = \\begin{bmatrix}a_1'x \\\\ a_2'x \\\\ \\vdots \\\\ a_m'x\\end{bmatrix}\n\\] Note, \\(f(x)=Ax\\in R^m\\), a \\(m\\times 1\\) vector. We can then define,\n\\[\n  \\begin{aligned}\n  \\frac{\\partial Ax}{\\partial x'} & = \\begin{bmatrix}\\frac{\\partial a_1'x}{\\partial x_1} & \\frac{\\partial a_1'x}{\\partial x_2} & \\dots & \\frac{\\partial a_1'x}{\\partial x_n}\\\\ \\frac{\\partial a_2'x}{\\partial x_1} & \\frac{\\partial a_2'x}{\\partial x_2} & \\dots & \\frac{\\partial a_2'x}{\\partial x_n}\\\\ \\vdots & \\vdots & \\ddots & \\vdots\\\\ \\frac{\\partial a_m'x}{\\partial x_1} & \\frac{\\partial a_m'x}{\\partial x_2} & \\dots & \\frac{\\partial a_m'x}{\\partial x_n}\\\\ \\end{bmatrix} \\\\\n  & = \\begin{bmatrix} a_{11} & a_{12} & \\dots & a_{1n} \\\\ a_{21} & a_{22} & \\dots & a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots\\\\ a_{m1} & a_{m2} & \\dots & a_{mn} \\\\ \\end{bmatrix} \\\\\n  & = A\n  \\end{aligned}\n\\]\nSince Ax is \\(m\\times 1\\) column vector, we take the derivative w.r.t. \\(x'\\) a row vector and not the column vector \\(x\\). This results in a matrix of partial derivatives.\n\n\n\nA second special case is where the function takes on the quadaratic form,\n\\[\nf(x) = x'Ax = \\sum_{i=1}^N\\sum_{j=1}^n a_{ij}x_ix_j\n\\]\nfor \\(n\\times n\\) (square) matrix A. As in the first linear case, \\(f(x)\\) is scalar.\nDefine \\(c = Ax\\), the \\(x'Ax = x'c\\). From the linear case, we know that,\n\\[\n\\frac{\\partial x'c}{\\partial x} = c\n\\]\nSimilarly, if we define \\(d = A'x\\) then \\(x'Ax = d'x\\). From the linear case, we know that,\n\\[\n\\frac{\\partial d'x}{\\partial x} = d\n\\]\nWe can define the total derivative as the sum of the partial derivatives w.r.t. to the first and second \\(x\\). Combining these two results, we have that,\n\\[\n\\frac{\\partial x'Ax}{\\partial x} = Ax + A'x\n\\]\nAnd if \\(A\\) is symmetric, this result simplifies to \\(2Ax\\)."
  },
  {
    "objectID": "material-linearalgebra.html#footnotes",
    "href": "material-linearalgebra.html#footnotes",
    "title": "Linear Algebra",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThese notes do not cover how to calculate the determinant of a square matrix. You should be able to find a definition easily online.↩︎\nA matrix is diagonalizable if it is similar to some other diagonal matrix. Matrices \\(B\\) and \\(C\\) are similar if \\(C=PBP^{-1}\\). A square matrix which is not diagonalizable is defective. This property relates closely to eigenvector decomposition.↩︎\nRecall, an eigenvalue and eigenvector pair, \\((\\lambda,c)\\), of matrix \\(A\\) satisfy:\n\\[\nAc = \\lambda c\\Rightarrow (A-\\lambda I_n)c=0\n\\]↩︎"
  }
]