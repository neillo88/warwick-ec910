[
  {
    "objectID": "problem-set-4.html",
    "href": "problem-set-4.html",
    "title": "Problem Set 4",
    "section": "",
    "text": "This problem set will revisit some of the material covered in Handouts 3 and 4. You will be required to work with a ‘raw’ dataset, downloaded from an online repository. For this reason, you should take care to check how the data is coded.\nYou will be using a version of the US Current Population Survey (CPS) called the Merged Outgoing Rotation Group (MORG). This data is compiled by the National Bureau of Economic Research (NBER) and has been used in many famous studies of the US economy. The CPS has a rather unique rotating panel design: “The monthly CPS is a rotating panel design; households are interviewed for four consecutive months, are not in the sample for the next eight months, and then are interviewed for four more consecutive months.” (source: IPUMS). The NBER’s MORG keeps only the outgoing rotation group’s observations.\nThe MORG .dta files can be found at: https://data.nber.org/morg/annual/."
  },
  {
    "objectID": "problem-set-4.html#preamble",
    "href": "problem-set-4.html#preamble",
    "title": "Problem Set 4",
    "section": "Preamble",
    "text": "Preamble\nCreate a do-file for this problem set and include a preamble that sets the directory and opens the data directly from the NBER website. Of course, this requires a good internet connection. For example,\n\nclear \n//or, to remove all stored values (including macros, matrices, scalars, etc.) \n*clear all\n\n* Replace $rootdir with the relevant path to on your local harddrive.\ncd \"$rootdir/problem-sets/ps-4\"\n\ncap log close\nlog using problem-set-4-log.txt, replace\n\nuse \"https://data.nber.org/morg/annual/morg19.dta\", clear\n\nYou can, of course, download the data and open it locally on your computer."
  },
  {
    "objectID": "problem-set-4.html#questions",
    "href": "problem-set-4.html#questions",
    "title": "Problem Set 4",
    "section": "Questions",
    "text": "Questions\n1. Create a new variable exper equal to age minus (years of education + 6). This is referred to as potential years of experience. Check how each variable defines missing values before proceeding. You will need to create a years of education variable for this. Here is he the suggested code:\n\ntab grade92, m\ngen eduyrs = .\n    replace eduyrs = .3 if grade92==31\n    replace eduyrs = 3.2 if grade92==32\n    replace eduyrs = 7.2 if grade92==33\n    replace eduyrs = 7.2 if grade92==34\n    replace eduyrs = 9  if grade92==35\n    replace eduyrs = 10 if grade92==36\n    replace eduyrs = 11 if grade92==37\n    replace eduyrs = 12 if grade92==38\n    replace eduyrs = 12 if grade92==39\n    replace eduyrs = 13 if grade92==40\n    replace eduyrs = 14 if grade92==41\n    replace eduyrs = 14 if grade92==42\n    replace eduyrs = 16 if grade92==43\n    replace eduyrs = 18 if grade92==44\n    replace eduyrs = 18 if grade92==45\n    replace eduyrs = 18 if grade92==46\n    lab var eduyrs \"completed education\"\ntab grade92, sum(eduyrs)\n\n2. Keep only those between the ages of 18 and 54. Check the distribution of `exper’ and replace any negative values to 0.\n3. Create a categorical variable that takes on 4 values: 1 “less than High School”; 2 “High School Diploma”; 3 “some Higher Education”; 4 “Bachelors”; 5 “Postgraduate”. This variable should be based on the the grade92 variable. You can find the value labels for this variable in this document: https://data.nber.org/morg/docs/cpsx.pdf. I suggest using the recode command, which allows you to create value labels while assigning values. Check the distributio of exper by education category.\n4. Create the variable lnwage equal to the (natural) log of weekly earnings. Create a figure that shows the predicted linear fit of lwage against exper, by educat. Try to place all 5 fitted lines in the same graph.\n5. Estimate a linear regression model that allows the slope coefficient on exper and constant term to vary by education category (educat). Let the base (excluded) education category be 2 “High School diploma”.\n\\[\n  \\ln(Wage_i) = \\alpha + \\sum_{j\\neq2}\\psi_j \\mathbf{1}\\{Educat_i=j\\} + \\beta Exper_i + \\sum_{j\\neq2}\\gamma_j Exper_i\\times\\mathbf{1}\\{Educat_i=j\\}+\\upsilon_i\n\\]\n6. Show that after 13 years of experience, those with some Higer Education (but no Bachelors), out earn those with just a high school diploma. You can assume that there are is a 2 year difference between the experience (education).\n7. Use the post-estimation test command to test the null hypothesis: \\(H_0: 15\\beta = 13(\\beta+\\gamma_3)+\\psi_3\\).\n8. Estimate a transformed version of the above model allowing you to test the above hypothesis using the coefficient from a single regressor. That is, the resulting test should be a simple t-test of \\(H_0: \\phi=0\\), where \\(\\phi\\) is the coefficient on the interaction of exper and a dummy variable for educat=3. This will be easier to do if you estimate the model using only the relevant sample: those with High School diplomas and some Higher Education. I suggest avoiding the use of factor notation to create the dummy variables and interaction terms for this exercise. For example, the following should replicate the relevant coefficients from Q5.\n\ngen hasHE = educat==3 if inlist(educat,2,3)\ngen hasHEexp = hasHE*exper\n\nreg lnwage exper hasHE hasHEexp\n\n9. Verify that the F-statistic from Q7 is the square of the above T-statistic.\n10. Use the restricted OLS approach to replicate the F-statistic and p-value from Q7.\n11. Use the restricted OLS approach to test the following hypothesis corresponding to the model in Q5:\n\\[\nH_0: \\gamma_j = 0\\qquad \\text{for}\\quad j=1,3,4,5\n\\] Compute the F-statistic and p-value. Verify your result using the post-estimation test command.\n12. Compute the relevant Chi-squared distributed test statistic and corresponding p-value for the above test, assuming \\(n\\) is large (enough).\n13. Using the data from Problem Set 2, estimate the simple linear regression model using OLS,\n\\[\n  \\ln(Wage_i) = \\beta_0 + \\beta_1 Educ_i + \\beta_2 Female_i + \\varepsilon_i\n\\]\n14. Estimate the model using Maximum Likelihood. Take a look at https://www.stata.com/manuals13/rmlexp.pdf, the documentation for the mlexp command. It has a discussion on estimating the CLRM using ML.1\n15. Estimate the model using Method of Moments. You can use the gmm command in Stata. Hint: the regressors will be their own instruments and use the onestep option.2"
  },
  {
    "objectID": "problem-set-4.html#postamble",
    "href": "problem-set-4.html#postamble",
    "title": "Problem Set 4",
    "section": "Postamble",
    "text": "Postamble\n\nlog close"
  },
  {
    "objectID": "problem-set-4.html#footnotes",
    "href": "problem-set-4.html#footnotes",
    "title": "Problem Set 4",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYou can also look at the following resource for a more flexible approach to ML estimation in Stata: https://www.stata.com/features/overview/maximum-likelihood-estimation/↩︎\nHere is a resource on GMM in Stata: https://www.stata.com/features/overview/generalized-method-of-moments/↩︎"
  },
  {
    "objectID": "problem-set-3.html",
    "href": "problem-set-3.html",
    "title": "Problem Set 3",
    "section": "",
    "text": "The purpose of this problem set is for you to see how the ordinary least squares (OLS) estimator behaves under various assumptions in a linear regression model where you know what the model is – since you are going to be generating the data from a known data generating process (DGP).\nThe models estimated are simple bivariate regressions but the properties of the OLS estimator with vary with each case. This is demonstrated by changing the (a) distributional properties of the error term (variance-covariance structure), and (b) inducing correlation between the regressor and the error term. Any resulting bias and/or inconsistency will depend on the DGP.\nTo achieve certain results we will have to use a serially-correlated error structure, which is only appropriate in a time-series setting. For this reason, the models will be written with subscript \\(t\\) and not \\(i\\).\nThe code has been provided for model 1. You can then modify the code for models 2-4."
  },
  {
    "objectID": "problem-set-3.html#preamble",
    "href": "problem-set-3.html#preamble",
    "title": "Problem Set 3",
    "section": "Preamble",
    "text": "Preamble\n\n\n\n\n\nYou do not need to load data for this problem set.\n\nclear \n//or, to remove all stored values (including macros, matrices, scalars, etc.) \n*clear all\n\n* Replace $rootdir with the relevant path to on your local harddrive.\ncd \"$rootdir/problem-sets/ps-3\"\n\ncap log close\nlog using problem-set-3-log.txt, replace\n\nHowever, since we are going to generate random variables, we should set a seed. This ensures replicability of the exercise. The number you choose is arbitrary, it simply ensures that any algorithms used to generate (pseudo) random variables start at the same place.\n\nset seed 981836"
  },
  {
    "objectID": "problem-set-3.html#model-1-clrm",
    "href": "problem-set-3.html#model-1-clrm",
    "title": "Problem Set 3",
    "section": "Model 1: CLRM",
    "text": "Model 1: CLRM\nThis is your classical linear regression model. OLS estimator is unbiased and consistent.\n\\[\nY_t = \\beta_1 + \\beta_2X_t + \\upsilon_t \\qquad \\text{with}\\quad \\upsilon_t\\sim N(0,\\sigma^2)\n\\]\nWe know that the OLS estimator for \\(\\beta_2\\) is given by,\n\\[\n\\begin{aligned}\n  \\hat{\\beta}_2 =& \\frac{\\sum_t \\big[(X_t-\\bar{X})(Y_t-\\bar{Y})\\big]}{\\sum_t (X_t-\\bar{X})^2} \\\\\n  =& \\beta_2 + \\frac{\\sum_t \\big[(X_t-\\bar{X})(\\upsilon_t-\\bar{\\upsilon})\\big]}{\\sum_t (X_t-\\bar{X})^2} \\\\\n  =& \\beta_2 + \\frac{\\sum_t \\tilde{X}_t\\tilde{\\upsilon}_t}{\\sum_t \\tilde{X}_t^2}\n\\end{aligned}  \n\\] where \\(\\tilde{X}_t\\) and \\(\\tilde{\\upsilon}_t\\) represent the demeaned counterparts of these variables. Alternatively, using linear algebra notation:\n\\[\n\\begin{aligned}\n  \\hat{\\beta}_2 =& \\frac{X'M_{\\ell}Y}{X'M_{\\ell}X} \\\\\n  =& \\beta_2 + \\frac{X'M_{\\ell}\\upsilon}{X'M_{\\ell}X} \\\\\n  =& \\beta_2 + \\frac{\\tilde{X}'\\tilde{\\upsilon}}{\\tilde{X}'\\tilde{X}}\n\\end{aligned}  \n\\] where \\(\\tilde{X} = M_{\\ell}X\\), \\(\\tilde{\\upsilon}= M_{\\ell}\\upsilon\\), and \\(M_{\\ell} = I_n-\\ell(\\ell'\\ell)^{-1}\\ell'\\) (the orthogonal projection of the constant regressor).\nWe know from Handouts 2 & 3,\n\n\\(E[\\hat{\\beta}_2] = \\beta_2\\) (i.e., unbiased)\n\\(p \\lim \\hat{\\beta}_2 = \\beta_2\\) (i.e., consistent)\n\nCan you demonstrate these results?\n\nSimulation\nBegin by designing a programme that takes the parameters of the model as arguments, generates the data, estimates the model, and then returns the stored values.\n\ncapture program drop mc1\nprogram define mc1, rclass\n    syntax [, obs(integer 1) s(real 1) b1(real 0) b2(real 0)  sigma(real 1)]\n    drop _all\n    set obs `obs'\n    gen u = rnormal(0,`sigma')            // sigma is the std deviation of the error distribution\n    gen x=uniform()*`s'                   // s is the std devation of the x distribution\n    gen y=`b1'+`b2'*x + u                   // this generates the dep variable y\n    reg y x\n    return scalar b1=_b[_cons]            // intercept estimate\n    return scalar b2=_b[x]                  // coeff on the x variable\n    return scalar se2 = _se[x]            // std error\n    return scalar t2 = _b[x]/_se[x]     // t ratio\n\nend\n\n\n\n\nUse the the simulate command in Stata to estimate the model 100 times:\n\nsimulate b1=r(b1) b2=r(b2) se2=r(se2) t2=r(t2), reps(100): mc1, obs(50) s(6) b1(4) b2(2) sigma(3)\n\n\n      Command: mc1, obs(50) s(6) b1(4) b2(2) sigma(3)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\n\n\nCalculate the bias and plot the distribution of the bias.\n\ngen bias2=b2-2\nsu b1 b2 se2 t2\nsu bias2\nhistogram bias2, normal xline(`r(mean)')\n\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n          b1 |        100    3.880226    .9977415   1.080851   6.090028\n          b2 |        100    2.041985     .271704   1.365155   2.673303\n         se2 |        100    .2520885    .0291596   .1814694   .3255497\n          t2 |        100    8.216185      1.4968   5.484826   12.60699\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n       bias2 |        100    .0419852     .271704  -.6348448   .6733029\n(bin=10, start=-.63484478, width=.13081477)"
  },
  {
    "objectID": "problem-set-3.html#model-2-serial-correlation",
    "href": "problem-set-3.html#model-2-serial-correlation",
    "title": "Problem Set 3",
    "section": "Model 2: Serial Correlation",
    "text": "Model 2: Serial Correlation\nRelax the assumption of an iid error term and allow for serial correlation. The OLS estimator is unbiased and consistent. However, the std errors are wrong since the software does not know that you have serially correlated errors and you are not taking this into account in the estimation.\n\\[\nY_t = \\beta_1 + \\beta_2X_t + \\upsilon_t \\qquad \\text{where}\\quad \\upsilon_t = \\rho \\upsilon_{t-1}+\\varepsilon_t \\quad\\text{and}\\quad \\varepsilon_t\\sim N(0,\\sigma^2)\n\\] We say that \\(U_t\\) follows an AR(1) process. You can show that \\(\\hat{\\beta}_2\\) remains unbiased and consistant. However, the standard homoskedastic-variance estimator is incorrect:\n\\[\nVar(\\hat{\\beta}_2) \\neq \\frac{\\sigma^2}{Var(X_i)}\n\\]\n\nSimulation\nYou will need to redesign the above programme. The challenging part is the simulation of the error term. This needs to follow an AR(1) process and must therefore be generated in sequence. You can do this as follows:\n\n  gen u=0 \n    gen time=_n\n    gen e = rnormal(0,`sigma')  \n    forvalues i=2/`obs'  {\n    replace u=`rho'*u[_n-1] + e if _n==`i'\n    }"
  },
  {
    "objectID": "problem-set-3.html#model-3-dynamic-model-without-serial-correlation",
    "href": "problem-set-3.html#model-3-dynamic-model-without-serial-correlation",
    "title": "Problem Set 3",
    "section": "Model 3: Dynamic model without serial correlation",
    "text": "Model 3: Dynamic model without serial correlation\nConsider a version of Model 1, where the regressor is the lag of the dependent variable. \\[\nY_t = \\beta_1 + \\beta_2Y_{t-1} + \\upsilon_t \\qquad \\text{with}\\quad \\upsilon_t\\sim N(0,\\sigma^2)\n\\] The OLS estimator is now, \\[\n  \\hat{\\beta}_2 = \\beta_2 + \\frac{\\sum_t \\tilde{Y}_{t-1}\\tilde{\\upsilon}_t}{\\sum_t \\tilde{Y}_{t-1}^2}\n\\] This model is biased, since\n\\[\nE\\bigg[\\frac{\\sum_t \\tilde{Y}_{t-1}\\tilde{\\upsilon}_t}{\\sum_t \\tilde{Y}_{t-1}^2}\\bigg] \\neq \\frac{E\\big[\\sum_t \\tilde{Y}_{t-1}\\tilde{\\upsilon}_t\\big]}{E\\big[\\sum_t \\tilde{Y}_{t-1}^2\\big]}\n\\] When the regressor was \\(X_t\\), the above statement was true given the Law of Iterated Expectations. However, you can use Slutsky’s theorem and the WLLN to show that \\(\\hat{\\beta}_2\\rightarrow_p \\beta_2\\). This result relies on the fact that \\(Y_{t-1}\\) is realized before \\(\\upsilon_t\\) which is iid. Thus, the bias goes to 0 as \\(n\\rightarrow \\infty\\).\n\nSimulation\nYou can use lag-operators in Stata to regress the outcome against its lagged value. For example:\n\n    gen time=_n\n    tsset time\n    reg y  L.y\n\nWhile the error term is serially uncorrelated (as in Model 1), you will need to generate the outcome sequentially (row value by row value). This is because the DGP has a lagged dependent variable structure."
  },
  {
    "objectID": "problem-set-3.html#model-4-dynamic-model-with-serial-correlation",
    "href": "problem-set-3.html#model-4-dynamic-model-with-serial-correlation",
    "title": "Problem Set 3",
    "section": "Model 4: Dynamic model with serial correlation",
    "text": "Model 4: Dynamic model with serial correlation\nConsider a version of Model 2, where the regressor is the lag of the dependent variable. \\[\nY_t = \\beta_1 + \\beta_2Y_{t-1} + \\upsilon_t \\text{where}\\quad \\upsilon_t = \\rho \\upsilon_{t-1}+\\varepsilon_t \\quad\\text{and}\\quad \\varepsilon_t\\sim N(0,\\sigma^2)\n\\] As with model 3, the OLS estimator will be biased. In addition, since \\(Cov(\\upsilon_t,\\upsilon_{t-1})\\neq0\\) and \\(Cov(Y_t,\\upsilon_{t})\\neq 0\\) (for any \\(t\\)), \\[\n\\Rightarrow Cov(Y_{t-1},\\upsilon_{t})\\neq 0\n\\] As a result \\(\\hat{\\beta}_2\\) is inconsistent.\n\nSimulation\nYou will need to use tricks from both models 2 and 3 to simulate this model."
  },
  {
    "objectID": "problem-set-3.html#postamble",
    "href": "problem-set-3.html#postamble",
    "title": "Problem Set 3",
    "section": "Postamble",
    "text": "Postamble\n\nlog close"
  },
  {
    "objectID": "problem-set-2.html",
    "href": "problem-set-2.html",
    "title": "Problem Set 2",
    "section": "",
    "text": "This problem set will take you through some Stata commands to estimate simple regression equations with dummy variables. You will learn how to interpret the estimated coefficients and test some linear hypotheses. Interpretation of these coefficients will be useful when we do treatment evaluation models later in term 1.\nThe hypothesis tests discussed in this problem set include standard T-tests and F-tests, which is assumed undergraduate knowledge for this module.\nYou will need to download the dataset problem-set-2-data.dta, which is available on Moodle."
  },
  {
    "objectID": "problem-set-2.html#conditional-expectation-function",
    "href": "problem-set-2.html#conditional-expectation-function",
    "title": "Problem Set 2",
    "section": "Conditional Expectation Function",
    "text": "Conditional Expectation Function\nConsider the Conditional Expectation Function (CEF), \\(E[Y_i|X_i]\\). If \\(X\\) takes on discrete values: \\(X_i\\in\\{x_1,x_2,...,x_m\\}\\), then\n\\[\n    E[Y_i|X_i] =  E[Y_i|X_i=x_1]\\cdot\\mathbf{1}\\{X_i = x_1\\}+...+E[Y_i|X_i=x_m]\\cdot\\mathbf{1}\\{X_i = x_m\\}\n\\] where \\(\\mathbf{1}\\{X_i = x_m\\}\\) is a dummy variable, \\(=1\\) when \\(X_i=x_m\\). Since the values of \\(X_i\\) are mutually exclusive there is no overlap of these dummy variables.\nNote, we do not need to assume that \\(X\\) is a single random variable. It can be a vector of random variables that takes on discrete values.\nWe can re-arrange this expression using anyone of the values of \\(X\\). The natural option is to choose the first, but this is arbitrary.\n\\[\n\\begin{aligned}\n    E[Y_i|X_i] =& E[Y_i|X_i=x_1]+ (E[Y_i|X_i=x_2]-E[Y_i|X_i=x_1])\\cdot\\mathbf{1}\\{X_i = x_2\\}+... \\\\\n&+(E[Y_i|X_i=x_m]-E[Y_i|X_i=x_1])\\cdot\\mathbf{1}\\{X_i = x_m\\}\n\\end{aligned}\n\\]\nSince, \\(E[Y_i|X_i = x_m]\\) is a constant (\\(X_i\\) is set to a specific value), we can express the CEF as a function that is linear in parameters.\n\\[\n    E[Y_i|X_i] = \\beta_1 + \\beta_2D_{i2} + ... + \\beta_m D_{im}\n\\]\nwhere \\(D_{im}=\\mathbf{1}\\{X_i = x_m\\}\\)."
  },
  {
    "objectID": "problem-set-2.html#preamble",
    "href": "problem-set-2.html#preamble",
    "title": "Problem Set 2",
    "section": "Preamble",
    "text": "Preamble\n\n\n\n\n\nCreate a do-file for this problem set and include a preamble that sets the directory and opens the data. For example,\n\nclear \n//or, to remove all stored values (including macros, matrices, scalars, etc.) \n*clear all\n\n* Replace $rootdir with the relevant path to on your local harddrive.\ncd \"$rootdir/problem-sets/ps-2\"\n\ncap log close\nlog using problem-set-2-log.txt, replace\n\nuse problem-set-2-data.dta"
  },
  {
    "objectID": "problem-set-2.html#questions",
    "href": "problem-set-2.html#questions",
    "title": "Problem Set 2",
    "section": "Questions",
    "text": "Questions\n1. Consider the \\(E[ln(Wage_i)|Gender_i]\\), where \\(Gender_i\\in\\{1 ``Male'', 2 ``Female''\\}\\). Show that this CEF implies a linear model,\n\\[\n    ln(Wage_i) = \\beta_1 + \\beta_2 D_{i2} + \\varepsilon_i\n\\]\nWhat do the parameters \\(\\beta_1\\) and \\(\\beta_2\\) imply?\n2. Regress lwage (log wage) on just a set of binary indicators that will enable you to test the hypothesis that males and females are on average, paid the same wage, ceteris paribus. Test this hypothesis.\n3. Extend the specification in (2) that will enable you to test the hypothesis that there is no difference in the wages between the following gender-ethnicity groups. Begin by defining the following dummy variables:\n\nfemale_black = female\\(\\times\\)black\nmale_black = (1-female)\\(\\times\\)black\nfemale_nonblack = female\\(\\times\\)(1-black)\nmale_nonblack = (1-female)\\(\\times\\)(1-black)\n\nThen estimate the following regressions:\n\nlwage on female_black, female_nonblack, male_black, male_nonblack (without a constant: option nocons)\nlwage on female, black, female_black\nlwage on female_black, female_nonblack, male_black\n\nFor some of these exercises you may be able to use Stata’s factor notation. However, in some instances you will need to manually create the above dummy-variable interactions.\nIn each case, identify the base category and write down the parameters of the (implied) model in terms of conditional expectations.\n4. Compare the estimated coefficients with the sample average values for the lwage for the four subgroups. What do you see?\n5. In each of the above models, describe the null hypothesis you would test to evaluate whether there is a significant earnings difference between the earnings of black and non-black females.\n6. Very your solution to 4. by performing a test using the three set of regression output. You can use the post-estimation test command.\n7. In each case, test equality across all four gender-ethnicity groups. Again, you should get the same result.\n8. Try to replicate the F-statistic for one of the above models. Hint, the F-stat for these models is the same as that of the whole model.\n9. Estimate the following model:\n\\[\n    lwage = \\beta_1 + \\beta_2F + \\beta_3B + \\beta_4F\\times B + \\beta_5exp + \\beta_6exp^2 + \\beta_7educ + \\varepsilon\n\\]\n\nInterpret the estimated coeffiecients \\(\\hat{\\beta}_7\\).\nInterpret the effect of experience variable exp. Use the median level of experience to make your calculation.\nA one unit incease in years of education is associated with an increase of 1.78% in expected wages, holding other regressors fixed.\n\n10. Theoretically, how would you test the following restrictions for the model below?\n\n\\(\\beta_2 = \\beta_3\\)\n\\(\\beta_4 + \\beta_5 = 1\\)\n\\(\\beta_2 = \\beta_3\\) and \\(\\beta_4 + \\beta_5 = 1\\)\n\n\\[\n    Y = \\beta_1 + \\beta_2X_2 + \\beta_3X_3 + \\beta_4X_4 + \\beta_5X_5 + \\varepsilon\n\\]"
  },
  {
    "objectID": "problem-set-2.html#postamble",
    "href": "problem-set-2.html#postamble",
    "title": "Problem Set 2",
    "section": "Postamble",
    "text": "Postamble\n\nlog close"
  },
  {
    "objectID": "material-linearalgebra.html",
    "href": "material-linearalgebra.html",
    "title": "Linear Algebra",
    "section": "",
    "text": "Consider a set of \\(k\\) \\(n\\)-dimensional vectors \\(\\{X_{1},X_{2},...,X_{k}\\}\\). These vectors are,\n\nDefinition 1 linearly dependent if there exists a set of scalars \\(\\{a_{1},a_{2},\\cdots,a_{k}\\}\\) such that\n\\[\na_1X_1 + a_2X_2+\\ldots+a_kX_k=0\n\\]\nwhere at least one \\(a_i\\neq0\\).\n\nAlternatively, they are,\n\nDefinition 2 linearly independent if the only set of scalars \\(\\{a_{1},a_{2},\\cdots,a_{k}\\}\\) that satisfies the above condition is \\(a_1,a_2,\\cdots,a_k=0\\).\n\nIf we collect these \\(k\\) column-vectors in a matrix, \\(X=[X_1\\;X_2 \\cdots X_k]\\), then the linear dependence condition can be written as,\n\\[\na_1X_1 + a_2X_2+\\ldots+a_kX_k=\\begin{bmatrix} X_1\\;X_2 \\cdots X_k\\end{bmatrix}\\begin{bmatrix}a_1\\\\a_2\\\\\\vdots\\\\a_k\\end{bmatrix}=Xa = 0\n\\]\nGiven any \\(n\\times k\\) matrix \\(X\\), its columns are,\n\nDefinition 3 linearly dependent if there exists a vector \\(a\\in\\mathbb{R}^k\\) such that \\(a\\neq0\\) and \\(Xa=0\\);\n\nor,\n\nDefinition 4 linearly independent if the only vector \\(a\\in\\mathbb{R}^k\\) such that \\(Xa=0\\) is \\(a=0\\).\n\nFor any matrix there may be more than one vector \\(a\\in\\mathbb{R}^{k}\\) such that \\(Xa=0\\). Indeed, if both \\(a_{1},a_{2}\\in\\mathbb{R}^{k}\\)\nsatisfy this condition and \\(a_{1}\\neq a_{2}\\) then you can show that any linear combination of \\(\\{a_{1},a_{2}\\}\\) satisfies the\ncondition \\(X(a_{1}b_{1}+a_{2}b_{2})=0\\) for \\(b_{1},b_{2}\\in\\mathbb{R}\\). Thus, there exists an entire set of vectors which satisfy this condition. This set is referred to as the,\n\nDefinition 5 null space of \\(X\\), \\[\n\\mathcal{N}(X) = \\{a\\in\\mathbb{R}^k:\\;Xa=0\\}\n\\]\n\nIt should be evident from the definition that if the columns of \\(X\\) are linearly independent then \\(\\mathcal{N}(X)=\\{0\\}\\), a singleton. That is, it just includes the 0-vector.\n\n\n\nHere, we concern ourselves only with real vectors from \\(\\mathbb{R}^n\\).\n\nDefinition 6 A vector space, denoted \\(\\mathcal{V}\\), refers to a set of vectors which is closed under finite addition and scalar multiplication.\n\n\nDefinition 7 A set of \\(k\\) linearly independent vectors, \\(\\{X_1,X_2,\\cdots,X_k\\}\\), forms a basis for vector space \\(\\mathcal{V}\\) if \\(\\forall\\;y\\in\\mathcal{V}\\) there exists a set of \\(k\\) scalars such that, \\[\ny=X_1b_1+X_2b_2+\\ldots+X_kb_k\n\\]\n\nBased on these definitions, it is evident that for the Euclidean space, \\(\\mathbb{E}^n\\), any \\(n\\) linearly independent vectors from \\(\\mathbb{R}^n\\) is a basis. For example, any point in \\(\\mathbb{E}^2\\) can be defined as a multiple of,\n\\[\n\\begin{bmatrix}1\\\\0\\end{bmatrix}\\quad \\text{and} \\quad\\begin{bmatrix}0\\\\1\\end{bmatrix}\n\\]\nConsider again the \\(n\\times k\\) matrix \\(X\\), where \\(k&lt;n\\). Then we define the,\n\nDefinition 8 column space (or span) of \\(X\\), denoted \\(\\mathcal{S}(X)\\), as the vector space generate by the \\(k\\) columns of \\(X\\). Formally, \\[\n\\mathcal{S}(X) = \\{y\\in\\mathbb{R}^n:\\;y=Xb\\quad\\text{for some }b\\in \\mathbb{R}^k\\}\n\\]\n\nA property to note about the span or column space \\(X\\) is,\nResult: \\(\\mathcal{S}(X)=\\mathcal{S}(XX')\\) :::\nwhere \\(XX'\\) is a \\(n\\times n\\) matrix.\nFinally, we can define the,\n\nDefinition 9 orthogonal column space (or orthogonal span) of \\(X\\) as, \\[\n\\mathcal{S}^{\\perp}(X) = \\{y\\in \\mathbb{R}^k:\\;y'x=0\\quad \\forall x\\in\\mathcal{S}(X)\\}\n\\]\n\n\n\n\nConsider a \\(n\\times k\\) matrix \\(X\\), the\n\nDefinition 10 row rank of \\(X\\) is the maximum number of linearly independent rows: \\[\nrowrank(X) \\leq n\n\\]\n\nWe say that matrix \\(X\\) has full row rank if \\(rowrank(X)=n\\).\nThe,\n\nDefinition 11 column rank of \\(X\\) is the maximum number of linearly independent columns:\n\\[\ncolrank(X) \\leq k\n\\]\n\nWe say that matrix \\(X\\) has full column rank if \\(colrank(X)=k\\).\nAn important result is,\n\nResult: the rank of \\(X\\): \\[\nr(X) = rowrank(X)=colrank(X) \\\\\n\\Rightarrow r(X)\\leq min\\{n,k\\}\n\\]\n\nIn addition, since the \\(r(X)\\) depends on the number of linearly independent columns, we can say that,\n\nResult: the dimension of \\(\\mathcal{S}(X)\\), \\(dim(\\mathcal{S}(X))\\), is given by the \\(r(X)\\).\n\nHere are a few additional results,\n\nResult: \\(r(X)=r(X')\\)\nResult: \\(r(XY)\\leq min\\{r(X),r(Y)\\}\\)\nResult: \\(r(XY)=r(X)\\) if \\(Y\\) is square and full rank\nResult: \\(r(X+Y)\\leq r(X) + r(Y)\\)\n\n\n\n\nConsider the case of a square, \\(n\\times n\\), matrix \\(A\\). We say that,\n\nDefinition 12 \\(A\\) is singular if the \\(r(A)&lt;n\\),\n\nor that,\n\nDefinition 13 \\(A\\) is non-singular if the \\(r(A)=n\\).\n\nThe singularity of a square matrix is important as it determines the invertibility of a matrix, which typically relates the existence of a unique solution in systems of linear equations. Here are a few key results,\n\nResult: There exists a matrix \\(B=A^{-1}\\), such that \\(AB=I_n\\) (where \\(I_n\\) is the identity matrix), if and only if \\(A\\) is non-singular.\nResult: \\(A\\) is non-singular if and only if the determinant of \\(A\\) is non-zero: \\(det(A)\\neq0\\).1\nResult: Likewise, \\(A\\) is singular if and only if \\(det(A)=0\\).\nResult: \\(AA^{-1}=A^{-1}A=I\\)\nResult: \\((A')^{-1}=(A^{-1})'\\)\nResult: If their respective inverses exist, then \\((AB)^{-1}=B^{-1}A^{-1}\\).\nResult: \\(det(AB)=det(A)det(B)\\)\nResult: \\(det(A^{-1})=det(A)^{-1}\\)\n\nFor any square matrix \\(A\\),\n\nDefinition 14 the trace of \\(A\\) is the sum of all diagonal elements: \\[\ntr(A) = \\sum_{i=1}^na_{ii}\n\\]\n\nRegarding the trace of a square matrix, here are a few important results:\n\nResult: \\(tr(A+B) = tr(A) + tr(B)\\)\nResult: \\(tr(\\lambda A) = \\lambda tr(A)\\) where \\(\\lambda\\) is a scalar\nResult: \\(tr(A) = tr(A')\\)\nResult: \\(tr(AB) = tr(BA)\\) where \\(AB\\) and \\(BA\\) are both square, but need not be of the same order.\nResult: \\(||A|| = (tr(A'A))^{1/2}\\)\n\n\n\n\nA symmetric matrix has the property that \\(A=A'\\). Therefore, \\(A\\) must be square.\nHere are a few important results concerning symmetric matrices.\n\nResult: \\(A^{-1}\\) exists if \\(det(A)\\neq 0\\) and \\(r(A)=n\\)\nResult: A is diagonalizable.2\nResult: The eigenvector decomposition of a square matrix gives you \\(A=C\\Lambda C^{-1}\\) where \\(\\Lambda\\) is a diagonal matrix of eigenvalues and $C$ a matrix of the corresponding eigenvectors. The symmetry of \\(A\\) gives you that \\(C^{-1}=C'\\Rightarrow A=C\\Lambda C'\\) with \\(C'C=CC'=I_{n}\\).3\n\nA key definition concerning symmetric matrices is their positive definiteness:\n\nDefinition 15 \\(A\\) is positive semi-definite if for any \\(x\\in\\mathbb{R}^n,\\;x'Ax\\geq0\\).\n\nGiven the eigenvector decomposition of a symmetric matrix, positive semi-definiteness implies \\(\\Lambda\\) is positive semi-definite: \\(\\lambda_i\\geq0\\quad\\forall i\\). Likewise,\n\nDefinition 16 \\(A\\) is positive definite if for any \\(x\\in\\mathbb{R}^n,\\;x'Ax&gt;0\\).\n\nAgain, based on the egeinvector decomposition, positive semi-definiteness implies \\(\\Lambda\\) is positive definite: \\(\\lambda_i&gt;0\\quad\\forall i\\).\nA few more results are:\n\nResult: \\(tr(A) = \\sum_{i=1}^n\\lambda_i\\)\nResult: \\(r(A) = r(\\Lambda)\\)\nResult: \\(det(A) = \\prod_{i=1}^n \\lambda_i\\)\n\nThis last result can be used to prove that any positive definite matrix is non-singular and therefore has an inverse.\nAny full-rank, positive semi-definite, symmetric matrix \\(B\\) has the additional properties:\n\nResult: \\(B=C\\Lambda C'\\) and \\(B^{-1} = C\\Lambda^{-1}C'\\)\nResult: We can define the square-root of \\(B\\) as \\(B^{1/2} = C\\Lambda^{1/2}C'\\). Similarly, \\(B^{-1/2} = C\\Lambda^{-1/2}C'\\).\n\n\n\n\nAn idempotent matrix has the property that \\(D=DD\\). Therefore, \\(D\\) must be square.\nHere are a few important results concerning idempotent matrices.\n\nResult: \\(D\\) is positive definite\nResult: \\(D\\) is diagonalizable\nResult: \\((I_n-D)\\) is also an idempotent matrix\nResult: With the exception of \\(I_n\\), all idempotent matrices are singular.\nResult: \\(r(D) = tr(D) = \\sum_{i=1}^n\\lambda_i\\)\nResult: \\(\\lambda_i\\in\\{0,1\\}\\quad \\forall\\;i\\)\n\nProjection matrices are idempotent, but need not be symmetric. However, for the purposes of this module we will deal exclusively with symmetric idempotent projection matrices.\n\n\n\nHere we will look at the derivatives of scalar with respect to (W.r.t.) a vector. You can also define other derivatives, such as the derivative of a vector w.r.t. a vector and the derivative of a scalar with respect to a matrix. However, these are not needed for these notes.\n\n\nSuppose \\(f(x)\\in R\\) (i.e. a scalar) and \\(x\\in R^n\\) (i.e. a \\(n\\times 1\\) vector). Then we can define the partial derivative of \\(f(x)\\) w.r.t. \\(x\\) as,\n\\[\n  \\frac{\\partial f(x)}{\\partial x}  = \\begin{bmatrix}\\frac{\\partial f(x)}{\\partial x_1} \\\\ \\frac{\\partial f(x)}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial f(x)}{\\partial x_n} \\end{bmatrix}\n\\]\n\n\n\nA special case is when \\(f(x)\\) is linear in \\(x\\),\n\\[\nf(x) = a'x = \\sum_{i=1}^n a_ix_i\n\\] for \\(a\\in R^n\\). The derivative of \\(a'x\\) with respect to the vector \\(x\\) can be defined as,\n\\[\n  \\begin{aligned}\n  \\frac{\\partial a'x}{\\partial x} & = \\begin{bmatrix}\\frac{\\partial a'x}{\\partial x_1} \\\\ \\frac{\\partial a'x}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial a'x}{\\partial x_n} \\end{bmatrix} \\\\\n  & = \\begin{bmatrix}a_1 \\\\ a_2 \\\\ \\vdots \\\\ a_n \\end{bmatrix} \\\\\n  & = a\n  \\end{aligned}\n\\] since the the partial derivate of \\(a'x = \\sum_{i=1}^n a_ix_i\\) w.r.t. \\(x_i\\) is just the scalar \\(a_i\\).\nThe derivative of a scalar w.r.t. to a vector yields a vector of partial derivatives.\nSince \\(a'x\\) is a scalar, it is by definition symmetric: \\(a'x = x'a\\). Thus,\n\\[\n\\frac{\\partial x'a}{\\partial x} = \\frac{\\partial a'x}{\\partial x} = a\n\\]\n\n\n\nSuppose \\(f(x)\\) is a linear transformation of \\(x\\),\n\\[\nf(x) = A'x\n\\] for any \\(m\\times n\\) matrix A,\n\\[\n  A = \\begin{bmatrix}a_1' \\\\ a_2' \\\\ \\vdots \\\\ a_m'\\end{bmatrix}\n\\] where \\(a_i\\in R^n\\;\\forall i=1,\\cdots,m\\) and,\n\\[\n  Ax = \\begin{bmatrix}a_1'x \\\\ a_2'x \\\\ \\vdots \\\\ a_m'x\\end{bmatrix}\n\\] Note, \\(f(x)=Ax\\in R^m\\), a \\(m\\times 1\\) vector. We can then define,\n\\[\n  \\begin{aligned}\n  \\frac{\\partial Ax}{\\partial x'} & = \\begin{bmatrix}\\frac{\\partial a_1'x}{\\partial x_1} & \\frac{\\partial a_1'x}{\\partial x_2} & \\cdots & \\frac{\\partial a_1'x}{\\partial x_n}\\\\ \\frac{\\partial a_2'x}{\\partial x_1} & \\frac{\\partial a_2'x}{\\partial x_2} & \\cdots & \\frac{\\partial a_2'x}{\\partial x_n}\\\\ \\vdots & \\vdots & \\ddots & \\vdots\\\\ \\frac{\\partial a_m'x}{\\partial x_1} & \\frac{\\partial a_m'x}{\\partial x_2} & \\cdots & \\frac{\\partial a_m'x}{\\partial x_n}\\\\ \\end{bmatrix} \\\\\n  & = \\begin{bmatrix} a_{11} & a_{12} & \\cdots & a_{1n} \\\\ a_{21} & a_{22} & \\cdots & a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots\\\\ a_{m1} & a_{m2} & \\cdots & a_{mn} \\\\ \\end{bmatrix} \\\\\n  & = A\n  \\end{aligned}\n\\]\nSince Ax is \\(m\\times 1\\) column vector, we take the derivative w.r.t. \\(x'\\) a row vector and not the column vector \\(x\\). This results in a matrix of partial derivatives.\n\n\n\nA second special case is where the function takes on the quadaratic form,\n\\[\nf(x) = x'Ax = \\sum_{i=1}^N\\sum_{j=1}^n a_{ij}x_ix_j\n\\]\nfor \\(n\\times n\\) (square) matrix A. As in the first linear case, \\(f(x)\\) is scalar.\nDefine \\(c = Ax\\), the \\(x'Ax = x'c\\). From the linear case, we know that,\n\\[\n\\frac{\\partial x'c}{\\partial x} = c\n\\]\nSimilarly, if we define \\(d = A'x\\) then \\(x'Ax = d'x\\). From the linear case, we know that,\n\\[\n\\frac{\\partial d'x}{\\partial x} = d\n\\]\nWe can define the total derivative as the sum of the partial derivatives w.r.t. to the first and second \\(x\\). Combining these two results, we have that,\n\\[\n\\frac{\\partial x'Ax}{\\partial x} = Ax + A'x\n\\]\nAnd if \\(A\\) is symmetric, this result simplifies to \\(2Ax\\)."
  },
  {
    "objectID": "material-linearalgebra.html#linear-dependence",
    "href": "material-linearalgebra.html#linear-dependence",
    "title": "Linear Algebra",
    "section": "",
    "text": "Consider a set of \\(k\\) \\(n\\)-dimensional vectors \\(\\{X_{1},X_{2},...,X_{k}\\}\\). These vectors are,\n\nDefinition 1 linearly dependent if there exists a set of scalars \\(\\{a_{1},a_{2},\\cdots,a_{k}\\}\\) such that\n\\[\na_1X_1 + a_2X_2+\\ldots+a_kX_k=0\n\\]\nwhere at least one \\(a_i\\neq0\\).\n\nAlternatively, they are,\n\nDefinition 2 linearly independent if the only set of scalars \\(\\{a_{1},a_{2},\\cdots,a_{k}\\}\\) that satisfies the above condition is \\(a_1,a_2,\\cdots,a_k=0\\).\n\nIf we collect these \\(k\\) column-vectors in a matrix, \\(X=[X_1\\;X_2 \\cdots X_k]\\), then the linear dependence condition can be written as,\n\\[\na_1X_1 + a_2X_2+\\ldots+a_kX_k=\\begin{bmatrix} X_1\\;X_2 \\cdots X_k\\end{bmatrix}\\begin{bmatrix}a_1\\\\a_2\\\\\\vdots\\\\a_k\\end{bmatrix}=Xa = 0\n\\]\nGiven any \\(n\\times k\\) matrix \\(X\\), its columns are,\n\nDefinition 3 linearly dependent if there exists a vector \\(a\\in\\mathbb{R}^k\\) such that \\(a\\neq0\\) and \\(Xa=0\\);\n\nor,\n\nDefinition 4 linearly independent if the only vector \\(a\\in\\mathbb{R}^k\\) such that \\(Xa=0\\) is \\(a=0\\).\n\nFor any matrix there may be more than one vector \\(a\\in\\mathbb{R}^{k}\\) such that \\(Xa=0\\). Indeed, if both \\(a_{1},a_{2}\\in\\mathbb{R}^{k}\\)\nsatisfy this condition and \\(a_{1}\\neq a_{2}\\) then you can show that any linear combination of \\(\\{a_{1},a_{2}\\}\\) satisfies the\ncondition \\(X(a_{1}b_{1}+a_{2}b_{2})=0\\) for \\(b_{1},b_{2}\\in\\mathbb{R}\\). Thus, there exists an entire set of vectors which satisfy this condition. This set is referred to as the,\n\nDefinition 5 null space of \\(X\\), \\[\n\\mathcal{N}(X) = \\{a\\in\\mathbb{R}^k:\\;Xa=0\\}\n\\]\n\nIt should be evident from the definition that if the columns of \\(X\\) are linearly independent then \\(\\mathcal{N}(X)=\\{0\\}\\), a singleton. That is, it just includes the 0-vector."
  },
  {
    "objectID": "material-linearalgebra.html#vector-spaces-bases-and-spans",
    "href": "material-linearalgebra.html#vector-spaces-bases-and-spans",
    "title": "Linear Algebra",
    "section": "",
    "text": "Here, we concern ourselves only with real vectors from \\(\\mathbb{R}^n\\).\n\nDefinition 6 A vector space, denoted \\(\\mathcal{V}\\), refers to a set of vectors which is closed under finite addition and scalar multiplication.\n\n\nDefinition 7 A set of \\(k\\) linearly independent vectors, \\(\\{X_1,X_2,\\cdots,X_k\\}\\), forms a basis for vector space \\(\\mathcal{V}\\) if \\(\\forall\\;y\\in\\mathcal{V}\\) there exists a set of \\(k\\) scalars such that, \\[\ny=X_1b_1+X_2b_2+\\ldots+X_kb_k\n\\]\n\nBased on these definitions, it is evident that for the Euclidean space, \\(\\mathbb{E}^n\\), any \\(n\\) linearly independent vectors from \\(\\mathbb{R}^n\\) is a basis. For example, any point in \\(\\mathbb{E}^2\\) can be defined as a multiple of,\n\\[\n\\begin{bmatrix}1\\\\0\\end{bmatrix}\\quad \\text{and} \\quad\\begin{bmatrix}0\\\\1\\end{bmatrix}\n\\]\nConsider again the \\(n\\times k\\) matrix \\(X\\), where \\(k&lt;n\\). Then we define the,\n\nDefinition 8 column space (or span) of \\(X\\), denoted \\(\\mathcal{S}(X)\\), as the vector space generate by the \\(k\\) columns of \\(X\\). Formally, \\[\n\\mathcal{S}(X) = \\{y\\in\\mathbb{R}^n:\\;y=Xb\\quad\\text{for some }b\\in \\mathbb{R}^k\\}\n\\]\n\nA property to note about the span or column space \\(X\\) is,\nResult: \\(\\mathcal{S}(X)=\\mathcal{S}(XX')\\) :::\nwhere \\(XX'\\) is a \\(n\\times n\\) matrix.\nFinally, we can define the,\n\nDefinition 9 orthogonal column space (or orthogonal span) of \\(X\\) as, \\[\n\\mathcal{S}^{\\perp}(X) = \\{y\\in \\mathbb{R}^k:\\;y'x=0\\quad \\forall x\\in\\mathcal{S}(X)\\}\n\\]"
  },
  {
    "objectID": "material-linearalgebra.html#rank",
    "href": "material-linearalgebra.html#rank",
    "title": "Linear Algebra",
    "section": "",
    "text": "Consider a \\(n\\times k\\) matrix \\(X\\), the\n\nDefinition 10 row rank of \\(X\\) is the maximum number of linearly independent rows: \\[\nrowrank(X) \\leq n\n\\]\n\nWe say that matrix \\(X\\) has full row rank if \\(rowrank(X)=n\\).\nThe,\n\nDefinition 11 column rank of \\(X\\) is the maximum number of linearly independent columns:\n\\[\ncolrank(X) \\leq k\n\\]\n\nWe say that matrix \\(X\\) has full column rank if \\(colrank(X)=k\\).\nAn important result is,\n\nResult: the rank of \\(X\\): \\[\nr(X) = rowrank(X)=colrank(X) \\\\\n\\Rightarrow r(X)\\leq min\\{n,k\\}\n\\]\n\nIn addition, since the \\(r(X)\\) depends on the number of linearly independent columns, we can say that,\n\nResult: the dimension of \\(\\mathcal{S}(X)\\), \\(dim(\\mathcal{S}(X))\\), is given by the \\(r(X)\\).\n\nHere are a few additional results,\n\nResult: \\(r(X)=r(X')\\)\nResult: \\(r(XY)\\leq min\\{r(X),r(Y)\\}\\)\nResult: \\(r(XY)=r(X)\\) if \\(Y\\) is square and full rank\nResult: \\(r(X+Y)\\leq r(X) + r(Y)\\)"
  },
  {
    "objectID": "material-linearalgebra.html#properties-of-square-matrices",
    "href": "material-linearalgebra.html#properties-of-square-matrices",
    "title": "Linear Algebra",
    "section": "",
    "text": "Consider the case of a square, \\(n\\times n\\), matrix \\(A\\). We say that,\n\nDefinition 12 \\(A\\) is singular if the \\(r(A)&lt;n\\),\n\nor that,\n\nDefinition 13 \\(A\\) is non-singular if the \\(r(A)=n\\).\n\nThe singularity of a square matrix is important as it determines the invertibility of a matrix, which typically relates the existence of a unique solution in systems of linear equations. Here are a few key results,\n\nResult: There exists a matrix \\(B=A^{-1}\\), such that \\(AB=I_n\\) (where \\(I_n\\) is the identity matrix), if and only if \\(A\\) is non-singular.\nResult: \\(A\\) is non-singular if and only if the determinant of \\(A\\) is non-zero: \\(det(A)\\neq0\\).1\nResult: Likewise, \\(A\\) is singular if and only if \\(det(A)=0\\).\nResult: \\(AA^{-1}=A^{-1}A=I\\)\nResult: \\((A')^{-1}=(A^{-1})'\\)\nResult: If their respective inverses exist, then \\((AB)^{-1}=B^{-1}A^{-1}\\).\nResult: \\(det(AB)=det(A)det(B)\\)\nResult: \\(det(A^{-1})=det(A)^{-1}\\)\n\nFor any square matrix \\(A\\),\n\nDefinition 14 the trace of \\(A\\) is the sum of all diagonal elements: \\[\ntr(A) = \\sum_{i=1}^na_{ii}\n\\]\n\nRegarding the trace of a square matrix, here are a few important results:\n\nResult: \\(tr(A+B) = tr(A) + tr(B)\\)\nResult: \\(tr(\\lambda A) = \\lambda tr(A)\\) where \\(\\lambda\\) is a scalar\nResult: \\(tr(A) = tr(A')\\)\nResult: \\(tr(AB) = tr(BA)\\) where \\(AB\\) and \\(BA\\) are both square, but need not be of the same order.\nResult: \\(||A|| = (tr(A'A))^{1/2}\\)"
  },
  {
    "objectID": "material-linearalgebra.html#properties-of-symmetric-matrices",
    "href": "material-linearalgebra.html#properties-of-symmetric-matrices",
    "title": "Linear Algebra",
    "section": "",
    "text": "A symmetric matrix has the property that \\(A=A'\\). Therefore, \\(A\\) must be square.\nHere are a few important results concerning symmetric matrices.\n\nResult: \\(A^{-1}\\) exists if \\(det(A)\\neq 0\\) and \\(r(A)=n\\)\nResult: A is diagonalizable.2\nResult: The eigenvector decomposition of a square matrix gives you \\(A=C\\Lambda C^{-1}\\) where \\(\\Lambda\\) is a diagonal matrix of eigenvalues and $C$ a matrix of the corresponding eigenvectors. The symmetry of \\(A\\) gives you that \\(C^{-1}=C'\\Rightarrow A=C\\Lambda C'\\) with \\(C'C=CC'=I_{n}\\).3\n\nA key definition concerning symmetric matrices is their positive definiteness:\n\nDefinition 15 \\(A\\) is positive semi-definite if for any \\(x\\in\\mathbb{R}^n,\\;x'Ax\\geq0\\).\n\nGiven the eigenvector decomposition of a symmetric matrix, positive semi-definiteness implies \\(\\Lambda\\) is positive semi-definite: \\(\\lambda_i\\geq0\\quad\\forall i\\). Likewise,\n\nDefinition 16 \\(A\\) is positive definite if for any \\(x\\in\\mathbb{R}^n,\\;x'Ax&gt;0\\).\n\nAgain, based on the egeinvector decomposition, positive semi-definiteness implies \\(\\Lambda\\) is positive definite: \\(\\lambda_i&gt;0\\quad\\forall i\\).\nA few more results are:\n\nResult: \\(tr(A) = \\sum_{i=1}^n\\lambda_i\\)\nResult: \\(r(A) = r(\\Lambda)\\)\nResult: \\(det(A) = \\prod_{i=1}^n \\lambda_i\\)\n\nThis last result can be used to prove that any positive definite matrix is non-singular and therefore has an inverse.\nAny full-rank, positive semi-definite, symmetric matrix \\(B\\) has the additional properties:\n\nResult: \\(B=C\\Lambda C'\\) and \\(B^{-1} = C\\Lambda^{-1}C'\\)\nResult: We can define the square-root of \\(B\\) as \\(B^{1/2} = C\\Lambda^{1/2}C'\\). Similarly, \\(B^{-1/2} = C\\Lambda^{-1/2}C'\\)."
  },
  {
    "objectID": "material-linearalgebra.html#properties-of-idempotent-matrices",
    "href": "material-linearalgebra.html#properties-of-idempotent-matrices",
    "title": "Linear Algebra",
    "section": "",
    "text": "An idempotent matrix has the property that \\(D=DD\\). Therefore, \\(D\\) must be square.\nHere are a few important results concerning idempotent matrices.\n\nResult: \\(D\\) is positive definite\nResult: \\(D\\) is diagonalizable\nResult: \\((I_n-D)\\) is also an idempotent matrix\nResult: With the exception of \\(I_n\\), all idempotent matrices are singular.\nResult: \\(r(D) = tr(D) = \\sum_{i=1}^n\\lambda_i\\)\nResult: \\(\\lambda_i\\in\\{0,1\\}\\quad \\forall\\;i\\)\n\nProjection matrices are idempotent, but need not be symmetric. However, for the purposes of this module we will deal exclusively with symmetric idempotent projection matrices."
  },
  {
    "objectID": "material-linearalgebra.html#vector-differentiation",
    "href": "material-linearalgebra.html#vector-differentiation",
    "title": "Linear Algebra",
    "section": "",
    "text": "Here we will look at the derivatives of scalar with respect to (W.r.t.) a vector. You can also define other derivatives, such as the derivative of a vector w.r.t. a vector and the derivative of a scalar with respect to a matrix. However, these are not needed for these notes.\n\n\nSuppose \\(f(x)\\in R\\) (i.e. a scalar) and \\(x\\in R^n\\) (i.e. a \\(n\\times 1\\) vector). Then we can define the partial derivative of \\(f(x)\\) w.r.t. \\(x\\) as,\n\\[\n  \\frac{\\partial f(x)}{\\partial x}  = \\begin{bmatrix}\\frac{\\partial f(x)}{\\partial x_1} \\\\ \\frac{\\partial f(x)}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial f(x)}{\\partial x_n} \\end{bmatrix}\n\\]\n\n\n\nA special case is when \\(f(x)\\) is linear in \\(x\\),\n\\[\nf(x) = a'x = \\sum_{i=1}^n a_ix_i\n\\] for \\(a\\in R^n\\). The derivative of \\(a'x\\) with respect to the vector \\(x\\) can be defined as,\n\\[\n  \\begin{aligned}\n  \\frac{\\partial a'x}{\\partial x} & = \\begin{bmatrix}\\frac{\\partial a'x}{\\partial x_1} \\\\ \\frac{\\partial a'x}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial a'x}{\\partial x_n} \\end{bmatrix} \\\\\n  & = \\begin{bmatrix}a_1 \\\\ a_2 \\\\ \\vdots \\\\ a_n \\end{bmatrix} \\\\\n  & = a\n  \\end{aligned}\n\\] since the the partial derivate of \\(a'x = \\sum_{i=1}^n a_ix_i\\) w.r.t. \\(x_i\\) is just the scalar \\(a_i\\).\nThe derivative of a scalar w.r.t. to a vector yields a vector of partial derivatives.\nSince \\(a'x\\) is a scalar, it is by definition symmetric: \\(a'x = x'a\\). Thus,\n\\[\n\\frac{\\partial x'a}{\\partial x} = \\frac{\\partial a'x}{\\partial x} = a\n\\]\n\n\n\nSuppose \\(f(x)\\) is a linear transformation of \\(x\\),\n\\[\nf(x) = A'x\n\\] for any \\(m\\times n\\) matrix A,\n\\[\n  A = \\begin{bmatrix}a_1' \\\\ a_2' \\\\ \\vdots \\\\ a_m'\\end{bmatrix}\n\\] where \\(a_i\\in R^n\\;\\forall i=1,\\cdots,m\\) and,\n\\[\n  Ax = \\begin{bmatrix}a_1'x \\\\ a_2'x \\\\ \\vdots \\\\ a_m'x\\end{bmatrix}\n\\] Note, \\(f(x)=Ax\\in R^m\\), a \\(m\\times 1\\) vector. We can then define,\n\\[\n  \\begin{aligned}\n  \\frac{\\partial Ax}{\\partial x'} & = \\begin{bmatrix}\\frac{\\partial a_1'x}{\\partial x_1} & \\frac{\\partial a_1'x}{\\partial x_2} & \\cdots & \\frac{\\partial a_1'x}{\\partial x_n}\\\\ \\frac{\\partial a_2'x}{\\partial x_1} & \\frac{\\partial a_2'x}{\\partial x_2} & \\cdots & \\frac{\\partial a_2'x}{\\partial x_n}\\\\ \\vdots & \\vdots & \\ddots & \\vdots\\\\ \\frac{\\partial a_m'x}{\\partial x_1} & \\frac{\\partial a_m'x}{\\partial x_2} & \\cdots & \\frac{\\partial a_m'x}{\\partial x_n}\\\\ \\end{bmatrix} \\\\\n  & = \\begin{bmatrix} a_{11} & a_{12} & \\cdots & a_{1n} \\\\ a_{21} & a_{22} & \\cdots & a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots\\\\ a_{m1} & a_{m2} & \\cdots & a_{mn} \\\\ \\end{bmatrix} \\\\\n  & = A\n  \\end{aligned}\n\\]\nSince Ax is \\(m\\times 1\\) column vector, we take the derivative w.r.t. \\(x'\\) a row vector and not the column vector \\(x\\). This results in a matrix of partial derivatives.\n\n\n\nA second special case is where the function takes on the quadaratic form,\n\\[\nf(x) = x'Ax = \\sum_{i=1}^N\\sum_{j=1}^n a_{ij}x_ix_j\n\\]\nfor \\(n\\times n\\) (square) matrix A. As in the first linear case, \\(f(x)\\) is scalar.\nDefine \\(c = Ax\\), the \\(x'Ax = x'c\\). From the linear case, we know that,\n\\[\n\\frac{\\partial x'c}{\\partial x} = c\n\\]\nSimilarly, if we define \\(d = A'x\\) then \\(x'Ax = d'x\\). From the linear case, we know that,\n\\[\n\\frac{\\partial d'x}{\\partial x} = d\n\\]\nWe can define the total derivative as the sum of the partial derivatives w.r.t. to the first and second \\(x\\). Combining these two results, we have that,\n\\[\n\\frac{\\partial x'Ax}{\\partial x} = Ax + A'x\n\\]\nAnd if \\(A\\) is symmetric, this result simplifies to \\(2Ax\\)."
  },
  {
    "objectID": "material-linearalgebra.html#footnotes",
    "href": "material-linearalgebra.html#footnotes",
    "title": "Linear Algebra",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThese notes do not cover how to calculate the determinant of a square matrix. You should be able to find a definition easily online.↩︎\nA matrix is diagonalizable if it is similar to some other diagonal matrix. Matrices \\(B\\) and \\(C\\) are similar if \\(C=PBP^{-1}\\). A square matrix which is not diagonalizable is defective. This property relates closely to eigenvector decomposition.↩︎\nRecall, an eigenvalue and eigenvector pair, \\((\\lambda,c)\\), of matrix \\(A\\) satisfy:\n\\[\nAc = \\lambda c\\Rightarrow (A-\\lambda I_n)c=0\n\\]↩︎"
  },
  {
    "objectID": "material-inference.html",
    "href": "material-inference.html",
    "title": "Statistical Inference",
    "section": "",
    "text": "This note reviews material that should be familiar from undergraduate texts. However, we will borrow the notation from Handouts 1-4 to make it relevant to this module.\nThe note reviews the following topics:\n\nconfidence intervals;\nsimple hypothesis tests;\np-values;\nstatistical power.\n\nIt does not cover (multiple) linear hypotheses, which is the subject of Handout 4.\n\n\n\nConsider the model, \\[\n  Y = \\beta_1X_1 + X_2\\beta_2 +u\n\\] where \\(\\beta_1\\) is a scalar. We know that under CLRM 1-6,1\n\\[\n  \\hat{\\beta}_1 |X\\sim N(\\beta_1,\\sigma^2/(X_1'M_2X_1))\n\\] We can write \\(\\sigma^2(X_1'M_2X_1)^{-1}\\) as a fraction in this way, since \\(\\hat{\\beta}_1\\) is a scalar.\nLet us define,\n\\[\n\\begin{aligned}\nV =& \\sigma^2/(X_1'M_2X_1) \\\\\n\\text{and}\\quad\\hat{V} =& s^2/(X_1'M_2X_1)\n\\end{aligned}\n\\] Where the difference is that in \\(\\hat{V}\\) includes, \\(s^2\\), the estimator for \\(\\sigma^2\\). Thus,\n\\[\n\\begin{aligned}\n\\frac{\\hat{\\beta}_1-\\beta_1}{\\sqrt{V}} & \\sim N(0,1) \\\\\n\\text{and}\\quad\\frac{\\hat{\\beta}_1-\\beta_1}{\\sqrt{\\hat{V}}} & \\sim T_{n-k}\n\\end{aligned}\n\\]\n\n\n\nHere we are going to descibe the the confidence interval (CI) for a single estimator. You can expand this to the vector case, but must then consider the joint distribution of the estimators.\nGiven the unknown population parameter, \\(\\beta_1\\), we want to define the CI such that,\n\\[\nPr(\\beta_1 \\in CI_{1-\\alpha}|X) = 1-\\alpha\n\\] We know that \\((\\hat{\\beta}_1-\\beta_1)/\\sqrt{V} \\sim N(0,1)\\). Therefore, using the percentiles of the standard normal distribution, we can say that,\n\\[\n\\begin{aligned}\n1-\\alpha =& Pr\\bigg(z_{\\alpha/2}\\leq \\frac{\\hat{\\beta}_1-\\beta_1}{\\sqrt{V}} \\leq z_{1-\\alpha/2}\\bigg) \\\\\n=& Pr\\big(z_{\\alpha/2}\\sqrt{V}\\leq \\hat{\\beta}_1-\\beta_1 \\leq z_{1-\\alpha/2}\\sqrt{V}\\big) \\\\\n=& Pr\\big(z_{\\alpha/2}\\sqrt{V}-\\hat{\\beta}_1\\leq -\\beta_1 \\leq z_{1-\\alpha/2}\\sqrt{V}-\\hat{\\beta}_1\\big) \\\\\n=& Pr\\big(\\hat{\\beta}_1-z_{1-\\alpha/2}\\sqrt{V}\\leq \\beta_1 \\leq \\hat{\\beta}_1-z_{\\alpha/2}\\sqrt{V}\\big)\n\\end{aligned}\n\\] Pay careful attention to the switch from lines 3 to 4. The multiplying of the inequalities by -1 switches the upper and lower thresholds.\nIn this case, we can use the symmetry of the normal distribution. Since the normal distribution is symmetric, \\(-z_{\\alpha/2} =  z_{1-\\alpha/2}\\). This gives us the symmetric CI:\n\\[\nCI_{1-\\alpha}= \\big[\\hat{\\beta}_1-z_{1-\\alpha/2}\\sqrt{V},\\hat{\\beta}_1+z_{1-\\alpha/2}\\sqrt{V}\\big]\n\\] However, not all distributions are symmetric. For example, can you solve the confidence interval of \\(\\sigma^2\\), using the estimator \\(s^2\\)?\nSince, the t-distribution is also symmetric, we can define the a very similar CI using \\(\\hat{V}\\):\n\\[\nCI_{1-\\alpha}= \\big[\\hat{\\beta}_1-t_{n-k,1-\\alpha/2}\\sqrt{V},\\hat{\\beta}_1+t_{n-k,1-\\alpha/2}\\sqrt{V}\\big]\n\\] where \\(t_{n-k,1-\\alpha/2}\\) is the \\(1-\\alpha/2\\) percentile of the t-distribution (with \\(n-k\\) dof).\n\n\n\nWe will consider both two-sided and one-sided hypothesis tests. The former are actually simpler than the latter, since they (typically) involve sharp null hypotheses.\n\n\nConsider the two-sided hypothesis:\n\\[\n\\begin{aligned}\n  &H_0: \\beta_1 = r \\\\\n  \\text{against}\\quad &H_1: \\beta_1 \\neq r\n\\end{aligned}\n\\] Here, \\(r\\) is just a constant (i.e. non-random scalar). For example, \\(r=0\\) is a simple test for whether \\(\\beta_1\\) is statistically significant.\nA test requires a rejection rule that determines when you reject \\(H_0\\) based on the value of the test-statistic. Consider, the T-statistic:\n\\[\n  \\text{T-stat} = \\frac{\\hat{\\beta}_1-r}{\\sqrt{\\hat{V}}}\n\\] Under \\(H_0: \\beta_1=r\\), implying that the T-statistic has a t-distribution. Note, it has this distribution only under \\(H_0\\). The statistic is a R.V. with infinite support. It has the continuous distribution over the real line, meaning that with a non-zero probability it can take on any value. If this is the case, any realization of the statistic (in by implication the estimator \\(\\hat{\\beta}_1\\)) is consistent with \\(H_0\\).\nThe goal is to control the probability of type 1 errors: the probability of rejecting \\(H_0\\) when it is true. We do so, be choosing a significance level \\(\\alpha\\) which will determine the size of the test.2\nConsider the rejection rule:\n\nReject \\(H_0\\) if \\(\\text{T-stat}\\leq t_{n-k,\\alpha/2}\\) or \\(\\text{T-stat}\\geq t_{n-k,1-\\alpha/2}\\).\n\nThe probability of a type 1 error is:\n\\[\n\\begin{aligned}\nPr(\\text{Reject}\\;H_0|\\beta_1 = r,X) =& Pr(\\text{T-stat}\\leq t_{n-k,\\alpha/2}|\\beta_1 = r,X) \\\\\n&+Pr(\\text{T-stat}\\geq t_{n-k,1-\\alpha/2}|\\beta_1 = r,X) \\\\\n=&\\alpha/2+\\alpha/2 \\\\\n=&\\alpha\n\\end{aligned}\n\\] This test has size \\(\\alpha\\). Moreover, given the symmetry of the t-distribution, we can write the rejection rule as:\n\nReject \\(H_0\\) if \\(|\\text{T-stat}|\\geq t_{n-k,1-\\alpha/2}\\).\n\n\n\n\nConsider the one-sided hypothesis:\n\\[\n\\begin{aligned}\n  &H_0: \\beta_1 \\leq r \\\\\n  \\text{against}\\quad &H_1: \\beta_1 &gt; r\n\\end{aligned}\n\\] The null hypothesis is no longer sharp. Any value of \\(\\leq\\) satisifies the null. It therefore becomes more difficult to think about the size of the test.\nConsider the rejection rule:\n\nReject \\(H_0\\) if \\(\\text{T-stat}\\geq t_{n-k,1-\\alpha}\\).\n\nWe reject \\(H_0\\) is the value of the test statistic is much greater than the \\(1-\\alpha\\) percentile of the t-distribution. Does this test have the right size?\n\\[\n\\begin{aligned}\nPr(\\text{Reject}\\;H_0|\\beta_1 \\leq r,X) =& Pr(\\text{T-stat}\\geq t_{n-k,1-\\alpha}|\\beta_1 \\leq r,X) \\\\\n\\leq&Pr(\\text{T-stat}\\geq t_{n-k,1-\\alpha}|\\beta_1 = r,X) \\\\\n=&\\alpha\n\\end{aligned}\n\\] The inequality appears, because for values of \\(\\beta_1&lt;r\\), the probability of rejection is strictly \\(&lt;\\alpha\\). This is not a problem, so long as for the threshold case \\(\\beta_1 = r\\), the probability is still \\(\\leq\\alpha\\).\n\n\n\n\nThe p-value of a test corresponds to the smallest significance level (i.e. \\(\\alpha\\)) at which you reject \\(H_0\\). It is an incredibly useful value to compute, because it can be used a rejection rule:\n\nReject \\(H_0\\) if p-value \\(\\leq \\alpha\\).\n\nThis provides you with a rejection rule that is independent of the distribution of the test-statistic. The critical values of a test-statistic will depend on the distribution, but the p-value not.\nIn the above two-sided test, the p-value is given by\n\\[\n\\text{p-value} = 2\\times(1-T_{n-k}(|\\text{T-stat}|))\n\\] where \\(T_{n-k}(\\cdot)\\) is the CDF of the t-distribution (with \\(n-k\\) dof).\nFor the one-sided test we considered, it would be:\n\\[\n\\text{p-value} = 1-T_{n-k}(\\text{T-stat})\n\\]\nWhile the p-value is computed using the CDF of the test-statistic, and is a value \\(\\in[0,1]\\), it is not a probability. However, since it is a function of the test-statistic, it is a random variable.\nOne interesting feature of the p-value is that it has a uniform distribution. Consider, the probability that the p-value is less than some value \\(\\rho\\). We will use the one-sided test to simplify things.\n\\[\n\\begin{aligned}\nPr(\\text{p-value}\\leq\\rho|X,\\beta_1=r) =& Pr(1-T_{n-k}(\\text{T-stat})\\leq\\rho|X,\\beta_1=r) \\\\\n=&Pr(T_{n-k}(\\text{T-stat})\\geq 1-\\rho|X,\\beta_1 = r) \\\\\n=&Pr(\\text{T-stat}\\geq T_{n-k}^{-1}(1-\\rho)|X,\\beta_1 = r) \\\\\n=&1-T_{n-k}\\big(T_{n-k}^{-1}(1-\\rho)\\big) \\\\\n=& 1-(1-\\rho) \\\\\n=&\\rho\n\\end{aligned}\n\\] Thus, the p-value has the characteristic of a uniformly distributed random variable: for \\(X\\sim U(0,1)\\Rightarrow Pr(X\\leq x) = x\\).\nThis fact is used to evaluate p-hacking and publication bias in published research. By collecting and plotting the distribution of p-values from published research, you can test how much the distribution varies from uniform.\n\n\n\nStastical power refers to the probability of rejecting \\(H_0\\) for a given value of the unknown parameter, which need not correspond to the value under the null hypothesis. Consider the sharp null from the two-sided test: \\(H_0:\\beta_1 = r\\). Suppose, the true value of \\(\\beta_1\\) is some other value \\(\\kappa\\).\n\\[\nPr(\\text{Reject}\\;H_0|\\beta_1 = \\kappa,X) =Pr(|\\text{T-stat}|\\geq t_{n-k,1-\\alpha/2}|\\beta_1 = \\kappa,X)\n\\] The T-statistic has a t-distribution only under the null hypothesis (\\(H_0\\) is true). If \\(\\beta_1=\\kappa\\neq r\\), then this probability is not \\(\\alpha\\). \\[\n\\begin{aligned}\nPr(\\text{Reject}\\;H_0|\\beta_1 = \\kappa,X) =& Pr\\bigg(\\frac{\\hat{\\beta}_1-r}{\\sqrt{\\hat{V}}}\\leq t_{n-k,\\alpha/2}\\bigg|\\beta_1 = \\kappa,X\\bigg) \\\\\n&+Pr\\bigg(\\frac{\\hat{\\beta}_1-r}{\\sqrt{\\hat{V}}}\\geq t_{n-k,1-\\alpha/2}\\bigg|\\beta_1 = \\kappa,X\\bigg) \\\\\n=& Pr\\bigg(\\frac{\\hat{\\beta}_1-\\kappa}{\\sqrt{\\hat{V}}}+\\frac{\\kappa-r}{\\sqrt{\\hat{V}}}\\leq t_{n-k,\\alpha/2}\\bigg|\\beta_1 = \\kappa,X\\bigg) \\\\\n&+Pr\\bigg(\\frac{\\hat{\\beta}_1-\\kappa}{\\sqrt{\\hat{V}}}+\\frac{\\kappa-r}{\\sqrt{\\hat{V}}}\\geq t_{n-k,1-\\alpha/2}\\bigg|\\beta_1 = \\kappa,X\\bigg) \\\\\n=& Pr\\bigg(\\frac{\\hat{\\beta}_1-\\kappa}{\\sqrt{\\hat{V}}}\\leq t_{n-k,\\alpha/2}-\\frac{\\kappa-r}{\\sqrt{\\hat{V}}}\\bigg|\\beta_1 = \\kappa,X\\bigg) \\\\\n&+Pr\\bigg(\\frac{\\hat{\\beta}_1-\\kappa}{\\sqrt{\\hat{V}}}\\geq t_{n-k,1-\\alpha/2}-\\frac{\\kappa-r}{\\sqrt{\\hat{V}}}\\bigg|\\beta_1 = \\kappa,X\\bigg)\n\\end{aligned}\n\\] Under the condition that \\(\\beta_1 = \\kappa\\), \\[\n\\frac{\\hat{\\beta}_1-\\kappa}{\\sqrt{\\hat{V}}}\\sim T_{n-k}\n\\]\n\\[\n\\begin{aligned}\nPr(\\text{Reject}\\;H_0|\\beta_1 = \\kappa,X) =& Pr\\bigg(T\\leq t_{n-k,\\alpha/2}-\\frac{\\kappa-r}{\\sqrt{\\hat{V}}}\\bigg|\\beta_1 = \\kappa,X\\bigg) \\\\\n&+Pr\\bigg(T\\geq t_{n-k,1-\\alpha/2}-\\frac{\\kappa-r}{\\sqrt{\\hat{V}}}\\bigg|\\beta_1 = \\kappa,X\\bigg) \\\\\n&\\geq \\alpha\n\\end{aligned}\n\\]\nWe know that this probability is at least as large as \\(\\alpha\\), the probability of a type 1 error. As a function of \\(\\kappa\\), this probability will tend towards 1, as \\(\\kappa\\) moves further from the null hypothesis. For a two-sided test, it does so symmetrically, creating a bell-shaped function in the case of the normal and t-distributions. Note, as \\(|\\kappa-r|\\) increases, the one probability increases, while the other decreases. Regardless, the power increases because the one probability increases faster than the other decreases. This is a result of the bell-shaped t-distribution.\nFor a one-sided test, the power function will asymptote to 1 only on the side of rejection. We say that a test is uniformly more powerful, if has greater statistical power for all possible values of the parameter.\nWhen comparing tests that have the same distribution, the difference in power will arise from the variance of the estimator \\(\\hat{V}\\). A more efficient estimator, will yield a more power test."
  },
  {
    "objectID": "material-inference.html#overview",
    "href": "material-inference.html#overview",
    "title": "Statistical Inference",
    "section": "",
    "text": "This note reviews material that should be familiar from undergraduate texts. However, we will borrow the notation from Handouts 1-4 to make it relevant to this module.\nThe note reviews the following topics:\n\nconfidence intervals;\nsimple hypothesis tests;\np-values;\nstatistical power.\n\nIt does not cover (multiple) linear hypotheses, which is the subject of Handout 4."
  },
  {
    "objectID": "material-inference.html#set-up",
    "href": "material-inference.html#set-up",
    "title": "Statistical Inference",
    "section": "",
    "text": "Consider the model, \\[\n  Y = \\beta_1X_1 + X_2\\beta_2 +u\n\\] where \\(\\beta_1\\) is a scalar. We know that under CLRM 1-6,1\n\\[\n  \\hat{\\beta}_1 |X\\sim N(\\beta_1,\\sigma^2/(X_1'M_2X_1))\n\\] We can write \\(\\sigma^2(X_1'M_2X_1)^{-1}\\) as a fraction in this way, since \\(\\hat{\\beta}_1\\) is a scalar.\nLet us define,\n\\[\n\\begin{aligned}\nV =& \\sigma^2/(X_1'M_2X_1) \\\\\n\\text{and}\\quad\\hat{V} =& s^2/(X_1'M_2X_1)\n\\end{aligned}\n\\] Where the difference is that in \\(\\hat{V}\\) includes, \\(s^2\\), the estimator for \\(\\sigma^2\\). Thus,\n\\[\n\\begin{aligned}\n\\frac{\\hat{\\beta}_1-\\beta_1}{\\sqrt{V}} & \\sim N(0,1) \\\\\n\\text{and}\\quad\\frac{\\hat{\\beta}_1-\\beta_1}{\\sqrt{\\hat{V}}} & \\sim T_{n-k}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "material-inference.html#confidence-interval",
    "href": "material-inference.html#confidence-interval",
    "title": "Statistical Inference",
    "section": "",
    "text": "Here we are going to descibe the the confidence interval (CI) for a single estimator. You can expand this to the vector case, but must then consider the joint distribution of the estimators.\nGiven the unknown population parameter, \\(\\beta_1\\), we want to define the CI such that,\n\\[\nPr(\\beta_1 \\in CI_{1-\\alpha}|X) = 1-\\alpha\n\\] We know that \\((\\hat{\\beta}_1-\\beta_1)/\\sqrt{V} \\sim N(0,1)\\). Therefore, using the percentiles of the standard normal distribution, we can say that,\n\\[\n\\begin{aligned}\n1-\\alpha =& Pr\\bigg(z_{\\alpha/2}\\leq \\frac{\\hat{\\beta}_1-\\beta_1}{\\sqrt{V}} \\leq z_{1-\\alpha/2}\\bigg) \\\\\n=& Pr\\big(z_{\\alpha/2}\\sqrt{V}\\leq \\hat{\\beta}_1-\\beta_1 \\leq z_{1-\\alpha/2}\\sqrt{V}\\big) \\\\\n=& Pr\\big(z_{\\alpha/2}\\sqrt{V}-\\hat{\\beta}_1\\leq -\\beta_1 \\leq z_{1-\\alpha/2}\\sqrt{V}-\\hat{\\beta}_1\\big) \\\\\n=& Pr\\big(\\hat{\\beta}_1-z_{1-\\alpha/2}\\sqrt{V}\\leq \\beta_1 \\leq \\hat{\\beta}_1-z_{\\alpha/2}\\sqrt{V}\\big)\n\\end{aligned}\n\\] Pay careful attention to the switch from lines 3 to 4. The multiplying of the inequalities by -1 switches the upper and lower thresholds.\nIn this case, we can use the symmetry of the normal distribution. Since the normal distribution is symmetric, \\(-z_{\\alpha/2} =  z_{1-\\alpha/2}\\). This gives us the symmetric CI:\n\\[\nCI_{1-\\alpha}= \\big[\\hat{\\beta}_1-z_{1-\\alpha/2}\\sqrt{V},\\hat{\\beta}_1+z_{1-\\alpha/2}\\sqrt{V}\\big]\n\\] However, not all distributions are symmetric. For example, can you solve the confidence interval of \\(\\sigma^2\\), using the estimator \\(s^2\\)?\nSince, the t-distribution is also symmetric, we can define the a very similar CI using \\(\\hat{V}\\):\n\\[\nCI_{1-\\alpha}= \\big[\\hat{\\beta}_1-t_{n-k,1-\\alpha/2}\\sqrt{V},\\hat{\\beta}_1+t_{n-k,1-\\alpha/2}\\sqrt{V}\\big]\n\\] where \\(t_{n-k,1-\\alpha/2}\\) is the \\(1-\\alpha/2\\) percentile of the t-distribution (with \\(n-k\\) dof)."
  },
  {
    "objectID": "material-inference.html#hypothesis-tests",
    "href": "material-inference.html#hypothesis-tests",
    "title": "Statistical Inference",
    "section": "",
    "text": "We will consider both two-sided and one-sided hypothesis tests. The former are actually simpler than the latter, since they (typically) involve sharp null hypotheses.\n\n\nConsider the two-sided hypothesis:\n\\[\n\\begin{aligned}\n  &H_0: \\beta_1 = r \\\\\n  \\text{against}\\quad &H_1: \\beta_1 \\neq r\n\\end{aligned}\n\\] Here, \\(r\\) is just a constant (i.e. non-random scalar). For example, \\(r=0\\) is a simple test for whether \\(\\beta_1\\) is statistically significant.\nA test requires a rejection rule that determines when you reject \\(H_0\\) based on the value of the test-statistic. Consider, the T-statistic:\n\\[\n  \\text{T-stat} = \\frac{\\hat{\\beta}_1-r}{\\sqrt{\\hat{V}}}\n\\] Under \\(H_0: \\beta_1=r\\), implying that the T-statistic has a t-distribution. Note, it has this distribution only under \\(H_0\\). The statistic is a R.V. with infinite support. It has the continuous distribution over the real line, meaning that with a non-zero probability it can take on any value. If this is the case, any realization of the statistic (in by implication the estimator \\(\\hat{\\beta}_1\\)) is consistent with \\(H_0\\).\nThe goal is to control the probability of type 1 errors: the probability of rejecting \\(H_0\\) when it is true. We do so, be choosing a significance level \\(\\alpha\\) which will determine the size of the test.2\nConsider the rejection rule:\n\nReject \\(H_0\\) if \\(\\text{T-stat}\\leq t_{n-k,\\alpha/2}\\) or \\(\\text{T-stat}\\geq t_{n-k,1-\\alpha/2}\\).\n\nThe probability of a type 1 error is:\n\\[\n\\begin{aligned}\nPr(\\text{Reject}\\;H_0|\\beta_1 = r,X) =& Pr(\\text{T-stat}\\leq t_{n-k,\\alpha/2}|\\beta_1 = r,X) \\\\\n&+Pr(\\text{T-stat}\\geq t_{n-k,1-\\alpha/2}|\\beta_1 = r,X) \\\\\n=&\\alpha/2+\\alpha/2 \\\\\n=&\\alpha\n\\end{aligned}\n\\] This test has size \\(\\alpha\\). Moreover, given the symmetry of the t-distribution, we can write the rejection rule as:\n\nReject \\(H_0\\) if \\(|\\text{T-stat}|\\geq t_{n-k,1-\\alpha/2}\\).\n\n\n\n\nConsider the one-sided hypothesis:\n\\[\n\\begin{aligned}\n  &H_0: \\beta_1 \\leq r \\\\\n  \\text{against}\\quad &H_1: \\beta_1 &gt; r\n\\end{aligned}\n\\] The null hypothesis is no longer sharp. Any value of \\(\\leq\\) satisifies the null. It therefore becomes more difficult to think about the size of the test.\nConsider the rejection rule:\n\nReject \\(H_0\\) if \\(\\text{T-stat}\\geq t_{n-k,1-\\alpha}\\).\n\nWe reject \\(H_0\\) is the value of the test statistic is much greater than the \\(1-\\alpha\\) percentile of the t-distribution. Does this test have the right size?\n\\[\n\\begin{aligned}\nPr(\\text{Reject}\\;H_0|\\beta_1 \\leq r,X) =& Pr(\\text{T-stat}\\geq t_{n-k,1-\\alpha}|\\beta_1 \\leq r,X) \\\\\n\\leq&Pr(\\text{T-stat}\\geq t_{n-k,1-\\alpha}|\\beta_1 = r,X) \\\\\n=&\\alpha\n\\end{aligned}\n\\] The inequality appears, because for values of \\(\\beta_1&lt;r\\), the probability of rejection is strictly \\(&lt;\\alpha\\). This is not a problem, so long as for the threshold case \\(\\beta_1 = r\\), the probability is still \\(\\leq\\alpha\\)."
  },
  {
    "objectID": "material-inference.html#p-values",
    "href": "material-inference.html#p-values",
    "title": "Statistical Inference",
    "section": "",
    "text": "The p-value of a test corresponds to the smallest significance level (i.e. \\(\\alpha\\)) at which you reject \\(H_0\\). It is an incredibly useful value to compute, because it can be used a rejection rule:\n\nReject \\(H_0\\) if p-value \\(\\leq \\alpha\\).\n\nThis provides you with a rejection rule that is independent of the distribution of the test-statistic. The critical values of a test-statistic will depend on the distribution, but the p-value not.\nIn the above two-sided test, the p-value is given by\n\\[\n\\text{p-value} = 2\\times(1-T_{n-k}(|\\text{T-stat}|))\n\\] where \\(T_{n-k}(\\cdot)\\) is the CDF of the t-distribution (with \\(n-k\\) dof).\nFor the one-sided test we considered, it would be:\n\\[\n\\text{p-value} = 1-T_{n-k}(\\text{T-stat})\n\\]\nWhile the p-value is computed using the CDF of the test-statistic, and is a value \\(\\in[0,1]\\), it is not a probability. However, since it is a function of the test-statistic, it is a random variable.\nOne interesting feature of the p-value is that it has a uniform distribution. Consider, the probability that the p-value is less than some value \\(\\rho\\). We will use the one-sided test to simplify things.\n\\[\n\\begin{aligned}\nPr(\\text{p-value}\\leq\\rho|X,\\beta_1=r) =& Pr(1-T_{n-k}(\\text{T-stat})\\leq\\rho|X,\\beta_1=r) \\\\\n=&Pr(T_{n-k}(\\text{T-stat})\\geq 1-\\rho|X,\\beta_1 = r) \\\\\n=&Pr(\\text{T-stat}\\geq T_{n-k}^{-1}(1-\\rho)|X,\\beta_1 = r) \\\\\n=&1-T_{n-k}\\big(T_{n-k}^{-1}(1-\\rho)\\big) \\\\\n=& 1-(1-\\rho) \\\\\n=&\\rho\n\\end{aligned}\n\\] Thus, the p-value has the characteristic of a uniformly distributed random variable: for \\(X\\sim U(0,1)\\Rightarrow Pr(X\\leq x) = x\\).\nThis fact is used to evaluate p-hacking and publication bias in published research. By collecting and plotting the distribution of p-values from published research, you can test how much the distribution varies from uniform."
  },
  {
    "objectID": "material-inference.html#statistical-power",
    "href": "material-inference.html#statistical-power",
    "title": "Statistical Inference",
    "section": "",
    "text": "Stastical power refers to the probability of rejecting \\(H_0\\) for a given value of the unknown parameter, which need not correspond to the value under the null hypothesis. Consider the sharp null from the two-sided test: \\(H_0:\\beta_1 = r\\). Suppose, the true value of \\(\\beta_1\\) is some other value \\(\\kappa\\).\n\\[\nPr(\\text{Reject}\\;H_0|\\beta_1 = \\kappa,X) =Pr(|\\text{T-stat}|\\geq t_{n-k,1-\\alpha/2}|\\beta_1 = \\kappa,X)\n\\] The T-statistic has a t-distribution only under the null hypothesis (\\(H_0\\) is true). If \\(\\beta_1=\\kappa\\neq r\\), then this probability is not \\(\\alpha\\). \\[\n\\begin{aligned}\nPr(\\text{Reject}\\;H_0|\\beta_1 = \\kappa,X) =& Pr\\bigg(\\frac{\\hat{\\beta}_1-r}{\\sqrt{\\hat{V}}}\\leq t_{n-k,\\alpha/2}\\bigg|\\beta_1 = \\kappa,X\\bigg) \\\\\n&+Pr\\bigg(\\frac{\\hat{\\beta}_1-r}{\\sqrt{\\hat{V}}}\\geq t_{n-k,1-\\alpha/2}\\bigg|\\beta_1 = \\kappa,X\\bigg) \\\\\n=& Pr\\bigg(\\frac{\\hat{\\beta}_1-\\kappa}{\\sqrt{\\hat{V}}}+\\frac{\\kappa-r}{\\sqrt{\\hat{V}}}\\leq t_{n-k,\\alpha/2}\\bigg|\\beta_1 = \\kappa,X\\bigg) \\\\\n&+Pr\\bigg(\\frac{\\hat{\\beta}_1-\\kappa}{\\sqrt{\\hat{V}}}+\\frac{\\kappa-r}{\\sqrt{\\hat{V}}}\\geq t_{n-k,1-\\alpha/2}\\bigg|\\beta_1 = \\kappa,X\\bigg) \\\\\n=& Pr\\bigg(\\frac{\\hat{\\beta}_1-\\kappa}{\\sqrt{\\hat{V}}}\\leq t_{n-k,\\alpha/2}-\\frac{\\kappa-r}{\\sqrt{\\hat{V}}}\\bigg|\\beta_1 = \\kappa,X\\bigg) \\\\\n&+Pr\\bigg(\\frac{\\hat{\\beta}_1-\\kappa}{\\sqrt{\\hat{V}}}\\geq t_{n-k,1-\\alpha/2}-\\frac{\\kappa-r}{\\sqrt{\\hat{V}}}\\bigg|\\beta_1 = \\kappa,X\\bigg)\n\\end{aligned}\n\\] Under the condition that \\(\\beta_1 = \\kappa\\), \\[\n\\frac{\\hat{\\beta}_1-\\kappa}{\\sqrt{\\hat{V}}}\\sim T_{n-k}\n\\]\n\\[\n\\begin{aligned}\nPr(\\text{Reject}\\;H_0|\\beta_1 = \\kappa,X) =& Pr\\bigg(T\\leq t_{n-k,\\alpha/2}-\\frac{\\kappa-r}{\\sqrt{\\hat{V}}}\\bigg|\\beta_1 = \\kappa,X\\bigg) \\\\\n&+Pr\\bigg(T\\geq t_{n-k,1-\\alpha/2}-\\frac{\\kappa-r}{\\sqrt{\\hat{V}}}\\bigg|\\beta_1 = \\kappa,X\\bigg) \\\\\n&\\geq \\alpha\n\\end{aligned}\n\\]\nWe know that this probability is at least as large as \\(\\alpha\\), the probability of a type 1 error. As a function of \\(\\kappa\\), this probability will tend towards 1, as \\(\\kappa\\) moves further from the null hypothesis. For a two-sided test, it does so symmetrically, creating a bell-shaped function in the case of the normal and t-distributions. Note, as \\(|\\kappa-r|\\) increases, the one probability increases, while the other decreases. Regardless, the power increases because the one probability increases faster than the other decreases. This is a result of the bell-shaped t-distribution.\nFor a one-sided test, the power function will asymptote to 1 only on the side of rejection. We say that a test is uniformly more powerful, if has greater statistical power for all possible values of the parameter.\nWhen comparing tests that have the same distribution, the difference in power will arise from the variance of the estimator \\(\\hat{V}\\). A more efficient estimator, will yield a more power test."
  },
  {
    "objectID": "material-inference.html#footnotes",
    "href": "material-inference.html#footnotes",
    "title": "Statistical Inference",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn Handout 4 we showed that \\(\\beta_1\\) can be written as \\(c'\\beta\\) where \\(c = \\begin{bmatrix}1 & 0 & \\cdots &0\\end{bmatrix}'\\). Therefore, it must be that \\((X_1'M_2X_1)^{-1} = c'(X'X)^{-1}c\\)↩︎\nWe say that a test has size \\(\\alpha\\) if the probability of a type 1 error is \\(\\leq \\alpha\\).↩︎"
  },
  {
    "objectID": "material-cef.html",
    "href": "material-cef.html",
    "title": "Conditional Expectation Function",
    "section": "",
    "text": "Consider the random variable \\(Y_i\\in\\mathbb{R}\\) and the random vector \\(X_i\\in\\mathbb{R}^k\\), \\(k\\geq1\\).1\n\n\nThe Conditional Expecation Function (CEF) - denoted \\(E[Y_i|X_i]\\) - is a random function. It is a function that returns the expected value of \\(Y_i\\) for each realized value of \\(X_i\\). Since \\(X_i\\) is a random vector the resulting function is random itself.\nIf we fix \\(X_i=x\\), then the value at which we are evaluating the function is no longer random. The result is a constant: the expected value of \\(Y_i\\) at the given \\(x\\).\n\\[\nE[Y_i|X_i=x] = \\int y \\cdot f_{Y}(y|X_i=x)dy = \\int y dF_{Y}(y|X_i=x)\n\\]\nThis follows the same logic that the expectation of a random variable is, \\(E[Y_i]\\), is not random.\nDiscrete case. The book devotes a lot time to the discussion of cases were \\(X_i\\) is a discrete random variable; using the notation \\(W_i\\in\\{0,1\\}\\) or \\(D_i\\in\\{0,1\\}\\). In this unique case, we can write the CEF as,\n\\[\nE[Y_i|D_i] = E[Y_i|D_i=0] + D_i\\cdot\\big(E[Y_i|D_i=1]-E[Y_i|D_i=0]\\big)\n\\]\nThe above function returns \\(E[Y_i|D_i=0]\\) when \\(D_i=0\\) and \\(E[Y_i|D_i=1]\\) when \\(D_i=1\\). This expression for the CEF will be useful in latter chapters of the book.\n\n\n\nThe Law of Iterated Expectations says that given two random variables2 \\([Y_i,X_i]\\), we can express the unconditional expected value of \\(Y_i\\) as the expected value of the conditional expectation of \\(Y_i\\) on \\(X_i\\).\n\\[\n        E[Y_i] = E\\big[E[Y_i|X_i]\\big]\n\\]\nWhere the outside expectation is with respect to \\(X_i\\),3 since the CEF is a random function of \\(X_i\\). We can expand this as follows,\n\\[\nE[Y_i] = \\int t \\cdot f_{Y_i}(t)dt = \\int\\int y \\cdot f_{Y_i|X}(y|x)dyf_X(x)dx = E\\big[E[Y_i|X_i]\\big]\n\\]\n\nExample 1 Suppose \\(Y_i\\) and \\(X_i\\) are both discrete, \\(Y_i\\in\\{1,2\\}\\) and \\(X_i\\in\\{3,4\\}\\), with the joint distribution:\n\n\\(f_{Y,X}\\)\n\n\n\n\\(X_i=3\\)\n\\(X_i=2\\)\n\n\n\\(Y_i=1\\)\n1/10\n3/10\n\n\n\\(Y_i=2\\)\n2/10\n4/10\n\n\n\nWe can then define the two marginal distributions,\n\n\\(f_Y\\)\n\n\n\\(Y_i=1\\)\n\\(Y_i=2\\)\n\n\n4/10\n6/10\n\n\n\nand,\n\n\\(f_X\\)\n\n\n\\(X_i=3\\)\n\\(X_i=4\\)\n\n\n3/10\n7/10\n\n\n\nLikewise, we know the conditional distribution \\(f_{Y|X}\\); which we get by dividing the joint distribution by the marginal distribution of \\(X_i\\). Each column of the conditional distribution should add up to 1.\n\n\\(f_{Y|X}\\)\n\n\n\n\\(X_i=3\\)\n\\(X_i=4\\)\n\n\n\\(Y_i=1\\)\n1/3\n3/7\n\n\n\\(Y_i=2\\)\n2/3\n4/7\n\n\n\nNow we can calculate the following objects:\n\n\\(E[Y_i]\\)\n\n\\[\n\\begin{aligned}\n        E[Y_i] =& 1\\cdot Pr(Y_i=1)+2\\cdot Pr(Y_i=2) \\\\\n        =&1\\cdot 4/10+2\\cdot 6/10 \\\\\n        =&16/10\n    \\end{aligned}\n\\]\n\n\\(E[Y_i|X_i=3]\\)\n\n\\[\n\\begin{aligned}\n        E[Y_i|X_i=3] =& 1\\cdot Pr(Y_i=1|X_i=3)+2\\cdot Pr(Y_i=2|X_i=3) \\\\\n        =&1\\cdot 1/3+2\\cdot 2/3 \\\\\n        =&5/3\n\\end{aligned}\n\\]\n\n\\(E[Y_i|X_i=4]\\)\n\n\\[\n\\begin{aligned}\n        E[Y_i|X_i=4] =& 1\\cdot Pr(Y_i=1|X_i=4)+2\\cdot Pr(Y_i=2|X_i=4) \\\\\n        =&1\\cdot 3/7+2\\cdot 4/7 \\\\\n        =&11/7\n    \\end{aligned}\n\\]\n\n\\(E\\big[E[Y_i|X_i]\\big]\\)\n\n\\[\n\\begin{aligned}\n        E\\big[E[Y_i|X_i]\\big] =& E[Y_i|X_i=3]\\cdot Pr(X_i=3)+ E[Y_i|X_i=4]\\cdot Pr(X_i=4) \\\\\n        =&5/3\\cdot3/10+11/7\\cdot 7/10 \\\\\n        =&16/10\n    \\end{aligned}\n\\]\nWe have therefore demonstrated the law of iterated expectations.\n\nWe can extend this principle to conditional expectations. Suppose you have three random variables/vectors \\(\\{Y_i,X_i,Z_i\\}\\), we can express the conditional expected value of \\(Y_i\\) on \\(X_i\\) as the (conditional) expected value of the conditional expectation of \\(Y_i\\) on \\(X_i\\) and \\(Z_i\\).\n\\[\n        E[Y_i|X_i] = E\\big[E[Y_i|X_i,Z_i]|X_i\\big]\n\\]\nHere the outside expectation is with respect \\(Z_i\\) conditional on \\(X_i\\). It utilizes the conditional distribution \\(f_{Z|X}\\) to form the outside expectation,\n\\[\nE[Y_i|X_i] = \\int y \\cdot f_{Y|X}(y|X_i)dt = \\int\\int y \\cdot f_{Y|X,Z}(y|X_i,z)dyf_{Z|X}(z|X_i)dz = E\\big[E[Y_i|X_i,Z_i]|X_i\\big]\n\\]\n\n\n\nThe following three theorems can be found in a range of Econometrics textbooks and Microeconometrics texts, including MM & MHE\n\nTheorem 1 We can express the observed outcome \\(Y_i\\) as a sum of \\(E[Y_i|X_i]+\\varepsilon_i\\) where \\(E[\\varepsilon_i|X_i]=0\\) (i.e., mean independent).\n\n\nProof. \n\n\\(E[\\varepsilon_i | X_i] = E[Y_i - E[Y_i | X_i] | X_i] = E[Y_i | X_i] - E[Y_i | X_i] = 0\\)\n\\(E[h(X_i)\\varepsilon_i] = E[h(X_i)E[\\varepsilon_i | X_i]] = E[h(X_i) \\times 0] = 0\\)\n\n\n\nTheorem 2 \\(E[Y_i|X_i]\\) is the best predictor of \\(Y_i\\).\n\n\nProof. \\[\n\\begin{aligned}\n(Y_i - m(X_i))^2 =& \\left((Y_i - E[Y_i | X_i]) + (E[Y_i | X_i] - m(X_i))\\right)^2 \\\\\n=& (Y_i - E[Y_i \\| X_i])^2 + (E[Y_i | X_i] - m(X_i))^2 \\\\&+ 2(Y_i - E[Y_i | X_i]) \\times (E[Y_i | X_i] - m(X_i))\n\\end{aligned}\n\\]\nThe last term (cross product) is mean zero. Thus, the function is minimized by setting \\(m(X_i) = E[Y_i | X_i]\\).\n\n\nTheorem 3 [ANOVA Theorem] The variance of \\(Y_i\\) can be decomposed as \\(V(E[Y_i|X_i])+E(V(Y_i|X_i))\\)\n\n\nProof. \\[\n\\begin{aligned}\n        V(Y_i)=&V(E[Y_i|X_i] + \\varepsilon_i) \\\\\n        =&V(E[Y_i|X_i])+V(\\varepsilon_i) \\\\\n        =&V(E[Y_i|X_i])+E[\\varepsilon_i^2]\n    \\end{aligned}\n\\] The second line follows from Theorem 1.1 (independence) and\n\\[\n        E[\\varepsilon_i^2]=E\\left[E[\\varepsilon_i^2|X_i]\\right]=E\\left[V(Y_i|X_i)\\right]\n\\]"
  },
  {
    "objectID": "material-cef.html#definition",
    "href": "material-cef.html#definition",
    "title": "Conditional Expectation Function",
    "section": "",
    "text": "The Conditional Expecation Function (CEF) - denoted \\(E[Y_i|X_i]\\) - is a random function. It is a function that returns the expected value of \\(Y_i\\) for each realized value of \\(X_i\\). Since \\(X_i\\) is a random vector the resulting function is random itself.\nIf we fix \\(X_i=x\\), then the value at which we are evaluating the function is no longer random. The result is a constant: the expected value of \\(Y_i\\) at the given \\(x\\).\n\\[\nE[Y_i|X_i=x] = \\int y \\cdot f_{Y}(y|X_i=x)dy = \\int y dF_{Y}(y|X_i=x)\n\\]\nThis follows the same logic that the expectation of a random variable is, \\(E[Y_i]\\), is not random.\nDiscrete case. The book devotes a lot time to the discussion of cases were \\(X_i\\) is a discrete random variable; using the notation \\(W_i\\in\\{0,1\\}\\) or \\(D_i\\in\\{0,1\\}\\). In this unique case, we can write the CEF as,\n\\[\nE[Y_i|D_i] = E[Y_i|D_i=0] + D_i\\cdot\\big(E[Y_i|D_i=1]-E[Y_i|D_i=0]\\big)\n\\]\nThe above function returns \\(E[Y_i|D_i=0]\\) when \\(D_i=0\\) and \\(E[Y_i|D_i=1]\\) when \\(D_i=1\\). This expression for the CEF will be useful in latter chapters of the book."
  },
  {
    "objectID": "material-cef.html#law-of-iterated-expectations",
    "href": "material-cef.html#law-of-iterated-expectations",
    "title": "Conditional Expectation Function",
    "section": "",
    "text": "The Law of Iterated Expectations says that given two random variables2 \\([Y_i,X_i]\\), we can express the unconditional expected value of \\(Y_i\\) as the expected value of the conditional expectation of \\(Y_i\\) on \\(X_i\\).\n\\[\n        E[Y_i] = E\\big[E[Y_i|X_i]\\big]\n\\]\nWhere the outside expectation is with respect to \\(X_i\\),3 since the CEF is a random function of \\(X_i\\). We can expand this as follows,\n\\[\nE[Y_i] = \\int t \\cdot f_{Y_i}(t)dt = \\int\\int y \\cdot f_{Y_i|X}(y|x)dyf_X(x)dx = E\\big[E[Y_i|X_i]\\big]\n\\]\n\nExample 1 Suppose \\(Y_i\\) and \\(X_i\\) are both discrete, \\(Y_i\\in\\{1,2\\}\\) and \\(X_i\\in\\{3,4\\}\\), with the joint distribution:\n\n\\(f_{Y,X}\\)\n\n\n\n\\(X_i=3\\)\n\\(X_i=2\\)\n\n\n\\(Y_i=1\\)\n1/10\n3/10\n\n\n\\(Y_i=2\\)\n2/10\n4/10\n\n\n\nWe can then define the two marginal distributions,\n\n\\(f_Y\\)\n\n\n\\(Y_i=1\\)\n\\(Y_i=2\\)\n\n\n4/10\n6/10\n\n\n\nand,\n\n\\(f_X\\)\n\n\n\\(X_i=3\\)\n\\(X_i=4\\)\n\n\n3/10\n7/10\n\n\n\nLikewise, we know the conditional distribution \\(f_{Y|X}\\); which we get by dividing the joint distribution by the marginal distribution of \\(X_i\\). Each column of the conditional distribution should add up to 1.\n\n\\(f_{Y|X}\\)\n\n\n\n\\(X_i=3\\)\n\\(X_i=4\\)\n\n\n\\(Y_i=1\\)\n1/3\n3/7\n\n\n\\(Y_i=2\\)\n2/3\n4/7\n\n\n\nNow we can calculate the following objects:\n\n\\(E[Y_i]\\)\n\n\\[\n\\begin{aligned}\n        E[Y_i] =& 1\\cdot Pr(Y_i=1)+2\\cdot Pr(Y_i=2) \\\\\n        =&1\\cdot 4/10+2\\cdot 6/10 \\\\\n        =&16/10\n    \\end{aligned}\n\\]\n\n\\(E[Y_i|X_i=3]\\)\n\n\\[\n\\begin{aligned}\n        E[Y_i|X_i=3] =& 1\\cdot Pr(Y_i=1|X_i=3)+2\\cdot Pr(Y_i=2|X_i=3) \\\\\n        =&1\\cdot 1/3+2\\cdot 2/3 \\\\\n        =&5/3\n\\end{aligned}\n\\]\n\n\\(E[Y_i|X_i=4]\\)\n\n\\[\n\\begin{aligned}\n        E[Y_i|X_i=4] =& 1\\cdot Pr(Y_i=1|X_i=4)+2\\cdot Pr(Y_i=2|X_i=4) \\\\\n        =&1\\cdot 3/7+2\\cdot 4/7 \\\\\n        =&11/7\n    \\end{aligned}\n\\]\n\n\\(E\\big[E[Y_i|X_i]\\big]\\)\n\n\\[\n\\begin{aligned}\n        E\\big[E[Y_i|X_i]\\big] =& E[Y_i|X_i=3]\\cdot Pr(X_i=3)+ E[Y_i|X_i=4]\\cdot Pr(X_i=4) \\\\\n        =&5/3\\cdot3/10+11/7\\cdot 7/10 \\\\\n        =&16/10\n    \\end{aligned}\n\\]\nWe have therefore demonstrated the law of iterated expectations.\n\nWe can extend this principle to conditional expectations. Suppose you have three random variables/vectors \\(\\{Y_i,X_i,Z_i\\}\\), we can express the conditional expected value of \\(Y_i\\) on \\(X_i\\) as the (conditional) expected value of the conditional expectation of \\(Y_i\\) on \\(X_i\\) and \\(Z_i\\).\n\\[\n        E[Y_i|X_i] = E\\big[E[Y_i|X_i,Z_i]|X_i\\big]\n\\]\nHere the outside expectation is with respect \\(Z_i\\) conditional on \\(X_i\\). It utilizes the conditional distribution \\(f_{Z|X}\\) to form the outside expectation,\n\\[\nE[Y_i|X_i] = \\int y \\cdot f_{Y|X}(y|X_i)dt = \\int\\int y \\cdot f_{Y|X,Z}(y|X_i,z)dyf_{Z|X}(z|X_i)dz = E\\big[E[Y_i|X_i,Z_i]|X_i\\big]\n\\]"
  },
  {
    "objectID": "material-cef.html#properties-of-the-cef",
    "href": "material-cef.html#properties-of-the-cef",
    "title": "Conditional Expectation Function",
    "section": "",
    "text": "The following three theorems can be found in a range of Econometrics textbooks and Microeconometrics texts, including MM & MHE\n\nTheorem 1 We can express the observed outcome \\(Y_i\\) as a sum of \\(E[Y_i|X_i]+\\varepsilon_i\\) where \\(E[\\varepsilon_i|X_i]=0\\) (i.e., mean independent).\n\n\nProof. \n\n\\(E[\\varepsilon_i | X_i] = E[Y_i - E[Y_i | X_i] | X_i] = E[Y_i | X_i] - E[Y_i | X_i] = 0\\)\n\\(E[h(X_i)\\varepsilon_i] = E[h(X_i)E[\\varepsilon_i | X_i]] = E[h(X_i) \\times 0] = 0\\)\n\n\n\nTheorem 2 \\(E[Y_i|X_i]\\) is the best predictor of \\(Y_i\\).\n\n\nProof. \\[\n\\begin{aligned}\n(Y_i - m(X_i))^2 =& \\left((Y_i - E[Y_i | X_i]) + (E[Y_i | X_i] - m(X_i))\\right)^2 \\\\\n=& (Y_i - E[Y_i \\| X_i])^2 + (E[Y_i | X_i] - m(X_i))^2 \\\\&+ 2(Y_i - E[Y_i | X_i]) \\times (E[Y_i | X_i] - m(X_i))\n\\end{aligned}\n\\]\nThe last term (cross product) is mean zero. Thus, the function is minimized by setting \\(m(X_i) = E[Y_i | X_i]\\).\n\n\nTheorem 3 [ANOVA Theorem] The variance of \\(Y_i\\) can be decomposed as \\(V(E[Y_i|X_i])+E(V(Y_i|X_i))\\)\n\n\nProof. \\[\n\\begin{aligned}\n        V(Y_i)=&V(E[Y_i|X_i] + \\varepsilon_i) \\\\\n        =&V(E[Y_i|X_i])+V(\\varepsilon_i) \\\\\n        =&V(E[Y_i|X_i])+E[\\varepsilon_i^2]\n    \\end{aligned}\n\\] The second line follows from Theorem 1.1 (independence) and\n\\[\n        E[\\varepsilon_i^2]=E\\left[E[\\varepsilon_i^2|X_i]\\right]=E\\left[V(Y_i|X_i)\\right]\n\\]"
  },
  {
    "objectID": "material-cef.html#footnotes",
    "href": "material-cef.html#footnotes",
    "title": "Conditional Expectation Function",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe subscript \\(i\\) is not necessary here. However, this notation is consistent with the rest of the book. In this book, \\(Y_i\\) denotes a random variable, \\(\\in \\mathbb{R}\\), and \\(Y\\) a random vector, \\(\\in \\mathbb{R}^n\\). Likewise, \\(X_i\\) is a random vector, \\(\\in \\mathbb{R}^k\\), while \\(X\\) will represent a random matrix, \\(\\in \\mathbb{R}^n \\times \\mathbb{R}^k\\).↩︎\nThis can be extended to random vectors.↩︎\nSome texts use the notation \\(E_X\\big[E[Y_i|X_i]\\big]\\) to demonstrate that the outside expectation is with respect to \\(X_i\\).↩︎"
  },
  {
    "objectID": "handout-5.html",
    "href": "handout-5.html",
    "title": "Panel Data Models",
    "section": "",
    "text": "In this handout we will see how to test static linear panel data models. We will review a number of estimators for these models, including:\n\nwithin-group;\nfirst differences;\nbetween-group;\nand feasible GLS.\n\nFurther reading can be found in:\n\nSection 21 of Cameron and Trivedi (2005)\nSection 10.1-10.3 of Verbeek (2017)"
  },
  {
    "objectID": "handout-5.html#overview",
    "href": "handout-5.html#overview",
    "title": "Panel Data Models",
    "section": "",
    "text": "In this handout we will see how to test static linear panel data models. We will review a number of estimators for these models, including:\n\nwithin-group;\nfirst differences;\nbetween-group;\nand feasible GLS.\n\nFurther reading can be found in:\n\nSection 21 of Cameron and Trivedi (2005)\nSection 10.1-10.3 of Verbeek (2017)"
  },
  {
    "objectID": "handout-5.html#model-specification",
    "href": "handout-5.html#model-specification",
    "title": "Panel Data Models",
    "section": "2 Model Specification",
    "text": "2 Model Specification\nA static linear model has the form,\n\\[\n  Y_{it} = X_{it}'\\beta + \\alpha_i + \\varepsilon_{it}\n\\]\nfor \\(i=1,..,n\\) and \\(t = 1,...,T\\). We will restrict this discussion to models where \\(T\\) is fixed. Asymptotically, this means that as \\(n\\) increases \\(T\\) remains fixed.\nIf collect all \\(T\\) observations of unit \\(i\\), we can describe them by the model,\n\\[\n  Y_{i} = X_{i}'\\beta + \\alpha_i\\ell + \\varepsilon_i\n\\] where \\(Y_i\\) is a \\(T\\times 1\\) random vector; \\(X_i\\) a \\(T\\times k\\) random matrix; and \\(\\ell\\) a \\(T\\times 1\\) vector of \\(1\\)’s.\nThis model has placed no restriction on the values of the outcome variable, \\(Y_i\\), and regressors, \\(X_i\\). In particular, the regressors may include time-varying as well as time-invariant variables.\nWe can extend this specification to include a linear or non-linear trend in time; for example, \\(\\phi t\\). However, the more common option is to include a very flexible time trend using fixed effects:\n\\[\n  \\delta_t = \\sum_{j=1}^T \\delta_j \\mathbf{1}\\{t = j\\}\n\\] Time fixed-effects are essentially a dummy variable for each time-period. This flexible function - sometimes referred to as saturated - can approximate any functional form of the underlying time trend. Models that include both \\(\\alpha_i\\) and \\(\\delta_t\\) are referred to as two-way fixed-effects models.\n\n2.1 Unobserved heterogeneity\nThe term \\(\\alpha_i\\) is particularly important. It represents time-invariant unobservables or unobserved heterogeneity across \\(i\\). I strongly recommend you read the discussion on pages 285-286 of Wooldridge (2010).\nThe \\(\\alpha_i\\) is unobserved, and therefore is a component of the error term \\(\\upsilon_{it} = \\alpha_i + \\varepsilon_{it}\\). The time-invariant component of the error is permanent; often referred to as unobserved heterogeneity or individual effect. The time-varying component is transient; sometimes referred to as the idiosyncratic error.\nAs with the time-varying part of the error term, \\(\\alpha_{i}\\) is \\(random\\). We should not think of \\(\\alpha_i\\) as a unit-specific intercept, since then it is just a non-random parameter. This would imply that the correlation between \\(\\alpha_i\\) and \\(X_i\\) is by definition 0.\nGiven the model specification, we must consider all three components - \\(\\{X_i,\\alpha_i,\\varepsilon_{it}\\}\\) - when making assumptions regarding exogeneity. We will make all assumptions conditional on \\(\\alpha_i\\). This is consistent with the idea that \\(\\alpha_i\\) is a permanent shock, and is therefore realized before \\(\\varepsilon_it\\).\nIf we do not condition on \\(\\alpha_i\\), then when we evaluate \\(E[Y_i|X_i]\\), we also need to consider both \\(E[\\varepsilon_i|X_i]\\) and \\(E[\\alpha_i|X_i]\\); not just \\(E[\\varepsilon_i|X_i,\\alpha_i]\\). Moreover, in general it will be that case that \\(E[\\alpha_i|X_i]\\neq E[\\alpha_i]\\) (i.e., not mean independent). For example, \\(\\alpha_i\\) may represent unobserved ability in a wage equation. This will correlated with regressors like education."
  },
  {
    "objectID": "handout-5.html#exogeneity",
    "href": "handout-5.html#exogeneity",
    "title": "Panel Data Models",
    "section": "3 Exogeneity",
    "text": "3 Exogeneity\nHaving assumed that the samples are independent across \\(i\\), we can define mean independence of the transient error term component for unit \\(i\\). There are three potential assumptions we can make:\n\nStrict exogeneity:\n\n\\[\n  E[\\varepsilon_i|X_i,\\alpha_i] = 0\n\\] or, \\[\n  E[\\varepsilon_{it}|X_{i1},X_{i2},...,X_{iT},\\alpha_i] = 0\\qquad \\forall\\;t\n\\] 2. Weak or sequential exogeneity:\n\\[\n  E[\\varepsilon_{it}|X_{i1},X_{i2},...,X_{it},\\alpha_i] = 0\\qquad \\forall\\;t\n\\] Exogeneity with respect to the past sequence of regressors (or predetermined regressors).\n\nContemporaneous exogeneity:\n\n\\[\n  E[\\varepsilon_{it}|X_{it},\\alpha_i] = 0\\qquad \\forall\\;t\n\\] Exogeneity only with respect to the contemporaneous value of \\(X_i\\).\n\n3.1 Strict exogeneity\nStrict exogeneity is a very strong assumption. It implies that \\(X\\) is uncorrelated with past, current, and future values of the transient error term (conditional on \\(\\alpha_i\\)): \\(E[X_{it},\\varepsilon_{is}|\\alpha_i] = 0\\;\\forall\\;t,s\\). Crucially, \\(X_{it}\\) cannot respond to the history of idiosyncratic shocks \\(\\varepsilon_{i1},\\varepsilon_{i2},...,\\varepsilon_{it}\\).\nThis assumption is required for unbiasedness of the linear estimator, but not for consistency. Under strict exogeneity,\n\\[\nE[Y_{it}|X_i,\\alpha_i] = E[Y_{it}|X_{it},\\alpha_i] = X_{it}'\\beta + \\alpha_i\n\\] The first equality implies that once you control for \\(X_{it}\\), there is no additional partial effect of \\(X_{is}\\) (\\(\\forall\\;s\\neq t\\)) on (the mean of) \\(Y_{it}\\). This assumption relates to the assumed static nature of the model: the model includes not lags or leads among the regressors.\nIt would be violated if the set of regressors included a lagged dependent variable. The assumption is also violated if the regressors are endogenous.\n\n\n3.2 Weak exogeneity\nWeak exogeneity, also referred to as sequential exogeneity, implies that the error term is uncorrelated with past and contemporaneous values of the regressors:\n\\[\nE[X_{is}\\varepsilon_{it}] = 0 \\qquad\\forall\\;s=1,...,t\n\\] In the static linear model, it also implies that,\n\\[\nE[Y_{it}|X_i,\\alpha_i] = E[Y_{it}|X_{it},\\alpha_i] = X_{it}'\\beta + \\alpha_i\n\\]\nThis structure can permit a lagged dependent variable amongst the regressors (assuming there is no serial correlation of the error term). However, the assumption remains violated if the regressors are endogenous.\n\n\n3.3 Contemporaneous exogeneity\nThis assumption implies that the error term is only uncorrelated with regressors in the same time period,\n\\[\nE[X_{it}\\varepsilon_{it}] = 0\n\\] Regardless, it still implies that, \\[\nE[Y_{it}|X_i,\\alpha_i] = E[Y_{it}|X_{it},\\alpha_i] = X_{it}'\\beta + \\alpha_i\n\\]"
  },
  {
    "objectID": "handout-5.html#basic-model",
    "href": "handout-5.html#basic-model",
    "title": "Panel Data Models",
    "section": "4 Basic model",
    "text": "4 Basic model\nWe begin by describe a basic model under the Static Linear Panel Model (SLPM) assumptions,\nSLPM 1: The model is static and linear in parameters:\n\\[\nY_{it} = X_{it}'\\beta + \\alpha_i + \\varepsilon_{it}\n\\]\nSLPM 2: Balanced panel: we observe each \\(i\\) in all \\(T\\) time periods.\nSLPM 3: Independent sampling along the cross-sectional dimension (\\(i\\)).\nSLPM 4: Strict exogeneity: \\(E[\\varepsilon_{it}|X_i,\\alpha_i] = 0\\; \\forall\\;t\\)\nSLPM 5: Conditional homoskedasticity and serial uncorrelatedness of the transient error term component.\n\\[\nVar(\\varepsilon_i|X_i,\\alpha_i) = \\begin{bmatrix}\\sigma^2_\\varepsilon & 0 & \\cdots & 0 \\\\ 0 & \\sigma^2_\\varepsilon & & \\\\ \\vdots & & \\ddots & \\\\ 0 & & & \\sigma^2_\\varepsilon \\end{bmatrix} = \\sigma^2_\\varepsilon I_T\n\\] Assumptions 4 & 5 are referred to as a ‘classical error term’ structure.\nUnder assumptions 1-5:\n\n\\(E[Y_i|X_i,\\alpha_i]  = X_i'\\beta + \\alpha_i\\ell\\)\n\\(Var(Y_i|X_i,\\alpha_i) = \\sigma^2_{\\varepsilon}I_T\\)\n\nAt this stage, the unanswered question is how to deal with the unobservables \\(\\alpha_i\\) in the equation. Generally, there are two approaches:\n\nAssume away the relationship between \\(\\alpha_i\\) and \\(X_i\\).\nRemove \\(\\alpha_i\\) from the equation prior to estimator; for example, within-group estimator and first-difference.\n\nAdopting approach (1), we will review the pooled OLS, between-group, and (feasible) Generalized Least Squares (GLS) estimators. Under approach (2), will review the within-group and first-difference estimators."
  },
  {
    "objectID": "handout-5.html#pooled-ols",
    "href": "handout-5.html#pooled-ols",
    "title": "Panel Data Models",
    "section": "5 Pooled OLS",
    "text": "5 Pooled OLS\nWe can ‘assume away’ the relationship between \\(\\alpha_i\\) and \\(X_i\\). Specifically, we will assume conditional mean independence:\n\nCLPM 6 \\(E[\\alpha_i|X_i] = E[\\alpha_i|X_{i1},X_{i2},...,X_{iT}] = E[\\alpha_i]=0\\)\n\nAs in the CLRM, the assumption that the unconditional mean of \\(\\alpha_i\\) is zero is not binding given the inclusion of a constant among the regressors. We will of course, need to assume that there is no perfect colinearity among the regressors.\n\nCLPM 7 \\(rank(X)=k\\)\n\nIn addition, we will need to make assumptions concerning \\(\\alpha_i\\). First, mean independence,\nThis implies uncorrelatedness. Under this assumptions, the model can be written as,\n\\[\n  Y_{it} = X_{it}'\\beta + \\upsilon_{it}\n\\] where \\(E[\\upsilon_{i}|X_i]=0\\). Alternatively, we can stack all \\(nT\\) observations together,\n\\[\n  Y = X\\beta + \\upsilon\n\\]\nSecond, we need to make an assumption regarding the variance of the unobserved heterogeneity:\n\nCLPM 8 \\(Var(\\alpha_i|X_i) = \\sigma^2_\\alpha\\)\n\nUnder these assumptions, the error term \\(\\upsilon_i = \\alpha_i\\ell + \\varepsilon_i\\) has the variance,\n\\[\nVar(\\upsilon_i |X_i) = E[\\upsilon_i\\upsilon_i' |X_i] = \\sigma^2_\\alpha\\ell\\ell' + \\sigma^2_\\varepsilon I_T = \\Omega\n\\] For all \\(s\\neq t\\) the \\(Cov(\\upsilon_{it},\\upsilon_{is}) = \\sigma^2_\\alpha\\). And the diagonal elements are given by \\(\\sigma^2_\\alpha + \\sigma^2_\\varepsilon\\).\nUnder these assumptions, the OLS estimator,\n\\[\n\\begin{aligned}\n\\hat{\\beta}^{OLS} =& (X'X)^{-1}X'Y \\\\\n=& \\beta + (X'X)^{-1}X'\\upsilon \\\\\n=& \\beta + \\big(\\sum_iX_i'X_i\\big)^{-1}\\sum_iX_i'\\upsilon_i\n\\end{aligned}\n\\] is both unbiased and consistent. This is because,\n\\[\np \\lim\\frac{1}{n}\\sum_iX_i'\\upsilon_i = \\sum_{t=1}^Tp \\lim\\frac{1}{n}\\sum_{i=1}^nX_{it}\\upsilon_{it} = \\sum_{t=1}^TE[X_{it}\\upsilon_{it}]=0\n\\] The asymptotic distribution is given by,\n\\[\n\\begin{aligned}\n\\sqrt{n}(\\hat{\\beta}^{OLS}-\\beta) =& \\bigg(\\frac{1}{n}\\sum_iX_i'X_i\\bigg)^{-1}\\frac{1}{\\sqrt{n}}\\sum_iX_i'\\upsilon_i \\\\\n\\rightarrow_d& N(0,V^{-1}\\Sigma V^{-1})\n\\end{aligned}\n\\] where,\n\n\\(V = E[X_i'X_i]\\)\n\\(\\Sigma = E[X_i'\\Omega X_i]\\)\n\nWe say that the approximate distribution of the pooled OLS estimator is given by,\n\\[\n\\hat{\\beta}^{OLS} \\overset{a}{\\sim} N\\bigg(\\beta, \\big(\\sum_iX_i'X_i\\big)^{-1}\\sum_iX_i'\\Omega X_i\\big(\\sum_iX_i'X_i\\big)^{-1}\\bigg)\n\\]\nHowever, it is NOT efficient. Moreover, the usual homoskedastic estimator for the variance will be biased and inconsistent. Unobserved heterogeneity will result in a serial correlation that is not accountant for by the standard estimator."
  },
  {
    "objectID": "handout-5.html#between-group-estimator",
    "href": "handout-5.html#between-group-estimator",
    "title": "Panel Data Models",
    "section": "6 Between-Group estimator",
    "text": "6 Between-Group estimator\nAn alternative to pooled OLS, is to collapse the multiple observations of unit \\(i\\) into a single cross-section aggregate. This transforms the model into,\n\\[\n\\bar{Y}_{i} = \\bar{X}_{i}'\\beta+\\bar{\\upsilon}_{i}\n\\] where \\(\\bar{\\upsilon}_{i} = \\alpha_i + \\bar{\\varepsilon}_i\\). The variane of this error term is,\n\\[\nE[\\bar{\\upsilon}_i^2|X_i] = \\sigma^2_\\alpha + \\frac{\\sigma^2_\\varepsilon}{T}\n\\] The OLS estimator for \\(\\beta\\) is given by,\n\\[\n\\hat{\\beta}^{BG} = (\\bar{X}'\\bar{X})^{-1}\\bar{X}'\\bar{Y} = \\big(\\sum_i\\bar{X}_i\\bar{X}_i'\\big)^{-1}\\sum_i\\bar{X}_i\\bar{Y}_i\n\\]\nSince the variance term is homoskedastic, the standard homoskedastic variance estimator will unbiased and consistent. This approach removes the problem of serially correlated error terms across repeated observations of \\(i\\) in pooled OLS by collapsing all observations to a single observation. However, it also reduces the information in the data and is therefore less efficient."
  },
  {
    "objectID": "handout-5.html#generalized-least-squares",
    "href": "handout-5.html#generalized-least-squares",
    "title": "Panel Data Models",
    "section": "7 Generalized Least Squares",
    "text": "7 Generalized Least Squares\nThis final approach takes seriously the structure of the composite error-term variance-covariance matrix,\n\\[\nE[\\upsilon_i\\upsilon_i' |X_i] = \\begin{bmatrix}\n\\sigma^2_\\alpha+\\sigma^2_\\varepsilon & \\sigma^2_\\alpha & \\cdots & \\sigma^2_\\alpha \\\\ \\sigma^2_\\alpha & \\sigma^2_\\alpha+\\sigma^2_\\varepsilon &  & \\\\\n\\vdots & & \\ddots & \\\\\n\\sigma^2_\\alpha &  &  & \\sigma^2_\\alpha+\\sigma^2_\\varepsilon\n\\end{bmatrix}\n\\] The \\(nT\\times nT\\) matrix, E[’ |X] is a block-diagonal matrix in which the off-diagonal values are \\(E[\\upsilon_{it}\\upsilon_{js} |X]=\\sigma^2_\\alpha\\) only for \\(i=j\\) and \\(s\\neq t\\); and zero otherwise.\nThe Generalized Least Squares solution say that if we transform the model in the following way, the resulting error term will be classical.\n\\[\n\\underbrace{Y_{it}-\\theta \\bar{Y}_i}_{Y_{it}^+} = \\underbrace{(X_{it}-\\theta\\bar{X}_i)}_{X_{it}^+}'\\beta + \\nu_{it}\n\\] where,\n\\[\n\\theta = 1- \\frac{\\sigma_\\varepsilon}{\\sqrt{T\\sigma^2_\\alpha+\\sigma^2_\\varepsilon}}\n\\]\nConsider the transformed error term \\(\\nu\\),\n\\[\n\\nu_{it} = \\upsilon_{it}-\\theta\\bar{\\upsilon}_i = (1-\\theta)\\alpha_i + \\varepsilon_{it}-\\frac{\\theta}{T}\\sum_t\\varepsilon_{it}\n\\]\nThe serial correlation of this error term is 0. Consider, for \\(t\\neq s\\) \\[\n\\begin{aligned}\nE[\\nu_{it}\\nu_{is}|X_i] =& E\\big[\\big((1-\\theta)\\alpha_i + \\varepsilon_{it}-\\frac{\\theta}{T}\\sum_{t'}\\varepsilon_{it'}\\big)\\big((1-\\theta)\\alpha_i + \\varepsilon_{is}-\\frac{\\theta}{T}\\sum_{t'}\\varepsilon_{it'}\\big)|X_i\\big] \\\\\n=&(1-\\theta)^2\\sigma^2_\\alpha  -2\\frac{\\theta}{T}\\sigma^2_\\varepsilon+\\frac{\\theta^2}{T^2}\\sum_{t'}\\sigma^2_\\varepsilon \\\\\n=&\\frac{\\sigma^2_\\varepsilon \\sigma^2_\\alpha}{T\\sigma^2_\\alpha+\\sigma^2_\\varepsilon} + \\frac{\\theta(\\theta-2)}{T}\\sigma^2_\\varepsilon \\\\\n=&\\frac{\\sigma^2_\\varepsilon \\sigma^2_\\alpha}{T\\sigma^2_\\alpha+\\sigma^2_\\varepsilon} - \\frac{\\sigma^2_\\varepsilon}{T}\\bigg(1-\\frac{\\sigma_\\varepsilon}{\\sqrt{T\\sigma^2_\\alpha+\\sigma^2_\\varepsilon}}\\bigg)\\bigg(1+\\frac{\\sigma_\\varepsilon}{\\sqrt{T\\sigma^2_\\alpha+\\sigma^2_\\varepsilon}}\\bigg) \\\\\n=& \\frac{\\sigma^2_\\varepsilon \\sigma^2_\\alpha}{T\\sigma^2_\\alpha+\\sigma^2_\\varepsilon}-\\frac{\\sigma^2_\\varepsilon}{T}\\bigg(1-\\frac{\\sigma^2_\\varepsilon}{T\\sigma^2_\\alpha+\\sigma^2_\\varepsilon}\\bigg) \\\\\n=&\\frac{\\sigma^2_\\varepsilon \\sigma^2_\\alpha}{T\\sigma^2_\\alpha+\\sigma^2_\\varepsilon}-\\frac{\\sigma^2_\\varepsilon \\sigma^2_\\alpha}{T\\sigma^2_\\alpha+\\sigma^2_\\varepsilon} \\\\\n=&0\n\\end{aligned}\n\\] The GLS estimator is then given by,\n\\[\n  \\hat{\\beta}^{GLS} = \\big[\\sum_iX_i^{+'}X_i^+\\big]^{-1}\\sum_iX_i^{+'}Y_i^+\n\\]\nYou can show that \\(\\hat{\\beta}^{GLS}\\) is a weighted average of \\(\\hat{\\beta}^{WG}\\) and \\(\\hat{\\beta}^{BG}\\). Also, take note of the fact that as \\(T\\rightarrow\\infty\\), \\(\\theta\\rightarrow 1\\). Thus, \\(\\hat{\\beta}_{GLS}\\rightarrow \\hat{\\beta}^{WG}\\) as \\(T\\rightarrow\\infty\\). In addition, if if \\(\\sigma^2_\\alpha=0\\), then \\(\\theta = 0\\) and \\(\\hat{\\beta}^{GLS}=\\hat{\\beta}^{OLS}\\), the pooled OLS estimator.\nHowever, this estimator is NOT feasible. This is because we do not observe \\(\\{\\sigma^2_\\alpha,\\sigma^2_\\varepsilon\\}\\).\n\n7.1 Feasible GLS\nA feasible version of the GLS estimator is given by the following steps:\n\nEstimate \\(\\sigma^2_\\varepsilon\\) using the WG estimator.\nUse the pooled OLS or BG estimator then estimate \\(\\sigma^2_\\alpha\\), using the value of \\(\\sigma^2_\\varepsilon\\) from step 1. For example, the RSS from pooled OLS (divided by \\(nT-k\\)) is a consistent estimator for \\(\\sigma^2_\\varepsilon+\\sigma^2_\\alpha\\). Similar, the RSS from BG estimator (divided by \\(n-k\\)) is a consistent estimator for \\(\\sigma^2_\\varepsilon/T+\\sigma^2_\\alpha\\).\nUsing the estimated \\(\\{\\hat{\\sigma}^2_\\alpha,\\hat{\\sigma}^2_\\varepsilon\\}\\), compute the transformed model and estimate using \\(\\hat{\\beta}^{FGLS}\\) using OLS."
  },
  {
    "objectID": "handout-5.html#within-group-estimator",
    "href": "handout-5.html#within-group-estimator",
    "title": "Panel Data Models",
    "section": "8 Within-Group Estimator",
    "text": "8 Within-Group Estimator\nThe approach taken here is to transform the linear model such that \\(\\alpha_i\\) is eliminate from the equation. Having done so, we do not need to make any assumption regarding \\(E[\\alpha_i|X_i]\\).\nHere we will exploit the fact that \\(\\alpha_i\\) is time-invariant. We begin by computing the unit-level average of the model. For the left-hand side,\n\\[\n\\bar{Y}_i = \\frac{1}{T}\\sum_{t=1}^T Y_{it}\n\\] and the right-hand side, \\[\n\\frac{1}{T}\\sum_{t=1}^T \\big(X_{it}'\\beta + \\alpha_i + \\varepsilon_{it}\\big) = \\bar{X}_{i}'\\beta + \\alpha_i + \\bar{\\varepsilon}_{i}\n\\] Next, subtract this from each value, to create a demeaned expression\n\\[\n\\underbrace{Y_{it}-\\bar{Y}_i}_{\\tilde{Y}_{it}} = \\underbrace{(X_{it}-\\bar{X}_i)'\\beta}_{\\tilde{X}_{it}'\\beta}+\\underbrace{\\alpha_{i}-\\alpha_i}_{=0}+ \\underbrace{\\varepsilon_{it}-\\bar{\\varepsilon}_i}_{\\tilde{\\varepsilon}_{it}}\n\\] The permanent error-term component drops out precisely because it is time-invariant. The transformed model is given by,\n\\[\n\\tilde{Y}_{it} = \\tilde{X}_{it}'\\beta+\\tilde{\\varepsilon}_{it}\n\\]\nThis model can be estimated by OLS. The solution is given by,\n\\[\n\\hat{\\beta}^{WG} = (\\tilde{X}'\\tilde{X})^{-1}\\tilde{X}'\\tilde{Y} = (\\sum_{i=1}^n\\tilde{X}_i'\\tilde{X}_i)^{-1}\\sum_{i=1}^n\\tilde{X}_i'\\tilde{Y}_i\n\\] where \\(\\tilde{X}_i\\) is a \\(T\\times k\\) matrix. Note, this is different to the standard cross-section expression$.\nThis solution assumes that the \\(k\\times k\\) matrix \\(\\sum_{i=1}^n\\tilde{X}_i'\\tilde{X}_i\\) is invertible. We must therefore assume,\n\nSLPM 7^b: \\(rank(\\tilde{X})=k\\)\n\nGiven that \\(\\tilde{X}\\) is the within-group demeaned vale of \\(X\\), this implies the regressors must all be time-varying. In addition, if the model includes a time trend (including time-FEs), the included variables cannot vary uniformly with time. An example of this is age. If all units’ age values increase by the same amount each period, then\n\\[\n\\tilde{age}_{it} = age_{it}-\\bar{age}_i = t - \\bar{t}\n\\] This is because an individuals age can be expressed as a time-invariant value of their year of birth \\(yob_i\\) plus a linear time-trend that has a unit-specific intercept. The demeaned value of age is perfectly colinear with a demeaned linear time-trend. It would also be perfectly colinear with a higher-order polynomial time-trend or year FEs.\n\\[\n  \\tilde{X}_i = \\begin{bmatrix}\n  \\tilde{X}_{i11} & \\tilde{X}_{i12} & \\cdots & \\tilde{X}_{i1k} \\\\\n  \\tilde{X}_{i21} & \\tilde{X}_{i22} &  & \\\\\n  \\vdots &  & \\ddots & \\\\\n  \\tilde{X}_{iT1} &  &  & \\tilde{X}_{iTk}\n  \\end{bmatrix} = \\begin{bmatrix}\\tilde{X}_{i1}' \\\\ \\tilde{X}_{i2}' \\\\ \\vdots \\\\ \\tilde{X}_{iT}'\\end{bmatrix}\n\\]\n\\(\\tilde{X}_i'\\tilde{X}_i\\) is therefore a \\(k\\times k\\) matrix, which can be expressed as,\n\\[\n\\tilde{X}_i'\\tilde{X}_i = \\sum_{t=1}^T \\tilde{X}_{it}\\tilde{X}_{it}' = \\begin{bmatrix} \\sum_{t}\\tilde{X}_{it1}^2 & \\sum_{t}\\tilde{X}_{it1}\\tilde{X}_{it2} & \\cdots & \\sum_{t}\\tilde{X}_{it1}\\tilde{X}_{itk} \\\\ \\sum_{t}\\tilde{X}_{it2}\\tilde{X}_{it1} & \\sum_{t}\\tilde{X}_{it2}^2 &  & \\\\ \\vdots & & \\ddots & \\\\ \\sum_{t}\\tilde{X}_{itk}\\tilde{X}_{it1} & & & \\sum_{t}\\tilde{X}_{itk}^2 \\end{bmatrix}\n\\]\nFinally, we can express \\(\\tilde{X}'\\tilde{X}\\) as,\n\\[\n\\tilde{X}'\\tilde{X} = \\sum_{i=1}^n \\tilde{X}_i'\\tilde{X}_i = \\begin{bmatrix} \\sum_i\\big(\\sum_{t}\\tilde{X}_{it1}^2\\big) & \\sum_i\\big(\\sum_{t}\\tilde{X}_{it1}\\tilde{X}_{it2}\\big) & \\cdots & \\sum_i\\big(\\sum_{t}\\tilde{X}_{it1}\\tilde{X}_{itk}\\big) \\\\ \\sum_i\\big(\\sum_{t}\\tilde{X}_{it2}\\tilde{X}_{it1}\\big) & \\sum_i\\big(\\sum_{t}\\tilde{X}_{it2}^2\\big) &  & \\\\ \\vdots & & \\ddots & \\\\ \\sum_i\\big(\\sum_{t}\\tilde{X}_{itk}\\tilde{X}_{it1}\\big) & & & \\sum_i\\big(\\sum_{t}\\tilde{X}_{itk}^2\\big) \\end{bmatrix}\n\\]\nWe can use the same method to describe \\(\\sum_{i=1}^n\\tilde{X}_i'\\tilde{Y}_i\\), a \\(k\\times 1\\) vector. Substituting in the definition of \\(\\tilde{Y}_i\\) from the transformed model, we get that,\n\\[\n\\hat{\\beta}^{WG} = \\beta +  (\\sum_{i=1}^n\\tilde{X}_i'\\tilde{X}_i)^{-1}\\sum_{i=1}^n\\tilde{X}_i'\\tilde{\\varepsilon}_i\n\\]"
  },
  {
    "objectID": "handout-5.html#conditional-variance",
    "href": "handout-5.html#conditional-variance",
    "title": "Panel Data Models",
    "section": "9 Conditional variance",
    "text": "9 Conditional variance\nGiven the demeaning of the model, the error term is no longer uncorrelated across \\(t\\) for the same \\(i\\):\n\\[\n\\begin{aligned}\nE[\\tilde{\\varepsilon}_{is}\\tilde{\\varepsilon}_{it}|X] =&E[(\\varepsilon_{is}-\\bar{\\varepsilon}_{i})(\\varepsilon_{it}-\\bar{\\varepsilon}_{i})|X] \\\\\n=&E\\bigg[\\bigg(\\varepsilon_{is}-\\frac{1}{T}\\sum_{s'}\\varepsilon_{is'}\\bigg)\\bigg(\\varepsilon_{it}-\\frac{1}{T}\\sum_{s'}\\varepsilon_{it'}\\bigg)\\bigg| X_i\\bigg] \\\\\n=&\\begin{cases} \\sigma^2_{\\varepsilon}(1-1/T) \\qquad \\text{for}\\quad s=t \\\\\n-\\sigma^2_{\\varepsilon}/T \\qquad \\text{for}\\quad s\\neq t\n\\end{cases}\n\\end{aligned}\n\\] The off-diagonal elements for the same \\(i\\) are the same for all time-periods. Collected together, this creates a block diagonal matrix, where covariance terms are zero for across different units.\n\\[\nE[\\tilde{\\varepsilon}_{is}\\tilde{\\varepsilon}_{it}X]= \\begin{bmatrix}\n\\Omega & 0 & \\cdots & 0 \\\\\n0 & \\Omega & & \\\\\n\\vdots & & \\ddots & \\\\\n0 & & & \\Omega\n\\end{bmatrix}\n\\] with each \\(T\\times T\\) \\(\\Omega\\) matrix given by,\n\\[\n\\begin{aligned}\n\\Omega=& E[\\tilde{\\varepsilon}_{i}\\tilde{\\varepsilon}_{i}'|X_i] \\\\\n=&\n\\begin{bmatrix}\n\\sigma^2_{\\varepsilon}(1-1/T) & -\\sigma^2_{\\varepsilon}/T & \\cdots & -\\sigma^2_{\\varepsilon}/T \\\\\n-\\sigma^2_{\\varepsilon}/T & \\sigma^2_{\\varepsilon}(1-1/T) & & \\\\\n\\vdots & & \\ddots & \\\\\n-\\sigma^2_{\\varepsilon}/T & & & \\sigma^2_{\\varepsilon}(1-1/T)\n\\end{bmatrix} \\\\\n=&\\sigma^{2}_\\varepsilon M_\\ell\n\\end{aligned}\n\\] where \\(M_\\ell = I_T-\\frac{\\ell\\ell'}{T}\\) for the \\(T\\times 1\\) vector of ones \\(\\ell\\). This follows from the fact that \\(\\tilde{\\varepsilon}_{i} = M_\\ell\\varepsilon_{i}\\). Which implies that,\n\\[\n\\begin{aligned}\nE[\\tilde{\\varepsilon}_{i}\\tilde{\\varepsilon}_{i}'|X_i] =& E[M_\\ell\\varepsilon_{i}\\varepsilon_{i}'M_\\ell'|X_i] \\\\\n=&M_\\ell E[\\varepsilon_{i}\\varepsilon_{i}'|X_i]M_\\ell \\\\\n=& M_\\ell\\sigma^2_\\varepsilon I_T M_\\ell \\\\\n=&\\sigma^{2}_\\varepsilon M_\\ell\n\\end{aligned}\n\\] Thus, \\[\nE[\\tilde{\\varepsilon}_{is}\\tilde{\\varepsilon}_{it}|X]= \\sigma^{2}_\\varepsilon\\begin{bmatrix}\nM_\\ell & 0 & \\cdots & 0 \\\\\n0 & M_\\ell & & \\\\\n\\vdots & & \\ddots & \\\\\n0 & & & M_\\ell\n\\end{bmatrix} = \\sigma^{2}_\\varepsilon M_{n\\cdot\\ell}\n\\] where the \\(rank(M_{n\\cdot\\ell})=nT-n\\). This follows from the fact that each \\(M_\\ell\\) has \\(rank(M_\\ell)=T-1\\). The matrix \\(M_{n\\cdot\\ell}\\) is a \\(nT\\times nT\\) matrix: a block diagonal matrix of \\(n\\) \\(M_\\ell\\) matrices. With this we can now solve for the conditional variance of the WG estimator.\n\\[\n\\begin{aligned}\nVar(\\hat{\\beta}^{WG}|X) =& (\\tilde{X}'\\tilde{X})^{-1}\\tilde{X}'E[\\tilde{\\varepsilon}_{is}\\tilde{\\varepsilon}_{it}|X]\\tilde{X}(\\tilde{X}'\\tilde{X})^{-1} \\\\\n=& \\sigma^2_\\varepsilon(\\tilde{X}'\\tilde{X})^{-1}\\tilde{X}'M_{n\\cdot\\ell}\\tilde{X}(\\tilde{X}'\\tilde{X})^{-1} \\\\\n=& \\sigma^2_\\varepsilon(\\tilde{X}'\\tilde{X})^{-1}\n\\end{aligned}\n\\] The final line follows from the fact that \\(\\tilde{X}=M_{n\\cdot\\ell}X\\) and \\(M_{n\\cdot\\ell}\\) is an idempotent (orthogonal) projection matrix.\nAn unbiased and consistent estimator for \\(\\sigma^2_\\varepsilon\\) is given by,\n\\[\n\\hat{\\sigma}^2_\\varepsilon = \\frac{RSS}{dof}=\\frac{\\sum_{i=1}^n\\sum_{t=1}^T\\big(Y_{it}-\\bar{Y}_i-(X_{it}-\\bar{X}_i)'\\hat{\\beta}^{WG}\\big)^2}{nT-n-k}\n\\] The residual degrees of freedom is equal to \\(nT-n-k\\), not \\(nT-k\\). While there are \\(nT\\) observations and \\(k\\) parameters, we must deduct the \\(n\\) unit-level means computed. Another way to see this is to use the above projection matrix decomposition:\n\\[\n\\begin{aligned}\nRSS =& \\hat{\\varepsilon}'\\hat{\\varepsilon} \\\\\n=& \\tilde{Y}'(I_{nT}-\\tilde{X}(\\tilde{X}'\\tilde{X})^{-1}\\tilde{X}')\\tilde{Y} \\\\\n=& Y'M_{n\\ell}(I_{nT}-M_{n\\ell}X(X'M_{n\\ell}X)^{-1}X'M_{n\\ell})M_{n\\ell}Y \\\\\n=& Y'\\underbrace{(M_{n\\ell}-M_{n\\ell}X(X'M_{n\\ell}X)^{-1}X'M_{n\\ell})}_{M_+}Y\n\\end{aligned}\n\\] This new matrix \\(M_+\\) is also an idempotent projection matrix, with rank \\(rank(M_+) = ran(M_{n\\ell})-min{rank(M_{n\\ell}),rank(X)} = nT-n-k\\). As we saw with the CLRM, this term with have a \\(\\chi^2\\) distribution with dof equal to the rank of the orthogonal projection matrix.\n\n9.1 Consistency and asymptotic distribution\nUnder assumptions SLPM 1-6; especially 4 & 5, \\(\\hat{\\beta}^{WG}\\) is consistent,\n\\[\n\\hat{\\beta}^{WG}\\rightarrow_p \\beta \\qquad \\text{as}\\quad n\\rightarrow\\infty\n\\] and asymptotically normal,\n\\[\n\\hat{\\beta}^{WG}\\overset{a}{\\sim} N\\big(\\beta,\\sigma^2_{\\varepsilon}\\big(\\sum_i\\tilde{X}_i'\\tilde{X}_i\\big)^{-1}\\big)\n\\]\nBoth results require that \\(E[\\tilde{X}_i,\\tilde{\\varepsilon}_i]=0\\). This is maintained by strict exogeneity (CLPM 4), which states that in addition to \\(E[X_{is},\\varepsilon_{it}]=0\\;\\forall\\;s,t\\),\n\\[\n  E[\\bar{X}_i,\\bar{\\varepsilon}_i]=0\n\\] This would not be true under the weak exogeneity assumption.\n\n\n9.2 Fixed-effects estimator\nThe above estimator is sometimes referred to as the (unit) Fixed-Effects estimator. This not not mean that \\(\\alpha_i\\) is non-random. It simply means that the within-group estimator is also given by a fixed-effects model. Consider the model,\n\\[\nY_{it} = \\sum_{j=1}^n\\phi_j\\mathbf{1}\\{i = j\\}+X_{it}'\\beta + \\upsilon_{it}\n\\]\nThis model includes a dummy variable for each unit. For each unit, only one dummy variable can be =1, the dummy variable with parameter \\(\\phi_i\\). Thus, the expression \\(\\sum_{j=1}^n\\phi_j\\mathbf{1}\\{i = j\\}=\\phi_i\\) for any \\(i\\). The model can therefore be written as, \\[\nY_{it} = \\phi_i+X_{it}'\\beta + \\upsilon_{it}\n\\]\nThis looks very similar to our SLPM, but with the distinguishing feature that \\(\\phi_i\\) is taken as a population parameter. This is sometimes referred as a unit-specific constant.\nIncluding a constant for each \\(i\\) has the same effect as demeaning the model. This alternative approach is referred to as the Least Squares Dummy Variable (LSDV) method.\n\\[\n\\hat{\\beta}^{WG} = \\hat{\\beta}^{LSDV}\n\\] This equivalence can be shown using Frisch Waugh Lovell Theorem (or partitioned regression). The LSDV estimator can be computed in two steps. First, regress each regressor and outcome on a setting of unit level dummies.\n\\[\nX_{k} = \\sum_{j=1}^n\\phi_j\\mathbf{1}\\{i = j\\} + \\xi\n\\] Each \\(\\mathbf{1}\\{i = j\\}\\) corresponds to a dummy variable where \\(T\\) values are =1 (for unit \\(i\\)), and the remainder 0. Next, use the residuals in the main equation. The residual from the regression is given by,\n\\[\nM_{n\\cdot\\ell}X_k  = \\tilde{X}_{k}\n\\] Thus, we regression is given by,\n\\[\nM_{n\\cdot\\ell}Y = \\tilde{Y} = M_{n\\cdot\\ell}X\\beta + \\varepsilon = \\tilde{X}\\beta + \\varepsilon\n\\] Employing the LSDV approach, one can estimated the unit-FE as,\n\\[\n\\hat{\\phi}_i = \\bar{Y}_i - \\bar{X}_i'\\hat{\\beta}^{LSDV}\n\\]\nWhile this estimator is unbiased, \\(E[\\hat{\\phi}_i|X_i]=\\phi\\), it is NOT consistent for fixed \\(T\\). It is only consistent if \\(T\\rightarrow \\infty\\) as \\(n\\rightarrow \\infty\\).\nThe equivalene of these approaches also explains why the degrees of freedom in the residual is \\(nT-n-k\\). By including \\(n\\) dummy variables, the number of parameters we need to estimate is \\(n+k\\)."
  },
  {
    "objectID": "handout-5.html#first-difference-estimator",
    "href": "handout-5.html#first-difference-estimator",
    "title": "Panel Data Models",
    "section": "10 First-Difference Estimator",
    "text": "10 First-Difference Estimator\nAs with the WG estimator, he first-difference (FD) estimator removes \\(\\alpha_i\\) from the model through differencing. However, this time the transformation is just difference over time:\n\\[\n\\underbrace{Y_{it}-Y_{it-1}}_{\\tilde{Y}_{it}} = \\underbrace{(X_{it}-X_{it-1})'\\beta}_{\\tilde{X}_{it}'\\beta}+\\underbrace{\\alpha_{i}-\\alpha_i}_{=0}+ \\underbrace{\\varepsilon_{it}-\\varepsilon_{it-1}}_{\\tilde{\\varepsilon}_{it}}\n\\] The estimation sample will include 1 less period: \\(t=2,...,T\\) for each \\(i\\).\nUnder CLPM 1-6, \\(\\hat{\\beta}^{FD}\\) is consistent and asympotitcally normal. However, we need to account for the error term structure. This is because,\n\\[\nE[\\tilde{\\varepsilon}_{is}\\tilde{\\varepsilon}_{it}] =\\begin{cases}E[(\\varepsilon_{is}-\\varepsilon_{is-1})(\\varepsilon_{it}-\\varepsilon_{it-1})] = 2\\sigma^2_{\\varepsilon} \\quad \\text{for}\\;s=t \\\\\nE[(\\varepsilon_{is}-\\varepsilon_{is-1})(\\varepsilon_{it}-\\varepsilon_{it-1})] = -\\sigma^2_{\\varepsilon} \\quad \\text{for}\\;s=t-1 \\\\\n0 \\quad \\text{otherwise}\\end{cases}\n\\]\nThis is a MA(1) error term structure, in which the first-order correlation is non-zero.\nFor \\(T=2\\), you can show that,\n\\[\n\\hat{\\beta}^{WG} = \\hat{\\beta}^{FD}\n\\]\nYou can also show that if you apply GLS to the first-differenced equation, you get the WG estimator (for \\(T\\geq2\\))."
  },
  {
    "objectID": "handout-5.html#wu-hausman-test",
    "href": "handout-5.html#wu-hausman-test",
    "title": "Panel Data Models",
    "section": "11 Wu-Hausman Test",
    "text": "11 Wu-Hausman Test\nThe Wu-Hausman test is used to test the exogeneity assumption underlying a particular estimator. You need two estimators: \\(\\{\\hat{\\beta}_1, \\hat{\\beta}_2\\}\\) such that,\n\nUnder \\(H_0: \\beta_1=\\beta_2\\)\n\\(\\hat{\\beta}_1\\) is consistent\n\\(\\hat{\\beta}_2\\) is consistent\n\\(Var(\\hat{\\beta}_1|X)&lt;Var(\\hat{\\beta}_2|X)\\): the former is more efficient\nUnder \\(H_1:  \\beta_1\\neq\\beta_2\\)\n\\(\\hat{\\beta}_1\\) is inconsistent\n\\(\\hat{\\beta}_2\\) is consistent\n\nThe test statistic is given by,\n\\[\n  \\text{Stat} = (\\hat{\\beta}_2-\\hat{\\beta}_1)'\\big(Var(\\hat{\\beta}_2-\\hat{\\beta}_1|X)\\big)^{-1}(\\hat{\\beta}_2-\\hat{\\beta}_1)\n\\] Under \\(H_0\\), this statistic converges in distribution to \\(\\Chi_k\\), where \\(k\\) is the number of regressors. The inner matrix is the inverse of the variance-covariance matrix.\n\\[\n\\begin{aligned}\nVar(\\hat{\\beta}_2-\\hat{\\beta}_1|X) =& Var(\\hat{\\beta}_2|X) + Var(\\hat{\\beta}_1|X)-2 Cov(\\hat{\\beta}_2,\\hat{\\beta}_1|X) \\\\\n=&Var(\\hat{\\beta}_2|X)-Var(\\hat{\\beta}_1|X)\n\\end{aligned}\n\\] Line 2 follows from line 1, because of a result demonstrated by Hausman (1978):\n\\[\n0 = Cov(\\hat{\\beta}_1,\\hat{\\beta}_1-\\hat{\\beta}_2) = Var(\\hat{\\beta}_1)-Cov(\\hat{\\beta}_1,\\hat{\\beta}_2)\n\\] This result holds in cases where the variances of the respectively estimators can be ranked: i.e. one estimator is more efficient than the other. As a result, the \\(Var(\\hat{\\beta}_2-\\hat{\\beta}_1|X)\\) matrix is positive definite. This implies that it’s inverse exists.\nThis test can be applied in this setting to test the null: \\(H_0: E[X_{it}\\alpha_i] = 0\\) (i.e. uncorrelatedness). This condition must hold for the (F)GLS estimator to be consistent. If \\(H_0\\) is false, then we know that the WG estimator is consistent, but that it is less efficient. To test this hypothesis we use the coefficients from the FGLS and WG estimators. Of course, this means that you can only test for restrictions on time-varying regressors (a restriction of WG estimator).1"
  },
  {
    "objectID": "handout-5.html#footnotes",
    "href": "handout-5.html#footnotes",
    "title": "Panel Data Models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis restriction can be dropped using Mundlak’s correction.↩︎"
  },
  {
    "objectID": "handout-3.html",
    "href": "handout-3.html",
    "title": "Estimation Methods",
    "section": "",
    "text": "In this handout we will look at several approaches to generate estimators:\n\nLeast Squares\nMethod of Moments\nMaximum Likelihood\n\nWe will discuss each approach in the context of the Classical Linear Regression Model discussed in Lecture 1. You may also wish to revise the notes on Linear Algrebra.\nFurther reading can be found in:\n\nSection 5.6 of Cameron and Trivedi (2005)\nSection 6.1 of Verbeek (2017)"
  },
  {
    "objectID": "handout-3.html#overview",
    "href": "handout-3.html#overview",
    "title": "Estimation Methods",
    "section": "",
    "text": "In this handout we will look at several approaches to generate estimators:\n\nLeast Squares\nMethod of Moments\nMaximum Likelihood\n\nWe will discuss each approach in the context of the Classical Linear Regression Model discussed in Lecture 1. You may also wish to revise the notes on Linear Algrebra.\nFurther reading can be found in:\n\nSection 5.6 of Cameron and Trivedi (2005)\nSection 6.1 of Verbeek (2017)"
  },
  {
    "objectID": "handout-3.html#review-of-clrm",
    "href": "handout-3.html#review-of-clrm",
    "title": "Estimation Methods",
    "section": "2 Review of CLRM",
    "text": "2 Review of CLRM\nThe classical linear regression model is states that the conditional expectation function \\(E[Y_i|X_i]\\) is linear in parameters.\nFor the random sample \\(i=1,...,n\\),\n\\[\nY_i = X_i'\\beta + u_i\n\\] where \\(X_i\\) is a random k-dimensional vector (k-vector) and \\(\\beta\\) a non-random k-vector of population parameters. Both \\(Y_i\\) and \\(u_i\\) are random scalars.\nAs we saw, we can stack each observation into a column vector:\n\\[\nY =\\underbrace{\\begin{bmatrix}Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix}}_{n\\times 1} = \\underbrace{\\begin{bmatrix}X_1'\\beta \\\\ X_2'\\beta \\\\ \\vdots \\\\ X_n'\\beta\\end{bmatrix}}_{n\\times 1}  + \\underbrace{\\begin{bmatrix}u_1 \\\\ u_2 \\\\ \\vdots \\\\ u_n\\end{bmatrix}}_{n\\times 1}  \n= \\underbrace{\\begin{bmatrix}X_{11} & X_{12} & \\cdots & X_{1k}\\\\ X_{21} & X_{22} && \\\\ \\vdots & & \\ddots & \\\\ X_{n1} & & & X_{nk} \\end{bmatrix}}_{n\\times k} \\underbrace{\\begin{bmatrix}\\beta_1 \\\\ \\beta_2 \\\\ \\vdots \\\\ \\beta_k\\end{bmatrix}}_{k\\times 1} + \\begin{bmatrix}u_1 \\\\ u_2 \\\\ \\vdots \\\\ u_n\\end{bmatrix} = X\\beta + u\n\\]\nwhere \\(X\\) is now a \\(n\\times k\\) random matrix, but \\(\\beta\\) remains a non-random k-vector of population parameters.\n\n\n\n\n\n\nData as a Matrix\n\n\n\nIf you have any experience working datasets in Stata/R, you will know that they tend to have a rectangular structure: each row typically represents an observation and each column a variable. This is the structure depicted above in matrix notation: each row of the \\(X\\) matrix depicts an observation and each column a regressor. The dataset you are using contains is a matrix of observations for both the outcome variable and regressors: \\([Y,X,]\\). Of course, we do not observe the error term.\n\n\nIn this section, we will employ the CLRM assumptions as we discuss the three approaches to estimation."
  },
  {
    "objectID": "handout-3.html#ordinary-least-squares",
    "href": "handout-3.html#ordinary-least-squares",
    "title": "Estimation Methods",
    "section": "3 (Ordinary) Least Squares",
    "text": "3 (Ordinary) Least Squares\nThe Ordinary Least Squares (OLS) estimator is the ‘work-horse’ of applied economics research.1 It is not the only Least Squares estimator, but as the simplest case is the most useful place to start. Other Least Squares estimators include Weighted Least Squares (WLS), (Feasible) Generalized Least Squares (GLS), Non-Linear Least Squares and Two-Stage Least Squares (2SLS).\nOLS is “ordinary” in the sense that there is no additional features to the method. For example, WLS applies a unique weight to each observation while OLS weights each observation equally. While OLS is arguably ‘vanilla’ in this way, it is efficient (as we shall see).\nIn general, LS estimators minimize a measure of ‘distance’ between the observed outcomes and the fitted values of the model. The measure of distance is sum of squared deviations (squared Euclidean or \\(\\ell_2\\) norm).2\n\n3.1 Application to CLRM\nIn the case of OLS estimator to the CLRM, the goal is to find the \\(b\\)-vector that minimizes,\n\\[\n\\sum_{i=1}^n(Y_i-\\underbrace{\\tilde{Y}_i}_{X_i'b})^2 = \\sum_{i=1}^n\\tilde{u}_i^2\n\\] This sum-of-squares can be written as the inner product of two-vectors:3\n\\[\n\\sum_{i=1}^n\\tilde{u}_i^2 = \\tilde{u}'\\tilde{u} = (Y-Xb)'(Y-Xb)\n\\]\nApplying the rules of matrix transposition, the inner product of these two matrices is given by,\n\\[\n(Y'-b'X')(Y-Xb) = Y'Y -b'X'Y-Y'Xb+b'X'Xb\n\\] Since all terms are scalars, \\(b'X'Y=Y'Xb\\); which then gives us, \\[\nY'Y -2b'X'Y+b'X'Xb\n\\] Thus, the (ordinary) least-squares estimator the vector that solves this linear expression. \\[\n\\hat{\\beta}^{OLS} = \\underset{b}{\\text{arg min}}\\quad Y'Y -2b'X'Y+b'X'Xb\n\\]\nUsing the rules of vector differentiation (see Linear Algrebra) we can find the first order conditions:\n\\[\n-2X'Y +2X'X\\hat{\\beta}^{OLS}= 0\n\\] If the \\(X\\) matrix is full rank (=\\(k\\)), then \\(X'X\\) is non-singular and its inverse exists. Recall, this was one of the CLRM assumptions. Then, \\[\n\\hat{\\beta}^{OLS} = (X'X)^{-1}X'Y\n\\]\n\n\n3.2 Bias\nIs the OLS estimator unbiased? The answer will depend on the assumption of the model. Here, we have assumed that the model being estimated is a CLRM. This means that we have assumed conditional mean independence of the error term:\n\\[\nE[u|X] = 0\n\\] The OLS-estimator is given by,\n\\[\n\\hat{\\beta}^{OLS} = (X'X)^{-1}X'Y = (X'X)^{-1}X'(X\\beta+u) = \\underbrace{(X'X)^{-1}X'X}_{I_n}\\beta+(X'X)^{-1}X'u=\\beta+(X'X)^{-1}X'u\n\\] plugging in the definition of \\(Y\\) from the model.\nHence,\n\\[\nE[\\hat{\\beta}^{OLS}|X] = E[\\beta+(X'X)^{-1}X'u|X] = \\beta+E[(X'X)^{-1}X'u|X]\n\\] Here we apply the linearity of the expectation operator and the factor that \\(\\beta\\) is a non-random vector. Next, we exploit the fact that conditional on \\(X\\), any function of \\(X\\) is non-random and can come out of the expectation operator.\n\\[\nE[\\hat{\\beta}^{OLS}|X] = \\beta+(X'X)^{-1}X'\\underbrace{E[u|X]}_{=0} = \\beta\n\\]\nNotice, we require the stronger assumption of conditional mean independence, \\(E[u|X]=0\\). Uncorrelateness, \\(E[X'u]=0\\), is insufficient for unbiasedness.\n\n\n\n\n\n\nImportant\n\n\n\nNotice, unbiasedness depends on the assumptions of the model and not any properties of the estimator. The estimator is simply a calculation using observed data. The properties and interpretation of this computation depend on the assumptions we make regarding the underlying model.\n\n\nIt is also worth noting that the unbiasedness of the OLS estimator does NOT depend on any assumptions regarding the variance or distribution of the error term.\n\n\n3.3 Efficiency\nThe OLS estimator is a \\(k\\)-dimensional random vector. The variance of this vector is a \\(k\\times k\\) variance-covariance matrix.\n\\[\nVar(\\hat{\\beta}) = E\\big[\\underbrace{(\\hat{\\beta}-E[\\hat{\\beta}])}_{k\\times 1}\\underbrace{(\\hat{\\beta}-E[\\hat{\\beta}])'}_{1\\times k}\\big]\n\\] The off-diagonals of the matrix are the covariances: \\(Cov(\\hat{\\beta}_j,\\hat{\\beta}_k)\\) for \\(j\\neq k\\).\nWe have just shown that \\(E[\\hat{\\beta}]=\\beta\\) and\n\\[\n\\hat{\\beta}^{OLS} -\\beta=(X'X)^{-1}X'u\n\\]\nThus, the (conditional) variance of this estimator is then given by,\n\\[\n\\begin{aligned}\nVar(\\hat{\\beta}^{OLS}|X) =& E\\big[(X'X)^{-1}X'uu'X(X'X)^{-1}|X\\big] \\\\\n=& (X'X)^{-1}X'E[uu'|X]X(X'X)^{-1} \\\\\n=& (X'X)^{-1}X'Var(u|X)X(X'X)^{-1}\n\\end{aligned}\n\\]\nThe variance of the estimator depends on the variance of the error term, the unexplained part of the model. In order to any further expressions for this variance calculation, we need to go back to the model. What assumptions did we make concerning the variance in the CLRM?\nUnder the assumption CLRM 3 of homoskedasticity,\n\\[\nVar(u|X) = \\sigma^2 I_n = \\begin{bmatrix}\\sigma^2& 0 & \\cdots & 0 \\\\\n0 & \\sigma^2 & & \\\\\n\\vdots & & \\ddots & \\\\\n0 & & & \\sigma^2\\end{bmatrix}\n\\]\nthe above expression simplies to\n\\[\n\\begin{aligned}\nVar(\\hat{\\beta}^{OLS}|X) =& (X'X)^{-1}X'\\sigma^2I_nX(X'X)^{-1} \\\\\n=&\\sigma^2(X'X)^{-1}X'X(X'X)^{-1} \\\\\n=&\\sigma^2(X'X)^{-1}\n\\end{aligned}\n\\]\nIf we made a different assumption of heteroskedasticty (CLRM 3), then\n\\[\nVar(u|X) = \\begin{bmatrix}\\sigma^2_1& 0 & \\cdots & 0 \\\\\n0 & \\sigma^2_2 & & \\\\\n\\vdots & & \\ddots & \\\\\n0 & & & \\sigma^2_n\\end{bmatrix} = \\Omega\n\\]\nthe variance matrix does not reduce to a scalar multiplied by the identity matrix. And,\n\\[\nVar(\\hat{\\beta}^{OLS}|X) = (X'X)^{-1}X'\\Omega X(X'X)^{-1}\n\\]\nThis is commonly referred to as a ‘sandwich’ formula, given the way \\(Var(u|X)=\\Omega\\) is sandwiched between two linear transformations. The Eicker-Huber-White estimator for heteroskedastic standard errors of \\(\\hat{\\beta}^{OLS}\\) replaces \\(Var(u|X)=E[uu'|X]\\) with \\(\\hat{u}\\hat{u}'\\), the OLS residuals.\n\n\n3.4 Finite-sample distribution\nThe finite sample distribution of the OLS estimator depends on the assumptions of the model. Under CLRM 5,\n\\[\nu|X \\sim N(0,\\sigma^2 I_n)\n\\] And we have already shown that the OLS estimator is simply a linear transformation of the error term, \\[\n\\hat{\\beta}^{OLS}=\\beta+(X'X)^{-1}X'u\n\\]\nThen, using the properties of the Normal distribution4\n\\[\n\\hat{\\beta}^{OLS}|X=N\\big(\\beta,\\sigma^2(X'X)^{-1}\\big)\n\\] assuming homoskedasticity. With heteroskedastic variance, you simply change the variance, as the assumption has no implications for biasedness.\n\n\n3.5 Consistency\nRecall from Lecture 2 that an estimator is consistent if it converges in probability to the parameter. In this case, we want to show that\n\\[\n\\hat{\\beta}^{OLS}\\rightarrow_p \\beta\\qquad\\text{as}\\qquad n\\rightarrow \\infty\n\\] Using the derivation \\(\\hat{\\beta}^{OLS} -\\beta=(X'X)^{-1}X'u\\), we need to show that \\((X'X)^{-1}X'u \\rightarrow_p 0\\). To emphasize the fact that \\(\\hat{\\beta}\\) is a function of the sample size, I am going to switch to the notation \\(\\hat{\\beta}_n\\) for this section.\nFor the consistency of the OLS estimator we require a few assumptions,\n\nCLRM 1: linear in parameters\nCLRM 2b: uncorrelatedness, \\(E[X_iu_i]=0\\)\nCLRM 4: \\(rank(X) = k\\)\nCLRM 6: data is iid\n(NEW) CLRM 7: \\(E[X_iX_i']\\) is a finite, positive-definite matrix.\n\nWe begin by re-writing the expression, \\(\\hat{\\beta}^{OLS}=\\beta+(X'X)^{-1}X'u\\) in summation notation and then scaling by \\(n\\),\n\\[\n\\hat{\\beta}_n=\\beta+\\bigg(\\sum_{i=1}^nX_iX_i'\\bigg)^{-1}\\sum_{i=1}^nX_iu_i = \\beta+\\bigg(n^{-1}\\sum_{i=1}^nX_iX_i'\\bigg)^{-1}n^{-1}\\sum_{i=1}^nX_iu_i\n\\]\nBy the WLLN,5\n\\[\nn^{-1}\\sum_{i=1}^nX_iu_i\\rightarrow_p E[X_1u_1] = 0\n\\]\nSimilarly, by WLLN, underpinned by finiteness of \\(E[X_1X_1']\\) (CLRM 7), \\[\nn^{-1}\\sum_{i=1}^nX_iX_i'\\rightarrow_p E[X_1X_1']\n\\]\nSince \\(E[X_1X_1']\\) is also positive definite (CLRM 7), then by Slutzky’s Theorem\n\\[\n\\bigg(n^{-1}\\sum_{i=1}^nX_iX_i'\\bigg)^{-1}\\rightarrow_p \\big(E[X_1X_1']\\big)^{-1}\n\\]\nHence, by Slutzky’s Theorem, which says that the product of two consistent estimators converges in probability to the product of their targets,\n\\[\n(X'X)^{-1}X'u \\rightarrow_p \\big(E[X_1X_1']\\big)^{-1}E[X_1u_1] = 0\n\\]\nThus,\n\\[\np \\lim(\\hat{\\beta}_n) = \\beta\n\\]\n\n\n\n\n\n\nImportant\n\n\n\nScaling each term by \\(n\\) is very important, as without it, both terms do not have a finite mean. Consider, under iid,\n\\[\nE\\bigg[\\sum_{i=1}^nX_iX_i'\\bigg] = nE[X_1X_1']\n\\] while, \\[\nE\\bigg[n^{-1}\\sum_{i=1}^nX_iX_i'\\bigg] = E[X_1X_1']\n\\]\n\n\n\n\n3.6 Asymptotic Distribution\nTo derive the asymptotic distribution of the OLS estimator we will need to apply the Central Limit Theorem. We will need to scale by \\(\\sqrt{n}\\), to derive the distribution of,\n\\[\n\\sqrt{n}(\\hat{\\beta}_n-\\beta)\n\\]\nRecall from Lecture 2 that by Cramer’s Convergence Theorem, \\(Y_nX_n\\rightarrow_d cX\\) where \\(X_n\\rightarrow_d\\) and \\(Y_n\\rightarrow_p c\\). This result holds for the case where \\(Y_n\\) is a random matrix. \\[\n\\sqrt{n}(\\hat{\\beta}_n-\\beta) = \\big(n^{-1}X'X\\big)^{-1}n^{-1/2}Xu\n\\] We have already established that, \\[\n\\big(n^{-1}X'X\\big)^{-1}\\rightarrow_p \\big(E[X_1X_1']\\big)^{-1}\n\\] under assumptions CLRM 1, 2b, 6, and 7.\nWe therefore need to consider the asymptotic distribution of \\(n^{-1/2}Xu\\). By CLRM 2b \\(E[X_1u_1]=0\\), fulfilling one of the CLT conditions. We then need the second moment to be finite: \\(Var(X_1u_1) = E[u_1^2X_1X_1']\\). This is a \\(k\\times k\\) matrix.\nWe will need to make some additional assumptions:\n\n(NEW) CLRM 8: \\(E[u_1^2X_1X_1']\\) is a finite positive-definite matrix.6\n\nUnder assumptions CLRM 1, 2, 6, and 8, by CLT,\n\\[\nn^{-1/2}Xu\\rightarrow N(0,E[u_1^2 X_1 X_1'])\n\\] There, \\[\n\\sqrt{n}(\\hat{\\beta}_n-\\beta) \\rightarrow_d N\\bigg(0,\\big(E[X_1X_1']\\big)^{-1}E[u_1^2X_1X_1']\\big(E[X_1X_1']\\big)^{-1}\\bigg)\n\\] Under the homoskedasticity, \\(E[u_1^2X_1X_1']=\\sigma^2 E[X_1X_1']\\), giving us, \\[\n\\sqrt{n}(\\hat{\\beta}_n-\\beta) \\rightarrow_d N\\bigg(0,\\sigma^2\\big(E[X_1X_1']\\big)^{-1}\\bigg)\n\\] We can approximate the asymptotic distribution \\(\\hat{\\beta}_n\\) by multiplying by \\(\\sqrt{n}\\) and replacing \\(\\big(E[X_1X_1']\\big)^{-1}\\) with the approximation \\(\\big(n^{-1}X'X\\big)^{-1}\\). \\[\n\\hat{\\beta}_n \\overset{a}{\\sim}N\\big(\\beta,\\sigma^2(X'X)^{-1}\\big)\n\\] The \\(n\\) in the variance formula is cancelled out by the pre-multiply of \\(\\sqrt{n}\\).\n\n\n3.7 Other properties\n\nAmong the class of unbiased linear estimators of the CLRM, the OLS is the Best Linear Unbiased Estimator (BLUE). “Best” here means lowest variance. Can you show this?"
  },
  {
    "objectID": "handout-3.html#method-of-moments",
    "href": "handout-3.html#method-of-moments",
    "title": "Estimation Methods",
    "section": "4 Method of Moments",
    "text": "4 Method of Moments\nThe method of moments (MM) approach is to match assumed ‘moments’ given by the model with their sample analogue. This is a very general approach and is used extensively in applied macroeconomics, where a structural model gives rise to moments between economic variables that can be matched in the data.\nA general principle of MM is that the number of moments, \\(m\\), must be \\(\\geq k\\), the number of parameters being estimated. This akin to saying, the number of equations must be greater or equal to the number of variables being solved for. If the number of moments exceeds the number of parameters, we say that the model is overidentified. In the case of instrumental variables, overidentification allows you two test certain model assumptions.\nIn term 2, you will study instrumental variables which adopts a GMM approach to estimation. MM approaches are also used extensively in time series. For now, we will apply the MM approach to the CLRM.\n\n4.1 General setup\nThe observed data is given by, \\(W_1,...,W_n\\), read \\(W_i\\) is a \\(p\\)-dimension random vector. Let \\(g(W_i,\\theta)\\) be a \\(l\\)-dimension function (i.e. \\(\\in \\mathbf{R}^l\\)) and \\(\\theta\\in\\mathbf{R}^k\\):\n\\[\ng(W_i,\\theta) = \\begin{bmatrix}g_1(W_i,\\theta) \\\\ \\vdots \\\\g_l(W_i,\\theta)\\end{bmatrix}\n\\] We assume that the true value of the parameter \\(\\theta_0\\in\\Theta\\subset \\mathbf{R}^{k}\\) satisifies the condition, \\[\nE\\big[g(W_i,\\theta_0)\\big] = 0\n\\] We say that the model is identified if there is a unique solution to the above equations. That is, \\(E\\big[g(W_i,\\theta)\\big] = E\\big[g(W_i,\\tilde{\\theta})\\big] = 0\\;\\Rightarrow\\;\\theta=\\tilde{\\theta}\\). A necessary condition for identifiction is \\(l\\geq k\\); i.e. the number of equations is at least as large as the number of unknown parameters. A model can be underidentified, which typically means that there is not a unique solution for some of the parameters.\n\n\n4.2 Estimator\nThe basic principle of MM estimation is to replacing the expectation operator with the average function and solve for \\(\\hat{\\theta}\\). \\[\nn^{-1}\\sum_{i=1}^n g(W_i,\\hat{\\theta}^{MM}) = 0\n\\] However, this only works when \\(l=k\\) (exactly identified cases). For \\(l&gt;k\\) (overidentified cases) there is no unique vector that solves all \\(l\\) equations.\nThe Generalized Method of Moments (GMM) approach applies a set of weights to the minimization problem. Let \\(A_n\\) be a \\(l\\times l\\) weight matrix, such that \\(A_n\\rightarrow_p A\\). Then,\n\\[\n\\hat{\\theta}^{GMM} = \\underset{\\theta\\in\\Theta}{\\text{arg min}}\\;\\bigg|\\bigg|A_n n^{-1}\\sum_{i=1}^n g(W_i,\\hat{\\theta}^{MM})\\bigg|\\bigg|^2\n\\]\nHere, \\(||v||\\) denotes the Euclidean norm of vector \\(v\\): \\(||v|| = \\sqrt{v'v}\\).\n\n\n4.3 Application to CLRM\nAssumption CLRM 2b tells us that the regressors are uncorrelated with the error term. \\[\nE[X_iu_i] = 0\n\\] This is the moment that gives rise to identification in the CLRM. Given CLRM 1, we can replace the error term in the above moment with \\(Y_i-X_i'\\beta\\). \\[\nE[X_i(Y_i-X_i'\\beta)] = 0\n\\] Thus, the \\(g(W_i,\\beta)=X_i(Y_i-X_i'\\beta)\\) for \\(W_i = [Y_i,X_i']'\\).\nHow many equations are there? Recall, \\(X_i\\) is a \\(k\\)-dimension random vector. So,\n\\[\nE[X_i(Y_i-X_i'\\beta)] = \\begin{bmatrix}E[X_{i1}(Y_i-X_i'\\beta)]\\\\E[X_{i2}(Y_i-X_i'\\beta)]\\\\ \\vdots \\\\ E[X_{ik}(Y_i-X_i'\\beta)]\\end{bmatrix}=0\n\\] The are \\(k\\)-moments (or equations), meaning that we can estimate up to \\(k\\) parameters. In this instance, we have a failure of identification if \\(rank(E[X_iX_i'])&lt;k\\); which is required for the invertibility of \\(E[X_iX_i']\\). This condition is met by assumption CLRM 4; ensuring exact identification.\nThe MM estimator for the CLRM is then given by the solution to,\n\\[\nn^{-1}\\sum_{i=1}^n X_i(Y_i-X_i'\\hat{\\beta}^{MM}) = 0\n\\]\nThe solution is equivalent to the OLS estimator: \\[\n\\hat{\\beta}^{MM} = \\bigg(n^{-1}\\sum_{i=1}^n X_iX_i'\\bigg)^{-1}n^{-1}\\sum_{i=1}^nX_iY_i = \\big(X'X\\big)^{-1}X'Y = \\hat{\\beta}^{OLS}\n\\]\n\n\n4.4 Consistency\nAs we have already established the consistency of the OLS estimator, we will briefly review the case of the GMM estimator here. A more detailed discussion will be provided in term 2.\nRecall, the assumption \\(A_n\\rightarrow_p A\\). Then,\n\\[\n\\bigg|\\bigg|A_n n^{-1}\\sum_{i=1}^n g(W_i,\\theta)\\bigg|\\bigg|^2\\rightarrow_p \\big|\\big|A E\\big[ g(W_i,\\theta)\\big]\\big|\\big|^2\n\\] In instance where identification is exact/unique, \\(E\\big[ g(W_i,\\theta)\\big]=0\\iff\\theta=\\theta_0\\). Which is to say that the true value of \\(\\theta\\) is the unique minimizer.\nThen,\n\\[\n\\begin{aligned}\n\\hat{\\theta}^{GMM} =&\\underset{\\theta\\in\\Theta}{\\text{arg min}}\\; \\bigg|\\bigg|A_n n^{-1}\\sum_{i=1}^n g(W_i,\\theta)\\bigg|\\bigg|^2 \\\\\n\\rightarrow_p&\\underset{\\theta\\in\\Theta}{\\text{arg min}}\\;\\big|\\big|A E\\big[ g(W_i,\\theta)\\big]\\big|\\big|^2 \\\\\n=&\\theta_0\n\\end{aligned}\n\\] The formal proof requires a number of additional regularity assumptions; including, the compactness of \\(\\Theta\\).\n\n\n4.5 Asymptotic Normality\nThe GMM estimator is asymptotically normal.\n\\[\n\\sqrt{n}\\big(\\hat{\\theta}^{GMM}-\\theta_0\\big)\\rightarrow_d N(0,V)\n\\] where, \\[\n\\begin{aligned}\nV =& (Q'A'AQ)^{-1}QA'A\\Omega A'AQ (Q'A'AQ)^{-1} \\\\\nQ =& E\\bigg[\\frac{\\partial g(W_i,\\theta_0)}{\\partial \\theta'}\\bigg] \\\\\n\\Omega =& E\\big[ g(W_i,\\theta)g(W_i,\\theta)'\\big]\n\\end{aligned}\n\\] Where does this result come from? We won’t go through the proof in details. However, it starts from the FOCs. The GMM estimator solves,\n\\[\n\\bigg[\\underbrace{n^{-1}\\sum_{i=1}^n\\frac{\\partial g(W_i,\\hat{\\theta}^{GMM})}{\\partial \\theta'}}_{Q_n\\big(\\hat{\\theta}^{GMM}\\big)}\\bigg]'A_n'A_n n^{-1}\\sum_{i=1}^{n}g(W_i,\\hat{\\theta}^{GMM}) = 0\n\\] In the above expression, the matrix of derivatives will converge (under some regularity conditions) in probability: \\(Q_n\\big(\\hat{\\theta}^{GMM}\\big)\\rightarrow_p Q\\). Second, since \\(E\\big[ g(W_i,\\theta)\\big]=0\\), by CLT we know,\n\\[\nn^{-1/2}\\sum_{i=1}^{n}g(W_i,\\theta_0)\\rightarrow_d N(0,\\underbrace{E\\big[ g(W_i,\\theta)g(W_i,\\theta)'\\big]}_\\Omega)\n\\] We can therefore see where the components of the variance come from. The proof requires a bit more work. First, we need the distribution of \\(\\sqrt{n}\\big(\\hat{\\theta}^{GMM}-\\theta_0\\big)\\), not \\(n^{-1/2}\\sum_{i=1}^{n}g(W_i,\\theta_0)\\). Second, the FOCs contain \\(n^{-1}\\sum_{i=1}^{n}g(W_i,\\hat{\\theta}^{GMM})\\) and not \\(n^{-1}\\sum_{i=1}^{n}g(W_i,\\theta_0)\\).\nThis is resolve using a mean value expansion:\n\\[\ng(W_i,,\\hat{\\theta}^{GMM}) = g(W_i,\\theta_0) + \\frac{\\partial g(W_i,\\hat{\\theta}^*)}{\\partial \\theta'}\\big(\\hat{\\theta}^{GMM}-\\theta_0\\big)\n\\] Plugging this expansion into the FOCs, you can rearrange to solve,\n\\[\n\\sqrt{n}\\big(\\hat{\\theta}^{GMM}-\\theta_0\\big) = -[Q_n\\big(\\hat{\\theta}^{GMM}\\big)'A_n'A_nQ_n\\big(\\hat{\\theta}^*\\big)]^{-1}Q_n\\big(\\hat{\\theta}^{GMM}\\big)'A_n'A_nn^{-1/2}\\sum_{i=1}^{n}g(W_i,\\theta_0)\n\\] Since \\(\\hat{\\theta}^*\\) is a mean value, it is a also consistent and \\(Q_n\\big(\\hat{\\theta}^*\\big)\\rightarrow_p Q\\).\n\n\n4.6 Additional comments\n\nThe targetted moments may be highly non-linear. For example, the Lucas Model pins down the rate of return on a risky asset \\(R_{j,t}\\) using the relative utility of consumption today and tomorrow. The equilibrium condition for assets \\(j=1,...,m\\) is given by,\n\\[\nE\\bigg[\\underbrace{\\delta\\bigg(\\frac{C_{t+1}}{C_t}\\bigg)^{-\\alpha}(1+R_{j,t})-1}_{g(W_{j,t},\\theta)}\\bigg]=0\n\\]\nwhere \\(W_{j,t}=[C_t,C_{t+1},R_{j,t}]'\\) and \\(\\theta = [\\alpha,\\delta]'\\). As the moment must hold for each asset, \\(\\theta\\) is identified so long as \\(m\\geq 2\\).\nGiven the non-linearity of the \\(g\\)-function, there is no closed for solution. Instead, the GMM estimator must be solved for using numerical optimization.\nSome (macroeconomic) models are not identified (or underidentified). For example, a simple RBC model (with a random government component) yields the following moment condition from Euler equation,7\n\\[\nE\\bigg[\\underbrace{\\beta\\frac{C_{t+1}}{C_t}\\big(f_K + (1-\\delta)\\big)-1}_{g(W_i,\\theta)}\\bigg]=0\n\\] where \\(W_{j,t}=[C_t,C_{t+1},f_K]\\) and \\(\\theta = [\\beta,\\delta]\\). In this application, there is only a single moment but two unknown parameters. For this reason, you will need to find an additional instrument that introduces an additional moment to identify both parameters."
  },
  {
    "objectID": "handout-3.html#maximum-likelihood",
    "href": "handout-3.html#maximum-likelihood",
    "title": "Estimation Methods",
    "section": "5 Maximum Likelihood",
    "text": "5 Maximum Likelihood\nMaximum Likelihood (ML or MLE) are a general class of estimators that exploit a knowledge of the underlying distribution of unobservables in the model. As the name suggests, the goal will be to maximize the likelihood (i.e. probability) of observing the a given sample of data, given the assumed distribution of the data, governed by a fixed set of parameters.\n\n5.1 General setup\nConsider an iid random sample of data: \\(W_1,...,W_n\\). We will assume that the data is drawn from a known distribution, \\(f(w_i;\\theta)\\), where \\(\\theta\\in\\Theta\\subset \\mathbf{R}^k\\) is an unknown vector of population parameters.8\n\n\n\n\n\n\nNotation\n\n\n\nThe notation used to describe ML estimation varies quite a bit across texts. One key difference appears to be how to denote a parameterized distribution. The density function, \\(f(w_i;\\theta)\\), is the density at \\(w_i\\) (the realized value for observation \\(i\\)), where the distribution is parameterized by \\(\\theta\\). Some texts use the conditional notation, \\(f(w_i|\\theta)\\), as the distribution depends on \\(\\theta\\). However, probabilistic conditions tend to be based on random variables and not non-random parameters. I found this StackExchange discussion on the topic quite interesting. Needless to say, there is much disagreement and notation appears to differ across Mathematics and Statistics, and among the Statisticians, between frequentists and Bayesians. Even the Wikipedia page on MLE uses a combination of the two notations. I will use ‘;’; which also happens to be the notation used by Wooldridge (2010).\n\n\nAs the sample is iid, the joint density (or pdf) of the realized observations is given by the product of marginals,\n\\[\nf(w;\\theta)=\\prod_{i=1}^n f(w_i;\\theta)\n\\] This is referred to as the likelihood function.\nSuppose \\(W_i = [Y_i,X_i']'\\), a vector contain a single outcome variable and a set of covariates. We can then define the joint conditional likelihood as,\n\\[\n\\begin{aligned}\n\\ell_i(\\theta) =& f(Y_i|X_i;\\theta) \\\\\nL_n(\\theta) =& \\prod_{i=1}^n f(Y_i|X_i;\\theta)\n\\end{aligned}\n\\] Here my notation differs from Wooldridge (2010), who uses \\(\\ell_i(\\theta)\\) to denote the conditional log-likelihood for observation \\(i\\) (see Wooldridge 2010, 471). Take note of the fact that the likelihood function is a random function of \\(\\theta\\), since it depends on the random variables \\(W_i = [Y_i,X_i']'\\).9\n\n\n5.2 Estimator\nThe goal of ML estimation is to solve the value of \\(\\hat{\\theta}\\) that maximizes the likelihood of observing the data.\n\\[\n\\hat{\\theta}^{ML} = \\underset{\\theta}{\\text{arg max}}\\;L_n(\\theta)\n\\] In practice, we apply a monotonic transformation to the likelihood function. By taking the log of the likelihood, the product of marginal distributions becomes a sum. As the transformation is monotonic, the solution to the above problem is equivalent to the solution to,\n\\[\n\\hat{\\theta}^{ML} = \\underset{\\theta}{\\text{arg max}}\\;n^{-1}\\log L_n(\\theta) = \\underset{\\theta}{\\text{arg max}}\\;n^{-1}\\sum_{i=1}^n\\log\\ell_i(\\theta)\n\\] In addition, the division by \\(n\\) makes this problem the sample analogue of, \\[\n      \\underset{\\theta\\in\\Theta}{\\text{max}}\\;E[\\log\\ell_i(\\theta)]\n\\] It turns out that the true value of the parameter, \\(\\theta_0\\), is the solution to the above problem [see Wooldridge (2010), pp. 473].10 We will prove this for the unconditional case when we discuss consistency of ML.\nAssuming a continuous, concave density function, we can solve for \\(\\hat{\\theta}^{ML}\\) using first-order conditions.\n\\[\n\\frac{1}{n}\\frac{\\partial \\log L_n(\\hat{\\theta})}{\\partial \\theta} =\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial \\log\\ell_i(\\hat{\\theta})}{\\partial \\theta}= n^{-1}S(\\hat{\\theta})=0\n\\]\nThe vector of partial derivatives is referred to as the score function: \\(S(\\theta)\\). When evaluated at the ML estimator, the score function is 0. This is a \\(k\\)-dimensional vector in which row is the partial derivative with respect to \\(\\theta_k\\).\n\n\n5.3 Application to CLRM\nUnder CLRM 5, \\(U|X \\sim N(0,\\sigma^2 I_n)\\). Together with CLRM 1 and 6, we know the conditional distribution of \\(Y\\). \\[\nY_i|X_i\\sim_{iid} N(X_i'\\beta,\\sigma^2)\n\\] where \\(X_i'\\beta\\) is the conditional mean of \\(Y_i\\). Therefore, the conditional likelihood of the data is given by, \\[\nL_n(\\beta,\\sigma^2) = \\prod_{i=1}^n \\bigg[\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\bigg(-\\frac{1}{2}\\bigg(\\frac{Y_i-X_i'\\beta}{\\sigma}\\bigg)^2\\bigg)\\bigg]\n\\] :::{.callout-important} We are working with the conditional likelihood. To define the likelihood of observing the entire sample, \\(W_1,...,W_n\\), we would also need to consider the distribution of \\(X_i\\). We would then define \\(f(W_i;\\theta) = f(Y_i|X_i;\\theta)\\cdot f(X_i)\\), where \\(f(X_i)\\) may be parameterized by its own set of parameters. \\(\\theta\\) is the set of parameters that parameterize the conditional distribution of \\(Y|X\\). ::: Taking the log transformation and divide by \\(n\\), we get, \\[\nn^{-1}\\log L_n(\\beta,\\sigma^2) =  -\\frac{1}{2}\\log(\\sigma^2)-\\frac{1}{2}\\log(2\\pi)-\\frac{1}{2n\\sigma^2} \\sum_{i=1}^n(Y_i-X_i'\\beta)^2\n\\] It should be immediately clear that maximizing this expression will be equivalent to minimizing the sum of squared errors.\nConsider the FOC’s set to 0 at the optimal point. First, w.r.t. \\(\\beta\\),\n\\[\n\\begin{aligned}\n\\frac{\\partial n^{-1}\\log L_n(\\hat{\\beta},\\hat{\\sigma}^2)}{\\partial\\beta} =& -\\frac{1}{n\\sigma^2} \\sum_{i=1}^nX_i(Y_i-X_i'\\hat{\\beta}) = 0 \\\\\n\\Rightarrow \\hat{\\beta}^{ML} =& \\bigg(\\sum_{i=1}^n X_iX_i'\\bigg)\\sum_{i=1}^nX_iY_i \\\\\n=& \\big(X'X\\big)^{-1}X'Y\n\\end{aligned}\n\\] In the case of the CLRM, \\(\\hat{\\beta}^{ML}=\\hat{\\beta}^{MM}=\\hat{\\beta}^{OLS}\\).\nSecond, w.r.t. \\(\\sigma^2\\), \\[\n\\begin{aligned}\n\\frac{\\partial n^{-1}\\log L_n(\\hat{\\beta},\\hat{\\sigma}^2)}{\\partial\\sigma^2} =& -\\frac{1}{2\\hat{\\sigma}^2}+ \\frac{1}{n2\\hat{\\sigma}^4}\\sum_{i=1}^n(Y_i-X_i'\\hat{\\beta})^2 = 0 \\\\\n\\Rightarrow \\hat{\\sigma}^{2}_{ML} =& n^{-1}\\sum_{i=1}^n(Y_i-X_i'\\hat{\\beta})^2\n\\end{aligned}\n\\] This estimator for the variance is consistent, but biased for small samples. This is because it scales by \\(n\\) and not \\(n-k\\), a distinction that is ignorable as \\(n\\rightarrow\\infty\\). For this reason, when conducting inference you should use the asymptotic distribution of the ML estimator.\n\n\n5.4 Consistency\nThe ML estimator is consistent. This can be shown in a couple of steps. To simplify notation we will examine the proof for the unconditional likelihood, but the same will hold for the conditional. The proof will require Jensen’s inequality:\n\nTheorem 1 For \\(h(\\cdot)\\) concave, then \\(E[h(X)]\\leq h(E[X])\\).\n\n\nProof. By the WLLN, for ALL values of \\(\\theta\\in\\Theta\\),\n\\[\n\\begin{aligned}\nn^{-1}\\sum_{i=1}^n\\log f(W_i;\\theta) \\rightarrow_p&\\;E\\big[\\log f(W_i;\\theta)\\big] \\\\\n=&\\int\\big(\\log f(w;\\theta)\\big)f(w;\\theta_0)dw\n\\end{aligned}\n\\] Note, an important distinction in the last line: the expectation is based on the density function parameterized by the true value, \\(\\theta_0\\). This is because the data is generated by the true density.\nWe have convergence for ALL values of \\(\\theta\\), but now need to establish convergence to the \\(\\theta_0\\). Consider the difference, \\[\n\\begin{aligned}\nE\\big[\\log f(W_i;\\theta)\\big]-E\\big[\\log f(W_i;\\theta_0)\\big]\n=&E\\bigg[\\log\\frac{f(W_i;\\theta)}{f(W_i;\\theta_0)}\\bigg] \\\\\n\\leq&\\log\\bigg[\\frac{f(W_i;\\theta)}{f(W_i;\\theta_0)}\\bigg] \\qquad \\text{by Jensen's} \\\\\n=&\\log \\int\\bigg(\\frac{f(w;\\theta)}{f(w,\\theta_0)}\\bigg)f(w;\\theta_0)dw \\\\\n=&\\log \\int f(w;\\theta)dw \\\\\n=&\\log 1 \\\\\n=&0\n\\end{aligned}\n\\] The inequality can be made strict if we assume that \\(Pr\\big(f(W_i;\\theta_0)\\neq f(W_i;\\theta)\\big)&gt;0\\) \\(\\forall \\theta\\neq\\theta_0\\). This ensures that \\(\\theta_0\\) is a unique solution. Since the difference is \\(\\leq 0\\), it follows that, \\[\n\\theta_0 = \\underset{\\theta\\in\\Theta}{\\text{arg max}} E[\\log f(W_i;\\theta)]\n\\] Which implies, \\[\n\\begin{aligned}\n\\hat{\\theta}^{ML}_n =& \\;\\underset{\\theta\\in\\Theta}{\\text{arg max}} \\;n^{-1}\\log L_n(\\theta) \\\\\n\\rightarrow_p& \\;\\underset{\\theta\\in\\Theta}{\\text{arg max}} E\\big[\\log f(W_i,\\theta)\\big]\\\\\n=& \\theta_0\n\\end{aligned}\n\\]\n\n\n\n5.5 Asymptotic Normality\nThe ML estimator is asymtotically normal. We will not prove this result, but rather focus on the form of the asymptotic variance and its estimator. The proof uses the Mean Value Theorem and CLT.\n\\[\n\\sqrt{n}\\big(\\hat{\\theta}^{ML}_n-\\theta_0\\big)\\rightarrow_d N(0,V)\n\\]\nwhere \\(V=[J(\\theta_0)]^{-1}\\). \\(J(\\theta)\\) is referred to as the information matrix, given by the expectation of the (Hessian) matrix of second-order derivatives:\n\\[\nJ(\\theta) = -E\\bigg[\\frac{\\partial^2}{\\partial\\theta\\partial\\theta'}\\log f(W_i,\\theta_0)\\bigg]\n\\] \\([nJ(\\theta_0)]^{-1}\\) is used to approximate the variance, but since \\(J\\) is not observed, it must be estimated. This is done by replacing the expectation in the information matrix with sample average:\n\\[\n\\hat{V}_H = \\bigg[\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial^2}{\\partial\\theta\\partial\\theta'}\\log f(W_i,\\hat{\\theta})\\bigg]^{-1}\n\\]\n\n\n5.6 Additional comments\n\nML estimators are invariant. If \\(\\hat{\\theta}\\) is the ML-estimator for \\(\\theta\\), then \\(\\ln(\\hat{\\theta})\\) is the ML-estimator for \\(\\ln(\\theta)\\).\nIn general, there are no closed form solutions for ML estimators; the CLRM being one exception. For this reason, ML estimation requires numerical optimization.\nThe ML estimator is efficient. That is, its variance is at least as small as any other consistent (and asymptotically normal) estimator.\nML estimators require as to know the true PDF, up to its parameters. For example, probit (logit) models assumes that the error term is normally (logistically) distributed.\nIn some cases, the estimator may be consistent even if the PDF is misspecified. As is the case for the OLS estimator of the linear model. These estimators are referred to as a quasi-ML estimators."
  },
  {
    "objectID": "handout-3.html#footnotes",
    "href": "handout-3.html#footnotes",
    "title": "Estimation Methods",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMachine Learning techniques, such as neural networks, use non-linear operators such as the Softmax function and Rectified Linear Unit (ReLU). Who knows, in a few years, we may no longer think of OLS as the ‘work-horse’ of applied statistics and research.↩︎\nA measure of distance must be positive. You can use the (sum of) absolute-value deviations, but the least squares has nice properties including ease of differentiation and overall efficiency (of the estimator).↩︎\nThe inner (or dot) product of two equal-length vectors, \\(a\\) and \\(b\\), is defined as: \\[\n\\langle a,b\\rangle=a\\cdot b = \\sum_{i=1}^k a_i\\times b_i = a'b\n\\]↩︎\nIf \\(Y\\sim N(\\mu,\\Sigma)\\), \\(Y\\in\\mathbf{R}^k\\), then \\(AY+b\\sim N(A\\mu+b,A\\Sigma A')\\) for any non-random \\(m\\times k\\) \\(A\\)-matrix and \\(m\\times 1\\) \\(b\\)-vector.↩︎\nThe assumptions are fulfilled by CLRM 2b and CLRM 6.↩︎\nThe finiteness of this matrix requires two assumptions:\n\n\\(E[X_{i,j}^4]&lt;\\infty\\) for all \\(j=1,...k\\) (i.e. each regressor has finite fourth moment)\n\\(E[U_j^4]&lt; \\infty\\)\n\nThese assumptions are sufficient for all elements of matrix \\(E[u_1^2 X_1 X_1']\\) to be finite. The proof is an application of the Cauchy-Schwartz Inequality, which we haven’t covered.↩︎\nThis example is taken from Canova (2007, ch. 5, p. 167).↩︎\n\\(\\Theta\\) is a parameter space and is typically assumed to be compact: a closed and bounded subset of Euclidean space.↩︎\nYou may also see the equivalent notation \\(L(W_i;\\theta)\\equiv L_n(\\theta)\\). The subscript-\\(n\\) implies that the function depends on the sample.↩︎\nThis is actually a non-trivial issue and beyond the scope of this module. As noted by Wooldridge (2010), we can arrive at the ML estimator by picking the value of \\(\\theta\\) to maximize the joint likelihood. However, this approach assumes that the true value of \\(\\theta\\in\\Theta\\), \\(\\theta_0\\), maximizes the joint likelihood. This is not immediately evident. Once established, we have a more robust basis of the ML estimator.↩︎"
  },
  {
    "objectID": "handout-1.html",
    "href": "handout-1.html",
    "title": "Classical Linear Regression Model & Ordinary Least Squares",
    "section": "",
    "text": "In this handout we will revisit the Classical Linear Regression Model (CLRM) (see Wooldridge 2010, chaps. 1–2). The goal of this week’s lecture is to:\n\nunderstand the model specification;\nit’s underlying assumptions;\nand the appropriate interpretation;\nthe OLS estimator, using linear algebra;\nthe geometry of OLS and partitioned regression result."
  },
  {
    "objectID": "handout-1.html#overview",
    "href": "handout-1.html#overview",
    "title": "Classical Linear Regression Model & Ordinary Least Squares",
    "section": "",
    "text": "In this handout we will revisit the Classical Linear Regression Model (CLRM) (see Wooldridge 2010, chaps. 1–2). The goal of this week’s lecture is to:\n\nunderstand the model specification;\nit’s underlying assumptions;\nand the appropriate interpretation;\nthe OLS estimator, using linear algebra;\nthe geometry of OLS and partitioned regression result."
  },
  {
    "objectID": "handout-1.html#model-specification",
    "href": "handout-1.html#model-specification",
    "title": "Classical Linear Regression Model & Ordinary Least Squares",
    "section": "2 Model Specification",
    "text": "2 Model Specification\nThe linear population regression model is given by,\n\\[\n\\begin{aligned}\n  Y_i =& X_i'\\beta+\\varepsilon_i \\\\\n  =& \\beta_1\\mathbf{1}+\\beta_2X_{i2}+\\beta_3X_{i3}+...+\\beta_kX_{ik}+\\varepsilon_i\n\\end{aligned}\n\\]\nfor \\(i = 1,2,...,n\\). Where,\n\n\\(i\\): unit of observation; e.g. individual, firm, union, political party, etc.\n\\(Y_i \\in \\mathbb{R}\\): scalar random variable.\n\\(X_i \\in \\mathbb{R}^k\\): \\(k\\)-dimensional (column1) vector of regressors, with \\(k&lt;n\\).2\n\\(\\beta\\): \\(k\\)-dimensional, non-random vector of unknown population parameters.\n\\(\\varepsilon_i\\): unobserved, random error term.3\n\nThe linear population regression equation is linear in parameters. This is an important assumption that does NOT restrict the model from being non-linear in regressors. For example, the equation\n\\[Y_i = \\beta_1 + \\beta_2X_{i2} + \\beta_3X_{i2}^{\\color{red}{2}} + \\varepsilon_i\\] non-linear in \\(X_{i2}\\), but still linear in parameters. In contrast, the equation\n\\[Y_i = \\beta_1 + \\beta_2X_{i2} + ({\\color{red}{\\beta_2\\beta_3}})X_{i3} + \\varepsilon_i\\] is non-linear in parameters.\n\n2.1 Intercept\nThe constant (intercept) in the equation serves an important purpose. While there is no a priori reason for the model to have a constant term, it does ensure that the error term is mean zero.\n\nProof. Suppose \\(E[\\varepsilon_i] = \\gamma\\).\nWe can then define a new error term, \\(\\upsilon_i = \\varepsilon_i - \\gamma\\), such \\(E[\\upsilon_i] = 0\\). The population regression model can be rewritten as, \\[ \\begin{aligned} Y_i =& X_ i'\\beta + v_i + \\gamma  \\\\\n=& \\underbrace{(\\beta_1+\\gamma)}_{\\tilde{\\beta}_1}\\mathbf{1} + \\beta_2X_{i2} + \\beta_3X_{i3} + ... + \\beta_kX_{ik} + v_i\n\\end{aligned}\n\\] The model has a new intercept \\(\\tilde{\\beta}_1=\\beta_1 + \\gamma\\), but the other parameters remain unchanged.\n\n\n\n2.2 Matrix notation\nFor a sample of \\(n\\) observations, we can stack the unit-level linear regression equation into a vector,\n\\[\nY =\\underbrace{\\begin{bmatrix}Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix}}_{n\\times 1} = \\underbrace{\\begin{bmatrix}X_1'\\beta \\\\ X_2'\\beta \\\\ \\vdots \\\\ X_n'\\beta\\end{bmatrix}}_{n\\times 1}  + \\underbrace{\\begin{bmatrix}\\varepsilon_1 \\\\ \\varepsilon_2 \\\\ \\vdots \\\\ \\varepsilon_n\\end{bmatrix}}_{n\\times 1}  \n= \\underbrace{\\begin{bmatrix}X_{11} & X_{12} & \\cdots & X_{1k}\\\\ X_{21} & X_{22} && \\\\ \\vdots & & \\ddots & \\\\ X_{n1} & & & X_{nk} \\end{bmatrix}}_{n\\times k} \\underbrace{\\begin{bmatrix}\\beta_1 \\\\ \\beta_2 \\\\ \\vdots \\\\ \\beta_k\\end{bmatrix}}_{k\\times 1} + \\begin{bmatrix}\\varepsilon_1 \\\\ \\varepsilon_2 \\\\ \\vdots \\\\ \\varepsilon_n\\end{bmatrix} = X\\beta + \\varepsilon\n\\] Notice, in matrix notation, you lose the transpose from \\(X_i'\\beta\\). Apart from the absence of the \\(i\\) subscript, this is a useful way of knowing the dimension of the equation (in my notes). You MUST always write \\(X\\beta\\) and not \\(\\beta X\\). For the scalar case, \\(X_i'\\beta = \\beta'X_i\\), but for the vector case \\(\\beta X\\) is not defined since \\(\\beta\\) is \\(k\\times 1\\) and \\(X\\) is \\(n\\times k\\)."
  },
  {
    "objectID": "handout-1.html#clrm-assumptions",
    "href": "handout-1.html#clrm-assumptions",
    "title": "Classical Linear Regression Model & Ordinary Least Squares",
    "section": "3 CLRM Assumptions",
    "text": "3 CLRM Assumptions\nAssumption CLRM 1. Population regression equation is linear in parameters: \\[Y = X\\beta+\\varepsilon\\]\nAssumption CLRM 2. Conditional mean independence of the error term: \\[E[\\varepsilon|X]=0\\]\nAssumption CLRM 2. is stronger than \\(E[\\varepsilon_i|X_i]\\) (mean independence for unit \\(i\\)). If all units were independent, then \\(E[\\varepsilon_i|X_i]\\) would imply \\(E[\\varepsilon|X]=0\\). However, since we have not (yet) assumed this, we need this stronger exogeneity assumption. Consider, if \\(i\\) represented units of time (\\(t\\)), as in time-series models, independence across \\(i\\) will not hold.\nTogether, CLRM 1. and CLRM 2. imply that\n\\[ E[Y|X] = X\\beta  \\] This means that the Conditional Expectation Function is known and linear in parameters.\nConditional mean independence implies - by the Law of Iterated Expectations - mean independence of the error term,\n\\[E[\\varepsilon|X]=0 \\Rightarrow E\\big[E[\\varepsilon|X]\\big]=E[\\varepsilon]=0\\]\nand uncorrelatedness,\n\\[E[\\varepsilon|X]=0 \\Rightarrow E[\\varepsilon X]=0\\] Note, neither of the above statements hold the other way around. Mean independence does not imply conditional mean independence and uncorrelatedness (zero correlation/covariance) does not imply conditional mean independence.\nUncorrelatedness rules out linear relationships between the regressors and error term while conditional mean independence rules out non-linear relationships too.\nIn general, distributional independence implies mean independence which then implies uncorrelatedness.\n\nIn the case joint-normally distributed random variables, uncorrelatedness implies independence. That is, if \\[ \\begin{bmatrix}Y_1 \\\\ Y_2\\end{bmatrix}\\sim N\\bigg(\\begin{bmatrix}\\mu_1 \\\\ \\mu_2\\end{bmatrix},\\begin{bmatrix}\\sigma_1^2 & \\sigma_{12}\\\\ \\sigma_{21} & \\sigma_2^2\\end{bmatrix}\\bigg)\\] Then \\(\\sigma_{12}=\\sigma_{21}=0 \\iff f_1*f_2 = f_{12}\\).\n\nWe will later show that uncorrelatedness is sufficient for consistency of the Ordinary Least Squares estimater, while conditional mean independence is required for unbiasedness of OLS.\nAssumption CLRM 3. Homoskedasticity: \\(Var(\\varepsilon|X) = E[\\varepsilon\\varepsilon'|X] =  \\sigma^2I_n\\)\nCLRM 3. states that the variance of the error term is independent of \\(X\\) and constant across units. The diagonal nature of the covariance matrix also implies that the error terms are uncorrelated across units in the data. Note, this does not imply independence of the error terms across units.\nModels with heteroskedasticity relax the assumption of constant variance, allowing for a richer variance-covariance matrix that typically depends on \\(X\\).\nThis assumption is unlikely to hold in time-series models where units represent repeated observations across time. Such violations are referred to as serial correlation or autocorrelation.\nEven in cross-sectional data settings, you can have non-zero correlations across units in the data. A common instance of this is the case of clustering. Clustering can occur when units experience common/correlated ‘shocks’; for example, the data contains groups of students from the same classroom who have a the same teacher. This can also be the result of clustered sampling, a common practice in multi-stage survey design.\nAssumption CLRM 4. Full rank: \\(rank(X)=k\\quad a.s.\\) a.s.4\nSince \\(X\\) is a random variable we should add to the assumption: \\(rank(X) = k\\) almost surely (abbreviated a.s.). This means that the set of events in which \\(X\\) is not full rank occur with probability 0. The reason for this addition is that such a set of events (in the sample space) may not be empty.\nCLRM 4. is some time referred to as the absence of perfect (or exact) collinearity. Do not confuse this with multicollinearity. Multicollinearity occurs when regressors are highly (linearly) correlated with one another, yielding imprecise estimates.\nAssumption CLRM 5. Normality of the error term: \\(\\varepsilon|X \\sim N(0,\\sigma^2 I_n)\\)\nAssumption CLRM 6. Observations \\(\\{(Y_i,X_i): i=1,...,n\\}\\) are independently and identically distributed (iid).\nCLRM 5 & 6 are not part of the Classical assumptions, but do simplify the problem of inference. Note, CLRM 5 implies independence across error terms, not implied by CLRM 3.\n\n3.1 Non-random \\(X\\)\nThere is an alternative version of the CLRM in which \\(X\\) is a non-random, matrix of regressors/predictors. With \\(X\\) fixed, the error term is the only only random variable in the model. CLRM assumptions 1 and 4 remain the same, while CLRM 2, 3, 5, and 6 become:\nAssumption CLRM 2a. Mean independence of the error term: \\[E[\\varepsilon]=0\\]\nAssumption CLRM 3a. Homoskedasticity: \\(Var(\\varepsilon) = \\sigma^2I_n\\)\nAssumption CLRM 5a. Normality of the error term: \\(\\varepsilon \\sim N(0,\\sigma^2 I_n)\\)\nAssumption CLRM 6a. Observations \\(\\{\\varepsilon_i: i=1,...,n\\}\\) are independently and identically distributed (iid).\n\n\n3.2 Identification\nCLRM 1,2 and 4. are the identifying assumptions of the model. These assumptions allow us to write the parameter of interest as a set of ‘observable’ moments in the data. We can demonstrate this as follows.\n\nProof. Start with CLRM 2.\n\\[\n    E[\\varepsilon_i|X_i]=0\n\\]\nPre-multiply by the vector \\(X_i\\), \\[\n        X_iE[\\varepsilon_i|X_i]=0\n\\] Since the expectation is conditional on \\(X_i\\), we can bring \\(X_i\\) inside the expectation function,\n\\[\n        E[X_i\\varepsilon_i|X_i]=0\n    \\] This conditional expectation is a random-function of \\(X_i\\). If we take the expectation of this function w.r.t. \\(X\\), we achieve the aforementioned result that conditional mean independence implies zero covariance, \\[\n        E\\left[E[X_i\\varepsilon_i|X_i]\\right]=E[X_i\\varepsilon_i]=0\n    \\]\nNow substitute in for \\(\\varepsilon_i\\) using the linear regression model from CLRM 1 and separate the resulting two terms,\n\\[\n\\begin{aligned}\n    &E[X_i(Y_i-X_i'\\beta)]=0 \\\\\n    \\Rightarrow &E[X_iX_i']\\beta=E[X_iY_i]\n\\end{aligned}\n\\]\nSince \\(\\beta\\) is a non-random vector, we can remove it from the expectation function.\nNow we have a system of linear equations (of the form \\(Av = b\\)) with a unique solution if and only if the matrix \\(E[X_iX_i']\\) is invertible. For the inverse of \\(E[X_iX_i']\\) to exist, we require CLRM 4, since \\(rank(X)=k\\quad a.s.\\Rightarrow rank(E[X_iX_i'])=k\\).5\n\\[\n    \\beta = E[X_iX_i']^{-1}E[X_iY_i]\n\\]\n\nWe cannot compute \\(\\beta\\) because we do not know the joint distribution of \\((Y_i,X_i)\\) needed to solve for the variance-covariance matrices. However, \\(\\beta\\) is (point) identified because both \\(Y\\) and \\(X\\) are observed in the data and the parameters are “pinned down” by a unique set of ‘observable’ moments in the data.\n\\(\\beta\\) is not identified if the above system of linear equations does not have a unique solution. This will occur if two or more of the regressors are perfectly colinear.6 \\(\\beta\\) is also not be identified if the resulting expression for \\(\\beta\\) includes ‘objects’ (moments, distribution/scale parameters) that are not ‘observed’ in the data. For example, if the error is not mean independent, the above expression will include a bias term that depends on \\(E[X_i'\\varepsilon_i]\\).\nIn this instance, the identification of \\(\\beta\\) is scale dependent. That is, if we multiply \\(Y_i\\) by a scalar, \\(\\beta\\) is multiplied by the same scalar. For example, in cases where a researcher is modelling standardized test-scores."
  },
  {
    "objectID": "handout-1.html#interpretation",
    "href": "handout-1.html#interpretation",
    "title": "Classical Linear Regression Model & Ordinary Least Squares",
    "section": "4 Interpretation",
    "text": "4 Interpretation\nIn this linear regression model each slope coefficient has a partial derivative interpretation,\n\\[\n\\beta_j = \\frac{\\partial E[Y_i|X_i]}{\\partial X_{ij}}\n\\] or, as a vector, \\[\n\\beta = \\frac{\\partial E[Y_i|X_i]}{\\partial X_{i}} = \\begin{bmatrix}\\frac{\\partial E[Y_i|X_i]}{\\partial X_{i1}}\\\\ \\vdots \\\\ \\frac{\\partial E[Y_i|X_i]}{\\partial X_{ik}}\\end{bmatrix} = \\begin{bmatrix}\\beta_1\\\\ \\vdots \\\\ \\beta_k\\end{bmatrix}\n\\]\nNote, the derivative is expressed in terms of changes in the expected value of \\(Y_i\\) (conditional on \\(X_i\\)), not \\(Y_i\\) itself. This is because \\(Y_i\\) is a random variable, but under CLRM 1 & 2\n\\[\nE[Y_i|X_i] = X_i'\\beta\n\\]\nFor a given value of \\(X_i\\), the above expression is non-random.\nAs \\(\\beta_j\\) is a partial derivative, its interpretation is one that “holds fixed” the value of other regressors (i.e. ceteris paribus). Because of this, many researchers apply the experimental language of control variables when interpretting regression coefficients. However, this is dependent on the assumed linearity of the CEF."
  },
  {
    "objectID": "handout-1.html#ordinary-least-squares",
    "href": "handout-1.html#ordinary-least-squares",
    "title": "Classical Linear Regression Model & Ordinary Least Squares",
    "section": "5 Ordinary Least Squares",
    "text": "5 Ordinary Least Squares\nOLS is an estimator for \\(\\beta\\). As will become evident in Lecture 3, it is not the only estimator for \\(\\beta\\).\nThe OLS estimator is the solution to,\n\\[\n\\min_b\\;\\sum_{i=1}^n(Y_i-X_i'b)^2\n\\]\nUsing vector notation, we can rewrite this as\n\\[\n\\begin{aligned}\n&\\min_b\\;(Y-Xb)'(Y-Xb)\\\\\n=&\\min_b\\;Y'Y-Y'Xb-b'X'Y+b'X'Xb \\\\\n=&\\min_b\\;Y'Y-2b'X'Y+b'X'Xb\n\\end{aligned}\n\\] From line 2 to 3 we use the fact that \\(Y'Xb\\) is a scalar and therefore symmetric: \\(Y'Xb=b'X'Y\\).7\nDifferentiating the above expression w.r.t. the vector \\(b\\) and setting the first-order conditions to \\(0\\), we find that the following condition must hold for \\(\\hat{\\beta}\\), the solution.\n\\[\n  \\begin{aligned}\n  &0=-2X'Y+2X'X\\hat{\\beta}\n  \\\\ \\Rightarrow& X'X\\hat{\\beta} = X'Y\n  \\end{aligned}\n\\]\n\nHow did we get this result? Deriving the first order conditions requires knowledge of how to solve for the derivative of a scalar respect to a column vector (in this case \\(b\\in R^k\\)). The extra material on Linear Algebra has some notes on vector differentiation.\nWe can ignore the first term \\(Y'Y\\) as it does not depend on \\(b\\). The second term is \\(-2b'X'Y\\). Here we can use the rule that, \\[\n  \\frac{\\partial z'a}{\\partial z} = \\frac{\\partial a'z}{\\partial z} = a\n\\] In this instance, \\(a = X'Y \\in R^k\\). Thus, \\[\n  \\frac{\\partial -2b'X'Y}{\\partial b} = -2\\frac{\\partial b'X'Y}{\\partial b} = -2X'Y\n\\] The third term is \\(b'X'Xb\\). This is what is commonly referred to as a quadratic form: \\(z'Az\\). We know that the derivative of this form is, \\[\n  \\frac{\\partial z'Az}{\\partial z} = Az + A'z\n\\] and if \\(A\\) is symmetric, the result simplies to \\(2Az\\). In this instance, \\(A = X'X\\) is symmetric and the derivative is given by, \\[\n  \\frac{\\partial b'X'Xb}{\\partial b} = 2X'X\n\\]\n\nIn order to solve for \\(\\hat{\\beta}\\) we need to move the \\(X'X\\) term to the right-hand side. If these were scalars we would simply divide both sides by the same constant. However, as \\(X'X\\) is a matrix, division is not possible. Instead, we need to pre-multiply both sides by the inverse of \\(X'X\\): \\((X'X)^{-1}\\). Here’s the issue: the inverse of a matrix need not exist.\nGiven a square \\(k\\times k\\) matrix \\(A\\), its inverse exists if and only if \\(A\\) is non-singular. For \\(A\\) to be non-singular its rank must have full rank: \\(r(A)=k\\), the number of rows/columns. This means that all \\(k\\) columns/rows must be linearly independent. (See Material on Linear Algebra for a more detailed discussion of all these terms.)\nIn our application, \\(A=X'X\\) and\n\\[ r(X'X) = r(X) = colrank(X)\\leq k \\]\nTo insure that the inverse of \\(X'X\\) exists, \\(X\\) must have full column rank: all column vectors must be linearly independent. In practice, this means that no regressor can be a perfect linear combination of others. However, we have this from\nCLRM 4: \\(rank(X)=k\\)\nYou may know this assumption by another name: the absence of perfect colinearity between regressors.\n\nThe rank condition is the reason we exclude a base category when working with categorical variables.\nRecall, most linear regression models are specified with a constant. Thus, the first column of \\(X\\) is\n\\[ X_1 = \\begin{bmatrix}1 \\\\ 1 \\\\ \\vdots \\\\ 1\\end{bmatrix} \\] a \\(n\\times 1\\) vector vector of \\(1\\)’s, denoted here as \\(\\ell\\). Suppose you have a categorical - for example, gender in an individual level dataset - that splits the same in two. The categories are assumed to be exhaustive and mutually exclusive. If you create two dummy variables, one for each category,\n\\[ X_2 = \\begin{bmatrix}1 \\\\ \\vdots \\\\1\\\\0\\\\ \\vdots \\\\ 0\\end{bmatrix}\\qquad\\text{and}\\qquad X_3 = \\begin{bmatrix}0 \\\\ \\vdots \\\\0\\\\1\\\\ \\vdots \\\\ 1\\end{bmatrix} \\]\nit is evident that \\(X_2+X_3 = \\ell\\). (Here I have depicted the sample as sorted along these two categories.) If \\(X=[X_1\\;X_2\\;X_3]\\), then it is rank-deficient: \\(r(X) = 2&lt;3\\), since \\(X_3=X_1-X_2\\). Thus, we can only include two of these three regressors. We can even exclude the constant and have \\(X=[X_2\\;X_3]\\).\n\nIf \\(X\\) is full rank, then \\((X'X)^{-1}\\) exists and,\n\\[\n\\hat{\\beta} = (X'X)^{-1}X'Y\n\\]\nThis relatively simple expression is the solution to least squares minimization problem. Just think, it would take less than three lines of code to programme this. That is the power of knowing a little linear algebra.\nWe can write the same expression in terms of summations over unit-level observations,\n\\[\n\\hat{\\beta} = \\bigg(\\sum_{i=1}^nX_iX_i'\\bigg)^{-1}\\sum_{i=1}^nX_iY_i\n\\]\nNote, the change in position of the transpose: \\(X_i\\) is a column vector \\(\\Rightarrow\\) \\(X_i'X_i\\) is a scalar while \\(X_iX_i'\\) is a \\(k\\times k\\) matrix. To match the first expression, the term inside the parenthesis must be a \\(k\\times k\\) matrix. Similarly, \\(X'Y\\) is a \\(k\\times 1\\) vector, as is \\(X_iY_i\\).\n\n5.1 Univariate case\nUndergraduate textbooks all teach a very similar expression for the OLS estimator of a uni-variate regression model (with a constant); typically, something like,8\n\\[\nY_i = \\beta_1+\\beta_2X_{i2}+\\varepsilon_i\n\\] We know that the OLS estimators are give by,\n\\[\n\\begin{aligned}\n\\tilde{\\beta}_2 =& \\frac{\\sum(Y_i-\\bar{Y})(X_{i2}-\\bar{X}_2)}{\\sum_{i=1}^n(X_{i2}-\\bar{X}_2)^2} \\\\\n\\text{and}\\qquad \\tilde{\\beta}_1 =& \\bar{Y}-\\tilde{\\beta_2}\\bar{X}_2\n\\end{aligned}\n\\]\nI am deliberately using the notation \\(\\tilde{\\beta}\\) to distinguish these two estimators from the expression below. Let us see if we can replicate this result, using vector notation. The the model is,\n\\[\n\\begin{aligned}\nY =& X\\beta+\\varepsilon \\\\\n=& \\begin{bmatrix}1&X_{12} \\\\ 1 & X_{22} \\\\ \\vdots & \\vdots \\\\ 1 & X_{n2}\\end{bmatrix}\\begin{bmatrix}\\beta_1 \\\\ \\beta_2 \\end{bmatrix} + \\varepsilon \\\\\n=& \\begin{bmatrix}\\ell &X_{2} \\end{bmatrix}\\begin{bmatrix}\\beta_1 \\\\ \\beta_2 \\end{bmatrix} + \\varepsilon\n\\end{aligned}\n\\]\nTherefore,\n\\[\n\\begin{aligned}\n\\hat{\\beta} = \\begin{bmatrix}\\hat{\\beta}_1 \\\\ \\hat{\\beta}_2\\end{bmatrix}=&(X'X)^{-1}X'Y \\\\\n=&\\bigg(\\begin{bmatrix}\\ell' \\\\ X_{2}' \\end{bmatrix}\\begin{bmatrix}\\ell &X_{2}\\end{bmatrix}\\bigg)^{-1}\\begin{bmatrix}\\ell'  \\\\ X_{2}' \\end{bmatrix}Y \\\\\n=&\\begin{bmatrix}\\ell'\\ell & \\ell'X_2 \\\\ X_{2}'\\ell & X_2'X_2 \\end{bmatrix}^{-1}\\begin{bmatrix}\\ell'Y  \\\\ X_{2}'Y \\end{bmatrix}\n\\end{aligned}\n\\]\nI went through this rather quickly, using a number of linear algebra rules that you may not be familiar with. Do not worry, the point of the exercise is not become a linear algebra master, but instead to focus on the element of each of each matrix/vector. Each element is a scalar (size \\(1\\times 1\\)).\nIf we right them each down as sums you they might be a little more familiar. First consider the \\(2\\times 2\\) matrix:\n\nelement [1,1]: \\(\\ell'\\ell = \\sum_{i=1}^n 1 = n\\)\nelement [1,2]: \\(\\ell'X_2 = \\sum_{i=1}^nX_{i2} = n\\bar{X}_2\\)\nelement [2,1]: \\(X_2'\\ell = \\sum_{i=1}^nX_{i2} = n\\bar{X}_2\\) (as above, since scalars are symmetric)\nelement [2,2]: \\(X_2'X_2=\\sum_{i=1}^nX_{i2}^2\\)\n\nNext, consider the final \\(2\\times 1\\) vector,\n\nelement [1,1]: \\(\\ell'Y = \\sum_{i=1}^n Y_i = n\\bar{Y}\\)\nelement [2,1]: \\(X_2'Y = \\sum_{i=1}^nY_iX_{i2}\\)\n\nOur OLS estimator is therefore,\n\\[\n\\hat{\\beta} = \\begin{bmatrix} n & n\\bar{X}_2 \\\\ n\\bar{X}_2 & \\sum_{i=1}^nX_{i2}^2 \\end{bmatrix}^{-1}\\begin{bmatrix}n\\bar{Y}  \\\\ \\sum_{i=1}^nY_iX_{i2} \\end{bmatrix}\n\\]\nWe now need to solve for the inverse of the \\(2\\times 2\\) matrix. You can easily find notes on how to do this online. Here, I will just provide the solution.\n\\[\n\\hat{\\beta} = \\frac{1}{n\\sum_{i=1}^nX_{i2}^2-n^2\\bar{X}_2^2}\\begin{bmatrix} \\sum_{i=1}^nX_{i2}^2 & -n\\bar{X}_2 \\\\ -n\\bar{X}_2 &  n\\end{bmatrix}\\begin{bmatrix}n\\bar{Y}  \\\\ \\sum_{i=1}^nY_iX_{i2} \\end{bmatrix}\n\\]\nRemember, this is still a \\(2\\times 1\\) vector. We can now solve for the final solution:\n\\[\n\\begin{aligned}\n\\hat{\\beta} =& \\frac{1}{n\\sum_{i=1}^nX_{i2}^2-n^2\\bar{X}_2^2}\\begin{bmatrix} n\\bar{Y}\\sum_{i=1}^nX_{i2}^2 -n\\bar{X}_2\\sum_{i=1}^nY_iX_{i2} \\\\ n\\sum_{i=1}^nY_iX_{i2}-n^2\\bar{X}_2\\bar{Y}\\end{bmatrix} \\\\\n=& \\frac{1}{n\\sum_{i=1}^n(X_{i2}-\\bar{X}_2)^2}\\begin{bmatrix} n\\bar{Y}\\sum_{i=1}^nX_{i2}^2 + n^2\\bar{Y}\\bar{X}^2 - n^2\\bar{Y}\\bar{X}^2 -n\\bar{X}_2\\sum_{i=1}^nY_iX_{i2} \\\\ n\\sum_{i=1}^n(Y_i-\\bar{Y})(X_{i2}-\\bar{X}_2) \\end{bmatrix} \\\\\n=& \\frac{1}{n\\sum_{i=1}^n(X_{i2}-\\bar{X}_2)^2}\\begin{bmatrix} n\\bar{Y}\\sum_{i=1}^n(X_{i2}-\\bar{X})^2 -n\\bar{X}_2\\sum_{i=1}^n(Y_i-\\bar{Y})(X_{i2}-\\bar{X}_2) \\\\ n\\sum_{i=1}^n(Y_i-\\bar{Y})(X_{i2}-\\bar{X}_2) \\end{bmatrix} \\\\\n=& \\begin{bmatrix} \\bar{Y}  -\\frac{n\\sum_{i=1}^n(Y_i-\\bar{Y})(X_{i2}-\\bar{X}_2)}{n\\sum_{i=1}^n(X_{i2}-\\bar{X}_2)^2}\\bar{X}_2 \\\\ \\frac{n\\sum_{i=1}^n(Y_i-\\bar{Y})(X_{i2}-\\bar{X}_2)}{n\\sum_{i=1}^n(X_{i2}-\\bar{X}_2)^2} \\end{bmatrix} \\\\\n=& \\begin{bmatrix} \\bar{Y}  -\\tilde{\\beta}_2\\bar{X}_2 \\\\ \\tilde{\\beta}_2 \\end{bmatrix} \\\\\n=& \\begin{bmatrix}\\tilde{\\beta}_1 \\\\ \\tilde{\\beta}_2 \\end{bmatrix}\n\\end{aligned}\n\\]\nThe math is a little involved, but it shows you these solutions are are the same. Unfortunately, the working gets even more arduous in a multivariate context. However, there are useful tools to help us with that we will discuss next.\n\n\n5.2 Geometry of OLS\nIn the last section we saw how the OLS estimator can, more generally, be described as a linear transformation of the \\(Y\\) vector.\n\\[\n\\hat{\\beta} = (X'X)^{-1}X'Y\n\\]\nWe also saw that in order for there to be a (unique) solution to the least squared problem, the \\(X\\) matrix must be full rank. This rules out any perfect colinearity between columns (i.e. regressors) in the \\(X\\) matrix, including the constant.\nGiven the vector of OLS coefficients, we can also estimate the residual,\n\\[\n\\begin{aligned}\n\\hat{\\varepsilon} =& Y - X\\hat{\\beta} \\\\\n=&Y-X(X'X)^{-1}X'Y \\\\\n=&(I_n-X(X'X)X^{-1})Y\n\\end{aligned}\n\\]\nby plugging the definition of \\(\\hat{\\beta}\\). Thus, the OLS estimator separates the vector \\(Y\\) into two components:\n\\[\n\\begin{aligned}\nY =& X\\hat{\\beta} + \\hat{\\varepsilon} \\\\\n=&\\underbrace{X(X'X)^{-1}X'}_{P_X}Y + (\\underbrace{I_n-X(X'X)X^{-1}}_{I_n-P_X = M_X})Y \\\\\n=&P_XY + M_XY\n\\end{aligned}\n\\]\nThe matrix \\(P_X = X(X'X)^{-1}X'\\) is a \\(n\\times n\\) projection matrix. It is a linear transformation that projects any vector into the span of \\(X\\): \\(S(X)\\subset\\mathbb{R}^n\\). (See for more information on these terms.) \\(S(X)\\) is the vector space spanned by the columns of \\(X\\). The dimensions of this vector space depends on the rank of \\(P_X\\),\n\\[\ndim(S(X)) = r(P_X) = r(X) = k\n\\]\nThe matrix \\(M_X = I_n-X(X'X)^{-1}X'\\) is also a \\(n\\times n\\) projection matrix. It projects any vector into \\(X\\)’s orthogonal span: \\(S^{\\perp}(X)\\). Any vector \\(z\\in S^{\\perp}(X)\\) is orthogonal to \\(X\\). This includes the estimated residual, which is by definition orthogonal to the predicted values and, indeed, any column of \\(X\\) (i.e. any regressor). The dimension of this orthogonal vector space depends on the rank of \\(M_X\\),\n\\[\ndim(S^{\\perp}(X)) = r(M_X) = r(I_n)-r(X) = n-k\n\\]\nThe orthogonality of these two projections can be easily shown, since projection matrices are idempotent (\\(P_XP_X = P_X\\)) and symmetric (\\(P_X' = P_X\\)). Consider the inner product of these two projections,\n\\[\nP_X'M_X = P_X(I_n-P_X) = P_X-P_XP_X = P_X-P_X = 0\n\\]\nThe least squares estimator is a projection of Y into two vector spaces: one the span of the columns of \\(X\\) and the other a space orthogonal to \\(X\\).\nWhy is this useful? Well, it helps us understand the “mechanics” (technically geometry) of OLS. When working with linear regression models, we typically assume either strict exogeneity - \\(E[\\varepsilon|X]=0\\) - or uncorrelatedness - \\(E[X'\\varepsilon]=0\\) - where the former implies the latter (but not the other way around).\nWhen we use OLS, we estimate the vector \\(\\hat{\\beta}\\) such that,\n\\[\nX'(Y-X\\hat{\\beta})=X'\\hat{\\varepsilon}=0 \\quad always\n\\]\nThis is true, not just in expectation, but by definition. The relationship is “mechanical”: the regressors and estimated residual are perfectly uncorrelated. This can be easily shown:\n\\[\n\\begin{aligned}\nX'\\hat{\\varepsilon} =& X'M_XY \\\\\n=& X'(I_n-P_X)Y \\\\\n=&X'I_nY-X'X(X'X)^{-1}X'Y \\\\\n=&X'Y-X'Y \\\\\n=&0\n\\end{aligned}\n\\]\nYou are essentially imposing the assumption of uncorrelatedness between the explained and unexplained components of Y on the data. This means that if the assumption is wrong, so is the projection.\n\n\n5.3 Partitioned regression\nThe tools of linear algebra can help us better understand partitioned regression. Indeed, I would go as far to to say that it is quite difficult to understand partitioned regression without an understanding of projection matrices. Moreover, we need to understand partitioned regression to really understand multivariate regression. The partitioned regression result is referred to as Frisch-Waugh-Lovell Theorem (FWL).\n\nTheorem 1 FWL says that if you have two sets of regressors, \\([X_1,X_2]\\), then \\(\\hat{\\beta}_1\\), the OLS estimator for \\(\\beta_1\\), from the regression,\n\\[\n        Y = X_1\\beta_1 + X_2\\beta_2 + \\varepsilon\n\\]\nis also given by the regression,\n\\[\n        M_2Y = M_2X_1\\beta_1 + \\xi\n\\]\n\nWe can demonstrate the FWL theorem result using projection matrices. Two simplify matters, we will divide the set of regressors into two groups: \\(X_1\\) a single regressor and \\(X_2\\) a \\(n\\times (k-1)\\) matrix. We can rewrite the linear model as,\n\\[\nY = X\\beta+ \\varepsilon =\\beta_1X_1+X_2\\beta_2+\\varepsilon\n\\]\nLet us begin by applying our existing knowledge. From above, we know that the residual from the regression of \\(X_1\\) on \\(X_2\\) is,\n\\[\n\\hat{\\upsilon} = M_2X_1\n\\]\nwhere \\(M_2 = I_n-X_2(X_2'X_2)^{-1}X_2'\\). It turns out, it does not matter if we residualize \\(Y\\) too. Can you see why? Thus, the model we estimate in the second step, is\n\\[\nY = \\gamma_1\\underbrace{M_2X_1}_{\\hat{\\upsilon}}+\\xi\n\\]\nWe know that \\(\\hat{\\gamma}_1 = (\\hat{\\upsilon}'\\hat{\\upsilon})^{-1}\\hat{\\upsilon}'Y\\). Replacing the value of the residual, we get,\n\\[\n\\begin{aligned}\n\\hat{\\gamma}_1 =& (\\hat{\\upsilon}'\\hat{\\upsilon})^{-1}\\hat{\\upsilon}'Y \\\\\n=&(X_1'M_2M_2X_1)^{-1}X_1'M_2Y \\\\\n=&(X_1'M_2X_1)^{-1}X_1'M_2Y\n\\end{aligned}\n\\] Here, we use both the symmetric and idempotent qualities of \\(M_2\\). Next we want to show that \\(\\hat{\\beta}_1\\) is given by the same value. This part is more complicated. Let’s start with by reminding ourselves of the following:\n\\[\n\\begin{aligned}\nX'X\\hat{\\beta} &= X'Y \\\\\n\\begin{bmatrix}X_1 & X_2\\end{bmatrix}'\\begin{bmatrix}X_1 & X_2\\end{bmatrix}\\begin{bmatrix}\\hat{\\beta}_1 \\\\ \\hat{\\beta}_2\\end{bmatrix} &= \\begin{bmatrix}X_1 & X_2\\end{bmatrix}'Y \\\\\n\\begin{bmatrix}X_1'X_1 & X_1'X_2 \\\\ X_2'X_1 & X_2'X_2 \\end{bmatrix}\\begin{bmatrix}\\hat{\\beta}_1 \\\\ \\hat{\\beta}_2\\end{bmatrix} &= \\begin{bmatrix}X_1'Y \\\\ X_2'Y\\end{bmatrix}\n\\end{aligned}\n\\]\nWe could solve for \\(\\hat{\\beta}_1\\) by solving for the inverse of \\(X'X\\); however, this will take a long time. An easier approach is to simply verify that \\(\\hat{\\beta}_1=(X_1'M_2X_1)^{-1}X_1'M_2Y\\). Recall, \\(\\hat{\\beta}\\) splits \\(Y\\) into two components:\n\\[\nY = \\hat{\\beta}_1X_1+X_2\\hat{\\beta}_2 + \\hat{\\varepsilon}\n\\]\nIf we plug this definition of \\(Y\\) into the above expression we get,\n\\[\n\\begin{aligned}\n&(X_1'M_2X_1)^{-1}X_1'M_2(\\hat{\\beta}_1X_1+X_2\\hat{\\beta}_2 + \\hat{\\varepsilon}) \\\\\n=&\\hat{\\beta}_1\\underbrace{(X_1'M_2X_1)^{-1}X_1'M_2X_1}_{=I_n} \\\\\n&+\\underbrace{(X_1'M_2X_1)^{-1}X_1'M_2X_2\\hat{\\beta_2}}_{=0} \\\\\n&+\\underbrace{(X_1'M_2X_1)^{-1}X_1'M_2\\hat{\\varepsilon}}_{=0} \\\\\n=&\\hat{\\beta}_1\n\\end{aligned}\n\\]\nIn line 2, I use the fact that \\(\\hat{\\beta}_1\\) is a scalar and can be moved to the front (since the order of multiplication does not matter with a scalar). In line 3, I use the fact that \\(M_2X_2=0\\) by definition. Line 4 uses the fact that \\(M_2\\hat{\\varepsilon}=\\hat{\\varepsilon}\\) which means that \\(X_1'M_2\\hat{\\varepsilon}=X_1'\\hat{\\varepsilon}=0\\).\nThe OLS estimator solves for \\(\\beta_1\\) using the variance in \\(X_1\\) that is orthogonal to \\(X_2\\). This is the manner in which we “hold \\(X_2\\) constant”: the variation in \\(M_2X_1\\) is orthogonal to \\(X_2\\). Changes in \\(M_2X_1\\) are uncorrelated with changes in \\(X_2\\); as if the variation in \\(M_2X_1\\) arose independently of \\(X_2\\). However, uncorrelatedness does NOT imply independence."
  },
  {
    "objectID": "handout-1.html#footnotes",
    "href": "handout-1.html#footnotes",
    "title": "Classical Linear Regression Model & Ordinary Least Squares",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMy notation assumes that \\(X_i\\) is a column vector, which makes \\(X_i'\\beta\\) a scalar. Wooldridge (2010) uses the notation \\(X_i\\beta\\), implying that \\(X_i\\) is a row vector. This is a matter of preference.↩︎\nYou might also refer to the vector of regressors or explanatory variables. The terms covariates or control variables are more common in Microeconometrics literature, where regressors are typically included for identification of causal effects. Some texts will use the term independent variables, but this name implies a specific relationship between \\(Y\\) and \\(X\\) that need not hold. Note, we will asssume in this term that \\(n&gt;k\\); i.e. this is “small” data.↩︎\nThis is NOT the residual.↩︎\nSee extra material on Linear Algebra to read more on rank.↩︎\nFor \\(n\\) large, \\(rank(E[X_iX_i'])=k\\Rightarrow rank(X)=k\\). This follows from Law of Large Numbers, since \\(plim(n^{-1}X'X) = E[X_iX_i']\\).↩︎\nThere are many such failures of (parametric) identification in models that include dummy variables (or fixed effects). Earlier we saw that the intercept is not separately identified from the mean of the error term. Mean independence of the error term, \\(E[\\varepsilon_i]=0\\), is required for us to separately ‘identify’ \\(\\beta_1\\).↩︎\nWhen working with vectors and matrices it is important to keep track of their size. You can only multiply two matrices/vectors if their column and row dimensions match. For example, if \\(A\\) and \\(B\\) are both \\(n\\times k\\) matrices (\\(n\\neq k\\)), then \\(AB\\) is not defined since \\(A\\) has \\(k\\) columns and \\(B\\) \\(n\\) rows. For the same reason \\(BA\\) is also not defined. However, you can pre-multiply \\(B\\) with \\(A'\\) as \\(A'\\) is a \\(k\\times n\\) matrix: \\(A'B\\) is therefore a \\((k\\times n)\\cdot (n\\times k)=k\\times k\\) matrix. Similarly, \\(B'A\\) is defined, but is a \\(n\\times n\\) matrix.\nOrder matters when working with matrices and vectors. Pre-multiplication and post-multiplication are not the same thing.\nKeep track of the size of each term to ensure they correspond to one another. In this instance, each term should be a scalar. For example, \\(-2b'X'Y\\) is the multiplication of a scalar (\\(-2\\): size \\(1\\times 1\\)), row vector (\\(b'\\): size \\(1\\times k\\)), matrix (\\(X'\\): size \\(k\\times n\\)), and column vector (\\(Y\\): size \\(n\\times 1\\)). Thus we have a \\((1\\times 1)\\cdot (1\\times k)\\cdot (k\\times n)\\cdot (n\\times 1)=1\\times 1\\).↩︎\nOnce you are familiar with vector notation, it is relatively easy to tell whether a model is uni- or multi-variate. This is because the notation \\(\\beta_2 X_{i2}\\) is not consistent with \\(X_{2i}\\) being a vector (row or column).\nIf \\(X_{i2}\\) is a \\(k\\times 1\\) vector then so is \\(\\beta_2\\). Thus, \\(\\beta_2 X_{i2}\\) is \\((k\\times 1)\\cdot (k\\times1)\\), which is not defined.\nIf \\(X_{i2}\\) is a row vector (as in Wooldridge, 2011), \\(\\beta_2 X_{i2}\\) will then be \\((k\\times 1)\\cdot (1\\times k)\\), a \\(k\\times k\\) matrix. This cannot be correct since the model is defined at the unit level.\nThus, if you see a model written with the parameter in front of the regressor, you know that this must be a single regressor. This is subtle, yet imporant, distinction that researchers often use to convey the structure of their model. Whenever \\(X_{i2}\\) is a vector, researchers will almost always use the notation \\(X_{i2}'\\beta\\) or \\(X_{i2}\\beta\\), depending on whether \\(X_{i2}\\) is assumed to be a column or row vector.↩︎"
  },
  {
    "objectID": "handout-2.html",
    "href": "handout-2.html",
    "title": "Estimators and their Properties",
    "section": "",
    "text": "Code\nlibrary(ggplot2)\nlibrary(latex2exp)"
  },
  {
    "objectID": "handout-2.html#overview",
    "href": "handout-2.html#overview",
    "title": "Estimators and their Properties",
    "section": "1 Overview",
    "text": "1 Overview\nIn this handout we will investigate the desirable properties of an estimator. Further reading can be found in:\n\nAppendix A of Cameron and Trivedi (2005)\nSection 2.6 of Verbeek (2017)"
  },
  {
    "objectID": "handout-2.html#an-estimator",
    "href": "handout-2.html#an-estimator",
    "title": "Estimators and their Properties",
    "section": "2 An estimator",
    "text": "2 An estimator\nAn estimator is a rule that tells you how to get your estimate given the observed data. As it is a that it is a function of random variables, an estimator is a random variable itself.\nWithout further information, we do not know anything about the distribution of this random variable and, in practice, will typically have to estimate the parameters of this distribution: mean, variance, skewness.\nThe assumed model (or data generating process) may pin down certain parameters or distributions. For example, under assumpions CLRM 5 & 6 the ordinary least squares estimator for \\(\\beta\\) will be normally distributed."
  },
  {
    "objectID": "handout-2.html#properties-of-estimators",
    "href": "handout-2.html#properties-of-estimators",
    "title": "Estimators and their Properties",
    "section": "3 Properties of estimators",
    "text": "3 Properties of estimators\nWhen evaluating an estimator you want to consider:\n\nbias: does the estimator yield estimates that ‘hit the right target’ on average?\nefficiency: do the estimates generated by the estimator have a limited dispersion/variance?\ndistribution: do you know the exact/approximate distribution of the estimator with which you can construct a valid hypothesis test?\n\nIt is important to consider both the small sample and large sample (asymptotic) properties of an estimator.\nUnbiasedness (property 1) is typically emphasized over properties 2 and 3. Afterall, a precise estimator of the wrong target is not particularly useful. That said, the expected value of an estimator is not always well defined.\n\n\n\n\n\n\nImportant\n\n\n\nThese are properties of the estimator, a random variable, and not the estimate. The estimate is a just a constant: a non-random realization of the estimator.\n\n\n\n3.1 Small vs large sample properties\nSometimes it is easier to study the large-sample, or asymptotic, properties of an estimator. That is, the properties of the estimator as the sample size gets large: \\(n\\rightarrow \\infty\\).\nSmall-sample properties include:\n\nBiasedness\nEfficiency/variance\nFinite-sample distribution (which might not be known)\n\nLarge-sample properties include:\n\nConsistency\nAsymptotic distribution\n\nAn estimator can have an asymptotic variance (efficiency); although, in EC910 will mostly discuss estimators with variances that shrink to zero as \\(n\\rightarrow \\infty\\).\n\n\n\n\n\n\nImportant\n\n\n\nThe phrase “finite-sample” is also used in other contexts. For example, in the Causal Inference (or Treatment Effects) literature “finite-sample” is used to describe estimands. This literature distinguishes between finite-sample and super-population estimands. The former relate to settings where the sample is treated as fixed, while the latter to settings where the sample is take as a draw from an unknown super population.\nThe estimator for a super-population estimand will have both finite and asymptotic properities.\n\n\n\n\n3.2 Bias\nThe estimator \\(\\hat{\\theta}\\) is unbiased for \\(\\theta\\) if,\n\\[\nE[\\hat{\\theta}] = \\theta\n\\] \\(\\hat{\\theta}\\) is a function of sample size, \\(n\\), while \\(\\theta\\) is not. Some texts will use the notation \\(\\hat{\\theta}_n\\) to emphasize this point.\n\n\nCode\nx_values &lt;- seq(-2, 3, length.out = 100)\nggplot(data.frame(x = x_values), aes(x = x)) +\n  stat_function(fun = dnorm, args = list(mean = 1, sd = 1), color = \"red\") + \n  stat_function(fun = dnorm, args = list(mean = 0, sd = .5), color = \"blue\") + \n  geom_vline(xintercept = 1, linetype = \"dashed\", color = \"red\", linewidth = 1) +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"blue\", linewidth = 1) + \n  labs(\n    title = NULL,\n    x = NULL,\n    y = \"Density\"\n  ) +\n  scale_x_continuous(\n    breaks = c(0, 1),            \n    labels = c(TeX(\"$\\\\theta$\"), TeX(\"$E[\\\\hat{\\\\theta}]$\"))  \n  ) +\n  theme_minimal() +\n  annotate(\"text\", x = 1.75, y = 0.35, label = \"Biased\", vjust = -1, hjust = 0.5, size = 5, color = \"red\") +\n  annotate(\"text\", x = -0.75, y = 0.6, label = \"Unbiased\", vjust = -1, hjust = 0.5, size = 5, color = \"blue\")\n\n\n\n\n\n\n\n\n\n\n\n3.3 Efficiency\nEfficiency is a relative concept. The estimator \\(\\hat{\\theta}\\) is more efficient the estimator \\(\\tilde{\\theta}\\), if both are unbiased for \\(\\theta\\) and\n\\[\nVar(\\hat{\\theta})&lt;Var(\\tilde{\\theta})\n\\] What if the estimator is biased? We can use Mean Square Error:\n\nDefinition 1 (MSE) \\(MSE = E\\big[(\\hat{\\theta}-\\theta)^2\\big] = \\text{bias}^2+\\text{variance}\\)\n\nNote, the above definition assumes that a finite variance.\n\nProof. \\[\n\\begin{aligned}\nE\\big[(\\hat{\\theta}-\\theta)^2\\big] &= E\\big[\\big((\\hat{\\theta}-E[\\hat{\\theta}])+(E[\\hat{\\theta}]-\\theta)\\big)^2\\big] \\\\\n&=E\\big[(\\hat{\\theta}-E[\\hat{\\theta}])^2\\big]+ E\\big[(\\theta-E[\\hat{\\theta}])^2\\big] + 2E\\big[(\\hat{\\theta}-E[\\hat{\\theta}])(\\theta-E[\\hat{\\theta}])\\big] \\\\\n&=Var(\\hat{\\theta}) + \\text{bias}^2\n\\end{aligned}\n\\]\n\nThe first line adds and subtracts the mean of the estimator, which need not be \\(\\theta\\), the targetted population parameter. The second expands the outside exponent over the two bracketted terms.\nThe cross-product term in line 2 is 0. Can you show this?\n\n\n3.4 Consistency\nConsistency is an asymptotic property of an estimator.\n\nDefinition 2 The estimator \\(\\hat{\\theta}_n\\) is a consistent estimator of \\(\\theta\\) if it converges in probability to \\(\\theta\\) as \\(n\\rightarrow\\infty\\),\n\\[\nPr(|\\hat{\\theta}_n-\\theta|\\geq\\varepsilon)\\rightarrow 0\\qquad n\\rightarrow \\infty\n\\] for \\(\\varepsilon\\) very small.1\n\nIntuitively, this means that tail area probabilities (i.e. probability of an estimator very far from the true value) goes to zero as the sample size gets large.\nWe use the notations, \\(p\\lim\\),\n\\[\np\\lim\\hat{\\theta}_n = \\theta\n\\]\nor convergence in probability,\n\\[\\hat{\\theta}_n \\rightarrow_p \\theta\\qquad\\text{as}\\qquad n\\rightarrow \\infty\\]\nAn important theorem regarding the consistency of estimators is Slutzky’s theorem concerning continuous functions of estimators. For example, we will use this theorem to prove the consistency of the OLS estimator.\n\nTheorem 1 (Slutzky’s Theorem) If \\(p\\lim\\hat{\\theta}_n = \\theta\\) and \\(h(\\cdot)\\) is a continuous function, then\n\\[\np\\lim\\;h(\\hat{\\theta}_n) = h\\big(p\\lim\\hat{\\theta}_n\\big)=h(\\theta)\n\\]\n\nHere are some useful examples,\n\nExample 1 Given two consistent estimators \\(\\big[\\hat{\\theta}_n,\\hat{\\beta}_n\\big]\\),\n\n\\(p\\lim\\;(a\\hat{\\theta}_n+ b\\hat{\\beta}_n) = a(p\\lim\\;\\hat{\\theta}_n)+b(p\\lim\\;\\hat{\\beta}_n) =a\\theta +b\\beta\\) for constants \\(a\\) and \\(b\\)\n\\(p\\lim\\;(\\hat{\\theta}_n\\times\\hat{\\beta}_n) = (p\\lim\\;\\hat{\\theta}_n)\\times (p\\lim\\;\\hat{\\beta}_n)=\\theta\\beta\\)\n\\(p\\lim\\;(\\hat{\\theta}_n^2) = (p\\lim\\;\\hat{\\theta}_n)^2 = \\theta^2\\)\n\\(p\\lim\\;\\bigg(\\frac{\\hat{\\theta}_n}{\\hat{\\beta}_n}\\bigg) = \\frac{\\theta}{\\beta}\\)\n\\(p\\lim\\;\\exp(\\hat{\\theta}_n) = \\exp(p\\lim\\;\\hat{\\theta}_n) = \\exp(\\theta)\\)\n\n\nMSE convergence (to zero) is a sufficient condition for consistency. However, it is not a necessary.\nFor an unbiased estimator,\n\\[\nVar(\\hat{\\theta}_n)\\rightarrow 0 \\quad \\text{as}\\quad n\\rightarrow \\infty \\implies p\\lim\\hat{\\theta}_n = \\theta\n\\]\nFor a biased estimator,\n\\[\nMSE(\\hat{\\theta}_n)\\rightarrow 0 \\quad \\text{as}\\quad n\\rightarrow \\infty \\implies p\\lim\\hat{\\theta}_n = \\theta\n\\]\n\nExercise 1 Given a sample of independently and identically distributed (iid) random variables, \\[\nX_1,...,X_n \\sim N(\\mu,\\sigma^2)\n\\] Show that the mean estimator - \\(\\overline{X} = \\frac{1}{n}\\sum_{i=1}X_i\\) is a consistent estimator of \\(\\mu\\); i.e. \\(p \\lim\\;\\overline{X} = \\mu\\).\n\nThe above exercise relates to one of the most important examples of convergence in probability:\n\nTheorem 2 (Weak Law of Large Numbers) Let \\(X_1,...,X_n\\) be a sample of iid random variables, such that \\(E[|X_1|]&lt;\\infty\\). Then, \\[\nn^{-1}\\sum_{i=1}^{n}X_i \\rightarrow_p E[X_1]\\qquad \\text{as}\\qquad n\\rightarrow \\infty\n\\]\n\nHere we use the notation \\(E[X_1]\\), since the data is iid and \\(E[X_i]=E[X_1]\\) for \\(i=1,...,n\\). We will prove a modified version of the WLLN theorem, assuming \\(E[X_1^2]&lt;\\infty\\). Since \\(E[X_1^2]&lt;\\infty\\implies\\) both \\(E[|X_1|]&lt;\\infty\\) and \\(Var(X_1)&lt;\\infty\\), we will have proven WLLN theorem.\n\nTheorem 3 Let \\(X_1,...,X_n\\) be a sample of iid random variables, such that \\(Var(X_1)&lt;\\infty\\). Then, \\[\nn^{-1}\\sum_{i=1}^{n}X_i \\rightarrow_p E[X_1]\\qquad \\text{as}\\qquad n\\rightarrow \\infty\n\\]\n\nTo complete the proof of WLLN, we will need to use Markov’s Inequality. We will not prove this lemma, but versions of the proof are readily available online.\n\nLemma 1 (Markov’s Inequality) Let \\(X\\) be a random variable. For \\(\\varepsilon&gt;0\\) and \\(r&gt;0\\), then\n\\[\nPr(|X|\\geq \\varepsilon)\\leq \\frac{E\\big[|X|^r\\big]}{\\varepsilon^r}\n\\]\n\nNow, we can complete the proof of WLLN, assuming a finite second moment.\n\nProof. \\[\n\\begin{aligned}\nPr\\bigg(\\bigg|n^{-1}\\sum_{i=1}^{n}X_i - E[X_1]\\bigg|\\geq \\varepsilon\\bigg) &= Pr\\bigg(\\bigg|n^{-1}\\sum_{i=1}^{n}(X_i - E[X_1])\\bigg|\\geq \\varepsilon\\bigg) \\\\\n&\\leq \\frac{E\\big[|\\sum_{i=1}^{n}(X_i - E[X_1])|^2\\big]}{n^2\\varepsilon^2} \\\\\n&=\\frac{\\sum_{i=1}^{n}\\sum_{j=1}^{n}E\\big[(X_i - E[X_1])(X_j - E[X_1])\\big]}{n^2\\varepsilon^2} \\\\\n&=\\frac{\\sum_{i=1}^{n}E\\big[(X_i - E[X_1])^2\\big]}{n^2\\varepsilon^2} \\\\\n&=\\frac{nVar(X_1)}{n^2\\varepsilon^2} \\\\\n&\\rightarrow 0 \\qquad \\text{as}\\qquad n\\rightarrow \\infty\n\\end{aligned}\n\\]\n\nNote, the WLLN holds under a weaker condition than iid. Between lines 3 and 4 we used the independence of observations to set correlations between units to 0. Thus, we required a weaker assumption of uncorrelateness: \\(Cov(X_i,X_j)=0\\;\\forall\\;i\\neq j\\).\nUnder the WLLN, you can show that the sample variance converges in probability to the population variance.\n\\[\np\\lim \\big(n^{-1}\\sum_{i=1}^{n}(X_i - \\overline{X})^2\\big) \\rightarrow_p Var(X_1)\n\\]\n\n\n3.5 Distribution\nIn this section we will focus on the asymptotic distribution of an estimator. If we know the joint distribution of the data, then we can potentially work out the distribution of the estimator in a finite sample. This is especially true when the random variables in question are drawn from known families of distributions.\nFor example, we know that the sum of Normal distributed random variables is Normally distributed itself. And the sum of the square of standard-Normally distributed random variables is Chi-squared distributed. We used these results do determine the distribution of test-statistics corresponding to the Classical Linear Regression Model.2\nHowever, what do you do when you do not know the underlying distribution of the random variables? Here we will rely on a the Central Limit Theorem, which tells us about the approximate distribution of an estimator when the sample is large.\nBefore discussing CLT, we must define a new convergence concept: convergence in distribution.\n\nDefinition 3 Let \\(X_1,...,X_n\\) be a sequence of random variables and let \\(F_n(x)\\) denote the marginal CDF of \\(X_n\\), \\[\nF_n(x) = Pr(X_n\\leq x)\n\\] Then, \\(X_n\\) converges in distribution to \\(X\\) if \\(F_n(x)\\rightarrow F(x)\\) as \\(n\\rightarrow \\infty\\), where \\(F(x)\\) is continuous.\n\nConvergence in distribution can be denoted,\n\\[\nX_n\\rightarrow_d X\n\\]\nwhere X is a random variable with distribution function \\(F(x)\\). Note, it is not the random variables that are converging, but rather the distributions of said random variables.\nAs with convergence in probability, there are some basic rules for manipulation.\n\nThe first is Cramer Convergence Theorem: Suppose \\(X_n\\rightarrow_dX\\) and \\(Y_n\\rightarrow_p c\\). then,\n\n\\(X_n+Y_n\\rightarrow_d X+c\\)\n\\(Y_nX_n\\rightarrow_d cX\\)\n\\(X_n/Y_n\\rightarrow_d X/c\\), for \\(c\\neq 0\\)\n\nIf \\(X_n\\rightarrow_p X\\), then \\(X_n\\rightarrow_d X\\). The converse is not true, with the exception:\nIf \\(X_n\\rightarrow_d C\\), a constant, then \\(X_n\\rightarrow_p C\\).\nIf \\(X_n-Y_n\\rightarrow_p 0\\), and \\(Y_n\\rightarrow_d Y\\), then \\(X_n\\rightarrow_d Y\\).\n\nUnlike with convergence in probability, \\(X_n\\rightarrow_d X\\) and \\(Y_n\\rightarrow_d Y\\) does NOT imply \\(X_n + Y_n \\rightarrow_d X+Y\\), unless joint convergences holds too. This is because the former are statements concerning the marginal distributions of \\(X_n\\) and \\(Y_n\\), while the distribution of \\(X_n + Y_n\\) depends on the joint distribution.\nIf \\((X_n,Y_n)\\rightarrow_d (X,Y)\\) (joint convergence), then \\(X_n+Y_n\\rightarrow_dX+Y\\). This result follows from the Central Mapping Theorem.\n\nTheorem 4 (Continuous Mapping Theorem) Suppose \\(X_n\\rightarrow_d X\\) and let \\(h(\\cdot)\\) be a continuous function. Then, \\(h(X_n)\\rightarrow_d h(X)\\).\n\nCMT holds for a vector of random variables as well as single random variable. Thus, if\n\\[\n\\begin{bmatrix}X_n \\\\ Y_n\\end{bmatrix}\\rightarrow_d \\begin{bmatrix}X \\\\ Y\\end{bmatrix}\n\\] then by CMT, \\[\nX_n+Y_n=\\begin{bmatrix}1 & 1\\end{bmatrix}\\begin{bmatrix}X_n \\\\ Y_n\\end{bmatrix} \\rightarrow_d\\begin{bmatrix}1 & 1\\end{bmatrix}\\begin{bmatrix}X \\\\ Y\\end{bmatrix} = X+Y\n\\]\nCan you show:\n\n\\((X_n,Y_n)\\rightarrow_d (X,Y)\\) (i.e. joint convergence) implies \\(X_n\\rightarrow_d X\\) (i.e. marginal convergence)?\n\\(\\sum_{j=1}^k X_{jn}^2 \\sim \\chi^2_k\\) for \\(X_n\\sim N(0,I_k)\\), \\(X_n\\in \\mathbf{R}^k\\).\n\nHaving discussed the CMT, we are now ready to discuss the Central Limit Theorem. Both are used extensively in Econometrics. We will not prove CLT as the proof requires a more detailed discussion of Moment Generating Functions.\nRecall, for an iid random sample, the sample average converges in probability to the population mean:\n\\[\np \\lim\\;\\overline{X} = E[X_1]\n\\]\nThe rate of convergence for this estimator is \\(\\sqrt{n}\\). The estimator is said to be root-\\(n\\)-consistent.\n\nTheorem 5 (Central Limit Theorem) Let \\(X_1,...,X_n\\) be a sample of iid random variables such that \\(E[X_1]=0\\) and \\(0&lt;E[X_1^2]&lt;\\infty\\). Then,\n\\[\nn^{-1/2}\\sum_{i=1}^{n}X_i\\rightarrow_d N(0,E[X_1^2])\n\\]\n\nConsider a random sample drawn independently from a distribution with mean \\(\\mu\\) and variance \\(\\sigma\\). Note, this distribution need not be Normal. It holds that, $X_1-,…,X_n-$ are iid and \\(E[X_1-\\mu]\\). In addition, \\(E[(X_1-\\mu)^2]=\\sigma^2&lt;\\infty\\). Therefore, by CLT\n\\[\nn^{1/2}(\\overline{X}-\\mu) = n^{-1/2}\\sum_{i=1}^{n}(X_i-\\mu)\\rightarrow_d N(0,\\sigma^2)\n\\]\nIn practice, we use CLT to determine the approximate distribution of an estimator in large samples. Based on the above result, we can say\n\\[\nn^{1/2}(\\overline{X}-\\mu)\\overset{a}{\\sim} N(0,\\sigma^2)\n\\] or,\n\\[\n\\overline{X}\\overset{a}{\\sim}N(\\mu,\\sigma^2/n)\n\\]\nwhere \\(\\sigma^2/n\\) is referred to as the asymptotic variance of \\(\\overline{X}\\). Here, the symbol \\(\\overset{a}{\\sim}\\) can be interpreted as “approximately in large samples”.\nWith CMT and CLT, we need one more theorem before we continue. We know from the WLLN that \\(\\overline{X}\\rightarrow_p E[X_1]=\\mu\\). Moreover, by Slutzky’s theorem we know that \\(h(\\overline{X})\\rightarrow_p h(\\mu)\\), for \\(h(\\cdot)\\) continuous. However, we do not know the approximate distribution of \\(h(\\overline{X})\\) in a large sample.\nConsider, \\(h(\\mu)\\) is a non-random constant and CLT applies to \\(n^{1/2}(\\overline{X}-\\mu)\\) and not \\(\\overline{X}\\). The latter implying that we cannot use CMT.\n\nTheorem 6 (Delta Method) Let \\(\\hat{\\theta}_n\\) be a random k-dimensional vector. Suppose that \\(n^{1/2}(\\hat{\\theta}_n-\\theta)\\rightarrow_d Y\\), where \\(\\theta\\) is a non-random k-dimensional vector and \\(Y\\) a random k-dimensional vector.\nLet \\(h: \\mathbf{R}^k\\rightarrow\\mathbf{R}^m\\) be a function (mapping) that is continuously differentiable on some open neighbourhood of \\(\\theta\\). Then,\n\\[\nn^{1/2}\\big(h(\\hat{\\theta}_n)-h(\\theta)\\big)\\rightarrow_d \\frac{\\partial h(\\theta)}{\\partial\\theta'}Y\n\\]\n\nThe proof involves Cramer’s Convergence Theorem, Slutzky’s Theorem, as well as the Mean Value Theorem (which we have not discussed). This result is used to derive the limiting distribution of non-linear models and their marginal effects (e.g. probit/logit), as well as non-linear tests of regression coefficients."
  },
  {
    "objectID": "handout-2.html#properties-of-the-expectation-operator",
    "href": "handout-2.html#properties-of-the-expectation-operator",
    "title": "Estimators and their Properties",
    "section": "4 Properties of the E[xpectation] operator",
    "text": "4 Properties of the E[xpectation] operator\n\nThe expectation of a continuously-distributed, random variable \\(X\\) can be defined as:\n\\[\nE[X] = \\int_{-\\infty}^{\\infty}tf_Xdt = \\int_{-\\infty}^{\\infty}tdF_X(t)\n\\]\nIf \\(X\\) takes on discrete values, \\(X \\in \\mathbf{X} = \\{x_1,x_2,...,x_m\\}\\), we can replace the integral with a summation and the probability density function (pdf: \\(f_X\\)) with a probability mass function (pmf: \\(p_X\\)).\n\\[\nE[X] = \\sum_{t\\in \\mathbf{X}}tp_X(t)\n\\]\nAn important property of the Exptation operator is that it is linear. Let \\(\\{a,b\\}\\) be two constants (non-random scalars), then\n\\[\nE[aX+b] = aE[X]+b\n\\]\nCan you show this, using the above definition of the expectation operator?\nSimilarly, consider two random varibales \\(X\\) and \\(Y\\). Then,\n\\[\nE[aX + bY] = E[aX] + E[bY] = aE[X] + bE[Y]\n\\]\nHowever, note\n\n\\(E[XY] = E[X] \\times E[Y]\\) if \\(X\\) and \\(Y\\) are independent\n\nThis follows from the fact that \\(f_{X,Y} = f_X\\cdot f_Y\\) if \\(X\\) and \\(Y\\) are independent. Note, this is not an iff (if-and-only-if) statement.\n\n\\(E\\left[\\frac{X}{Y}\\right]\\neq\\frac{E[X]}{E[Y]}\\) for \\(E[Y]\\neq0\\)\n\\(E[\\ln(X)]\\neq \\ln(E[Y])\\)\n\nIn general, \\(E[h(X)]\\neq h(E[Y])\\) with the exception of \\(h(\\cdot)\\) linear function."
  },
  {
    "objectID": "handout-2.html#footnotes",
    "href": "handout-2.html#footnotes",
    "title": "Estimators and their Properties",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n\\(|x-a|&gt;b\\; \\implies \\;-b&gt;x-a\\quad\\text{and}\\quad b&lt;x-a\\)↩︎\nIn this setting, we make an assumption around the distribution of the error term in the model.↩︎"
  },
  {
    "objectID": "handout-4.html",
    "href": "handout-4.html",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "In this handout we will see how to test linear hypotheses. These could be,\n\nsingle hypothesis concerning single parameter;\nsingle hypothesis concerning multiple parameters;\nmultiple hypothesis concerning more than one parameter.\n\nFurther reading can be found in:\n\nSection 7.1-7.4 of Cameron and Trivedi (2005)\nSection 6.2 & 6.3 of Verbeek (2017)"
  },
  {
    "objectID": "handout-4.html#overview",
    "href": "handout-4.html#overview",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "In this handout we will see how to test linear hypotheses. These could be,\n\nsingle hypothesis concerning single parameter;\nsingle hypothesis concerning multiple parameters;\nmultiple hypothesis concerning more than one parameter.\n\nFurther reading can be found in:\n\nSection 7.1-7.4 of Cameron and Trivedi (2005)\nSection 6.2 & 6.3 of Verbeek (2017)"
  },
  {
    "objectID": "handout-4.html#linear-hypotheses",
    "href": "handout-4.html#linear-hypotheses",
    "title": "Hypothesis Testing",
    "section": "2 Linear Hypotheses",
    "text": "2 Linear Hypotheses\nAssume that the underlying model is linear in parameters, \\[\n  Y = X\\beta + u\n\\] Consider the linear hypothesis, \\[\n  R\\beta = r\n\\] where \\(R\\) is an \\(q\\times k\\), non-random matrix and \\(r\\) a \\(q\\times 1\\) non-random vector of constants.\nFor example, consider the linear model from Problem Set 2, \\[\nY = \\beta_1 + \\beta_2X_2 + \\beta_3X_3 + \\beta_4X_4 + \\beta_5X_5 + u\n\\] we can express the single (\\(q=1\\)) linear hypothesis \\(H_0: \\beta_2 = \\beta_3 \\Rightarrow\\beta_2-\\beta_3 = 0\\) as \\[\nH_0: R\\beta = \\begin{bmatrix}0&1&-1&0&0\\end{bmatrix}\\begin{bmatrix}\\beta_1 \\\\ \\beta_2 \\\\ \\beta_3 \\\\ \\beta_4 \\\\ \\beta_5\\end{bmatrix} = 0\n\\] In the case of a single linear hypothesis, we can also use the notation \\(c'\\beta = r\\) where \\(c\\) is a \\(k\\times 1\\) non-random vector.\nWe can express the multiple (\\(q=2\\)) linear hypotheses from this exercise \\[\nH_0: \\beta_2 = \\beta_3\\quad\\text{and}\\quad\\beta_4 + \\beta_5 = 1\n\\] as, \\[\nR\\beta = \\begin{bmatrix}0&1&-1&0&0 \\\\ 0&0&0&1&1\\end{bmatrix}\\begin{bmatrix}\\beta_1 \\\\ \\beta_2 \\\\ \\beta_3 \\\\ \\beta_4 \\\\ \\beta_5\\end{bmatrix} = \\begin{bmatrix}0 \\\\1 \\end{bmatrix}\n\\]"
  },
  {
    "objectID": "handout-4.html#distribution-under-h_0",
    "href": "handout-4.html#distribution-under-h_0",
    "title": "Hypothesis Testing",
    "section": "3 Distribution under \\(H_0\\)",
    "text": "3 Distribution under \\(H_0\\)\nLet us assume the CLRM assumptions, including normality and homoskedasticity\n\nCLRM 5: \\(u|X \\sim N(0,\\sigma^2 I_n)\\)\n\nWe know that under these assumptions the OLS estimator is normally distributed,\n\\[\n\\hat{\\beta}|X \\sim N(\\beta,\\sigma^2 (X'X)^{-1})\n\\]\nThis implies that,1\n\\[\nR\\hat{\\beta}|X \\sim N(R\\beta,\\sigma^2 R(X'X)^{-1}R')\n\\]\nUnder the null hypothesis, \\(H_0: R\\beta = r\\),\n\\[\nR\\hat{\\beta}|X \\sim N(r,\\sigma^2 R(X'X)^{-1}R')\n\\] From Handout 3, we know the asymptotic distribution of \\(\\hat{\\beta}\\) is normal, even if the (conditional) finite sample distribution of \\(u\\) is not. Applying the Delta Method (see Handout 2) to this result,\n\\[\n  \\sqrt{n} (R\\hat{\\beta}_n-R\\beta)\\rightarrow_d N(0,\\sigma^2R(E[X_1X_1'])^{-1}R')\\qquad\\text{as}\\quad n\\rightarrow\\infty\n\\]\nWhich means that, under \\(H_0\\),\n\\[\n  \\sqrt{n} (R\\hat{\\beta}_n-r)\\rightarrow_d N(0,\\sigma^2R(E[X_1X_1'])^{-1}R')\\qquad\\text{as}\\quad n\\rightarrow\\infty\n\\]"
  },
  {
    "objectID": "handout-4.html#single-linear-hypothesis",
    "href": "handout-4.html#single-linear-hypothesis",
    "title": "Hypothesis Testing",
    "section": "4 Single linear hypothesis",
    "text": "4 Single linear hypothesis\nAssuming CLRM 5, we know that for any \\(k\\times 1\\) non-random vector \\(c\\),\n\\[\nc'\\hat{\\beta}|X \\sim N(c'\\beta,\\sigma^2 c'(X'X)^{-1}c)\n\\] Which then implies, \\[\n\\frac{c'\\hat{\\beta}-c'\\beta}{\\sqrt{\\sigma^2 c'(X'X)^{-1}c}}|X \\sim N(0,1)\n\\] Under \\(H_0: c'\\beta = r\\), \\[\n\\frac{c'\\hat{\\beta}-r}{\\sqrt{\\sigma^2 c'(X'X)^{-1}c}}|X \\overset{H_0}{\\sim} N(0,1)\n\\] Note, this statement is only true under \\(H_0\\), while the former is true always (under CLRM assumptions).\nWhen evaluating against the alternative hypothesis, \\(H_1: c'\\beta\\neq r\\), the valid two-sided rejection rule is given by,\n\nReject \\(H_0\\) if \\(|\\text{Z-stat}|&gt;z_{1-\\alpha/2}\\)\n\nwhere \\(\\alpha\\) is the chosen significance level (e.g., 1%, 5%, or 10%) and \\(z_{1-\\alpha/2}\\) the \\(1-\\alpha/2\\) percentile of the standard normal distribution. We can write the two sided rejection in this way given the symmetry of the normal distribution. In general, the rejection rule is,\n\nReject \\(H_0\\) if \\(\\text{Z-stat}&lt;z_{\\alpha/2}\\) or \\(\\text{Z-stat}&gt;z_{1-\\alpha/2}\\)\n\nHowever, given the symmetric of \\(N(0,1)\\), \\(z_{\\alpha/2}=-z_{1-\\alpha/2}\\):\n\\[\n\\begin{aligned}\n\\alpha =& Pr(Z&lt;z_{\\alpha/2})+Pr(Z&gt;z_{1-\\alpha/2}) \\\\\n=& Pr(Z&lt;-z_{1-\\alpha/2})+Pr(Z&gt;z_{1-\\alpha/2}) \\\\\n=& Pr(|Z|&gt;z_{1-\\alpha/2})\n\\end{aligned}\n\\] For asymmetric distributions, this is not the case, and you must write the two rejection cases separately.\nThis test is valid for the true \\(\\sigma^2\\), the homoskedastic variance of the error term. We typically do not know this variance and need to replace it with the estimator,\n\\[\ns^2 = \\frac{1}{n-k}\\sum_{i=1}^{n}\\hat{u}_i^2 = \\frac{\\hat{u}'\\hat{u}}{n-k}\n\\] where \\(\\hat{u}\\) is the residual from the linear model: \\(\\hat{u} = Y-X\\hat{\\beta}\\). This begs the question: what is the conditional distribution of,\n\\[\n\\text{T-stat} = \\frac{c'\\hat{\\beta}-r}{\\sqrt{s^2 c'(X'X)^{-1}c}}\n\\] under \\(H_0\\). You will know from undergraduate texts that it is \\(t\\)-distributed. We can demonstrate this result using the idempotent properties of projection matrices.\nFirst-off, substitue back in \\(\\sigma^2\\)\n\\[\n\\text{T-stat} = \\frac{c'\\hat{\\beta}-r}{\\sqrt{\\sigma^2 c'(X'X)^{-1}c}}\\times\\sqrt{\\frac{s^2}{\\sigma^2}}\n\\] The T-statistic can be rewritten as the Z-statistic (which has a standard normal distribution under \\(H_0\\)) multiplied by the square-root of the ratio of \\(s^2\\) and \\(\\sigma^2\\). Consider the distribution of this ratio:\n\\[\n  \\frac{s^2}{\\sigma^2} = \\frac{1}{n-k} \\frac{\\hat{u}'\\hat{u}}{\\sigma^2}\n\\] We know that \\(\\hat{u} = M_X Y = M_X u\\), where \\(M_X\\) is idempotent and symmetric. Therefore,\n\\[\n  \\frac{s^2}{\\sigma^2} = \\frac{1}{n-k} \\frac{u'M_Xu}{\\sigma^2}=\\frac{1}{n-k} \\bigg(\\frac{u}{\\sigma}\\bigg)'M_X\\bigg(\\frac{u}{\\sigma}\\bigg)\n\\] By the properties of symmetric matrices (see Linear Algebra notes), \\(M_X\\) has a eigenvector decomposition\n\\[\nM_X = C\\Lambda C' \\qquad\\text{where} \\quad CC' = C'C = I_n\n\\] Moreover, since \\(M_X\\) is idempotent, all eigenvalues are \\(\\lambda_j\\in\\{0,1\\}\\) for \\(j=1,...,n\\). This implies that,\n\\[\nrank(M_X) = tr(M_X) = \\sum_{j=1}^n\\lambda_j\n\\] We know that the \\(rank(M_X) = rank(I_n-P_X) = rank(I_n)-rank(P_X) = n-rank(X) = n-k\\). Therefore, \\(M_X\\) has \\(n-k\\) non-zero eigenvalues. Applying this eigenvector decomposition, we get that, \\[\n  \\frac{s^2}{\\sigma^2} = \\frac{1}{n-k} \\bigg(\\frac{u}{\\sigma}\\bigg)'C\\Lambda C'\\bigg(\\frac{u}{\\sigma}\\bigg) = \\frac{1}{n-k} \\bigg(\\frac{C'u}{\\sigma}\\bigg)'\\Lambda \\bigg(\\frac{C'u}{\\sigma}\\bigg)\n\\]\nSince \\(u|X\\sim N(0,\\sigma^2 I_n)\\),\n\\[\n\\frac{C'u}{\\sigma}\\sim N(0,\\underbrace{C'I_nC}_{=C'C=I_n})\n\\] Thus,\n\\[\n  \\frac{s^2}{\\sigma^2} =  \\frac{1}{n-k} Z'\\Lambda Z = \\frac{1}{n-k}\\sum_{j=1}^n \\lambda_jZ_j^2\n\\] Under the \\(H_0\\), the T-statistic’s distribution can be described by the ratio of two random variables: \\(Z\\sim N(0,1)\\) and \\(W\\sim\\chi^2_{n-k}\\).\n\\[\n\\text{T-stat} = \\frac{Z}{\\sqrt{W/(n-k)}} \\sim T_{n-k}\n\\] This distribution is known as the \\(t\\)-distribution, with \\(n-k\\) degrees of freedom (dof). Note, the degrees of freedom correspond to the rank of the \\(M_X\\) matrix, since this determines the number of squared standard normal distributions summed in the denominator.\nFor a two-sided test, we reject \\(H_0\\) when,\n\\[\n  |\\text{T-stat}| &gt;t_{n-k,1-\\alpha/2}\n\\] where \\(t_{n-k,1-\\alpha}\\) is the \\(1-\\alpha/2\\) percentile of the \\(t\\)-distribution (with \\(n-k\\) dof). As with the standard normal distribution, the \\(t\\)-distribution is symmetric.\n\n4.1 Asymptotic distribution\nWe will see that for \\(n\\) large, it does not make a difference that we do not know \\(\\sigma^2\\). The fact that \\(s^2\\rightarrow_p \\sigma^2\\) (as \\(n\\rightarrow\\infty\\)) means that the T-statistic will be normally distributed in the limit.\nTo demonstrate this result we just need to concern ourself with,\n\\[\n  \\frac{s^2}{\\sigma^2} =  \\frac{1}{n-k}\\sum_{j=1}^n \\lambda_jZ_j^2\\sim \\chi^2_{n-k}/(n-k)\n\\] The \\(E[\\sum_{j=1}^n \\lambda_jZ_j^2] = n-k\\), since \\(E[Z_1^2] = 1\\) for any standard normal random variable. Moreover, \\(E[(\\sum_{j=1}^n \\lambda_jZ_j^2)^2]=2(n-k)\\). Therefore, by the WLLN,\n\\[\n\\frac{1}{n-k}\\sum_{j=1}^n \\lambda_jZ_j^2\\rightarrow_p 1\n\\] As a result, under \\(H_0\\),\n\\[\n  \\text{T-stat}\\rightarrow_d N(0,1)\n\\]"
  },
  {
    "objectID": "handout-4.html#multiple-linear-hypotheses",
    "href": "handout-4.html#multiple-linear-hypotheses",
    "title": "Hypothesis Testing",
    "section": "5 Multiple Linear Hypotheses",
    "text": "5 Multiple Linear Hypotheses\nWe will follow similar steps to above. For any non-random \\(q\\times k\\) matrix \\(R\\),\n\\[\nR\\hat{\\beta}|X \\sim N(R\\beta,\\sigma^2 R(X'X)^{-1}R')\n\\] Under the CLRM assumptions, this statement is true always, while under \\(H_0: R\\beta = r\\),\n\\[\nR\\hat{\\beta}|X \\overset{H_0}{\\sim} N(r,\\sigma^2 R(X'X)^{-1}R')\n\\]\nTo construct a scalar test static for these \\(q\\) hypotheses we will first need to consider the distribution of,\n\\[\n(R\\hat{\\beta}-r)'\\big(\\sigma^2 R(X'X)^{-1}R'\\big)^{-1}(R\\hat{\\beta}-r)\\sim \\chi^2_{q}\n\\] We can prove this result as follows.\n\nProof. Suppose \\(U\\sim N(0,\\Omega)\\). Then, \\(\\Omega\\) is a positive-definite, symmetric variance-covariance matrix. By symmetry, this matrix has an eigenvector decomposition,\n\\[\n  \\Omega = C\\Lambda C' \\qquad\\text{where} \\quad CC' = C'C =I_n\n  \\] Given it’s positive definiteness, all eigenvalues in \\(\\lambda\\) are strictly positive. We can therefore define,\n\\[\n  \\Lambda^{1/2} = \\begin{bmatrix}\\lambda_1^{1/2} & 0 & \\cdots & 0 \\\\ 0 & \\lambda_2^{1/2} &  & \\\\\n  \\vdots & & \\ddots & \\\\ 0 & & & \\lambda_q^{1/2}\\end{bmatrix}\n  \\] Likewise, \\[\n  \\Lambda^{-1/2} = \\begin{bmatrix}\\lambda_1^{-1/2} & 0 & \\cdots & 0 \\\\ 0 & \\lambda_2^{-1/2} &  & \\\\\n  \\vdots & & \\ddots & \\\\ 0 & & & \\lambda_q^{-1/2}\\end{bmatrix}\n  \\] This allows us to define,\n\\[\n  \\Omega^{1/2} = C\\Lambda^{1/2}C' \\qquad \\text{since}\\quad C\\Lambda^{1/2}C'C\\Lambda^{1/2}C' = C\\Lambda C' = \\Omega\n\\] and \\[\n  \\Omega^{-1/2} = C\\Lambda^{-1/2}C' \\qquad \\text{since}\\quad C\\Lambda^{-1/2}C'C\\Lambda^{-1/2}C' = C\\Lambda C' = \\Omega^{-1}\n\\] This allows us to standardize the distribution of \\(U\\).\n\\[\n\\Omega^{-1/2}U \\sim N(0,\\Omega^{-1/2}\\Omega\\Omega^{-1/2}) = N(0,I_q)\n\\] and \\[\n(\\Omega^{-1/2}U)'\\Omega^{-1/2}U =  U'\\Omega^{-1}U  = \\sum_{j=1}^qZ_i^2\\sim \\chi^2_q\n\\]\n\nUsing this result, we can then define the F-statistic as,\n\\[\n  \\text{F-stat} = (R\\hat{\\beta}-r)'\\big(s^2 R(X'X)^{-1}R'\\big)^{-1}(R\\hat{\\beta}-r)/q\n\\] where we replace the unknown \\(\\sigma^2\\) by \\(s^2\\) and scale by \\(q\\), the \\(rank(R)\\) (or number of hypotheses).\nAs above, we can rewrite this test-statistic as,\n\\[\n\\begin{aligned}\n  \\text{F-stat} =& \\frac{(R\\hat{\\beta}-r)'\\big(\\sigma^2 R(X'X)^{-1}R'\\big)^{-1}(R\\hat{\\beta}-r)/q}{s^2/\\sigma^2} \\\\\n  \\sim&\\frac{\\chi_q/q}{\\chi_{n-k}/(n-k)}\\\\\n  =&F_{q,n-k}\n\\end{aligned}  \n\\] We reject \\(H_0\\) when,\n\\[\n  \\text{F-stat} &gt;f_{q,n-k,1-\\alpha}\n\\] where \\(f_{q,n-k,1-\\alpha}\\) is the \\(1-\\alpha\\) percentile of the \\(F\\)-distribution with dof \\(q\\) and \\(n-k\\). This is a one-sided test since the F-statistic is strictly positive.\nNote, for \\(q=1\\), the F-statistic is given by the square of the T-statistic.\n\n5.1 Asymptotic distribution\nAs before, the replacement of \\(\\sigma^2\\) with \\(s^2\\) has no baring on the asymptotic distribution of the test. Since, \\(p \\lim (s_n^2/\\sigma^2) = 1\\),\n\\[\n\\text{F-stat} \\rightarrow_d \\chi^2_{q}/q\\qquad \\text{as}\\quad n\\rightarrow\\infty\n\\]"
  },
  {
    "objectID": "handout-4.html#restricted-ols",
    "href": "handout-4.html#restricted-ols",
    "title": "Hypothesis Testing",
    "section": "6 Restricted OLS",
    "text": "6 Restricted OLS\nYou will likely be familiar with a second description of the F-statistic:\n\\[\n\\text{F-stat} = \\frac{(RSS_R - RSS_U)/q}{RSS_U/(n-k)}\n\\]\nThis expression is equivalent to above and can be derived by solving the restricted (ordinary) least squares problem.\n\\[\n  \\underset{b}{\\min} \\;(Y-Xb)'(Y-Xb) \\qquad \\text{s.t.}\\quad Rb = r\n\\]\nThis is a constrained optimization problem that can be solved using the Lagrangian function,\n\\[\nL(b,\\lambda) = (Y-Xb)'(Y-Xb) + 2\\lambda'(Rb-r)\n\\] The F.O.C.’s (w.r.t. \\(b\\) and \\(\\lambda\\)), state at the optimum it must be that,\n\\[\n\\begin{aligned}\n0 =& 2X'X\\tilde{\\beta}-2X'Y-2R'\\tilde{\\lambda} \\\\\n0 =& R\\tilde{\\beta}-r\n\\end{aligned}\n\\]\nFrom (1), we have,\n\\[\n\\begin{aligned}\n\\tilde{\\beta} =& (X'X)^{-1}(X'Y-R'\\tilde{\\lambda}) \\\\\n=& \\underbrace{(X'X)^{-1}X'Y}_\\text{OLS}- (X'X)^{-1}R'\\tilde{\\lambda} \\\\\n=& \\hat{\\beta} - (X'X)^{-1}R'\\tilde{\\lambda}\n\\end{aligned}\n\\] Pluggin this solution into (2), you get,\n\\[\n\\begin{aligned}\nr =& R\\hat{\\beta}-R(X'X)^{-1}R'\\tilde{\\lambda} \\\\\n\\Rightarrow \\tilde{\\lambda} =& (R(X'X)^{-1}R')^{-1}(R\\hat{\\beta}-r) \\\\\n\\Rightarrow \\tilde{\\beta} =& \\hat{\\beta}- (X'X)^{-1}R'(R(X'X)^{-1}R')^{-1}(R\\hat{\\beta}-r)\n\\end{aligned}\n\\]\nThe residuals from the restricted model are given by,\n\\[\n\\begin{aligned}\n\\tilde{U} =& Y-X\\tilde{\\beta} \\\\\n=&\\underbrace{Y-X\\hat{\\beta}}_{M_XY}+X(X'X)^{-1}R'(R(X'X)^{-1}R')^{-1}(R\\hat{\\beta}-r) \\\\\n=&\\hat{U}+X(X'X)^{-1}R'(R(X'X)^{-1}R')^{-1}(R\\hat{\\beta}-r)\n\\end{aligned}\n\\] The Residual Sum of Squares (RSS) from the restricted model is then given by, \\[\n\\begin{aligned}\n\\tilde{U}'\\tilde{U} =& \\big(\\hat{U}+X(X'X)^{-1}R'(R(X'X)^{-1}R')^{-1}(R\\hat{\\beta}-r)\\big)'\\big(\\hat{U}+X(X'X)^{-1}R'(R(X'X)^{-1}R')^{-1}(R\\hat{\\beta}-r)\\big) \\\\\n=&\\hat{U}'\\hat{U} + (R\\hat{\\beta}-r)'(R(X'X)^{-1}R')^{-1}(R\\hat{\\beta}-r)\n\\end{aligned}\n\\] The cross-product terms cancel, because \\(\\hat{U}'X(X'X)^{-1}R'(R(X'X)^{-1}R')^{-1}(R\\hat{\\beta}-r) = Y'M_XX(X'X)^{-1}R'(R(X'X)^{-1}R')^{-1}(R\\hat{\\beta}-r) = 0\\). Thus,\n\\[\n  (R\\hat{\\beta}-r)'(R(X'X)^{-1}R')^{-1}(R\\hat{\\beta}-r) = \\underbrace{\\tilde{U}'\\tilde{U}}_{RSS_R}-\\underbrace{\\hat{U}'\\hat{U}}_{RSS_U}\n\\] Since, \\(s^2 = RSS_U/(n-k)\\), a scalar,\n\\[\n  \\text{F-stat} = \\frac{(RSS_R - RSS_U)/q}{RSS_U/(n-k)}\n\\]\nWhere \\(q = dof_r-dof_u\\), the difference in residual degrees of freedom for the two models.\nYou can estimate the restricted model by imposing the restrictions of \\(H_0\\) on the model. For example, in Problem Set 2, we tested \\(H_0: \\beta_2 = \\beta_3\\) and \\(\\beta_4 + \\beta_5 = 1\\), by estimating the restricted model,\n\\[\n    (Y-X_4) = \\gamma_1 + \\gamma_2(X_2+X_3) + \\gamma_5(X_5-X_4) + \\varepsilon\n\\]\n\n6.1 Testing full model\nIn the unique case where the hypothesis is given by,\n\\[\nH_0: \\beta_2 = ... = \\beta_k = 0\n\\]\nAnd the model includes a constant (i.e. \\(X_1 = \\mathbf{1}\\)). Then, the restricted model amounts to a regression of \\(Y\\) on a constant: \\(Y = \\beta_1 + u\\).\n\\[\n  RSS_R = (Y-\\bar{Y})'(Y-\\bar{Y}) = TSS\n\\]\nIn this unique case, the F-statistic can be written in terms of \\(R^2\\) (of the unrestricted model),\n\\[\n  \\text{F-stat} = \\frac{R^2/(k-1)}{(1 - R^2)/(n-k)}\n\\]\nThis is the F-statistic reported by Stata when you estimate a linear regression model."
  },
  {
    "objectID": "handout-4.html#heteroskedasticity",
    "href": "handout-4.html#heteroskedasticity",
    "title": "Hypothesis Testing",
    "section": "7 Heteroskedasticity",
    "text": "7 Heteroskedasticity\nBoth the t- and F-tests described above have their corresponding finite sample distributions under the assumptions that the error term in the CLRM is normally distributed (conditional on X) and that its variance is homoskedastic.\nThese distributions are not valid, for the finite sample, if the error terms are heteroskedastic. However, the tests do have the same asymptotic distributions with heteroskedasticity. For this reason, you should not use the t-distribution or F-distribution if your SEs are computed assuming heteroskedasticity. Instead, use the normal and chi-squared distributions."
  },
  {
    "objectID": "handout-4.html#footnotes",
    "href": "handout-4.html#footnotes",
    "title": "Hypothesis Testing",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf \\(X\\sim N(\\mu,\\Omega)\\) then the linear transformation of \\(X\\), \\(Y = AX +b\\), is distributed \\(Y\\sim N(A\\mu+b,A\\Omega A')\\).↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Quantitative Methods: Econometrics B",
    "section": "",
    "text": "This is not the official module website. The EC910 (or EC987, 2024/25) Moodle-page is the primary source of communication and resources for this module. All material available on this website can be found there.\nI designed this website as a means of testing out the Quarto package, developed by RStudio.1 This was partly as a way to develop my own skills and better incorporate the R language into the module. However, there are also accessibility benefits to this approach. Mathematical notation published in pdf’s using LaTeX cannot be read by a screen reader, while LaTeX published in html can. In addition, the website has built-in dark-mode option (see top-right toggle). For those who prefer pdf (for printing and notetaking), each page contains a unique downloadable link.\nPlease let me know if you find any mistakes in my notes or code. In addition, I welcome suggestions on how to improve the R (or Stata) code provided in this module. I will endeavor to do my best to acknowledge the many people who have contributed to this material and my wider knowledge of this subject."
  },
  {
    "objectID": "index.html#welcome-to-metrics-b",
    "href": "index.html#welcome-to-metrics-b",
    "title": "Quantitative Methods: Econometrics B",
    "section": "",
    "text": "This is not the official module website. The EC910 (or EC987, 2024/25) Moodle-page is the primary source of communication and resources for this module. All material available on this website can be found there.\nI designed this website as a means of testing out the Quarto package, developed by RStudio.1 This was partly as a way to develop my own skills and better incorporate the R language into the module. However, there are also accessibility benefits to this approach. Mathematical notation published in pdf’s using LaTeX cannot be read by a screen reader, while LaTeX published in html can. In addition, the website has built-in dark-mode option (see top-right toggle). For those who prefer pdf (for printing and notetaking), each page contains a unique downloadable link.\nPlease let me know if you find any mistakes in my notes or code. In addition, I welcome suggestions on how to improve the R (or Stata) code provided in this module. I will endeavor to do my best to acknowledge the many people who have contributed to this material and my wider knowledge of this subject."
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Quantitative Methods: Econometrics B",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nI am building off material developed by Prof. Wiji Arulampalam, who taught this module from 2011-2024. Since arriving at Warwick in 2021, I have learned so much from Prof. Arulampalam’s expertise and years of research and professional experience. More importantly, I have so enjoyed working alongside her as both colleague and friend. I wish her well in her retirement.\nI have also borrowed from material developed by Prof. Vadim Marmer (Vancouver School of Economics, UBC) who taught the Econometrics module I attended during my own Master’s degree. I was the teaching assistant for this module from 2017-2019 and developed some additional notes found here. I am hugely indebted to Prof. Marmer, whose teaching and professional guidance have been foundational to my academic career.\nI hope you enjoy this module!\nNeil Lloyd"
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Quantitative Methods: Econometrics B",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.↩︎"
  },
  {
    "objectID": "material-dummy.html",
    "href": "material-dummy.html",
    "title": "Dummy Variables",
    "section": "",
    "text": "Dummy variables are used extensively in Econometrics and applied research. They are used to model the impact (treatment effect) of public policies, randomized control trials, and ‘natural’ experiments. They are also used to model categorical variables and fixed-effects (used to explain unobserved heterogeneity in cross-section and panel models). Given their proclivity, it is important that we understand their mechanics.\nYou might be surprised by how much ‘real-world’ data is not continuous. For example, if you downloaded a survey dataset you will likely find that most variables are in fact categorical. This is the result of a number of factors, including accuracy (avoiding reporting bias) and just the nature of real-world variables. For example, sureys often ask people to report their income on a binned scale (e.g., $0-$10,000; $10,000-$20,000; …) as a lot of people do not know their exact annual or monthly salary. Variables about a households structure (e.g. “2 parents, with children”; “Single parent, without children”; …) are naturally categorical.\nEven if a variable may have (close to) continuous measure (e.g. age in months/days; years of education), you might choose to model the variable as categorical since it may not be cardinal. For example, a bachelors degree may correspond to a 15 years of education, compared to 12 years of education for a high school diploma. However, is this 3-year difference equivalent to the difference in education of individuals with 5 and 2 years of education? You could say this about income (measured in $’s), height/weight/BMI, and employment tenure, as these measures are cardinal.\n\n\nAny dummy variable is a discrete random variable (often denoted \\(D\\)) that takes on two values: \\(D_i\\in\\{0,1\\}\\). It corresponds to a true-false statement:\n\\[\nD_i = \\mathbf{1}\\{\\text{``true\"}\\}\n\\] This simple function returns the value 1 if the statement inside is true (for unit \\(i\\)), and 0 otherwise. The For example, suppose there was a categorical variable that took on \\(m\\) distinct values, each with their own label,\n\\[\nX_{i}\\in\\{x_1\\;\\text{``label 1\"},x_2\\;\\text{``label 2\"},...,x_m\\;\\text{``label m\"}\\}\n\\] For example, \\(favouritecolour_i \\in\\{1\\;\\text{``red\"}, 2\\;\\text{``blue\"}, 3\\;\\text{``yellow\"}, 4\\;\\text{``green\"}\\}\\). Then for each value of \\(X_i\\), you can define a dummy variable:\n\\[\n  D_{im} = \\mathbf{1}\\{X_i = x_m\\}\n\\]\nA common usage of dummy variables is in the evaluation of randomized and ‘natural’ experiments. Researchers will typically define a single dummy variable to denote treatment status: \\(D_i = \\mathbf{1}\\{\\text{``treated\"}\\}\\). The dummy variable splits the sample into two groups: treated (\\(D_i=1\\)) and control (\\(D_i=0\\)).\n\n\n\nIn a basic setting the use a of dummy variable might be relatively straight forward. For example, consider the above exmple of a single treatment-status dummy variable. We can include this in a univariate regression model (including a constant term),\n\\[\nY_i = \\beta_{1}+\\beta_{2} D_{i} + \\varepsilon_i\n\\] We can show that \\(\\beta_2\\) can be interpretted as the difference between the mean of \\(Y\\) for the treated and control group (see material on interpretting linear models),\n\\[\n  \\beta_2 = E[Y_i|D_i=1] - E[Y_i|D_i=0]\n\\] Unfortunately, this notation is not going to help us move forward, so let us be a little more formal/detailed. Suppose, a categorical regressor takes on two values \\(X\\in\\{1,2\\}\\). We can then define two dummy variables,\n\\[\n\\begin{aligned}\n  D_{i1} =& \\mathbf{1}\\{X_i = 1\\} \\\\\n  D_{i2} =& \\mathbf{1}\\{X_i = 2\\}\n\\end{aligned}\n\\]\nSince the values of \\(X\\) are exhaustive and mutually exclusive, it must be that,\n\\[\n  D_{i1} + D_{i2} = 1\n\\]\nFor every unit, their value of \\(X\\) is either 1 or 2.\nNow, let us consider the linear model that includes these dummy variables as regressors:\n\\[\nY_i = \\beta_1 + \\beta_2D_{i1} + \\beta_3D_{i2} + \\varepsilon_i\n\\] We immediately have a problem. This model is not identified. This is due to a rank violation (perfect colinearity between regressors).\nSuppose the data was sorted on \\(X\\). Then consider the matrix of regressors in this model:\n\\[\nX=[\\ell,D_1,D_2]=\\begin{bmatrix}1 & 0 &1 \\\\ \\vdots & \\vdots & \\vdots \\\\ 1 & 0 &1 \\\\ 1 & 1 &0\\\\ \\vdots & \\vdots & \\vdots \\\\ 1 & 1 & 0 \\end{bmatrix}\n\\] This matrix has 3 columns, but its rank is 2. Column 3 (\\(D_2\\)) is given by column 1 (\\(\\ell\\)) minus column 2 (\\(D_1\\)).\n\\[\n\\begin{bmatrix}1 \\\\ \\vdots \\\\ 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{bmatrix}=\\begin{bmatrix}0 \\\\ \\vdots \\\\ 0 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{bmatrix}+\\begin{bmatrix}1 \\\\ \\vdots \\\\ 1 \\\\ 0\\\\  \\vdots \\\\  0 \\end{bmatrix}\n\\]\nIf \\(X\\) is not full rank, then the \\(k\\times k\\) matrix \\(X'X\\) is not invertible. And neither is \\(E[X_iX_i']\\), implying a failure of identification. We cannot seperately identify all three \\(\\beta\\)-parameters \\([\\beta_1,\\beta_2,\\beta_3]\\).\nWe can demonstrate the result in another way. Given that \\(D_{i1} + D_{i2} = 1\\) (where 1 represents the constant regressor), we can substitute \\(D_{i2}\\) with \\(1-D_{i1}\\).\n\\[\n\\begin{aligned}\n  Y_i =& \\beta_1 + \\beta_2D_{i1} + \\beta_3(1-D_{i1}) + \\varepsilon_i \\\\\n  =& (\\beta_1 + \\beta_3) + (\\beta_2-\\beta_3)D_{i1} +\\varepsilon_i\n\\end{aligned}\n\\] which we can rewrite as,\n\\[\n  Y_i = \\beta_{1(2)} + \\beta_{2(2)} D_{i1} + \\varepsilon_i\n\\] I’m using the notation of an additional subscript-\\((2)\\) to denote the parameters corresponding to the model where we exclude the dummy variable \\(D_{i2}\\).\nNow, the \\(X_2\\) matrix (denoted as such because it excludes \\(D_2\\)) has two columns and is full rank. Thus, \\([\\beta_{1(2)}, \\beta_{2(2)}]\\) are both identified.\n\\[\nX_{(2)}=[\\ell,D_1]=\\begin{bmatrix}1 & 0 \\\\ \\vdots & \\vdots \\\\ 1 & 0 \\\\ 1 & 1 \\\\ \\vdots & \\vdots \\\\ 1 & 1 \\end{bmatrix}\n\\] Thus, the model is given by,\n\\[\n  Y = X_{(2)}\\beta_{(2)} + \\varepsilon\n\\]\nWe can have also substituted \\(D_{i1}\\) with \\(1-D_{i2}\\). This would give us the model,\n\\[\n  Y_i = \\beta_{1(1)} + \\beta_{2(1)} D_{i2} + \\varepsilon_i\n\\] where \\(\\beta_{1(1)} = \\beta_1 + \\beta_2\\) and \\(\\beta_{2(1)} = \\beta_3-\\beta_2\\).\nor,\n\\[\n  Y = X_{(1)}\\beta_{(1)} + \\varepsilon\n\\] Note, the vectors \\(\\beta_{(1)}\\) and \\(\\beta_{(2)}\\) are not the same. However, you can recover the one from the other. For example,\n\\[\n\\begin{aligned}\n\\beta_{1(1)} =& \\beta_1 + \\beta_2 = \\beta_1 + \\beta_3 + (\\beta_2-\\beta_3) = \\beta_{1(2)}-\\beta_{2(2)} \\\\\n\\beta_{2(1)} =& \\beta_3-\\beta_2 = -\\beta_{2(2)}\n\\end{aligned}\n\\]\nFinally, we can substitute in the constant with \\(D_{i1}+D_{i2}\\). This gives us the model,\n\\[\n  Y_i = \\beta_{1(0)}D_{i1} + \\beta_{2(0)} D_{i2} + \\varepsilon_i\n\\] where \\(\\beta_{1(0)} = \\beta_1 + \\beta_2\\) and \\(\\beta_{2(0)} = \\beta_1+\\beta_3\\).\nor,\n\\[\n  Y = X_{(0)}\\beta_{(0)} + \\varepsilon\n\\] While it might seem strange to exclude the constant term, it is implicitly included. This is because the regressors add up to one.\n\n\n\nWe have considered three different models, each with a different excluded variable:\n\n\\(Y = X_{(2)}\\beta_{(2)} + \\varepsilon\\)\n\\(Y = X_{(1)}\\beta_{(1)} + \\varepsilon\\)\n\\(Y = X_{(0)}\\beta_{(0)} + \\varepsilon\\)\n\nEach model corresponds to a different projection matrix: \\(P_X = X(X'X)^{-1}X'\\). I will denote these \\(P_{(2)}\\), \\(P_{(1)}\\), and \\(P_{(0)}\\). It turns out that,\n\\[\nP_{(2)} = P_{(1)} = P_{(0)}\n\\] Implying the same result for the orthogonal projections: \\(M_{(2)}\\), \\(M_{(1)}\\), and \\(M_{(0)}\\). It implies that the predicted values and residuals are the same for all three models.\n\n\n\nThe above result has important implications for discrete/categorical covariates. Suppose you had a regression with a single a continuous regressors \\(X_{1}\\) and then a single categorical variable \\(X_{2}\\in\\{x_1,...,x_m\\}\\). There will be \\(k = m+1\\) parameters in this model. You would define a set of dummy variables for each category and include \\(m-1\\) in the model, assuming a constant is also included.1 \\[\nY = \\beta_1 X_{1} + \\beta_{21(1)} + \\beta_{22(1)} D_{2} + ... + \\beta_{2m(1)}D_m + \\upsilon\n\\] This can be written as,\n\\[\n  Y = X_1\\beta_1 + X_{2(1)}\\beta_{2(1)} + \\upsilon\n\\] where, \\[\n  X_{2(1)} = [1,D_2,D_3,...,D_m]\n\\]\nThe partitioned regression result tells us that the OLS-estimator for \\(\\beta_1\\) is given by,\n\\[\n  (M_{2(1)}Y) = (M_{2(1)}X_1)\\beta_1 + \\xi\n\\] where \\(M_{2(1)}\\) is the orthogonal projection matrix of all other regressors in the model. In this case, that is the orthogonal projection matrix of the matrix,\nIn this matrix, \\(D_1\\) has been excluded as the base category. Above, we showed that,\n\\[\nM_{2(0)} = M_{2(1)} = M_{2(2)} = ... = M_{2(m)}\n\\] Thus, the choice of base category for the covariate regressors has no impact on our estimate of \\(\\beta_1\\), the parameter of interest. We could have written the same model, excluding \\(D_2\\)\n\\[\nY = \\beta_1 X_{1} + \\beta_{21(2)} + \\beta_{22(2)} D_{1} + \\beta_{23(2)} D_{3} + ... + \\beta_{2m(2)}D_m + \\upsilon\n\\]\nOr even excluding the constant,\n\\[\nY = \\beta_1 X_{1} + \\beta_{21(0)} D_{1} + \\beta_{22(0)} D_{2} +  ... + \\beta_{2m(0)}D_m + \\upsilon\n\\]\nRegardless, of the base category, the estimator for \\(\\beta_1\\) remains unchanged.\n\n\n\nThe above result helps rationalize a fairly common notation in applied econometrics. Suppose you were studying the relationship between (log of) wages and education. We know that there is a lot of spatial heteroegeneity in wages: some occupations in certain cities pay a lot more than the same occupation in a different city. This could be for a whole range of reasons - some individual (differences in education/training, etc.) and some environmental (local ammenities, labour competition, number of employers etc.). Some of these location factors may be obserable, but some not. If you are willing to assume that these characteristics relate to the local area and not the specific individual, then you could model them as,\n\\[\n  A_{l} = [A_{l1},...,A_{ls}]'\n\\] Where \\(A_l\\) is a \\(s\\times 1\\) vector of \\(l\\)-location’s characteristics. You can use the notation \\(A_{l(i)}\\) to link the individual to a specific location. Recall, these are common to all individuals in this location. We can think of this linear combination of location specific variables as a single location-specific parameter \\(\\alpha_l\\).\n\\[\n  \\alpha_l = A_{l(i)}'\\gamma\n\\] By doing this, we remove the need to observe all the variables in \\(A_l\\). However, we must observe multiple people from the same location.\nSuppose the model you have in mind is given by,\n\\[\n\\ln(w_i) = \\alpha + \\beta edu_i + A_{l(i)}'\\gamma + \\varepsilon_{i}\n\\] We can’t estimate this model because we can’t observe all variables in \\(A\\). However, we can estimate the model,\n\\[\n\\ln(w_i) = \\alpha_1 + \\beta edu_i + \\sum_{j=2}^m \\alpha_{2j}\\mathbf{1}\\{location_i = j\\} + \\varepsilon_i\n\\] where we exclude the first (arbitrary) location. Or, we can drop the constant and estimate,\n\\[\n\\ln(w_i) = \\beta edu_i + \\sum_{j=1}^m \\alpha_{j}\\mathbf{1}\\{location_i = j\\} + \\varepsilon_i\n\\] Since, an individual will have only one location (in a given period of time), unit \\(i\\) in location \\(j\\) will just return,\n\\[\n\\sum_{j=1}^m \\alpha_{j}\\mathbf{1}\\{location_i = j\\} = \\alpha_j\n\\] So, the model can be written as,\n\\[\n\\ln(w_{ij}) = \\beta edu_{ij} + \\alpha_j + \\varepsilon_{ij}\n\\] You can think of \\(\\alpha_j\\) as a location-specific constant. This is also why it makes sense to drop the constant whe using fixed-effects notation. This constant explains all the unobserved spatial heterogeneity in the earnings of individuals.\nFrom a modelling perspective, it is important to remember that this notation essentially hides a whole set of dummy variables that are implicitly included in the model. The number of regressors in this model will be \\(k = 1 + m\\) where \\(m\\) is the number of locations."
  },
  {
    "objectID": "material-dummy.html#notation",
    "href": "material-dummy.html#notation",
    "title": "Dummy Variables",
    "section": "",
    "text": "Any dummy variable is a discrete random variable (often denoted \\(D\\)) that takes on two values: \\(D_i\\in\\{0,1\\}\\). It corresponds to a true-false statement:\n\\[\nD_i = \\mathbf{1}\\{\\text{``true\"}\\}\n\\] This simple function returns the value 1 if the statement inside is true (for unit \\(i\\)), and 0 otherwise. The For example, suppose there was a categorical variable that took on \\(m\\) distinct values, each with their own label,\n\\[\nX_{i}\\in\\{x_1\\;\\text{``label 1\"},x_2\\;\\text{``label 2\"},...,x_m\\;\\text{``label m\"}\\}\n\\] For example, \\(favouritecolour_i \\in\\{1\\;\\text{``red\"}, 2\\;\\text{``blue\"}, 3\\;\\text{``yellow\"}, 4\\;\\text{``green\"}\\}\\). Then for each value of \\(X_i\\), you can define a dummy variable:\n\\[\n  D_{im} = \\mathbf{1}\\{X_i = x_m\\}\n\\]\nA common usage of dummy variables is in the evaluation of randomized and ‘natural’ experiments. Researchers will typically define a single dummy variable to denote treatment status: \\(D_i = \\mathbf{1}\\{\\text{``treated\"}\\}\\). The dummy variable splits the sample into two groups: treated (\\(D_i=1\\)) and control (\\(D_i=0\\))."
  },
  {
    "objectID": "material-dummy.html#dummy-regressors",
    "href": "material-dummy.html#dummy-regressors",
    "title": "Dummy Variables",
    "section": "",
    "text": "In a basic setting the use a of dummy variable might be relatively straight forward. For example, consider the above exmple of a single treatment-status dummy variable. We can include this in a univariate regression model (including a constant term),\n\\[\nY_i = \\beta_{1}+\\beta_{2} D_{i} + \\varepsilon_i\n\\] We can show that \\(\\beta_2\\) can be interpretted as the difference between the mean of \\(Y\\) for the treated and control group (see material on interpretting linear models),\n\\[\n  \\beta_2 = E[Y_i|D_i=1] - E[Y_i|D_i=0]\n\\] Unfortunately, this notation is not going to help us move forward, so let us be a little more formal/detailed. Suppose, a categorical regressor takes on two values \\(X\\in\\{1,2\\}\\). We can then define two dummy variables,\n\\[\n\\begin{aligned}\n  D_{i1} =& \\mathbf{1}\\{X_i = 1\\} \\\\\n  D_{i2} =& \\mathbf{1}\\{X_i = 2\\}\n\\end{aligned}\n\\]\nSince the values of \\(X\\) are exhaustive and mutually exclusive, it must be that,\n\\[\n  D_{i1} + D_{i2} = 1\n\\]\nFor every unit, their value of \\(X\\) is either 1 or 2.\nNow, let us consider the linear model that includes these dummy variables as regressors:\n\\[\nY_i = \\beta_1 + \\beta_2D_{i1} + \\beta_3D_{i2} + \\varepsilon_i\n\\] We immediately have a problem. This model is not identified. This is due to a rank violation (perfect colinearity between regressors).\nSuppose the data was sorted on \\(X\\). Then consider the matrix of regressors in this model:\n\\[\nX=[\\ell,D_1,D_2]=\\begin{bmatrix}1 & 0 &1 \\\\ \\vdots & \\vdots & \\vdots \\\\ 1 & 0 &1 \\\\ 1 & 1 &0\\\\ \\vdots & \\vdots & \\vdots \\\\ 1 & 1 & 0 \\end{bmatrix}\n\\] This matrix has 3 columns, but its rank is 2. Column 3 (\\(D_2\\)) is given by column 1 (\\(\\ell\\)) minus column 2 (\\(D_1\\)).\n\\[\n\\begin{bmatrix}1 \\\\ \\vdots \\\\ 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{bmatrix}=\\begin{bmatrix}0 \\\\ \\vdots \\\\ 0 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{bmatrix}+\\begin{bmatrix}1 \\\\ \\vdots \\\\ 1 \\\\ 0\\\\  \\vdots \\\\  0 \\end{bmatrix}\n\\]\nIf \\(X\\) is not full rank, then the \\(k\\times k\\) matrix \\(X'X\\) is not invertible. And neither is \\(E[X_iX_i']\\), implying a failure of identification. We cannot seperately identify all three \\(\\beta\\)-parameters \\([\\beta_1,\\beta_2,\\beta_3]\\).\nWe can demonstrate the result in another way. Given that \\(D_{i1} + D_{i2} = 1\\) (where 1 represents the constant regressor), we can substitute \\(D_{i2}\\) with \\(1-D_{i1}\\).\n\\[\n\\begin{aligned}\n  Y_i =& \\beta_1 + \\beta_2D_{i1} + \\beta_3(1-D_{i1}) + \\varepsilon_i \\\\\n  =& (\\beta_1 + \\beta_3) + (\\beta_2-\\beta_3)D_{i1} +\\varepsilon_i\n\\end{aligned}\n\\] which we can rewrite as,\n\\[\n  Y_i = \\beta_{1(2)} + \\beta_{2(2)} D_{i1} + \\varepsilon_i\n\\] I’m using the notation of an additional subscript-\\((2)\\) to denote the parameters corresponding to the model where we exclude the dummy variable \\(D_{i2}\\).\nNow, the \\(X_2\\) matrix (denoted as such because it excludes \\(D_2\\)) has two columns and is full rank. Thus, \\([\\beta_{1(2)}, \\beta_{2(2)}]\\) are both identified.\n\\[\nX_{(2)}=[\\ell,D_1]=\\begin{bmatrix}1 & 0 \\\\ \\vdots & \\vdots \\\\ 1 & 0 \\\\ 1 & 1 \\\\ \\vdots & \\vdots \\\\ 1 & 1 \\end{bmatrix}\n\\] Thus, the model is given by,\n\\[\n  Y = X_{(2)}\\beta_{(2)} + \\varepsilon\n\\]\nWe can have also substituted \\(D_{i1}\\) with \\(1-D_{i2}\\). This would give us the model,\n\\[\n  Y_i = \\beta_{1(1)} + \\beta_{2(1)} D_{i2} + \\varepsilon_i\n\\] where \\(\\beta_{1(1)} = \\beta_1 + \\beta_2\\) and \\(\\beta_{2(1)} = \\beta_3-\\beta_2\\).\nor,\n\\[\n  Y = X_{(1)}\\beta_{(1)} + \\varepsilon\n\\] Note, the vectors \\(\\beta_{(1)}\\) and \\(\\beta_{(2)}\\) are not the same. However, you can recover the one from the other. For example,\n\\[\n\\begin{aligned}\n\\beta_{1(1)} =& \\beta_1 + \\beta_2 = \\beta_1 + \\beta_3 + (\\beta_2-\\beta_3) = \\beta_{1(2)}-\\beta_{2(2)} \\\\\n\\beta_{2(1)} =& \\beta_3-\\beta_2 = -\\beta_{2(2)}\n\\end{aligned}\n\\]\nFinally, we can substitute in the constant with \\(D_{i1}+D_{i2}\\). This gives us the model,\n\\[\n  Y_i = \\beta_{1(0)}D_{i1} + \\beta_{2(0)} D_{i2} + \\varepsilon_i\n\\] where \\(\\beta_{1(0)} = \\beta_1 + \\beta_2\\) and \\(\\beta_{2(0)} = \\beta_1+\\beta_3\\).\nor,\n\\[\n  Y = X_{(0)}\\beta_{(0)} + \\varepsilon\n\\] While it might seem strange to exclude the constant term, it is implicitly included. This is because the regressors add up to one."
  },
  {
    "objectID": "material-dummy.html#dummy-projections",
    "href": "material-dummy.html#dummy-projections",
    "title": "Dummy Variables",
    "section": "",
    "text": "We have considered three different models, each with a different excluded variable:\n\n\\(Y = X_{(2)}\\beta_{(2)} + \\varepsilon\\)\n\\(Y = X_{(1)}\\beta_{(1)} + \\varepsilon\\)\n\\(Y = X_{(0)}\\beta_{(0)} + \\varepsilon\\)\n\nEach model corresponds to a different projection matrix: \\(P_X = X(X'X)^{-1}X'\\). I will denote these \\(P_{(2)}\\), \\(P_{(1)}\\), and \\(P_{(0)}\\). It turns out that,\n\\[\nP_{(2)} = P_{(1)} = P_{(0)}\n\\] Implying the same result for the orthogonal projections: \\(M_{(2)}\\), \\(M_{(1)}\\), and \\(M_{(0)}\\). It implies that the predicted values and residuals are the same for all three models."
  },
  {
    "objectID": "material-dummy.html#dummy-covariates",
    "href": "material-dummy.html#dummy-covariates",
    "title": "Dummy Variables",
    "section": "",
    "text": "The above result has important implications for discrete/categorical covariates. Suppose you had a regression with a single a continuous regressors \\(X_{1}\\) and then a single categorical variable \\(X_{2}\\in\\{x_1,...,x_m\\}\\). There will be \\(k = m+1\\) parameters in this model. You would define a set of dummy variables for each category and include \\(m-1\\) in the model, assuming a constant is also included.1 \\[\nY = \\beta_1 X_{1} + \\beta_{21(1)} + \\beta_{22(1)} D_{2} + ... + \\beta_{2m(1)}D_m + \\upsilon\n\\] This can be written as,\n\\[\n  Y = X_1\\beta_1 + X_{2(1)}\\beta_{2(1)} + \\upsilon\n\\] where, \\[\n  X_{2(1)} = [1,D_2,D_3,...,D_m]\n\\]\nThe partitioned regression result tells us that the OLS-estimator for \\(\\beta_1\\) is given by,\n\\[\n  (M_{2(1)}Y) = (M_{2(1)}X_1)\\beta_1 + \\xi\n\\] where \\(M_{2(1)}\\) is the orthogonal projection matrix of all other regressors in the model. In this case, that is the orthogonal projection matrix of the matrix,\nIn this matrix, \\(D_1\\) has been excluded as the base category. Above, we showed that,\n\\[\nM_{2(0)} = M_{2(1)} = M_{2(2)} = ... = M_{2(m)}\n\\] Thus, the choice of base category for the covariate regressors has no impact on our estimate of \\(\\beta_1\\), the parameter of interest. We could have written the same model, excluding \\(D_2\\)\n\\[\nY = \\beta_1 X_{1} + \\beta_{21(2)} + \\beta_{22(2)} D_{1} + \\beta_{23(2)} D_{3} + ... + \\beta_{2m(2)}D_m + \\upsilon\n\\]\nOr even excluding the constant,\n\\[\nY = \\beta_1 X_{1} + \\beta_{21(0)} D_{1} + \\beta_{22(0)} D_{2} +  ... + \\beta_{2m(0)}D_m + \\upsilon\n\\]\nRegardless, of the base category, the estimator for \\(\\beta_1\\) remains unchanged."
  },
  {
    "objectID": "material-dummy.html#fixed-effects",
    "href": "material-dummy.html#fixed-effects",
    "title": "Dummy Variables",
    "section": "",
    "text": "The above result helps rationalize a fairly common notation in applied econometrics. Suppose you were studying the relationship between (log of) wages and education. We know that there is a lot of spatial heteroegeneity in wages: some occupations in certain cities pay a lot more than the same occupation in a different city. This could be for a whole range of reasons - some individual (differences in education/training, etc.) and some environmental (local ammenities, labour competition, number of employers etc.). Some of these location factors may be obserable, but some not. If you are willing to assume that these characteristics relate to the local area and not the specific individual, then you could model them as,\n\\[\n  A_{l} = [A_{l1},...,A_{ls}]'\n\\] Where \\(A_l\\) is a \\(s\\times 1\\) vector of \\(l\\)-location’s characteristics. You can use the notation \\(A_{l(i)}\\) to link the individual to a specific location. Recall, these are common to all individuals in this location. We can think of this linear combination of location specific variables as a single location-specific parameter \\(\\alpha_l\\).\n\\[\n  \\alpha_l = A_{l(i)}'\\gamma\n\\] By doing this, we remove the need to observe all the variables in \\(A_l\\). However, we must observe multiple people from the same location.\nSuppose the model you have in mind is given by,\n\\[\n\\ln(w_i) = \\alpha + \\beta edu_i + A_{l(i)}'\\gamma + \\varepsilon_{i}\n\\] We can’t estimate this model because we can’t observe all variables in \\(A\\). However, we can estimate the model,\n\\[\n\\ln(w_i) = \\alpha_1 + \\beta edu_i + \\sum_{j=2}^m \\alpha_{2j}\\mathbf{1}\\{location_i = j\\} + \\varepsilon_i\n\\] where we exclude the first (arbitrary) location. Or, we can drop the constant and estimate,\n\\[\n\\ln(w_i) = \\beta edu_i + \\sum_{j=1}^m \\alpha_{j}\\mathbf{1}\\{location_i = j\\} + \\varepsilon_i\n\\] Since, an individual will have only one location (in a given period of time), unit \\(i\\) in location \\(j\\) will just return,\n\\[\n\\sum_{j=1}^m \\alpha_{j}\\mathbf{1}\\{location_i = j\\} = \\alpha_j\n\\] So, the model can be written as,\n\\[\n\\ln(w_{ij}) = \\beta edu_{ij} + \\alpha_j + \\varepsilon_{ij}\n\\] You can think of \\(\\alpha_j\\) as a location-specific constant. This is also why it makes sense to drop the constant whe using fixed-effects notation. This constant explains all the unobserved spatial heterogeneity in the earnings of individuals.\nFrom a modelling perspective, it is important to remember that this notation essentially hides a whole set of dummy variables that are implicitly included in the model. The number of regressors in this model will be \\(k = 1 + m\\) where \\(m\\) is the number of locations."
  },
  {
    "objectID": "material-dummy.html#footnotes",
    "href": "material-dummy.html#footnotes",
    "title": "Dummy Variables",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI have deliberately moved the order of the variables so that the constant and dummy variables can be grouped together as a single set of regressors \\(X_2\\). In this way, the model follows the notation from Handout 1, in which \\(Y = X_1\\beta_1 + X_2\\beta_2 + \\varepsilon\\).↩︎"
  },
  {
    "objectID": "material-interpretation.html",
    "href": "material-interpretation.html",
    "title": "Interpretation of Linear Models",
    "section": "",
    "text": "In this short handout we will consider the interpretation of linear regression model coefficients in models with different combinations of outcome and regressor variables:\n\ncontinuous level-level\ncontinuous-discrete\ndiscrete-continuous\ndiscrete-discrete\nlog-level\nlevel-log\nlog-log\n\nIn all instances, we will work on the CLRM model assumptions 1 & 2, which tell us that the conditional expectation function is linear in parameters:\n\\[\nE[Y_i|X_i] = X_i'\\beta\n\\]\n\n\nIf \\(Y_i\\) and \\(X_i\\) are both continuously distributed random variables then,\n\\[\n\\beta_j = \\frac{\\partial E[Y_i|X_i]}{\\partial X_{ij}}\n\\] or, as a vector,\n\\[\n\\beta = \\frac{\\partial E[Y_i|X_i]}{\\partial X_{i}} = \\begin{bmatrix}\\frac{\\partial E[Y_i|X_i]}{\\partial X_{i1}}\\\\ \\vdots \\\\ \\frac{\\partial E[Y_i|X_i]}{\\partial X_{ik}}\\end{bmatrix} = \\begin{bmatrix}\\beta_1\\\\ \\vdots \\\\ \\beta_k\\end{bmatrix}\n\\]\nThe regression parameter has a partial derivative interpretation with respect to the CEF. As discussed in Handout 1, this is often used to motivate the experimental language of ceteris paribus: “holding all else fixed.\n\n\n\nConsider a case where there is a single discrete regressor: \\(D_i \\in \\{0,1\\}\\). For example,\n\\[\n  Y_i = \\beta_1 + \\beta_2 D_i + \\varepsilon_i\n\\] We cannot apply the partial derivative interpretation since \\(D\\) is not continuous. Instead, we will look at differences:\n\\[\n\\begin{aligned}\n    E[Y_i|D_i=1] =& \\beta_1 + \\beta_2 \\\\\n    E[Y_i|D_i=0] =& \\beta_1 \\\\   \n    \\Rightarrow \\beta_2 =& E[Y_i|D_i=1] - E[Y_i|D_i=0]\n\\end{aligned}\n\\]\nWe can easily extend this the case where the model includes additional (discrete or continuous) covariates, as well as case where the variable takes on multiple discrete values.\n\n\n\nIf the outcome is discrete (\\(Y_i\\in\\{0,1\\}\\)) while the regressors are continuous, the resulting linear model is referred to as a linear probability model.\n\\[\nE[Y_i|X_i] = Pr(Y_i = 1|X_i) = X_i'\\beta\n\\] This is differentiable, since \\(X\\) is continuous and the same partial derivative interpretation follows.\n\\[\n\\beta_j = \\frac{\\partial Pr(Y_i=1|X_i)}{\\partial X_{ij}}\n\\]\nNote, the unit of \\(Y\\) is probability-points (\\(\\in[0,1]\\)), not %-points (\\(\\in[0,100]\\)). Of course, the conversion of units can be made by \\(\\times 100\\) to measure in %-points.\n\n\n\nIf both the outcome and regressor(s) are discrete, then the parameter identifies a difference in conditional probabilities, \\[\n\\beta_2 = Pr(Y_i|D_i=1) - Pr(Y_i=1|D_i=0)\n\\] Note, the unit of \\(Y\\) is probability-points (\\(\\in[0,1]\\)), not %-points (\\(\\in[0,100]\\)).\n\n\n\nConsider the model,\n\\[\n  \\ln(Y_i) = X_i'\\beta + \\varepsilon_i\n\\] Then,\n\\[\n  X_i'\\beta = E[\\ln(Y_i)|X_i]\n\\]\n\\[\n\\beta_j = \\frac{\\partial E[\\ln(Y_i)|X_i]}{\\partial X_{ij}}\n\\]\nThe coefficient is therefore measured in log-units of \\(Y\\). The relation to a change in the (expected) level of \\(Y\\) is given by,\n\\[\n\\%\\Delta E[Y_i|X_i] = (exp(\\beta)-1)\\times 100\n\\] For reasonably small values of \\(\\beta\\) (i.e. within the range \\([-0.1,0.1]\\)) this can be approximated by,\n\\[\n\\%\\Delta E[Y_i|X_i] = \\beta\\times 100\n\\] A 1-unit change in \\(X_{i1}\\) is associated with a \\(\\beta_1\\) percent change in the expected value of \\(Y\\).\nThis referred to as a semi-elasticity.\n\n\n\nIf the regressor(s) is measure in log-units; for example,\n\\[\n  Y_i = \\beta_1 + \\beta_2 \\ln(X_i)_i + \\varepsilon_i\n\\] Then,\n\\[\n\\beta_2 = \\frac{\\partial E[Y_i|X_i]}{\\partial \\ln(X_{i})}\n\\]\nA 1 percent increase in \\(X\\) is given by \\(X\\times1.01\\). This is equivalent to a change in \\(\\ln(X)\\) of,\n\\[\n\\ln(X_i\\times1.01) - \\ln(X_i) = \\ln(1.01) \\approx 0.01\n\\] Thus, a 1 percent increase in the level of \\(X\\) is associated with a \\(\\beta_2/100\\) increase in the expected value of \\(Y\\). Or, more accurate\n\\[\n  \\Delta E[Y_i|X_i] = \\beta_2\\times \\ln(1.01)\n\\] This is also a semi-elasticity.\n\n\n\nIn models where both the outcome and regressor are log-transformed with an elasticity interpretation.\n\\[\n    \\ln(Y_i) = \\beta_1 + \\beta_2 \\ln(X_i)_i + \\varepsilon_i\n\\] \\[\n\\beta_2 = \\frac{\\partial E[\\ln(Y_i)|X_i]}{\\partial \\ln(X_{i})}\n\\] \\(\\beta_2\\) is a the % change in the expected value of \\(Y\\) from a 1 % change in \\(X\\)."
  },
  {
    "objectID": "material-interpretation.html#continuous-level-level-models",
    "href": "material-interpretation.html#continuous-level-level-models",
    "title": "Interpretation of Linear Models",
    "section": "",
    "text": "If \\(Y_i\\) and \\(X_i\\) are both continuously distributed random variables then,\n\\[\n\\beta_j = \\frac{\\partial E[Y_i|X_i]}{\\partial X_{ij}}\n\\] or, as a vector,\n\\[\n\\beta = \\frac{\\partial E[Y_i|X_i]}{\\partial X_{i}} = \\begin{bmatrix}\\frac{\\partial E[Y_i|X_i]}{\\partial X_{i1}}\\\\ \\vdots \\\\ \\frac{\\partial E[Y_i|X_i]}{\\partial X_{ik}}\\end{bmatrix} = \\begin{bmatrix}\\beta_1\\\\ \\vdots \\\\ \\beta_k\\end{bmatrix}\n\\]\nThe regression parameter has a partial derivative interpretation with respect to the CEF. As discussed in Handout 1, this is often used to motivate the experimental language of ceteris paribus: “holding all else fixed."
  },
  {
    "objectID": "material-interpretation.html#continuous-discrete-models",
    "href": "material-interpretation.html#continuous-discrete-models",
    "title": "Interpretation of Linear Models",
    "section": "",
    "text": "Consider a case where there is a single discrete regressor: \\(D_i \\in \\{0,1\\}\\). For example,\n\\[\n  Y_i = \\beta_1 + \\beta_2 D_i + \\varepsilon_i\n\\] We cannot apply the partial derivative interpretation since \\(D\\) is not continuous. Instead, we will look at differences:\n\\[\n\\begin{aligned}\n    E[Y_i|D_i=1] =& \\beta_1 + \\beta_2 \\\\\n    E[Y_i|D_i=0] =& \\beta_1 \\\\   \n    \\Rightarrow \\beta_2 =& E[Y_i|D_i=1] - E[Y_i|D_i=0]\n\\end{aligned}\n\\]\nWe can easily extend this the case where the model includes additional (discrete or continuous) covariates, as well as case where the variable takes on multiple discrete values."
  },
  {
    "objectID": "material-interpretation.html#discrete-continuous-models",
    "href": "material-interpretation.html#discrete-continuous-models",
    "title": "Interpretation of Linear Models",
    "section": "",
    "text": "If the outcome is discrete (\\(Y_i\\in\\{0,1\\}\\)) while the regressors are continuous, the resulting linear model is referred to as a linear probability model.\n\\[\nE[Y_i|X_i] = Pr(Y_i = 1|X_i) = X_i'\\beta\n\\] This is differentiable, since \\(X\\) is continuous and the same partial derivative interpretation follows.\n\\[\n\\beta_j = \\frac{\\partial Pr(Y_i=1|X_i)}{\\partial X_{ij}}\n\\]\nNote, the unit of \\(Y\\) is probability-points (\\(\\in[0,1]\\)), not %-points (\\(\\in[0,100]\\)). Of course, the conversion of units can be made by \\(\\times 100\\) to measure in %-points."
  },
  {
    "objectID": "material-interpretation.html#discrete-discrete-models",
    "href": "material-interpretation.html#discrete-discrete-models",
    "title": "Interpretation of Linear Models",
    "section": "",
    "text": "If both the outcome and regressor(s) are discrete, then the parameter identifies a difference in conditional probabilities, \\[\n\\beta_2 = Pr(Y_i|D_i=1) - Pr(Y_i=1|D_i=0)\n\\] Note, the unit of \\(Y\\) is probability-points (\\(\\in[0,1]\\)), not %-points (\\(\\in[0,100]\\))."
  },
  {
    "objectID": "material-interpretation.html#log-level-models",
    "href": "material-interpretation.html#log-level-models",
    "title": "Interpretation of Linear Models",
    "section": "",
    "text": "Consider the model,\n\\[\n  \\ln(Y_i) = X_i'\\beta + \\varepsilon_i\n\\] Then,\n\\[\n  X_i'\\beta = E[\\ln(Y_i)|X_i]\n\\]\n\\[\n\\beta_j = \\frac{\\partial E[\\ln(Y_i)|X_i]}{\\partial X_{ij}}\n\\]\nThe coefficient is therefore measured in log-units of \\(Y\\). The relation to a change in the (expected) level of \\(Y\\) is given by,\n\\[\n\\%\\Delta E[Y_i|X_i] = (exp(\\beta)-1)\\times 100\n\\] For reasonably small values of \\(\\beta\\) (i.e. within the range \\([-0.1,0.1]\\)) this can be approximated by,\n\\[\n\\%\\Delta E[Y_i|X_i] = \\beta\\times 100\n\\] A 1-unit change in \\(X_{i1}\\) is associated with a \\(\\beta_1\\) percent change in the expected value of \\(Y\\).\nThis referred to as a semi-elasticity."
  },
  {
    "objectID": "material-interpretation.html#level-log-models",
    "href": "material-interpretation.html#level-log-models",
    "title": "Interpretation of Linear Models",
    "section": "",
    "text": "If the regressor(s) is measure in log-units; for example,\n\\[\n  Y_i = \\beta_1 + \\beta_2 \\ln(X_i)_i + \\varepsilon_i\n\\] Then,\n\\[\n\\beta_2 = \\frac{\\partial E[Y_i|X_i]}{\\partial \\ln(X_{i})}\n\\]\nA 1 percent increase in \\(X\\) is given by \\(X\\times1.01\\). This is equivalent to a change in \\(\\ln(X)\\) of,\n\\[\n\\ln(X_i\\times1.01) - \\ln(X_i) = \\ln(1.01) \\approx 0.01\n\\] Thus, a 1 percent increase in the level of \\(X\\) is associated with a \\(\\beta_2/100\\) increase in the expected value of \\(Y\\). Or, more accurate\n\\[\n  \\Delta E[Y_i|X_i] = \\beta_2\\times \\ln(1.01)\n\\] This is also a semi-elasticity."
  },
  {
    "objectID": "material-interpretation.html#log-log-models",
    "href": "material-interpretation.html#log-log-models",
    "title": "Interpretation of Linear Models",
    "section": "",
    "text": "In models where both the outcome and regressor are log-transformed with an elasticity interpretation.\n\\[\n    \\ln(Y_i) = \\beta_1 + \\beta_2 \\ln(X_i)_i + \\varepsilon_i\n\\] \\[\n\\beta_2 = \\frac{\\partial E[\\ln(Y_i)|X_i]}{\\partial \\ln(X_{i})}\n\\] \\(\\beta_2\\) is a the % change in the expected value of \\(Y\\) from a 1 % change in \\(X\\)."
  },
  {
    "objectID": "problem-set-2-solutions.html",
    "href": "problem-set-2-solutions.html",
    "title": "Problem Set 2 (SOLUTIONS)",
    "section": "",
    "text": "This problem set will take you through some Stata commands to estimate simple regression equations with dummy variables. You will learn how to interpret the estimated coefficients and test some linear hypotheses. Interpretation of these coefficients will be useful when we do treatment evaluation models later in term 1.\nThe hypothesis tests discussed in this problem set include standard T-tests and F-tests, which is assumed undergraduate knowledge for this module.\nYou will need to download the dataset problemset2.dta, which is available on Moodle."
  },
  {
    "objectID": "problem-set-2-solutions.html#conditional-expectation-function",
    "href": "problem-set-2-solutions.html#conditional-expectation-function",
    "title": "Problem Set 2 (SOLUTIONS)",
    "section": "Conditional Expectation Function",
    "text": "Conditional Expectation Function\nConsider the Conditional Expectation Function (CEF), \\(E[Y_i|X_i]\\). If \\(X\\) takes on discrete values: \\(X_i\\in\\{x_1,x_2,...,x_m\\}\\), then\n\\[\n    E[Y_i|X_i] =  E[Y_i|X_i=x_1]\\cdot\\mathbf{1}\\{X_i = x_1\\}+...+E[Y_i|X_i=x_m]\\cdot\\mathbf{1}\\{X_i = x_m\\}\n\\] where \\(\\mathbf{1}\\{X_i = x_m\\}\\) is a dummy variable, \\(=1\\) when \\(X_i=x_m\\). Since the values of \\(X_i\\) are mutually exclusive there is no overlap of these dummy variables.\nNote, we do not need to assume that \\(X\\) is a single random variable. It can be a vector of random variables that takes on discrete values.\nWe can re-arrange this expression using anyone of the values of \\(X\\). The natural option is to choose the first, but this is arbitrary.\n\\[\n\\begin{aligned}\n    E[Y_i|X_i] =& E[Y_i|X_i=x_1]+ (E[Y_i|X_i=x_2]-E[Y_i|X_i=x_1])\\cdot\\mathbf{1}\\{X_i = x_2\\}+... \\\\\n&+(E[Y_i|X_i=x_m]-E[Y_i|X_i=x_1])\\cdot\\mathbf{1}\\{X_i = x_m\\}\n\\end{aligned}\n\\]\nSince, \\(E[Y_i|X_i = x_m]\\) is a constant (\\(X_i\\) is set to a specific value), we can express the CEF as a function that is linear in parameters.\n\\[\n    E[Y_i|X_i] = \\beta_1 + \\beta_2D_{i2} + ... + \\beta_m D_{im}\n\\]\nwhere \\(D_{im}=\\mathbf{1}\\{X_i = x_m\\}\\)."
  },
  {
    "objectID": "problem-set-2-solutions.html#preamble",
    "href": "problem-set-2-solutions.html#preamble",
    "title": "Problem Set 2 (SOLUTIONS)",
    "section": "Preamble",
    "text": "Preamble\n\n\n\n\n\nCreate a do-file for this problem set and include a preamble that sets the directory and opens the data. For example,\n\nclear \n//or, to remove all stored values (including macros, matrices, scalars, etc.) \n*clear all\n\n* Replace $rootdir with the relevant path to on your local harddrive.\ncd \"$rootdir/problem-sets/ps-2\"\n\ncap log close\nlog using problem-set-2-log.txt, replace\n\nuse problem-set-2-data.dta"
  },
  {
    "objectID": "problem-set-2-solutions.html#questions",
    "href": "problem-set-2-solutions.html#questions",
    "title": "Problem Set 2 (SOLUTIONS)",
    "section": "Questions",
    "text": "Questions\n1. Consider the \\(E[ln(Wage_i)|Gender_i]\\), where \\(Gender_i\\in\\{1 ``Male'', 2 ``Female''\\}\\). Show that this CEF implies a linear model,\n\\[\n    ln(Wage_i) = \\beta_1 + \\beta_2 D_{i2} + \\varepsilon_i\n\\]\nWhat do the parameters \\(\\beta_1\\) and \\(\\beta_2\\) imply?\n\\[\nE[ln(Wage_i)|G_i] = E[ln(Wage_i)|G_i=1]+ (E[ln(Wage_i)|G_i=2]-E[ln(Wage_i)|G_i=1])\\cdot\\mathbf{1}\\{G_i = 2\\}\n\\] \\[\n= \\beta_1+ \\beta_2D_{i2}\n\\] \\(\\Rightarrow E[\\varepsilon_i|D_{i2}]=0\\).\n2. Regress lwage (log wage) on just a set of binary indicators that will enable you to test the hypothesis that males and females are on average, paid the same wage, ceteris paribus. Test this hypothesis.\n\nreg lwage female\n\n* or\n\ngen male=1-female\nreg lwage male\n\n\n      Source |       SS           df       MS      Number of obs   =     4,165\n-------------+----------------------------------   F(1, 4163)      =    491.72\n       Model |  93.6914807         1  93.6914807   Prob &gt; F        =    0.0000\n    Residual |  793.213421     4,163  .190538895   R-squared       =    0.1056\n-------------+----------------------------------   Adj R-squared   =    0.1054\n       Total |  886.904902     4,164  .212993492   Root MSE        =    .43651\n\n------------------------------------------------------------------------------\n       lwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      female |  -.4744661   .0213967   -22.17   0.000     -.516415   -.4325171\n       _cons |   6.729774     .00718   937.29   0.000     6.715697     6.74385\n------------------------------------------------------------------------------\n\n      Source |       SS           df       MS      Number of obs   =     4,165\n-------------+----------------------------------   F(1, 4163)      =    491.72\n       Model |  93.6914807         1  93.6914807   Prob &gt; F        =    0.0000\n    Residual |  793.213421     4,163  .190538895   R-squared       =    0.1056\n-------------+----------------------------------   Adj R-squared   =    0.1054\n       Total |  886.904902     4,164  .212993492   Root MSE        =    .43651\n\n------------------------------------------------------------------------------\n       lwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        male |   .4744661   .0213967    22.17   0.000     .4325171     .516415\n       _cons |   6.255308    .020156   310.34   0.000     6.215791    6.294824\n------------------------------------------------------------------------------\n\n\nAlternatively, you could use Stata’s factor notation:\n\nreg lwage i.female\n\n//note: defaults to smallest value as base category. This can be changed as follows.\n\nreg lwage ib1.female\n\n\n      Source |       SS           df       MS      Number of obs   =     4,165\n-------------+----------------------------------   F(1, 4163)      =    491.72\n       Model |  93.6914807         1  93.6914807   Prob &gt; F        =    0.0000\n    Residual |  793.213421     4,163  .190538895   R-squared       =    0.1056\n-------------+----------------------------------   Adj R-squared   =    0.1054\n       Total |  886.904902     4,164  .212993492   Root MSE        =    .43651\n\n------------------------------------------------------------------------------\n       lwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n    1.female |  -.4744661   .0213967   -22.17   0.000     -.516415   -.4325171\n       _cons |   6.729774     .00718   937.29   0.000     6.715697     6.74385\n------------------------------------------------------------------------------\n\n      Source |       SS           df       MS      Number of obs   =     4,165\n-------------+----------------------------------   F(1, 4163)      =    491.72\n       Model |  93.6914807         1  93.6914807   Prob &gt; F        =    0.0000\n    Residual |  793.213421     4,163  .190538895   R-squared       =    0.1056\n-------------+----------------------------------   Adj R-squared   =    0.1054\n       Total |  886.904902     4,164  .212993492   Root MSE        =    .43651\n\n------------------------------------------------------------------------------\n       lwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n    0.female |   .4744661   .0213967    22.17   0.000     .4325171     .516415\n       _cons |   6.255308    .020156   310.34   0.000     6.215791    6.294824\n------------------------------------------------------------------------------\n\n\nIt is evident from the test p-value that the difference is statistically significantly. Note, the standard reg command assume homoskedastic SEs. If we believe that the variance varies of (log of) wages varies with gender, we should estimate heteroskedastic SEs. However, in this instance it will not make a difference to the conclusion.\n\nreg lwage female, r\n\n\nLinear regression                               Number of obs     =      4,165\n                                                F(1, 4163)        =     520.64\n                                                Prob &gt; F          =     0.0000\n                                                R-squared         =     0.1056\n                                                Root MSE          =     .43651\n\n------------------------------------------------------------------------------\n             |               Robust\n       lwage | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      female |  -.4744661   .0207939   -22.82   0.000    -.5152332   -.4336989\n       _cons |   6.729774   .0072089   933.53   0.000      6.71564    6.743907\n------------------------------------------------------------------------------\n\n\n3. Extend the specification in (2) that will enable you to test the hypothesis that there is no difference in the wages between the following gender-ethnicity groups. Begin by defining the following dummy variables:\n\nfemale_black = female\\(\\times\\)black\nmale_black = (1-female)\\(\\times\\)black\nfemale_nonblack = female\\(\\times\\)(1-black)\nmale_nonblack = (1-female)\\(\\times\\)(1-black)\n\n\ngen female_black=female*black\ngen female_nonblack=female*(1-black)\ngen male_black=(1-female)*black\ngen male_nonblack=(1-female)*(1-black)\n\nThen estimate the following regressions:\n\nlwage on female_black, female_nonblack, male_black, male_nonblack (without a constant: option nocons)\nlwage on female, black, female_black\nlwage on female_black, female_nonblack, male_black\n\nFor some of these exercises you may be able to use Stata’s factor notation. However, in some instances you will need to manually create the above dummy-variable interactions.\nIn each case, identify the base category and write down the parameters of the (implied) model in terms of conditional expectations.\n\n* (a)\nreg lwage female_black female_nonblack male_black male_nonblack\n\n// note, Stata has dropped one variable due to perfect collinearity\n\nreg lwage female_black female_nonblack male_black male_nonblack, nocons\n\nnote: male_black omitted because of collinearity.\n\n      Source |       SS           df       MS      Number of obs   =     4,165\n-------------+----------------------------------   F(3, 4161)      =    191.17\n       Model |  107.436063         3  35.8120209   Prob &gt; F        =    0.0000\n    Residual |  779.468839     4,161  .187327287   R-squared       =    0.1211\n-------------+----------------------------------   Adj R-squared   =    0.1205\n       Total |  886.904902     4,164  .212993492   Root MSE        =    .43281\n\n------------------------------------------------------------------------------\n       lwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nfemale_black |  -.4434409   .0523433    -8.47   0.000    -.5460617     -.34082\nfemale_non~k |  -.2101553   .0383456    -5.48   0.000    -.2853332   -.1349774\n  male_black |          0  (omitted)\nmale_nonbl~k |   .2239593   .0317691     7.05   0.000     .1616749    .2862436\n       _cons |   6.517691   .0309152   210.82   0.000     6.457081    6.578301\n------------------------------------------------------------------------------\n\n      Source |       SS           df       MS      Number of obs   =     4,165\n-------------+----------------------------------   F(4, 4161)      &gt;  99999.00\n       Model |  185756.485         4  46439.1213   Prob &gt; F        =    0.0000\n    Residual |  779.468839     4,161  .187327287   R-squared       =    0.9958\n-------------+----------------------------------   Adj R-squared   =    0.9958\n       Total |  186535.954     4,165  44.7865436   Root MSE        =    .43281\n\n------------------------------------------------------------------------------\n       lwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nfemale_black |    6.07425   .0422382   143.81   0.000     5.991441     6.15706\nfemale_non~k |   6.307536   .0226856   278.04   0.000      6.26306    6.352012\n  male_black |   6.517691   .0309152   210.82   0.000     6.457081    6.578301\nmale_nonbl~k |    6.74165   .0073159   921.51   0.000     6.727307    6.755993\n------------------------------------------------------------------------------\n\n\nThis model returns four parameter-estimates, each corresponding to the four gender-ethnicity groups. These are essentially conditional mean estimates.\n\n* (b)\nreg lwage female black female_black\n\n// or, using factor notation:\n\nreg lwage i.female##i.black\n\n\n      Source |       SS           df       MS      Number of obs   =     4,165\n-------------+----------------------------------   F(3, 4161)      =    191.17\n       Model |  107.436063         3  35.8120209   Prob &gt; F        =    0.0000\n    Residual |  779.468839     4,161  .187327287   R-squared       =    0.1211\n-------------+----------------------------------   Adj R-squared   =    0.1205\n       Total |  886.904902     4,164  .212993492   Root MSE        =    .43281\n\n------------------------------------------------------------------------------\n       lwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      female |  -.4341146   .0238361   -18.21   0.000     -.480846   -.3873832\n       black |  -.2239593   .0317691    -7.05   0.000    -.2862436   -.1616749\nfemale_black |  -.0093263    .057515    -0.16   0.871    -.1220865    .1034339\n       _cons |    6.74165   .0073159   921.51   0.000     6.727307    6.755993\n------------------------------------------------------------------------------\n\n      Source |       SS           df       MS      Number of obs   =     4,165\n-------------+----------------------------------   F(3, 4161)      =    191.17\n       Model |  107.436063         3  35.8120209   Prob &gt; F        =    0.0000\n    Residual |  779.468839     4,161  .187327287   R-squared       =    0.1211\n-------------+----------------------------------   Adj R-squared   =    0.1205\n       Total |  886.904902     4,164  .212993492   Root MSE        =    .43281\n\n------------------------------------------------------------------------------\n       lwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n    1.female |  -.4341146   .0238361   -18.21   0.000     -.480846   -.3873832\n     1.black |  -.2239593   .0317691    -7.05   0.000    -.2862436   -.1616749\n             |\nfemale#black |\n        1 1  |  -.0093263    .057515    -0.16   0.871    -.1220865    .1034339\n             |\n       _cons |    6.74165   .0073159   921.51   0.000     6.727307    6.755993\n------------------------------------------------------------------------------\n\n\nIn this model, we have the following:\n\n\\(\\beta_1 = E[ln(Wage_i)|F = 0,B = 0]\\)\n\\(\\beta_2 = E[ln(Wage_i)|F = 1,B = 0]-E[ln(Wage_i)|F = 0,B = 0]\\)\n\\(\\beta_3 = E[ln(Wage_i)|F = 0,B = 1]-E[ln(Wage_i)|F = 0,B = 0]\\)\n\\(\\beta_4 = (E[ln(Wage_i)|F = 1,B = 1]-E[ln(Wage_i)|F = 0,B = 1])-(E[ln(Wage_i)|F = 1,B = 0]-E[ln(Wage_i)|F = 0,B = 0])\\)\n\n\n* (c) \nreg lwage female_black female_nonblack male_black  \n\n// note, this one is harder to replicate using factor notation. \n\n\n      Source |       SS           df       MS      Number of obs   =     4,165\n-------------+----------------------------------   F(3, 4161)      =    191.17\n       Model |  107.436063         3  35.8120209   Prob &gt; F        =    0.0000\n    Residual |  779.468839     4,161  .187327287   R-squared       =    0.1211\n-------------+----------------------------------   Adj R-squared   =    0.1205\n       Total |  886.904902     4,164  .212993492   Root MSE        =    .43281\n\n------------------------------------------------------------------------------\n       lwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nfemale_black |  -.6674001   .0428671   -15.57   0.000    -.7514426   -.5833576\nfemale_non~k |  -.4341146   .0238361   -18.21   0.000     -.480846   -.3873832\n  male_black |  -.2239593   .0317691    -7.05   0.000    -.2862436   -.1616749\n       _cons |    6.74165   .0073159   921.51   0.000     6.727307    6.755993\n------------------------------------------------------------------------------\n\n\nIn this model, we have the following:\n\n\\(\\beta_1 = E[ln(Wage_i)|F = 0,B = 0]\\)\n\\(\\beta_2 = E[ln(Wage_i)|F = 1,B = 1]-E[ln(Wage_i)|F = 0,B = 0]\\)\n\\(\\beta_3 = E[ln(Wage_i)|F = 1,B = 0]-E[ln(Wage_i)|F = 0,B = 0]\\)\n\\(\\beta_4 = E[ln(Wage_i)|F = 0,B = 1]-E[ln(Wage_i)|F = 0,B = 0]\\)\n\n4. Compare the estimated coefficients with the sample average values for the lwage for the four subgroups. What do you see?\n\ntable female black, stat(mean lwage)\n\n\n------------------------------------------------\n               |               black            \n               |         0          1      Total\n---------------+--------------------------------\nfemale or male |                                \n  0            |   6.74165   6.517691   6.729774\n  1            |  6.307536    6.07425   6.255308\n  Total        |  6.700755   6.363002   6.676346\n------------------------------------------------\n\n\nYou can compute the coefficients from each model simply using these averages.\n5. In each of the above models, describe the null hypothesis you would test to evaluate whether there is a significant earnings difference between the earnings of black and non-black females.\n\n\\(H_0: \\beta_1 = \\beta_2\\)\n\\(H_0: \\beta_3 + \\beta_4 = 0\\)\n\\(H_0: \\beta_2 - \\beta_3 = 0\\)\n\n6. Verify your solution to 4 by performing a test using the three set of regression output. You can use the post-estimation test command.\n\nreg lwage female_black female_nonblack male_black male_nonblack, nocons\n\ntest female_black = female_nonblack\n\nreg lwage female black female_black\n\ntest female_black + black = 0\n\nreg lwage female_black female_nonblack male_black  \n\ntest female_black - female_nonblack = 0\n\n\n      Source |       SS           df       MS      Number of obs   =     4,165\n-------------+----------------------------------   F(4, 4161)      &gt;  99999.00\n       Model |  185756.485         4  46439.1213   Prob &gt; F        =    0.0000\n    Residual |  779.468839     4,161  .187327287   R-squared       =    0.9958\n-------------+----------------------------------   Adj R-squared   =    0.9958\n       Total |  186535.954     4,165  44.7865436   Root MSE        =    .43281\n\n------------------------------------------------------------------------------\n       lwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nfemale_black |    6.07425   .0422382   143.81   0.000     5.991441     6.15706\nfemale_non~k |   6.307536   .0226856   278.04   0.000      6.26306    6.352012\n  male_black |   6.517691   .0309152   210.82   0.000     6.457081    6.578301\nmale_nonbl~k |    6.74165   .0073159   921.51   0.000     6.727307    6.755993\n------------------------------------------------------------------------------\n\n ( 1)  female_black - female_nonblack = 0\n\n       F(  1,  4161) =   23.68\n            Prob &gt; F =    0.0000\n\n      Source |       SS           df       MS      Number of obs   =     4,165\n-------------+----------------------------------   F(3, 4161)      =    191.17\n       Model |  107.436063         3  35.8120209   Prob &gt; F        =    0.0000\n    Residual |  779.468839     4,161  .187327287   R-squared       =    0.1211\n-------------+----------------------------------   Adj R-squared   =    0.1205\n       Total |  886.904902     4,164  .212993492   Root MSE        =    .43281\n\n------------------------------------------------------------------------------\n       lwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      female |  -.4341146   .0238361   -18.21   0.000     -.480846   -.3873832\n       black |  -.2239593   .0317691    -7.05   0.000    -.2862436   -.1616749\nfemale_black |  -.0093263    .057515    -0.16   0.871    -.1220865    .1034339\n       _cons |    6.74165   .0073159   921.51   0.000     6.727307    6.755993\n------------------------------------------------------------------------------\n\n ( 1)  black + female_black = 0\n\n       F(  1,  4161) =   23.68\n            Prob &gt; F =    0.0000\n\n      Source |       SS           df       MS      Number of obs   =     4,165\n-------------+----------------------------------   F(3, 4161)      =    191.17\n       Model |  107.436063         3  35.8120209   Prob &gt; F        =    0.0000\n    Residual |  779.468839     4,161  .187327287   R-squared       =    0.1211\n-------------+----------------------------------   Adj R-squared   =    0.1205\n       Total |  886.904902     4,164  .212993492   Root MSE        =    .43281\n\n------------------------------------------------------------------------------\n       lwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nfemale_black |  -.6674001   .0428671   -15.57   0.000    -.7514426   -.5833576\nfemale_non~k |  -.4341146   .0238361   -18.21   0.000     -.480846   -.3873832\n  male_black |  -.2239593   .0317691    -7.05   0.000    -.2862436   -.1616749\n       _cons |    6.74165   .0073159   921.51   0.000     6.727307    6.755993\n------------------------------------------------------------------------------\n\n ( 1)  female_black - female_nonblack = 0\n\n       F(  1,  4161) =   23.68\n            Prob &gt; F =    0.0000\n\n\n7. In each case, test equality across all four gender-ethnicity groups. Again, you should get the same result.\n\nreg lwage female_black female_nonblack male_black male_nonblack, nocons\n\ntest female_black = female_nonblack = male_black = male_nonblack\n\nreg lwage female black female_black\n\ntest female_black = black = female = 0\n\nreg lwage female_black female_nonblack male_black  \n\ntest female_black = female_nonblack = male_black = 0\n\n\n      Source |       SS           df       MS      Number of obs   =     4,165\n-------------+----------------------------------   F(4, 4161)      &gt;  99999.00\n       Model |  185756.485         4  46439.1213   Prob &gt; F        =    0.0000\n    Residual |  779.468839     4,161  .187327287   R-squared       =    0.9958\n-------------+----------------------------------   Adj R-squared   =    0.9958\n       Total |  186535.954     4,165  44.7865436   Root MSE        =    .43281\n\n------------------------------------------------------------------------------\n       lwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nfemale_black |    6.07425   .0422382   143.81   0.000     5.991441     6.15706\nfemale_non~k |   6.307536   .0226856   278.04   0.000      6.26306    6.352012\n  male_black |   6.517691   .0309152   210.82   0.000     6.457081    6.578301\nmale_nonbl~k |    6.74165   .0073159   921.51   0.000     6.727307    6.755993\n------------------------------------------------------------------------------\n\n ( 1)  female_black - female_nonblack = 0\n ( 2)  female_black - male_black = 0\n ( 3)  female_black - male_nonblack = 0\n\n       F(  3,  4161) =  191.17\n            Prob &gt; F =    0.0000\n\n      Source |       SS           df       MS      Number of obs   =     4,165\n-------------+----------------------------------   F(3, 4161)      =    191.17\n       Model |  107.436063         3  35.8120209   Prob &gt; F        =    0.0000\n    Residual |  779.468839     4,161  .187327287   R-squared       =    0.1211\n-------------+----------------------------------   Adj R-squared   =    0.1205\n       Total |  886.904902     4,164  .212993492   Root MSE        =    .43281\n\n------------------------------------------------------------------------------\n       lwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      female |  -.4341146   .0238361   -18.21   0.000     -.480846   -.3873832\n       black |  -.2239593   .0317691    -7.05   0.000    -.2862436   -.1616749\nfemale_black |  -.0093263    .057515    -0.16   0.871    -.1220865    .1034339\n       _cons |    6.74165   .0073159   921.51   0.000     6.727307    6.755993\n------------------------------------------------------------------------------\n\n ( 1)  - black + female_black = 0\n ( 2)  - female + female_black = 0\n ( 3)  female_black = 0\n\n       F(  3,  4161) =  191.17\n            Prob &gt; F =    0.0000\n\n      Source |       SS           df       MS      Number of obs   =     4,165\n-------------+----------------------------------   F(3, 4161)      =    191.17\n       Model |  107.436063         3  35.8120209   Prob &gt; F        =    0.0000\n    Residual |  779.468839     4,161  .187327287   R-squared       =    0.1211\n-------------+----------------------------------   Adj R-squared   =    0.1205\n       Total |  886.904902     4,164  .212993492   Root MSE        =    .43281\n\n------------------------------------------------------------------------------\n       lwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nfemale_black |  -.6674001   .0428671   -15.57   0.000    -.7514426   -.5833576\nfemale_non~k |  -.4341146   .0238361   -18.21   0.000     -.480846   -.3873832\n  male_black |  -.2239593   .0317691    -7.05   0.000    -.2862436   -.1616749\n       _cons |    6.74165   .0073159   921.51   0.000     6.727307    6.755993\n------------------------------------------------------------------------------\n\n ( 1)  female_black - female_nonblack = 0\n ( 2)  female_black - male_black = 0\n ( 3)  female_black = 0\n\n       F(  3,  4161) =  191.17\n            Prob &gt; F =    0.0000\n\n\n8. Try to replicate the F-statistic for one of the above models. Hint, the F-stat for these models is the same as that of the whole model.\n\nreg lwage female black female_black\nereturn list\nscalar fstat = (e(r2)*e(df_r))/((1-e(r2))*e(df_m))\nscalar list fstat\n\n\n      Source |       SS           df       MS      Number of obs   =     4,165\n-------------+----------------------------------   F(3, 4161)      =    191.17\n       Model |  107.436063         3  35.8120209   Prob &gt; F        =    0.0000\n    Residual |  779.468839     4,161  .187327287   R-squared       =    0.1211\n-------------+----------------------------------   Adj R-squared   =    0.1205\n       Total |  886.904902     4,164  .212993492   Root MSE        =    .43281\n\n------------------------------------------------------------------------------\n       lwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      female |  -.4341146   .0238361   -18.21   0.000     -.480846   -.3873832\n       black |  -.2239593   .0317691    -7.05   0.000    -.2862436   -.1616749\nfemale_black |  -.0093263    .057515    -0.16   0.871    -.1220865    .1034339\n       _cons |    6.74165   .0073159   921.51   0.000     6.727307    6.755993\n------------------------------------------------------------------------------\n\nscalars:\n                  e(N) =  4165\n               e(df_m) =  3\n               e(df_r) =  4161\n                  e(F) =  191.1735422330181\n                 e(r2) =  .1211359442526956\n               e(rmse) =  .4328132235793649\n                e(mss) =  107.4360627542734\n                e(rss) =  779.4688391479763\n               e(r2_a) =  .1205023003768864\n                 e(ll) =  -2419.902951629166\n               e(ll_0) =  -2688.805870567022\n               e(rank) =  4\n\nmacros:\n            e(cmdline) : \"regress lwage female black female_black\"\n              e(title) : \"Linear regression\"\n          e(marginsok) : \"XB default\"\n                e(vce) : \"ols\"\n             e(depvar) : \"lwage\"\n                e(cmd) : \"regress\"\n         e(properties) : \"b V\"\n            e(predict) : \"regres_p\"\n              e(model) : \"ols\"\n          e(estat_cmd) : \"regress_estat\"\n\nmatrices:\n                  e(b) :  1 x 4\n                  e(V) :  4 x 4\n               e(beta) :  1 x 3\n\nfunctions:\n             e(sample)   \n     fstat =  191.17354\n\n\nIn the case where the F-test corresponds to the test of the entire model, you can write the F-statistic in terms of \\(R^2\\).\n9. Estimate the following model:\n\\[\n    lwage = \\beta_1 + \\beta_2F + \\beta_3B + \\beta_4F\\times B + \\beta_5exp + \\beta_6exp^2 + \\beta_7educ + \\varepsilon\n\\]\n\nInterpret the estimated coeffiecients \\(\\hat{\\beta}_7\\).\nInterpret the effect of experience variable exp. Use the median level of experience to make your calculation.\n\n\nsum educ, det\nreg lwage i.female##i.black exper expsq educ\n\ndi (exp(_b[educ])-1)*100\n\n\n                     years of education\n-------------------------------------------------------------\n      Percentiles      Smallest\n 1%            6              4\n 5%            8              4\n10%            9              4       Obs               4,165\n25%           12              4       Sum of wgt.       4,165\n\n50%           12                      Mean           12.84538\n                        Largest       Std. dev.      2.787995\n75%           16             17\n90%           17             17       Variance       7.772916\n95%           17             17       Skewness      -.2581161\n99%           17             17       Kurtosis        2.71273\n\n      Source |       SS           df       MS      Number of obs   =     4,165\n-------------+----------------------------------   F(6, 4158)      =    411.81\n       Model |     330.586         6  55.0976667   Prob &gt; F        =    0.0000\n    Residual |  556.318902     4,158   .13379483   R-squared       =    0.3727\n-------------+----------------------------------   Adj R-squared   =    0.3718\n       Total |  886.904902     4,164  .212993492   Root MSE        =    .36578\n\n------------------------------------------------------------------------------\n       lwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n    1.female |  -.4032047   .0203208   -19.84   0.000    -.4430444    -.363365\n     1.black |  -.1551546   .0269249    -5.76   0.000    -.2079417   -.1023674\n             |\nfemale#black |\n        1 1  |   -.002071   .0488405    -0.04   0.966    -.0978246    .0936826\n             |\n       exper |   .0427346   .0022404    19.07   0.000     .0383422     .047127\n       expsq |  -.0006982   .0000494   -14.14   0.000    -.0007951   -.0006014\n        educ |   .0731837   .0020983    34.88   0.000     .0690698    .0772975\n       _cons |   5.303667   .0362462   146.32   0.000     5.232606    5.374729\n------------------------------------------------------------------------------\n7.5928119\n\n\n\nA one unit increase in years of educ is associated with an increase of 7.59% in expected wages, holding other regressors fixed.\n\n\nsum exper, det\nreturn list\ndi (exp(_b[exper]+2*r(p50)*_b[expsq])-1)*100\n\n\n             years of full-time work experience\n-------------------------------------------------------------\n      Percentiles      Smallest\n 1%            3              1\n 5%            5              1\n10%            7              1       Obs               4,165\n25%           11              1       Sum of wgt.       4,165\n\n50%           18                      Mean           19.85378\n                        Largest       Std. dev.      10.96637\n75%           29             50\n90%           36             50       Variance       120.2613\n95%           39             50       Skewness       .4000014\n99%           44             51       Kurtosis       2.072064\n\nscalars:\n                  r(N) =  4165\n              r(sum_w) =  4165\n               r(mean) =  19.85378151260504\n                r(Var) =  120.2612759224727\n                 r(sd) =  10.96637022548814\n           r(skewness) =  .4000013893781186\n           r(kurtosis) =  2.072064120792274\n                r(sum) =  82691\n                r(min) =  1\n                r(max) =  51\n                 r(p1) =  3\n                 r(p5) =  5\n                r(p10) =  7\n                r(p25) =  11\n                r(p50) =  18\n                r(p75) =  29\n                r(p90) =  36\n                r(p95) =  39\n                r(p99) =  44\n1.7754427\n\n\n\nA one unit incease in years of experience is associated with an increase of 1.78% in expected wages, holding other regressors fixed.\n\n10. Theoretically, how would you test the following restrictions for the model below?\n\n\\(\\beta_2 = \\beta_3\\)\n\\(\\beta_4 + \\beta_5 = 1\\)\n\\(\\beta_2 = \\beta_3\\) and \\(\\beta_4 + \\beta_5 = 1\\)\n\n\\[\n    Y = \\beta_1 + \\beta_2X_2 + \\beta_3X_3 + \\beta_4X_4 + \\beta_5X_5 + \\varepsilon\n\\]\n\nRestrictions 1: \\(H_0: \\beta_2 = \\beta_3\\Rightarrow \\beta_2-\\beta_3 = 0\\). This can be written as a simple T-test (or F-tests),\n\n\\[\n\\text{T-stat}  = \\frac{\\hat{\\beta}_2-\\hat{\\beta}_3}{\\sqrt{\\hat{Var}(\\hat{\\beta}_2-\\hat{\\beta}_3)}}\n\\]\nwhere,\n\\[\nVar(\\hat{\\beta}_2-\\hat{\\beta}_3) = Var(\\hat{\\beta}_2) + Var(\\hat{\\beta}_3)-2\\cdot Cov(\\hat{\\beta}_2,\\hat{\\beta}_3)\n\\]\nAlternatively, rewrite the model, adding and subtracting \\(\\beta_3X_2\\) (or \\(\\beta_2X_3\\)):\n\\[\n    Y = \\beta_1 + (\\beta_2-\\beta_3)X_2 + \\beta_3(X_2+X_3) + \\beta_4X_4 + \\beta_5X_5 + \\varepsilon\n\\]\nThen test the hypothese that the coefficient on \\(X_2\\) is \\(=0\\).\n\nRestrictions 1: \\(H_0: \\beta_4 + \\beta_5 = 1\\). This can be written as a simple T-test,\n\n\\[\n\\text{T-stat} = \\frac{\\hat{\\beta}_4+\\hat{\\beta}_5-1}{\\sqrt{\\hat{Var}(\\hat{\\beta}_4+\\hat{\\beta}_5})}\n\\]\nwhere,\n\\[\nVar(\\hat{\\beta}_4+\\hat{\\beta}_5) = Var(\\hat{\\beta}_4) + Var(\\hat{\\beta}_5)+2\\cdot Cov(\\hat{\\beta}_4,\\hat{\\beta}_5)\n\\]\nAlternatively, rewrite the model, adding and subtracting \\(\\beta_5X_4-X_5\\) (or \\(\\beta_2X_3\\)):\n\\[\n    Y-X_4 = \\beta_1 + \\beta_2X_2 + \\beta_3X_3 + (\\beta_4+\\beta_5-1)X_4 + (\\beta_5)(X_5-X_4) + \\varepsilon\n\\]\nThen test the hypothese that the coefficient on \\(X_4\\) is \\(=0\\).\n\nTo test both of the linear restrictions simultaneously, we would use an F-test.\n\nStep 1: estimate the unrestricted model\n\\[\n    Y = \\beta_1 + \\beta_2X_2 + \\beta_3X_3 + \\beta_4X_4 + \\beta_5X_5 + \\varepsilon\n\\]\nStore the \\(SSR_U\\)\nStep 2: estimate the restricted model\n\\[\n    (Y-X_4) = \\gamma_1 + \\gamma_2(X_2+X_3) + \\gamma_5(X_5-X_4) + \\varepsilon\n\\]\nStore the \\(SSR_R\\).\nStep 3: Compute the F-statistic\n\\[\n\\text{F-stat} = \\frac{(SSR_R-SSR_U)/(df_R-df_U)}{SSR_U/df_U}\n\\]"
  },
  {
    "objectID": "problem-set-2-solutions.html#postamble",
    "href": "problem-set-2-solutions.html#postamble",
    "title": "Problem Set 2 (SOLUTIONS)",
    "section": "Postamble",
    "text": "Postamble\n\nlog close"
  },
  {
    "objectID": "problem-set-3-solutions.html",
    "href": "problem-set-3-solutions.html",
    "title": "Problem Set 3 (SOLUTIONS)",
    "section": "",
    "text": "The purpose of this problem set is for you to see how the ordinary least squares (OLS) estimator behaves under various assumptions in a linear regression model where you know what the model is – since you are going to be generating the data from a known data generating process (DGP).\nThe models estimated are simple bivariate regressions but the properties of the OLS estimator with vary with each case. This is demonstrated by changing the (a) distributional properties of the error term (variance-covariance structure), and (b) inducing correlation between the regressor and the error term. Any resulting bias and/or inconsistency will depend on the DGP.\nTo achieve certain results we will have to use a serially-correlated error structure, which is only appropriate in a time-series setting. For this reason, the models will be written with subscript \\(t\\) and not \\(i\\).\nThe code has been provided for model 1. You can then modify the code for models 2-4."
  },
  {
    "objectID": "problem-set-3-solutions.html#preamble",
    "href": "problem-set-3-solutions.html#preamble",
    "title": "Problem Set 3 (SOLUTIONS)",
    "section": "Preamble",
    "text": "Preamble\n\n\n\n\n\nYou do not need to load data for this problem set.\n\nclear \n//or, to remove all stored values (including macros, matrices, scalars, etc.) \n*clear all\n\n* Replace $rootdir with the relevant path to on your local harddrive.\ncd \"$rootdir/problem-sets/ps-3\"\n\ncap log close\nlog using problem-set-3-log.txt, replace\n\nHowever, since we are going to generate random variables, we should set a seed. This ensures replicability of the exercise. The number you choose is arbitrary, it simply ensures that any algorithms used to generate (pseudo) random variables start at the same place.\n\nset seed 981836"
  },
  {
    "objectID": "problem-set-3-solutions.html#model-1-clrm",
    "href": "problem-set-3-solutions.html#model-1-clrm",
    "title": "Problem Set 3 (SOLUTIONS)",
    "section": "Model 1: CLRM",
    "text": "Model 1: CLRM\nThis is your classical linear regression model. OLS estimator is unbiased and consistent.\n\\[\nY_t = \\beta_1 + \\beta_2X_t + \\upsilon_t \\qquad \\text{with}\\quad \\upsilon_t\\sim N(0,\\sigma^2)\n\\]\nWe know that the OLS estimator for \\(\\beta_2\\) is given by,\n\\[\n\\begin{aligned}\n  \\hat{\\beta}_2 =& \\frac{\\sum_t \\big[(X_t-\\bar{X})(Y_t-\\bar{Y})\\big]}{\\sum_t (X_t-\\bar{X})^2} \\\\\n  =& \\beta_2 + \\frac{\\sum_t \\big[(X_t-\\bar{X})(\\upsilon_t-\\bar{\\upsilon})\\big]}{\\sum_t (X_t-\\bar{X})^2} \\\\\n  =& \\beta_2 + \\frac{\\sum_t \\tilde{X}_t\\tilde{\\upsilon}_t}{\\sum_t \\tilde{X}_t^2}\n\\end{aligned}  \n\\] where \\(\\tilde{X}_t\\) and \\(\\tilde{\\upsilon}_t\\) represent the demeaned counterparts of these variables. Alternatively, using linear algebra notation:\n\\[\n\\begin{aligned}\n  \\hat{\\beta}_2 =& \\frac{X'M_{\\ell}Y}{X'M_{\\ell}X} \\\\\n  =& \\beta_2 + \\frac{X'M_{\\ell}\\upsilon}{X'M_{\\ell}X} \\\\\n  =& \\beta_2 + \\frac{\\tilde{X}'\\tilde{\\upsilon}}{\\tilde{X}'\\tilde{X}}\n\\end{aligned}  \n\\] where \\(\\tilde{X} = M_{\\ell}X\\), \\(\\tilde{\\upsilon}= M_{\\ell}\\upsilon\\), and \\(M_{\\ell} = I_n-\\ell(\\ell'\\ell)^{-1}\\ell'\\) (the orthogonal projection of the constant regressor).\nWe know from Handouts 2 & 3,\n\n\\(E[\\hat{\\beta}_2] = \\beta_2\\) (i.e., unbiased)\n\\(p \\lim \\hat{\\beta}_2 = \\beta_2\\) (i.e., consistent)\n\nCan you demonstrate these results?\n\nSimulation\nBegin by designing a programme that takes the parameters of the model as arguments, generates the data, estimates the model, and then returns the stored values.\n\ncap prog drop mc1\nprogram define mc1, rclass\n    syntax [, obs(integer 1) s(real 1) b1(real 0) b2(real 0)  sigma(real 1)]\n    drop _all\n    set obs `obs'\n    gen u = rnormal(0,`sigma')            // sigma is the std deviation of the error distribution\n    gen x=uniform()*`s'                   // s is the std devation of the x distribution\n    gen y=`b1'+`b2'*x + u                   // this generates the dep variable y\n    reg y x\n    return scalar b1=_b[_cons]            // intercept estimate\n    return scalar b2=_b[x]                  // coeff on the x variable\n    return scalar se2 = _se[x]            // std error\n    return scalar t2 = _b[x]/_se[x]     // t ratio\nend\n\n\n\n\nUse the the simulate command in Stata to estimate the model 100 times:\n\nsimulate b1=r(b1) b2=r(b2) se2=r(se2) t2=r(t2), reps(100): mc1, obs(50) s(6) b1(4) b2(2) sigma(3)\n\n\n      Command: mc1, obs(50) s(6) b1(4) b2(2) sigma(3)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\n\n\nCalculate the bias and plot the distribution of the bias.\n\ngen bias2=b2-2\nsu b1 b2 se2 t2\nsu bias2\nhistogram bias2, normal xline(`r(mean)')\n\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n          b1 |        100    3.880226    .9977415   1.080851   6.090028\n          b2 |        100    2.041985     .271704   1.365155   2.673303\n         se2 |        100    .2520885    .0291596   .1814694   .3255497\n          t2 |        100    8.216185      1.4968   5.484826   12.60699\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n       bias2 |        100    .0419852     .271704  -.6348448   .6733029\n(bin=10, start=-.63484478, width=.13081477)\n\n\n\n\n\n\n\n\n\nThe above simulation is a for a fixed sample size. To demonstrate consistency we need to repeat the exercise for larger and larger \\(n\\)’s.\n\ntempfile simdata\nforvalues n = 100(100)1000 {\n    simulate b1=r(b1) b2=r(b2) se2=r(se2) t2=r(t2), reps(100): mc1, obs(`n') s(6) b1(4) b2(2) sigma(3) \n    gen size = `n'\n    if `n'==100 save `simdata', replace\n    else {\n        append using `simdata'\n        save `simdata', replace\n    }\n}\ngen bias2=b2-2\nhistogram bias2, normal xline(0) by(size)\n\n\n      Command: mc1, obs(100) s(6) b1(4) b2(2) sigma(3)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\n(file C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_000005.tmp not found)\nfile C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_000005.tmp saved as .dta\n    format\n\n      Command: mc1, obs(200) s(6) b1(4) b2(2) sigma(3)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\nfile C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_000005.tmp saved as .dta\n    format\n\n      Command: mc1, obs(300) s(6) b1(4) b2(2) sigma(3)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\nfile C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_000005.tmp saved as .dta\n    format\n\n      Command: mc1, obs(400) s(6) b1(4) b2(2) sigma(3)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\nfile C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_000005.tmp saved as .dta\n    format\n\n      Command: mc1, obs(500) s(6) b1(4) b2(2) sigma(3)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\nfile C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_000005.tmp saved as .dta\n    format\n\n      Command: mc1, obs(600) s(6) b1(4) b2(2) sigma(3)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\nfile C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_000005.tmp saved as .dta\n    format\n\n      Command: mc1, obs(700) s(6) b1(4) b2(2) sigma(3)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\nfile C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_000005.tmp saved as .dta\n    format\n\n      Command: mc1, obs(800) s(6) b1(4) b2(2) sigma(3)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\nfile C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_000005.tmp saved as .dta\n    format\n\n      Command: mc1, obs(900) s(6) b1(4) b2(2) sigma(3)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\nfile C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_000005.tmp saved as .dta\n    format\n\n      Command: mc1, obs(1000) s(6) b1(4) b2(2) sigma(3)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\nfile C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_000005.tmp saved as .dta\n    format"
  },
  {
    "objectID": "problem-set-3-solutions.html#model-2-serial-correlation",
    "href": "problem-set-3-solutions.html#model-2-serial-correlation",
    "title": "Problem Set 3 (SOLUTIONS)",
    "section": "Model 2: Serial Correlation",
    "text": "Model 2: Serial Correlation\nRelax the assumption of an iid error term and allow for serial correlation. The OLS estimator is unbiased and consistent. However, the std errors are wrong since the software does not know that you have serially correlated errors and you are not taking this into account in the estimation.\n\\[\nY_t = \\beta_1 + \\beta_2X_t + \\upsilon_t \\qquad \\text{where}\\quad \\upsilon_t = \\rho \\upsilon_{t-1}+\\varepsilon_t \\quad\\text{and}\\quad \\varepsilon_t\\sim N(0,\\sigma^2)\n\\] We say that \\(U_t\\) follows an AR(1) process. You can show that \\(\\hat{\\beta}_2\\) remains unbiased and consistant. However, the standard homoskedastic-variance estimator is incorrect:\n\\[\nVar(\\hat{\\beta}_2) \\neq \\frac{\\sigma^2}{Var(X_t)}\n\\]\n\nSimulation\n\ncap prog drop mc2\nprogram define mc2, rclass\n    syntax [, obs(integer 1) s(real 1) b1(real 0) b2(real 0) bias2(real 0) sigma(real 1) rho(real 0)]\n    drop _all\n    set obs `obs'\n    gen u=0 \n    gen time=_n\n    tsset time\n    gen e = rnormal(0,`sigma')  \n    forvalues i=2/`obs'  {\n    replace u=`rho'*u[_n-1] + e if _n==`i'\n    }\n    gen x=uniform()*`s'\n    gen y=`b1'+`b2'*x + u\n    reg y x     \n    return scalar b1=_b[_cons]\n    return scalar b2=_b[x]\n    return scalar se2 = _se[x]\n    return scalar t2 = _b[x]/_se[x]\nend\n\nsimulate b1=r(b1) b2=r(b2) se2=r(se2) t2=r(t2), reps(100): mc2, obs(50) s(6) b1(4) b2(2) sigma(3) rho(0.2) \ngen bias2=b2-2\nsu b2 t2 se2\nsu bias2\nhistogram bias2, normal xline(`r(mean)')\n\n\n\n      Command: mc2, obs(50) s(6) b1(4) b2(2) sigma(3) rho(0.2)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n          b2 |        100    1.975448    .2406981   1.466939   2.656462\n          t2 |        100    8.010763      1.3815   4.681076   10.78957\n         se2 |        100    .2500731    .0286469   .2042868   .3313515\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n       bias2 |        100   -.0245523    .2406981  -.5330607   .6564617\n(bin=10, start=-.53306067, width=.11895224)\n\n\n\n\n\n\n\n\n\nTo demonstrate consistency we need to repeat the exercise for larger and larger \\(n\\)’s.\n\ntempfile simdata\nforvalues n = 100(100)1000 {\n    simulate b1=r(b1) b2=r(b2) se2=r(se2) t2=r(t2), reps(100): mc2, obs(`n') s(6) b1(4) b2(2) sigma(3) rho(0.2)\n    gen size = `n'\n    if `n'==100 save `simdata', replace\n    else {\n        append using `simdata'\n        save `simdata', replace\n    }\n}\ngen bias2=b2-2\nhistogram bias2, normal xline(0) by(size)\n\n\n      Command: mc2, obs(100) s(6) b1(4) b2(2) sigma(3) rho(0.2)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\n(file C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_000009.tmp not found)\nfile C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_000009.tmp saved as .dta\n    format\n\n      Command: mc2, obs(200) s(6) b1(4) b2(2) sigma(3) rho(0.2)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\nfile C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_000009.tmp saved as .dta\n    format\n\n      Command: mc2, obs(300) s(6) b1(4) b2(2) sigma(3) rho(0.2)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\nfile C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_000009.tmp saved as .dta\n    format\n\n      Command: mc2, obs(400) s(6) b1(4) b2(2) sigma(3) rho(0.2)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\nfile C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_000009.tmp saved as .dta\n    format\n\n      Command: mc2, obs(500) s(6) b1(4) b2(2) sigma(3) rho(0.2)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\nfile C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_000009.tmp saved as .dta\n    format\n\n      Command: mc2, obs(600) s(6) b1(4) b2(2) sigma(3) rho(0.2)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\nfile C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_000009.tmp saved as .dta\n    format\n\n      Command: mc2, obs(700) s(6) b1(4) b2(2) sigma(3) rho(0.2)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\nfile C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_000009.tmp saved as .dta\n    format\n\n      Command: mc2, obs(800) s(6) b1(4) b2(2) sigma(3) rho(0.2)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\nfile C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_000009.tmp saved as .dta\n    format\n\n      Command: mc2, obs(900) s(6) b1(4) b2(2) sigma(3) rho(0.2)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\nfile C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_000009.tmp saved as .dta\n    format\n\n      Command: mc2, obs(1000) s(6) b1(4) b2(2) sigma(3) rho(0.2)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\nfile C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_000009.tmp saved as .dta\n    format"
  },
  {
    "objectID": "problem-set-3-solutions.html#model-3-dynamic-model-without-serial-correlation",
    "href": "problem-set-3-solutions.html#model-3-dynamic-model-without-serial-correlation",
    "title": "Problem Set 3 (SOLUTIONS)",
    "section": "Model 3: Dynamic model without serial correlation",
    "text": "Model 3: Dynamic model without serial correlation\nConsider a version of Model 1, where the regressor is the lag of the dependent variable. \\[\nY_t = \\beta_1 + \\beta_2Y_{t-1} + \\upsilon_t \\qquad \\text{with}\\quad \\upsilon_t\\sim N(0,\\sigma^2)\n\\] The OLS estimator is now, \\[\n  \\hat{\\beta}_2 = \\beta_2 + \\frac{\\sum_t \\tilde{Y}_{t-1}\\tilde{\\upsilon}_t}{\\sum_t \\tilde{Y}_{t-1}^2}\n\\] This model is biased, since\n\\[\nE\\bigg[\\frac{\\sum_t \\tilde{Y}_{t-1}\\tilde{\\upsilon}_t}{\\sum_t \\tilde{Y}_{t-1}^2}\\bigg] \\neq \\frac{E\\big[\\sum_t \\tilde{Y}_{t-1}\\tilde{\\upsilon}_t\\big]}{E\\big[\\sum_t \\tilde{Y}_{t-1}^2\\big]}\n\\] When the regressor was \\(X_t\\), the above statement was true given the Law of Iterated Expectations. However, you can use Slutsky’s theorem and the WLLN to show that \\(\\hat{\\beta}_2\\rightarrow_p \\beta_2\\). This result relies on the fact that \\(Y_{t-1}\\) is realized before \\(\\upsilon_t\\) which is iid. Thus, the bias goes to 0 as \\(n\\rightarrow \\infty\\).\n\nSimulation\n\ncap prog drop mc3\nprogram define mc3, rclass\n    syntax [, obs(integer 1)  b1(real 0)  b2(real 0)  sigma(real 1)]\n    drop _all\n    set obs `obs'\n    gen y=0\n    gen u = rnormal(0,`sigma') \n    gen time=_n\n    tsset time\n    forvalues i=2/`obs'  {\n    replace y=`b1'+ `b2'* y[_n-1] + u  if _n==`i'\n    }\n    reg y  L.y\n    return scalar b1=_b[_cons]\n    return scalar b2=_b[L.y]\n    return scalar se2 = _se[L.y]\n  return scalar t2 = _b[L.y]/_se[L.y]\nend\n\n\nsimulate b1=r(b1) b2=r(b2) se2=r(se2) t2=r(t2), reps(100): mc3, obs(50) b1(4)  b2(0.2) sigma(3)  \ngen bias2=b2-0.2\nsum b2 t2 se2 \nsum bias2\nhistogram bias2, normal xline(`r(mean)')\n\n\n\n      Command: mc3, obs(50) b1(4) b2(0.2) sigma(3)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n          b2 |        100     .167145    .1275283  -.1321701   .4612297\n          t2 |        100    1.213486    .9468185  -.9446563   3.532131\n         se2 |        100    .1400517    .0042463   .1278178   .1539907\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n       bias2 |        100    -.032855    .1275283  -.3321701   .2612297\n(bin=10, start=-.33217007, width=.05933998)\n\n\n\n\n\n\n\n\n\nTo demonstrate consistency we need to repeat the exercise for larger and larger \\(n\\)’s.\n\ntempfile simdata\nforvalues n = 100(100)1000 {\n    simulate b1=r(b1) b2=r(b2) se2=r(se2) t2=r(t2), reps(100): mc3, obs(`n') b1(4) b2(0.2) sigma(3) \n    gen size = `n'\n    if `n'==100 save `simdata', replace\n    else {\n        append using `simdata'\n        save `simdata', replace\n    }\n}\ngen bias2=b2-0.2\nhistogram bias2, normal xline(0) by(size)\n\n\n      Command: mc3, obs(100) b1(4) b2(0.2) sigma(3)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\n(file C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_00000d.tmp not found)\nfile C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_00000d.tmp saved as .dta\n    format\n\n      Command: mc3, obs(200) b1(4) b2(0.2) sigma(3)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\nfile C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_00000d.tmp saved as .dta\n    format\n\n      Command: mc3, obs(300) b1(4) b2(0.2) sigma(3)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\nfile C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_00000d.tmp saved as .dta\n    format\n\n      Command: mc3, obs(400) b1(4) b2(0.2) sigma(3)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\nfile C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_00000d.tmp saved as .dta\n    format\n\n      Command: mc3, obs(500) b1(4) b2(0.2) sigma(3)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\nfile C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_00000d.tmp saved as .dta\n    format\n\n      Command: mc3, obs(600) b1(4) b2(0.2) sigma(3)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\nfile C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_00000d.tmp saved as .dta\n    format\n\n      Command: mc3, obs(700) b1(4) b2(0.2) sigma(3)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\nfile C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_00000d.tmp saved as .dta\n    format\n\n      Command: mc3, obs(800) b1(4) b2(0.2) sigma(3)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\nfile C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_00000d.tmp saved as .dta\n    format\n\n      Command: mc3, obs(900) b1(4) b2(0.2) sigma(3)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\nfile C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_00000d.tmp saved as .dta\n    format\n\n      Command: mc3, obs(1000) b1(4) b2(0.2) sigma(3)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\nfile C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_00000d.tmp saved as .dta\n    format"
  },
  {
    "objectID": "problem-set-3-solutions.html#model-4-dynamic-model-with-serial-correlation",
    "href": "problem-set-3-solutions.html#model-4-dynamic-model-with-serial-correlation",
    "title": "Problem Set 3 (SOLUTIONS)",
    "section": "Model 4: Dynamic model with serial correlation",
    "text": "Model 4: Dynamic model with serial correlation\nConsider a version of Model 2, where the regressor is the lag of the dependent variable. \\[\nY_t = \\beta_1 + \\beta_2Y_{t-1} + \\upsilon_t \\text{where}\\quad \\upsilon_t = \\rho \\upsilon_{t-1}+\\varepsilon_t \\quad\\text{and}\\quad \\varepsilon_t\\sim N(0,\\sigma^2)\n\\] As with model 3, the OLS estimator will be biased. In addition, since \\(Cov(\\upsilon_t,\\upsilon_{t-1})\\neq0\\) and \\(Cov(Y_t,\\upsilon_{t})\\neq 0\\) (for any \\(t\\)), \\[\n\\Rightarrow Cov(Y_{t-1},\\upsilon_{t})\\neq 0\n\\] As a result \\(\\hat{\\beta}_2\\) is inconsistent.\n\nSimulation\n\ncap prog drop mc4\nprogram define mc4, rclass\n    syntax [, obs(integer 1)  b1(real 0)  b2(real 0)  sigma(real 1) rho(real 0) ]\n    drop _all\n    set obs `obs'\n    gen y=0\n    gen u=0\n    gen e = rnormal(0,`sigma') \n    gen time=_n\n    tsset time\n    \n    forvalues i=2/`obs'  {\n    replace u=`rho'*u[_n-1] + e if _n==`i'\n    replace y=`b1'+ `b2'* y[_n-1] + u  if _n==`i'\n    }\n    reg y  L.y\n    return scalar b1=_b[_cons]\n    return scalar b2=_b[L.y]\n    return scalar se2 = _se[L.y]\n  return scalar t2 = _b[L.y]/_se[L.y]\nend\n\nsimulate b1=r(b1) b2=r(b2) se2=r(se2) t2=r(t2), reps(100): mc4, obs(50) b1(4) b2(0.2) sigma(3) rho(0.2)\ngen bias2=b2-0.2\nsum b2 t2 se2\nsum bias2\nhistogram bias2, normal xline(`r(mean)')\n\n\n\n      Command: mc4, obs(50) b1(4) b2(0.2) sigma(3) rho(0.2)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n          b2 |        100    .3424452    .1554043  -.1707231   .6607195\n          t2 |        100    2.666883    1.363437  -1.216084   6.210446\n         se2 |        100    .1323436    .0083629   .1063884   .1438691\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n       bias2 |        100    .1424452    .1554043  -.3707231   .4607195\n(bin=10, start=-.37072313, width=.08314427)\n\n\n\n\n\n\n\n\n\nTo demonstrate consistency we need to repeat the exercise for larger and larger \\(n\\)’s.\n\ntempfile simdata\nforvalues n = 100(100)1000 {\n    simulate b1=r(b1) b2=r(b2) se2=r(se2) t2=r(t2), reps(100): mc4, obs(`n') b1(4) b2(0.2) sigma(3) rho(0.2)\n    gen size = `n'\n    if `n'==100 save `simdata', replace\n    else {\n        append using `simdata'\n        save `simdata', replace\n    }\n}\ngen bias2=b2-0.2\nhistogram bias2, normal xline(0) by(size)\n\n\n      Command: mc4, obs(100) b1(4) b2(0.2) sigma(3) rho(0.2)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\n(file C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_00000h.tmp not found)\nfile C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_00000h.tmp saved as .dta\n    format\n\n      Command: mc4, obs(200) b1(4) b2(0.2) sigma(3) rho(0.2)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\nfile C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_00000h.tmp saved as .dta\n    format\n\n      Command: mc4, obs(300) b1(4) b2(0.2) sigma(3) rho(0.2)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\nfile C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_00000h.tmp saved as .dta\n    format\n\n      Command: mc4, obs(400) b1(4) b2(0.2) sigma(3) rho(0.2)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\nfile C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_00000h.tmp saved as .dta\n    format\n\n      Command: mc4, obs(500) b1(4) b2(0.2) sigma(3) rho(0.2)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\nfile C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_00000h.tmp saved as .dta\n    format\n\n      Command: mc4, obs(600) b1(4) b2(0.2) sigma(3) rho(0.2)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\nfile C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_00000h.tmp saved as .dta\n    format\n\n      Command: mc4, obs(700) b1(4) b2(0.2) sigma(3) rho(0.2)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\nfile C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_00000h.tmp saved as .dta\n    format\n\n      Command: mc4, obs(800) b1(4) b2(0.2) sigma(3) rho(0.2)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\nfile C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_00000h.tmp saved as .dta\n    format\n\n      Command: mc4, obs(900) b1(4) b2(0.2) sigma(3) rho(0.2)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\nfile C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_00000h.tmp saved as .dta\n    format\n\n      Command: mc4, obs(1000) b1(4) b2(0.2) sigma(3) rho(0.2)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\nfile C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_00000h.tmp saved as .dta\n    format"
  },
  {
    "objectID": "problem-set-3-solutions.html#postamble",
    "href": "problem-set-3-solutions.html#postamble",
    "title": "Problem Set 3 (SOLUTIONS)",
    "section": "Postamble",
    "text": "Postamble\n\nlog close"
  },
  {
    "objectID": "problem-set-4-solutions.html",
    "href": "problem-set-4-solutions.html",
    "title": "Problem Set 4 (SOLUTIONS)",
    "section": "",
    "text": "This problem set will revisit some of the material covered in Handouts 3 and 4. You will be required to work with a ‘raw’ dataset, downloaded from an online repository. For this reason, you should take care to check how the data is coded.\nYou will be using a version of the US Current Population Survey (CPS) called the Merged Outgoing Rotation Group (MORG). This data is compiled by the National Bureau of Economic Research (NBER) and has been used in many famous studies of the US economy. The CPS has a rather unique rotating panel design: “The monthly CPS is a rotating panel design; households are interviewed for four consecutive months, are not in the sample for the next eight months, and then are interviewed for four more consecutive months.” (source: IPUMS). The NBER’s MORG keeps only the outgoing rotation group’s observations.\nThe MORG .dta files can be found at: https://data.nber.org/morg/annual/.\n\n\n\n\n\n\n\nCreate a do-file for this problem set and include a preamble that sets the directory and opens the data directly from the NBER website. Of course, this requires a good internet connection. For example,\n\nclear \n//or, to remove all stored values (including macros, matrices, scalars, etc.) \n*clear all\n\n* Replace $rootdir with the relevant path to on your local harddrive.\ncd \"$rootdir/problem-sets/ps-4\"\n\ncap log close\nlog using problem-set-4-log.txt, replace\n\nuse \"https://data.nber.org/morg/annual/morg19.dta\", clear\n\nYou can, of course, download the data and open it locally on your computer.\n\n\n\n1. Create a new variable exper equal to age minus (years of education + 6). This is referred to as potential years of experience. Check how each variable defines missing values before proceeding. You will need to create a years of education variable for this. Here is he the suggested code:\n\ntab grade92, m\ngen eduyrs = .\n    replace eduyrs = .3 if grade92==31\n    replace eduyrs = 3.2 if grade92==32\n    replace eduyrs = 7.2 if grade92==33\n    replace eduyrs = 7.2 if grade92==34\n    replace eduyrs = 9  if grade92==35\n    replace eduyrs = 10 if grade92==36\n    replace eduyrs = 11 if grade92==37\n    replace eduyrs = 12 if grade92==38\n    replace eduyrs = 12 if grade92==39\n    replace eduyrs = 13 if grade92==40\n    replace eduyrs = 14 if grade92==41\n    replace eduyrs = 14 if grade92==42\n    replace eduyrs = 16 if grade92==43\n    replace eduyrs = 18 if grade92==44\n    replace eduyrs = 18 if grade92==45\n    replace eduyrs = 18 if grade92==46\n    lab var eduyrs \"completed education\"\ntab grade92, sum(eduyrs)\n\n\n    Highest |\n      grade |\n  completed |      Freq.     Percent        Cum.\n------------+-----------------------------------\n         31 |        814        0.28        0.28\n         32 |      1,495        0.51        0.79\n         33 |      3,071        1.05        1.85\n         34 |      4,123        1.41        3.26\n         35 |      5,244        1.80        5.06\n         36 |      7,824        2.69        7.75\n         37 |      9,271        3.18       10.93\n         38 |      4,226        1.45       12.38\n         39 |     82,795       28.41       40.79\n         40 |     50,112       17.20       57.99\n         41 |     12,392        4.25       62.24\n         42 |     16,161        5.55       67.79\n         43 |     59,438       20.40       88.19\n         44 |     25,374        8.71       96.89\n         45 |      3,785        1.30       98.19\n         46 |      5,265        1.81      100.00\n------------+-----------------------------------\n      Total |    291,390      100.00\n(291,390 missing values generated)\n(814 real changes made)\n(1,495 real changes made)\n(3,071 real changes made)\n(4,123 real changes made)\n(5,244 real changes made)\n(7,824 real changes made)\n(9,271 real changes made)\n(4,226 real changes made)\n(82,795 real changes made)\n(50,112 real changes made)\n(12,392 real changes made)\n(16,161 real changes made)\n(59,438 real changes made)\n(25,374 real changes made)\n(3,785 real changes made)\n(5,265 real changes made)\n\n    Highest |\n      grade |   Summary of completed education\n  completed |        Mean   Std. dev.       Freq.\n------------+------------------------------------\n         31 |   .30000001           0         814\n         32 |         3.2           0       1,495\n         33 |   7.1999998           0       3,071\n         34 |   7.1999998           0       4,123\n         35 |           9           0       5,244\n         36 |          10           0       7,824\n         37 |          11           0       9,271\n         38 |          12           0       4,226\n         39 |          12           0      82,795\n         40 |          13           0      50,112\n         41 |          14           0      12,392\n         42 |          14           0      16,161\n         43 |          16           0      59,438\n         44 |          18           0      25,374\n         45 |          18           0       3,785\n         46 |          18           0       5,265\n------------+------------------------------------\n      Total |   13.556855   2.7030576     291,390\n\n\n\ntab age, m\ngen exper = age-(eduyrs+6)\n\n\n        Age |      Freq.     Percent        Cum.\n------------+-----------------------------------\n         16 |      4,661        1.60        1.60\n         17 |      4,630        1.59        3.19\n         18 |      4,417        1.52        4.70\n         19 |      4,039        1.39        6.09\n         20 |      3,915        1.34        7.43\n         21 |      3,996        1.37        8.81\n         22 |      3,918        1.34       10.15\n         23 |      3,950        1.36       11.51\n         24 |      4,194        1.44       12.94\n         25 |      4,185        1.44       14.38\n         26 |      4,325        1.48       15.87\n         27 |      4,476        1.54       17.40\n         28 |      4,600        1.58       18.98\n         29 |      4,633        1.59       20.57\n         30 |      4,829        1.66       22.23\n         31 |      4,735        1.62       23.85\n         32 |      4,601        1.58       25.43\n         33 |      4,748        1.63       27.06\n         34 |      4,646        1.59       28.66\n         35 |      4,730        1.62       30.28\n         36 |      4,742        1.63       31.91\n         37 |      4,848        1.66       33.57\n         38 |      4,550        1.56       35.13\n         39 |      4,735        1.62       36.76\n         40 |      4,667        1.60       38.36\n         41 |      4,503        1.55       39.90\n         42 |      4,390        1.51       41.41\n         43 |      4,309        1.48       42.89\n         44 |      4,193        1.44       44.33\n         45 |      4,253        1.46       45.79\n         46 |      4,266        1.46       47.25\n         47 |      4,447        1.53       48.78\n         48 |      4,563        1.57       50.34\n         49 |      4,698        1.61       51.96\n         50 |      4,646        1.59       53.55\n         51 |      4,477        1.54       55.09\n         52 |      4,555        1.56       56.65\n         53 |      4,523        1.55       58.20\n         54 |      4,736        1.63       59.83\n         55 |      5,010        1.72       61.55\n         56 |      5,035        1.73       63.27\n         57 |      4,976        1.71       64.98\n         58 |      5,030        1.73       66.71\n         59 |      5,066        1.74       68.45\n         60 |      5,124        1.76       70.20\n         61 |      5,067        1.74       71.94\n         62 |      5,035        1.73       73.67\n         63 |      4,927        1.69       75.36\n         64 |      4,892        1.68       77.04\n         65 |      4,554        1.56       78.60\n         66 |      4,526        1.55       80.16\n         67 |      4,344        1.49       81.65\n         68 |      4,328        1.49       83.13\n         69 |      4,100        1.41       84.54\n         70 |      4,058        1.39       85.93\n         71 |      4,008        1.38       87.31\n         72 |      3,897        1.34       88.65\n         73 |      3,147        1.08       89.73\n         74 |      2,815        0.97       90.69\n         75 |      2,809        0.96       91.66\n         76 |      2,623        0.90       92.56\n         77 |      2,373        0.81       93.37\n         78 |      2,201        0.76       94.13\n         79 |      1,977        0.68       94.80\n         80 |      7,799        2.68       97.48\n         85 |      7,340        2.52      100.00\n------------+-----------------------------------\n      Total |    291,390      100.00\n\n\n2. Keep only those between the ages of 18 and 54. Check the distribution of `exper’ and replace any negative values to 0.\n\nkeep if inrange(age,18,54)\nsum exper, det\nreplace exper=0 if exper&lt;0\n\n(126,352 observations deleted)\n\n                            exper\n-------------------------------------------------------------\n      Percentiles      Smallest\n 1%            0             -6\n 5%            1             -5\n10%            2             -4       Obs             165,038\n25%            7             -4       Sum of wgt.     165,038\n\n50%           16                      Mean           16.50623\n                        Largest       Std. dev.      10.57401\n75%           25           47.7\n90%           31           47.7       Variance       111.8097\n95%           34           47.7       Skewness       .1296908\n99%           36           47.7       Kurtosis       1.887811\n(1,078 real changes made)\n\n\n3. Create a categorical variable that takes on 4 values: 1 “less than High School”; 2 “High School Diploma”; 3 “some Higher Education”; 4 “Bachelors”; 5 “Postgraduate”. This variable should be based on the the grade92 variable. You can find the value labels for this variable in this document: https://data.nber.org/morg/docs/cpsx.pdf. I suggest using the recode command, which allows you to create value labels while assigning values. Check the distributio of exper by education category.\n\nrecode grade92 (31/38 = 1 \"&lt;HS\") (39 = 2 \"HS\") (40/42 = 3 \"HS+\") (43 = 4 \"BA\") (44/46 = 5 \"PG\"), gen(educat)\ntab grade92 educat, m\n\ntab educat, sum(exper)\n\n(165,038 differences between grade92 and educat)\n\n   Highest |\n     grade |      RECODE of grade92 (Highest grade completed)\n completed |       &lt;HS         HS        HS+         BA         PG |     Total\n-----------+-------------------------------------------------------+----------\n        31 |       398          0          0          0          0 |       398 \n        32 |       578          0          0          0          0 |       578 \n        33 |     1,515          0          0          0          0 |     1,515 \n        34 |     1,571          0          0          0          0 |     1,571 \n        35 |     1,971          0          0          0          0 |     1,971 \n        36 |     2,198          0          0          0          0 |     2,198 \n        37 |     4,373          0          0          0          0 |     4,373 \n        38 |     2,440          0          0          0          0 |     2,440 \n        39 |         0     45,013          0          0          0 |    45,013 \n        40 |         0          0     30,934          0          0 |    30,934 \n        41 |         0          0      7,154          0          0 |     7,154 \n        42 |         0          0      9,708          0          0 |     9,708 \n        43 |         0          0          0     37,557          0 |    37,557 \n        44 |         0          0          0          0     14,804 |    14,804 \n        45 |         0          0          0          0      2,021 |     2,021 \n        46 |         0          0          0          0      2,803 |     2,803 \n-----------+-------------------------------------------------------+----------\n     Total |    15,044     45,013     47,796     37,557     19,628 |   165,038 \n\n  RECODE of |\n    grade92 |\n   (Highest |\n      grade |          Summary of exper\n completed) |        Mean   Std. dev.       Freq.\n------------+------------------------------------\n        &lt;HS |    19.29665    12.82301      15,044\n         HS |   17.726435   11.044222      45,013\n        HS+ |   15.643589   10.806949      47,796\n         BA |   15.376841   9.3440749      37,557\n         PG |   15.900092   8.1569821      19,628\n------------+------------------------------------\n      Total |   16.514468   10.560511     165,038\n\n\n4. Create the variable lnwage equal to the (natural) log of weekly earnings. Create a figure that shows the predicted linear fit of lwage against exper, by educat. Try to place all 5 fitted lines in the same graph.\n\ngen lnwage = ln(earnwke)\ntwoway (lfit lnwage exper if educat==1) (lfit lnwage exper if educat==2)  (lfit lnwage exper if educat==3) (lfit lnwage exper if educat==4) (lfit lnwage exper if educat==5) , legend(order(1 \"&lt;HS\" 2 \"HS\" 3 \"HS+\" 4 \"BA\" 5 \"PG\") pos(6) r(1)) xtitle(Years of experience) \n\n(49,686 missing values generated)\n\n\n\n\n\n\n\n\n\n5. Estimate a linear regression model that allows the slope coefficient on exper and constant term to vary by education category (educat). Let the base (excluded) education category be 2 “High School diploma”.\n\\[\n  \\ln(Wage_i) = \\alpha + \\sum_{j\\neq2}\\psi_j \\mathbf{1}\\{Educat_i=j\\} + \\beta Exper_i + \\sum_{j\\neq2}\\gamma_j Exper_i\\times\\mathbf{1}\\{Educat_i=j\\}+\\upsilon_i\n\\]\n\nreg lnwage ib2.educat##c.exper\n\n\n      Source |       SS           df       MS      Number of obs   =   115,352\n-------------+----------------------------------   F(9, 115342)    =   4039.95\n       Model |  17435.5509         9  1937.28343   Prob &gt; F        =    0.0000\n    Residual |  55310.0589   115,342  .479530951   R-squared       =    0.2397\n-------------+----------------------------------   Adj R-squared   =    0.2396\n       Total |  72745.6098   115,351   .63064568   Root MSE        =    .69248\n\n------------------------------------------------------------------------------\n      lnwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      educat |\n        &lt;HS  |  -.3882762   .0178644   -21.73   0.000    -.4232902   -.3532622\n        HS+  |  -.0839435   .0104229    -8.05   0.000    -.1043722   -.0635149\n         BA  |   .6145964   .0109763    55.99   0.000     .5930831    .6361097\n         PG  |   .9055692   .0141961    63.79   0.000      .877745    .9333933\n             |\n       exper |   .0182621   .0003709    49.24   0.000     .0175351    .0189891\n             |\n      educat#|\n     c.exper |\n        &lt;HS  |    .001037   .0007627     1.36   0.174     -.000458    .0025319\n        HS+  |   .0095156   .0005186    18.35   0.000     .0084991    .0105321\n         BA  |  -.0032067   .0005743    -5.58   0.000    -.0043324   -.0020811\n         PG  |  -.0051215   .0007698    -6.65   0.000    -.0066302   -.0036128\n             |\n       _cons |   6.101809   .0077485   787.48   0.000     6.086622    6.116996\n------------------------------------------------------------------------------\n\n\n6. Show that after 13 years of experience, those with some Higer Education (but no Bachelors), out earn those with just a high school diploma. You can assume that there are is a 2 year difference between the experience (education).\n\ndis _b[exper]*14\ndis (_b[exper] + _b[3.educat#exper])*12 + _b[3.educat]\n\n\ndis _b[exper]*15\ndis (_b[exper] + _b[3.educat#exper])*13 + _b[3.educat]\n\n.25566943\n.24938901\n.27393153\n.27716672\n\n\n7. Use the post-estimation test command to test the null hypothesis: \\(H_0: 15\\beta = 13(\\beta+\\gamma_3)+\\psi_3\\).\n\ntest exper*15 = (exper+3.educat#exper)*13+3.educat\n\n\n ( 1)  - 3.educat + 2*exper - 13*3.educat#c.exper = 0\n\n       F(  1,115342) =    0.32\n            Prob &gt; F =    0.5734\n\n\n8. Estimate a transformed version of the above model allowing you to test the above hypothesis using the coefficient from a single regressor. That is, the resulting test should be a simple t-test of \\(H_0: \\phi=0\\), where \\(\\phi\\) is the coefficient on the interaction of exper and a dummy variable for educat=3. This will be easier to do if you estimate the model using only the relevant sample: those with High School diplomas and some Higher Education. I suggest avoiding the use of factor notation to create the dummy variables and interaction terms for this exercise. For example, the following should replicate the relevant coefficients from Q5.\n\ngen hasHE = educat==3 if inlist(educat,2,3)\ngen hasHEexp = hasHE*exper\n\nreg lnwage exper hasHE hasHEexp\n\n(72,229 missing values generated)\n(72,229 missing values generated)\n\n      Source |       SS           df       MS      Number of obs   =    62,811\n-------------+----------------------------------   F(3, 62807)     =   2860.46\n       Model |  4000.16053         3  1333.38684   Prob &gt; F        =    0.0000\n    Residual |  29277.0845    62,807  .466143655   R-squared       =    0.1202\n-------------+----------------------------------   Adj R-squared   =    0.1202\n       Total |  33277.2451    62,810  .529808073   Root MSE        =    .68275\n\n------------------------------------------------------------------------------\n      lnwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       exper |   .0182621   .0003657    49.94   0.000     .0175453    .0189789\n       hasHE |  -.0839435   .0102764    -8.17   0.000    -.1040852   -.0638019\n    hasHEexp |   .0095156   .0005113    18.61   0.000     .0085134    .0105178\n       _cons |   6.101809   .0076396   798.71   0.000     6.086835    6.116782\n------------------------------------------------------------------------------\n\n\n\ngen experR = exper+hasHEexp*2/13\ngen hasHER = hasHE-hasHEexp/13\nreg lnwage experR hasHER hasHEexp\n\n(72,229 missing values generated)\n(72,229 missing values generated)\n\n      Source |       SS           df       MS      Number of obs   =    62,811\n-------------+----------------------------------   F(3, 62807)     =   2860.46\n       Model |  4000.16052         3  1333.38684   Prob &gt; F        =    0.0000\n    Residual |  29277.0845    62,807  .466143655   R-squared       =    0.1202\n-------------+----------------------------------   Adj R-squared   =    0.1202\n       Total |  33277.2451    62,810  .529808073   Root MSE        =    .68275\n\n------------------------------------------------------------------------------\n      lnwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      experR |   .0182621   .0003657    49.94   0.000     .0175453    .0189789\n      hasHER |  -.0839435   .0102764    -8.17   0.000    -.1040852   -.0638019\n    hasHEexp |   .0002489   .0004358     0.57   0.568    -.0006053     .001103\n       _cons |   6.101809   .0076396   798.71   0.000     6.086835    6.116782\n------------------------------------------------------------------------------\n\n\n9. Verify that the F-statistic from Q7 is the square of the above T-statistic.\n\ndis (_b[hasHEexp]/_se[hasHEexp])^2\n\n.32610143\n\n\n10. Use the restricted OLS approach to replicate the F-statistic and p-value from Q7.\n\nreg lnwage exper hasHE hasHEexp\nscalar RSSu = e(rss)\nscalar DOFu = e(df_r)\nreg lnwage experR hasHER \nscalar RSSr = e(rss)\nscalar DOFr = e(df_r)\n\nscalar Fstat = ((RSSr-RSSu)/(DOFr-DOFu))/(RSSu/DOFu)\nscalar pval = Ftail(1,DOFu,Fstat)\n\nscalar list Fstat pval\n\n\n      Source |       SS           df       MS      Number of obs   =    62,811\n-------------+----------------------------------   F(3, 62807)     =   2860.46\n       Model |  4000.16053         3  1333.38684   Prob &gt; F        =    0.0000\n    Residual |  29277.0845    62,807  .466143655   R-squared       =    0.1202\n-------------+----------------------------------   Adj R-squared   =    0.1202\n       Total |  33277.2451    62,810  .529808073   Root MSE        =    .68275\n\n------------------------------------------------------------------------------\n      lnwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       exper |   .0182621   .0003657    49.94   0.000     .0175453    .0189789\n       hasHE |  -.0839435   .0102764    -8.17   0.000    -.1040852   -.0638019\n    hasHEexp |   .0095156   .0005113    18.61   0.000     .0085134    .0105178\n       _cons |   6.101809   .0076396   798.71   0.000     6.086835    6.116782\n------------------------------------------------------------------------------\n\n      Source |       SS           df       MS      Number of obs   =    62,811\n-------------+----------------------------------   F(2, 62808)     =   4290.58\n       Model |  4000.00851         2  2000.00425   Prob &gt; F        =    0.0000\n    Residual |  29277.2366    62,808  .466138654   R-squared       =    0.1202\n-------------+----------------------------------   Adj R-squared   =    0.1202\n       Total |  33277.2451    62,810  .529808073   Root MSE        =    .68274\n\n------------------------------------------------------------------------------\n      lnwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      experR |   .0182233   .0003593    50.71   0.000      .017519    .0189277\n      hasHER |  -.0882792   .0069253   -12.75   0.000    -.1018527   -.0747056\n       _cons |   6.104077   .0065255   935.42   0.000     6.091287    6.116867\n------------------------------------------------------------------------------\n     Fstat =  .32612066\n      pval =   .5679544\n\n\n11. Use the restricted OLS approach to test the following hypothesis corresponding to the model in Q5:\n\\[\nH_0: \\gamma_j = 0\\qquad \\text{for}\\quad j=1,3,4,5\n\\] Compute the F-statistic and p-value. Verify your result using the post-estimation test command.\n\nreg lnwage ib2.educat##c.exper\nscalar RSSu = e(rss)\nscalar DOFu = e(df_r)\nreg lnwage ib2.educat exper \nscalar RSSr = e(rss)\nscalar DOFr = e(df_r)\n\nscalar Fstat = ((RSSr-RSSu)/(DOFr-DOFu))/(RSSu/DOFu)\nscalar pval = Ftail(DOFr-DOFu,DOFu,Fstat)\n\nscalar list Fstat pval\n\n** verify\nreg lnwage ib2.educat##c.exper\ntest 1.educat#exper 3.educat#exper 4.educat#exper 5.educat#exper\n\n\n      Source |       SS           df       MS      Number of obs   =   115,352\n-------------+----------------------------------   F(9, 115342)    =   4039.95\n       Model |  17435.5509         9  1937.28343   Prob &gt; F        =    0.0000\n    Residual |  55310.0589   115,342  .479530951   R-squared       =    0.2397\n-------------+----------------------------------   Adj R-squared   =    0.2396\n       Total |  72745.6098   115,351   .63064568   Root MSE        =    .69248\n\n------------------------------------------------------------------------------\n      lnwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      educat |\n        &lt;HS  |  -.3882762   .0178644   -21.73   0.000    -.4232902   -.3532622\n        HS+  |  -.0839435   .0104229    -8.05   0.000    -.1043722   -.0635149\n         BA  |   .6145964   .0109763    55.99   0.000     .5930831    .6361097\n         PG  |   .9055692   .0141961    63.79   0.000      .877745    .9333933\n             |\n       exper |   .0182621   .0003709    49.24   0.000     .0175351    .0189891\n             |\n      educat#|\n     c.exper |\n        &lt;HS  |    .001037   .0007627     1.36   0.174     -.000458    .0025319\n        HS+  |   .0095156   .0005186    18.35   0.000     .0084991    .0105321\n         BA  |  -.0032067   .0005743    -5.58   0.000    -.0043324   -.0020811\n         PG  |  -.0051215   .0007698    -6.65   0.000    -.0066302   -.0036128\n             |\n       _cons |   6.101809   .0077485   787.48   0.000     6.086622    6.116996\n------------------------------------------------------------------------------\n\n      Source |       SS           df       MS      Number of obs   =   115,352\n-------------+----------------------------------   F(5, 115346)    =   7085.67\n       Model |  17093.4651         5  3418.69301   Prob &gt; F        =    0.0000\n    Residual |  55652.1448   115,346  .482480058   R-squared       =    0.2350\n-------------+----------------------------------   Adj R-squared   =    0.2349\n       Total |  72745.6098   115,351   .63064568   Root MSE        =    .69461\n\n------------------------------------------------------------------------------\n      lnwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      educat |\n        &lt;HS  |  -.3724213   .0090073   -41.35   0.000    -.3900754   -.3547671\n        HS+  |   .0726614   .0055618    13.06   0.000     .0617604    .0835624\n         BA  |   .5714202   .0057563    99.27   0.000     .5601379    .5827025\n         PG  |   .8295498   .0068114   121.79   0.000     .8161996    .8428999\n             |\n       exper |   .0201711   .0002025    99.60   0.000     .0197742    .0205681\n       _cons |   6.067685   .0054116  1121.24   0.000     6.057079    6.078292\n------------------------------------------------------------------------------\n     Fstat =  178.34399\n      pval =  1.32e-152\n\n      Source |       SS           df       MS      Number of obs   =   115,352\n-------------+----------------------------------   F(9, 115342)    =   4039.95\n       Model |  17435.5509         9  1937.28343   Prob &gt; F        =    0.0000\n    Residual |  55310.0589   115,342  .479530951   R-squared       =    0.2397\n-------------+----------------------------------   Adj R-squared   =    0.2396\n       Total |  72745.6098   115,351   .63064568   Root MSE        =    .69248\n\n------------------------------------------------------------------------------\n      lnwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      educat |\n        &lt;HS  |  -.3882762   .0178644   -21.73   0.000    -.4232902   -.3532622\n        HS+  |  -.0839435   .0104229    -8.05   0.000    -.1043722   -.0635149\n         BA  |   .6145964   .0109763    55.99   0.000     .5930831    .6361097\n         PG  |   .9055692   .0141961    63.79   0.000      .877745    .9333933\n             |\n       exper |   .0182621   .0003709    49.24   0.000     .0175351    .0189891\n             |\n      educat#|\n     c.exper |\n        &lt;HS  |    .001037   .0007627     1.36   0.174     -.000458    .0025319\n        HS+  |   .0095156   .0005186    18.35   0.000     .0084991    .0105321\n         BA  |  -.0032067   .0005743    -5.58   0.000    -.0043324   -.0020811\n         PG  |  -.0051215   .0007698    -6.65   0.000    -.0066302   -.0036128\n             |\n       _cons |   6.101809   .0077485   787.48   0.000     6.086622    6.116996\n------------------------------------------------------------------------------\n\n ( 1)  1.educat#c.exper = 0\n ( 2)  3.educat#c.exper = 0\n ( 3)  4.educat#c.exper = 0\n ( 4)  5.educat#c.exper = 0\n\n       F(  4,115342) =  178.34\n            Prob &gt; F =    0.0000\n\n\n12. Compute the relevant Chi-squared distributed test statistic and corresponding p-value for the above test, assuming \\(n\\) is large (enough).\n\nscalar Cstat = Fstat*(DOFr-DOFu)\nscalar pval = chi2tail(DOFr-DOFu,Cstat)\nscalar list Cstat pval\n\n     Cstat =  713.37597\n      pval =  4.42e-153\n\n\n13. Using the data from Problem Set 2, estimate the simple linear regression model using OLS,\n\\[\n  \\ln(Wage_i) = \\beta_0 + \\beta_1 Educ_i + \\beta_2 Female_i + \\varepsilon_i\n\\]\n\nuse \"$rootdir/problem-sets/ps-2/problem-set-2-data.dta\", clear\nreg lwage educ female\nest sto ols\nestadd scalar sigma = e(rmse)\n\n(PSID wage data 1976-82 from Baltagi and Khanti-Akom (1990))\n\n      Source |       SS           df       MS      Number of obs   =     4,165\n-------------+----------------------------------   F(2, 4162)      =    732.99\n       Model |  231.021419         2   115.51071   Prob &gt; F        =    0.0000\n    Residual |  655.883483     4,162  .157588535   R-squared       =    0.2605\n-------------+----------------------------------   Adj R-squared   =    0.2601\n       Total |  886.904902     4,164  .212993492   Root MSE        =    .39697\n\n------------------------------------------------------------------------------\n       lwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        educ |   .0651382   .0022066    29.52   0.000     .0608121    .0694642\n      female |  -.4737645   .0194589   -24.35   0.000    -.5119143   -.4356147\n       _cons |    5.89297   .0290891   202.58   0.000      5.83594        5.95\n------------------------------------------------------------------------------\n\nadded scalar:\n              e(sigma) =  .39697422\n\n\n14. Estimate the Mincer equation using Maximum Likelihood. Take a look at https://www.stata.com/manuals13/rmlexp.pdf, the documentation for the mlexp command. It has a discussion on estimating the CLRM using ML.1\n\nmlexp (ln(normalden(lwage, {xb: educ female _cons}, exp({theta}))))\nereturn list\nnlcom (sigma: exp(_b[/theta]))\nestadd scalar sigma = r(b)[1,1]\neststo ml\n\n\nInitial:      Log likelihood = -97095.356\nAlternative:  Log likelihood = -35297.969\nRescale:      Log likelihood = -12999.606\nRescale eq:   Log likelihood = -7350.6222\nIteration 0:  Log likelihood = -7350.6222  (not concave)\nIteration 1:  Log likelihood =  -3936.054  \nIteration 2:  Log likelihood = -2187.1092  (backed up)\nIteration 3:  Log likelihood = -2073.0429  \nIteration 4:  Log likelihood = -2060.4271  \nIteration 5:  Log likelihood = -2060.4019  \nIteration 6:  Log likelihood = -2060.4019  \n\nMaximum likelihood estimation\n\nLog likelihood = -2060.4019                              Number of obs = 4,165\n\n------------------------------------------------------------------------------\n             | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nxb           |\n        educ |   .0651382   .0022058    29.53   0.000      .060815    .0694614\n      female |  -.4737645   .0194519   -24.36   0.000    -.5118895   -.4356396\n       _cons |    5.89297   .0290786   202.66   0.000     5.835977    5.949963\n-------------+----------------------------------------------------------------\n      /theta |  -.9242442   .0109566   -84.35   0.000    -.9457188   -.9027696\n------------------------------------------------------------------------------\n\nscalars:\n               e(rank) =  4\n                  e(N) =  4165\n                 e(ic) =  6\n                  e(k) =  4\n               e(k_eq) =  2\n          e(converged) =  1\n                 e(rc) =  0\n                 e(ll) =  -2060.40189888505\n              e(k_aux) =  1\n               e(df_m) =  4\n         e(k_eq_model) =  0\n\nmacros:\n            e(cmdline) : \"mlexp (ln(normalden(lwage, {xb: educ female _cons..\"\n                e(cmd) : \"mlexp\"\n            e(predict) : \"mlexp_p\"\n          e(estat_cmd) : \"mlexp_estat\"\n       e(marginsnotok) : \"SCores\"\n          e(marginsok) : \"default xb\"\n        e(marginsprop) : \"nochainrule\"\n               e(lexp) : \"ln(normalden(lwage,{xb:},exp({theta:})))\"\n             e(params) : \"xb:educ xb:female xb:_cons theta:_cons\"\n                e(opt) : \"moptimize\"\n                e(vce) : \"oim\"\n          e(ml_method) : \"lf0\"\n          e(technique) : \"nr\"\n         e(properties) : \"b V\"\n\nmatrices:\n                  e(b) :  1 x 4\n                  e(V) :  4 x 4\n               e(init) :  1 x 4\n               e(ilog) :  1 x 20\n           e(gradient) :  1 x 4\n\nfunctions:\n             e(sample)   \n\n       sigma: exp(_b[/theta])\n\n------------------------------------------------------------------------------\n             | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       sigma |   .3968312   .0043479    91.27   0.000     .3883094     .405353\n------------------------------------------------------------------------------\n\nadded scalar:\n              e(sigma) =  .39683123\n\n\n15. Estimate the Mincer equation using Method of Moments. You can use the gmm command in Stata. Hint: the regressors will be their own instruments and use the onestep option.2\n\ngmm (lwage - {xb: educ female _cons}), instruments(educ female) onestep\neststo mm\n\nesttab ols ml mm, se drop(theta:_cons) scalar(N sigma) mtitle(OLS ML MM)\n\n\nStep 1\nIteration 0:  GMM criterion Q(b) =  44.629069  \nIteration 1:  GMM criterion Q(b) =  2.101e-24  \nIteration 2:  GMM criterion Q(b) =  1.368e-31  \n\nnote: model is exactly identified.\n\nGMM estimation \n\nNumber of parameters =   3\nNumber of moments    =   3\nInitial weight matrix: Unadjusted                 Number of obs   =      4,165\n\n------------------------------------------------------------------------------\n             |               Robust\n             | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        educ |   .0651382   .0023187    28.09   0.000     .0605935    .0696828\n      female |  -.4737645   .0177811   -26.64   0.000    -.5086148   -.4389143\n       _cons |    5.89297   .0300924   195.83   0.000      5.83399     5.95195\n------------------------------------------------------------------------------\nInstruments for equation 1: educ female _cons\n\n------------------------------------------------------------\n                      (1)             (2)             (3)   \n                      OLS              ML              MM   \n------------------------------------------------------------\nmain                                                        \neduc               0.0651***       0.0651***       0.0651***\n                (0.00221)       (0.00221)       (0.00232)   \n\nfemale             -0.474***       -0.474***       -0.474***\n                 (0.0195)        (0.0195)        (0.0178)   \n\n_cons               5.893***        5.893***        5.893***\n                 (0.0291)        (0.0291)        (0.0301)   \n------------------------------------------------------------\nN                    4165            4165            4165   \nsigma               0.397           0.397                   \n------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001\n\n\n\n\n\n\nlog close"
  },
  {
    "objectID": "problem-set-4-solutions.html#preamble",
    "href": "problem-set-4-solutions.html#preamble",
    "title": "Problem Set 4 (SOLUTIONS)",
    "section": "",
    "text": "Create a do-file for this problem set and include a preamble that sets the directory and opens the data directly from the NBER website. Of course, this requires a good internet connection. For example,\n\nclear \n//or, to remove all stored values (including macros, matrices, scalars, etc.) \n*clear all\n\n* Replace $rootdir with the relevant path to on your local harddrive.\ncd \"$rootdir/problem-sets/ps-4\"\n\ncap log close\nlog using problem-set-4-log.txt, replace\n\nuse \"https://data.nber.org/morg/annual/morg19.dta\", clear\n\nYou can, of course, download the data and open it locally on your computer."
  },
  {
    "objectID": "problem-set-4-solutions.html#questions",
    "href": "problem-set-4-solutions.html#questions",
    "title": "Problem Set 4 (SOLUTIONS)",
    "section": "",
    "text": "1. Create a new variable exper equal to age minus (years of education + 6). This is referred to as potential years of experience. Check how each variable defines missing values before proceeding. You will need to create a years of education variable for this. Here is he the suggested code:\n\ntab grade92, m\ngen eduyrs = .\n    replace eduyrs = .3 if grade92==31\n    replace eduyrs = 3.2 if grade92==32\n    replace eduyrs = 7.2 if grade92==33\n    replace eduyrs = 7.2 if grade92==34\n    replace eduyrs = 9  if grade92==35\n    replace eduyrs = 10 if grade92==36\n    replace eduyrs = 11 if grade92==37\n    replace eduyrs = 12 if grade92==38\n    replace eduyrs = 12 if grade92==39\n    replace eduyrs = 13 if grade92==40\n    replace eduyrs = 14 if grade92==41\n    replace eduyrs = 14 if grade92==42\n    replace eduyrs = 16 if grade92==43\n    replace eduyrs = 18 if grade92==44\n    replace eduyrs = 18 if grade92==45\n    replace eduyrs = 18 if grade92==46\n    lab var eduyrs \"completed education\"\ntab grade92, sum(eduyrs)\n\n\n    Highest |\n      grade |\n  completed |      Freq.     Percent        Cum.\n------------+-----------------------------------\n         31 |        814        0.28        0.28\n         32 |      1,495        0.51        0.79\n         33 |      3,071        1.05        1.85\n         34 |      4,123        1.41        3.26\n         35 |      5,244        1.80        5.06\n         36 |      7,824        2.69        7.75\n         37 |      9,271        3.18       10.93\n         38 |      4,226        1.45       12.38\n         39 |     82,795       28.41       40.79\n         40 |     50,112       17.20       57.99\n         41 |     12,392        4.25       62.24\n         42 |     16,161        5.55       67.79\n         43 |     59,438       20.40       88.19\n         44 |     25,374        8.71       96.89\n         45 |      3,785        1.30       98.19\n         46 |      5,265        1.81      100.00\n------------+-----------------------------------\n      Total |    291,390      100.00\n(291,390 missing values generated)\n(814 real changes made)\n(1,495 real changes made)\n(3,071 real changes made)\n(4,123 real changes made)\n(5,244 real changes made)\n(7,824 real changes made)\n(9,271 real changes made)\n(4,226 real changes made)\n(82,795 real changes made)\n(50,112 real changes made)\n(12,392 real changes made)\n(16,161 real changes made)\n(59,438 real changes made)\n(25,374 real changes made)\n(3,785 real changes made)\n(5,265 real changes made)\n\n    Highest |\n      grade |   Summary of completed education\n  completed |        Mean   Std. dev.       Freq.\n------------+------------------------------------\n         31 |   .30000001           0         814\n         32 |         3.2           0       1,495\n         33 |   7.1999998           0       3,071\n         34 |   7.1999998           0       4,123\n         35 |           9           0       5,244\n         36 |          10           0       7,824\n         37 |          11           0       9,271\n         38 |          12           0       4,226\n         39 |          12           0      82,795\n         40 |          13           0      50,112\n         41 |          14           0      12,392\n         42 |          14           0      16,161\n         43 |          16           0      59,438\n         44 |          18           0      25,374\n         45 |          18           0       3,785\n         46 |          18           0       5,265\n------------+------------------------------------\n      Total |   13.556855   2.7030576     291,390\n\n\n\ntab age, m\ngen exper = age-(eduyrs+6)\n\n\n        Age |      Freq.     Percent        Cum.\n------------+-----------------------------------\n         16 |      4,661        1.60        1.60\n         17 |      4,630        1.59        3.19\n         18 |      4,417        1.52        4.70\n         19 |      4,039        1.39        6.09\n         20 |      3,915        1.34        7.43\n         21 |      3,996        1.37        8.81\n         22 |      3,918        1.34       10.15\n         23 |      3,950        1.36       11.51\n         24 |      4,194        1.44       12.94\n         25 |      4,185        1.44       14.38\n         26 |      4,325        1.48       15.87\n         27 |      4,476        1.54       17.40\n         28 |      4,600        1.58       18.98\n         29 |      4,633        1.59       20.57\n         30 |      4,829        1.66       22.23\n         31 |      4,735        1.62       23.85\n         32 |      4,601        1.58       25.43\n         33 |      4,748        1.63       27.06\n         34 |      4,646        1.59       28.66\n         35 |      4,730        1.62       30.28\n         36 |      4,742        1.63       31.91\n         37 |      4,848        1.66       33.57\n         38 |      4,550        1.56       35.13\n         39 |      4,735        1.62       36.76\n         40 |      4,667        1.60       38.36\n         41 |      4,503        1.55       39.90\n         42 |      4,390        1.51       41.41\n         43 |      4,309        1.48       42.89\n         44 |      4,193        1.44       44.33\n         45 |      4,253        1.46       45.79\n         46 |      4,266        1.46       47.25\n         47 |      4,447        1.53       48.78\n         48 |      4,563        1.57       50.34\n         49 |      4,698        1.61       51.96\n         50 |      4,646        1.59       53.55\n         51 |      4,477        1.54       55.09\n         52 |      4,555        1.56       56.65\n         53 |      4,523        1.55       58.20\n         54 |      4,736        1.63       59.83\n         55 |      5,010        1.72       61.55\n         56 |      5,035        1.73       63.27\n         57 |      4,976        1.71       64.98\n         58 |      5,030        1.73       66.71\n         59 |      5,066        1.74       68.45\n         60 |      5,124        1.76       70.20\n         61 |      5,067        1.74       71.94\n         62 |      5,035        1.73       73.67\n         63 |      4,927        1.69       75.36\n         64 |      4,892        1.68       77.04\n         65 |      4,554        1.56       78.60\n         66 |      4,526        1.55       80.16\n         67 |      4,344        1.49       81.65\n         68 |      4,328        1.49       83.13\n         69 |      4,100        1.41       84.54\n         70 |      4,058        1.39       85.93\n         71 |      4,008        1.38       87.31\n         72 |      3,897        1.34       88.65\n         73 |      3,147        1.08       89.73\n         74 |      2,815        0.97       90.69\n         75 |      2,809        0.96       91.66\n         76 |      2,623        0.90       92.56\n         77 |      2,373        0.81       93.37\n         78 |      2,201        0.76       94.13\n         79 |      1,977        0.68       94.80\n         80 |      7,799        2.68       97.48\n         85 |      7,340        2.52      100.00\n------------+-----------------------------------\n      Total |    291,390      100.00\n\n\n2. Keep only those between the ages of 18 and 54. Check the distribution of `exper’ and replace any negative values to 0.\n\nkeep if inrange(age,18,54)\nsum exper, det\nreplace exper=0 if exper&lt;0\n\n(126,352 observations deleted)\n\n                            exper\n-------------------------------------------------------------\n      Percentiles      Smallest\n 1%            0             -6\n 5%            1             -5\n10%            2             -4       Obs             165,038\n25%            7             -4       Sum of wgt.     165,038\n\n50%           16                      Mean           16.50623\n                        Largest       Std. dev.      10.57401\n75%           25           47.7\n90%           31           47.7       Variance       111.8097\n95%           34           47.7       Skewness       .1296908\n99%           36           47.7       Kurtosis       1.887811\n(1,078 real changes made)\n\n\n3. Create a categorical variable that takes on 4 values: 1 “less than High School”; 2 “High School Diploma”; 3 “some Higher Education”; 4 “Bachelors”; 5 “Postgraduate”. This variable should be based on the the grade92 variable. You can find the value labels for this variable in this document: https://data.nber.org/morg/docs/cpsx.pdf. I suggest using the recode command, which allows you to create value labels while assigning values. Check the distributio of exper by education category.\n\nrecode grade92 (31/38 = 1 \"&lt;HS\") (39 = 2 \"HS\") (40/42 = 3 \"HS+\") (43 = 4 \"BA\") (44/46 = 5 \"PG\"), gen(educat)\ntab grade92 educat, m\n\ntab educat, sum(exper)\n\n(165,038 differences between grade92 and educat)\n\n   Highest |\n     grade |      RECODE of grade92 (Highest grade completed)\n completed |       &lt;HS         HS        HS+         BA         PG |     Total\n-----------+-------------------------------------------------------+----------\n        31 |       398          0          0          0          0 |       398 \n        32 |       578          0          0          0          0 |       578 \n        33 |     1,515          0          0          0          0 |     1,515 \n        34 |     1,571          0          0          0          0 |     1,571 \n        35 |     1,971          0          0          0          0 |     1,971 \n        36 |     2,198          0          0          0          0 |     2,198 \n        37 |     4,373          0          0          0          0 |     4,373 \n        38 |     2,440          0          0          0          0 |     2,440 \n        39 |         0     45,013          0          0          0 |    45,013 \n        40 |         0          0     30,934          0          0 |    30,934 \n        41 |         0          0      7,154          0          0 |     7,154 \n        42 |         0          0      9,708          0          0 |     9,708 \n        43 |         0          0          0     37,557          0 |    37,557 \n        44 |         0          0          0          0     14,804 |    14,804 \n        45 |         0          0          0          0      2,021 |     2,021 \n        46 |         0          0          0          0      2,803 |     2,803 \n-----------+-------------------------------------------------------+----------\n     Total |    15,044     45,013     47,796     37,557     19,628 |   165,038 \n\n  RECODE of |\n    grade92 |\n   (Highest |\n      grade |          Summary of exper\n completed) |        Mean   Std. dev.       Freq.\n------------+------------------------------------\n        &lt;HS |    19.29665    12.82301      15,044\n         HS |   17.726435   11.044222      45,013\n        HS+ |   15.643589   10.806949      47,796\n         BA |   15.376841   9.3440749      37,557\n         PG |   15.900092   8.1569821      19,628\n------------+------------------------------------\n      Total |   16.514468   10.560511     165,038\n\n\n4. Create the variable lnwage equal to the (natural) log of weekly earnings. Create a figure that shows the predicted linear fit of lwage against exper, by educat. Try to place all 5 fitted lines in the same graph.\n\ngen lnwage = ln(earnwke)\ntwoway (lfit lnwage exper if educat==1) (lfit lnwage exper if educat==2)  (lfit lnwage exper if educat==3) (lfit lnwage exper if educat==4) (lfit lnwage exper if educat==5) , legend(order(1 \"&lt;HS\" 2 \"HS\" 3 \"HS+\" 4 \"BA\" 5 \"PG\") pos(6) r(1)) xtitle(Years of experience) \n\n(49,686 missing values generated)\n\n\n\n\n\n\n\n\n\n5. Estimate a linear regression model that allows the slope coefficient on exper and constant term to vary by education category (educat). Let the base (excluded) education category be 2 “High School diploma”.\n\\[\n  \\ln(Wage_i) = \\alpha + \\sum_{j\\neq2}\\psi_j \\mathbf{1}\\{Educat_i=j\\} + \\beta Exper_i + \\sum_{j\\neq2}\\gamma_j Exper_i\\times\\mathbf{1}\\{Educat_i=j\\}+\\upsilon_i\n\\]\n\nreg lnwage ib2.educat##c.exper\n\n\n      Source |       SS           df       MS      Number of obs   =   115,352\n-------------+----------------------------------   F(9, 115342)    =   4039.95\n       Model |  17435.5509         9  1937.28343   Prob &gt; F        =    0.0000\n    Residual |  55310.0589   115,342  .479530951   R-squared       =    0.2397\n-------------+----------------------------------   Adj R-squared   =    0.2396\n       Total |  72745.6098   115,351   .63064568   Root MSE        =    .69248\n\n------------------------------------------------------------------------------\n      lnwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      educat |\n        &lt;HS  |  -.3882762   .0178644   -21.73   0.000    -.4232902   -.3532622\n        HS+  |  -.0839435   .0104229    -8.05   0.000    -.1043722   -.0635149\n         BA  |   .6145964   .0109763    55.99   0.000     .5930831    .6361097\n         PG  |   .9055692   .0141961    63.79   0.000      .877745    .9333933\n             |\n       exper |   .0182621   .0003709    49.24   0.000     .0175351    .0189891\n             |\n      educat#|\n     c.exper |\n        &lt;HS  |    .001037   .0007627     1.36   0.174     -.000458    .0025319\n        HS+  |   .0095156   .0005186    18.35   0.000     .0084991    .0105321\n         BA  |  -.0032067   .0005743    -5.58   0.000    -.0043324   -.0020811\n         PG  |  -.0051215   .0007698    -6.65   0.000    -.0066302   -.0036128\n             |\n       _cons |   6.101809   .0077485   787.48   0.000     6.086622    6.116996\n------------------------------------------------------------------------------\n\n\n6. Show that after 13 years of experience, those with some Higer Education (but no Bachelors), out earn those with just a high school diploma. You can assume that there are is a 2 year difference between the experience (education).\n\ndis _b[exper]*14\ndis (_b[exper] + _b[3.educat#exper])*12 + _b[3.educat]\n\n\ndis _b[exper]*15\ndis (_b[exper] + _b[3.educat#exper])*13 + _b[3.educat]\n\n.25566943\n.24938901\n.27393153\n.27716672\n\n\n7. Use the post-estimation test command to test the null hypothesis: \\(H_0: 15\\beta = 13(\\beta+\\gamma_3)+\\psi_3\\).\n\ntest exper*15 = (exper+3.educat#exper)*13+3.educat\n\n\n ( 1)  - 3.educat + 2*exper - 13*3.educat#c.exper = 0\n\n       F(  1,115342) =    0.32\n            Prob &gt; F =    0.5734\n\n\n8. Estimate a transformed version of the above model allowing you to test the above hypothesis using the coefficient from a single regressor. That is, the resulting test should be a simple t-test of \\(H_0: \\phi=0\\), where \\(\\phi\\) is the coefficient on the interaction of exper and a dummy variable for educat=3. This will be easier to do if you estimate the model using only the relevant sample: those with High School diplomas and some Higher Education. I suggest avoiding the use of factor notation to create the dummy variables and interaction terms for this exercise. For example, the following should replicate the relevant coefficients from Q5.\n\ngen hasHE = educat==3 if inlist(educat,2,3)\ngen hasHEexp = hasHE*exper\n\nreg lnwage exper hasHE hasHEexp\n\n(72,229 missing values generated)\n(72,229 missing values generated)\n\n      Source |       SS           df       MS      Number of obs   =    62,811\n-------------+----------------------------------   F(3, 62807)     =   2860.46\n       Model |  4000.16053         3  1333.38684   Prob &gt; F        =    0.0000\n    Residual |  29277.0845    62,807  .466143655   R-squared       =    0.1202\n-------------+----------------------------------   Adj R-squared   =    0.1202\n       Total |  33277.2451    62,810  .529808073   Root MSE        =    .68275\n\n------------------------------------------------------------------------------\n      lnwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       exper |   .0182621   .0003657    49.94   0.000     .0175453    .0189789\n       hasHE |  -.0839435   .0102764    -8.17   0.000    -.1040852   -.0638019\n    hasHEexp |   .0095156   .0005113    18.61   0.000     .0085134    .0105178\n       _cons |   6.101809   .0076396   798.71   0.000     6.086835    6.116782\n------------------------------------------------------------------------------\n\n\n\ngen experR = exper+hasHEexp*2/13\ngen hasHER = hasHE-hasHEexp/13\nreg lnwage experR hasHER hasHEexp\n\n(72,229 missing values generated)\n(72,229 missing values generated)\n\n      Source |       SS           df       MS      Number of obs   =    62,811\n-------------+----------------------------------   F(3, 62807)     =   2860.46\n       Model |  4000.16052         3  1333.38684   Prob &gt; F        =    0.0000\n    Residual |  29277.0845    62,807  .466143655   R-squared       =    0.1202\n-------------+----------------------------------   Adj R-squared   =    0.1202\n       Total |  33277.2451    62,810  .529808073   Root MSE        =    .68275\n\n------------------------------------------------------------------------------\n      lnwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      experR |   .0182621   .0003657    49.94   0.000     .0175453    .0189789\n      hasHER |  -.0839435   .0102764    -8.17   0.000    -.1040852   -.0638019\n    hasHEexp |   .0002489   .0004358     0.57   0.568    -.0006053     .001103\n       _cons |   6.101809   .0076396   798.71   0.000     6.086835    6.116782\n------------------------------------------------------------------------------\n\n\n9. Verify that the F-statistic from Q7 is the square of the above T-statistic.\n\ndis (_b[hasHEexp]/_se[hasHEexp])^2\n\n.32610143\n\n\n10. Use the restricted OLS approach to replicate the F-statistic and p-value from Q7.\n\nreg lnwage exper hasHE hasHEexp\nscalar RSSu = e(rss)\nscalar DOFu = e(df_r)\nreg lnwage experR hasHER \nscalar RSSr = e(rss)\nscalar DOFr = e(df_r)\n\nscalar Fstat = ((RSSr-RSSu)/(DOFr-DOFu))/(RSSu/DOFu)\nscalar pval = Ftail(1,DOFu,Fstat)\n\nscalar list Fstat pval\n\n\n      Source |       SS           df       MS      Number of obs   =    62,811\n-------------+----------------------------------   F(3, 62807)     =   2860.46\n       Model |  4000.16053         3  1333.38684   Prob &gt; F        =    0.0000\n    Residual |  29277.0845    62,807  .466143655   R-squared       =    0.1202\n-------------+----------------------------------   Adj R-squared   =    0.1202\n       Total |  33277.2451    62,810  .529808073   Root MSE        =    .68275\n\n------------------------------------------------------------------------------\n      lnwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       exper |   .0182621   .0003657    49.94   0.000     .0175453    .0189789\n       hasHE |  -.0839435   .0102764    -8.17   0.000    -.1040852   -.0638019\n    hasHEexp |   .0095156   .0005113    18.61   0.000     .0085134    .0105178\n       _cons |   6.101809   .0076396   798.71   0.000     6.086835    6.116782\n------------------------------------------------------------------------------\n\n      Source |       SS           df       MS      Number of obs   =    62,811\n-------------+----------------------------------   F(2, 62808)     =   4290.58\n       Model |  4000.00851         2  2000.00425   Prob &gt; F        =    0.0000\n    Residual |  29277.2366    62,808  .466138654   R-squared       =    0.1202\n-------------+----------------------------------   Adj R-squared   =    0.1202\n       Total |  33277.2451    62,810  .529808073   Root MSE        =    .68274\n\n------------------------------------------------------------------------------\n      lnwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      experR |   .0182233   .0003593    50.71   0.000      .017519    .0189277\n      hasHER |  -.0882792   .0069253   -12.75   0.000    -.1018527   -.0747056\n       _cons |   6.104077   .0065255   935.42   0.000     6.091287    6.116867\n------------------------------------------------------------------------------\n     Fstat =  .32612066\n      pval =   .5679544\n\n\n11. Use the restricted OLS approach to test the following hypothesis corresponding to the model in Q5:\n\\[\nH_0: \\gamma_j = 0\\qquad \\text{for}\\quad j=1,3,4,5\n\\] Compute the F-statistic and p-value. Verify your result using the post-estimation test command.\n\nreg lnwage ib2.educat##c.exper\nscalar RSSu = e(rss)\nscalar DOFu = e(df_r)\nreg lnwage ib2.educat exper \nscalar RSSr = e(rss)\nscalar DOFr = e(df_r)\n\nscalar Fstat = ((RSSr-RSSu)/(DOFr-DOFu))/(RSSu/DOFu)\nscalar pval = Ftail(DOFr-DOFu,DOFu,Fstat)\n\nscalar list Fstat pval\n\n** verify\nreg lnwage ib2.educat##c.exper\ntest 1.educat#exper 3.educat#exper 4.educat#exper 5.educat#exper\n\n\n      Source |       SS           df       MS      Number of obs   =   115,352\n-------------+----------------------------------   F(9, 115342)    =   4039.95\n       Model |  17435.5509         9  1937.28343   Prob &gt; F        =    0.0000\n    Residual |  55310.0589   115,342  .479530951   R-squared       =    0.2397\n-------------+----------------------------------   Adj R-squared   =    0.2396\n       Total |  72745.6098   115,351   .63064568   Root MSE        =    .69248\n\n------------------------------------------------------------------------------\n      lnwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      educat |\n        &lt;HS  |  -.3882762   .0178644   -21.73   0.000    -.4232902   -.3532622\n        HS+  |  -.0839435   .0104229    -8.05   0.000    -.1043722   -.0635149\n         BA  |   .6145964   .0109763    55.99   0.000     .5930831    .6361097\n         PG  |   .9055692   .0141961    63.79   0.000      .877745    .9333933\n             |\n       exper |   .0182621   .0003709    49.24   0.000     .0175351    .0189891\n             |\n      educat#|\n     c.exper |\n        &lt;HS  |    .001037   .0007627     1.36   0.174     -.000458    .0025319\n        HS+  |   .0095156   .0005186    18.35   0.000     .0084991    .0105321\n         BA  |  -.0032067   .0005743    -5.58   0.000    -.0043324   -.0020811\n         PG  |  -.0051215   .0007698    -6.65   0.000    -.0066302   -.0036128\n             |\n       _cons |   6.101809   .0077485   787.48   0.000     6.086622    6.116996\n------------------------------------------------------------------------------\n\n      Source |       SS           df       MS      Number of obs   =   115,352\n-------------+----------------------------------   F(5, 115346)    =   7085.67\n       Model |  17093.4651         5  3418.69301   Prob &gt; F        =    0.0000\n    Residual |  55652.1448   115,346  .482480058   R-squared       =    0.2350\n-------------+----------------------------------   Adj R-squared   =    0.2349\n       Total |  72745.6098   115,351   .63064568   Root MSE        =    .69461\n\n------------------------------------------------------------------------------\n      lnwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      educat |\n        &lt;HS  |  -.3724213   .0090073   -41.35   0.000    -.3900754   -.3547671\n        HS+  |   .0726614   .0055618    13.06   0.000     .0617604    .0835624\n         BA  |   .5714202   .0057563    99.27   0.000     .5601379    .5827025\n         PG  |   .8295498   .0068114   121.79   0.000     .8161996    .8428999\n             |\n       exper |   .0201711   .0002025    99.60   0.000     .0197742    .0205681\n       _cons |   6.067685   .0054116  1121.24   0.000     6.057079    6.078292\n------------------------------------------------------------------------------\n     Fstat =  178.34399\n      pval =  1.32e-152\n\n      Source |       SS           df       MS      Number of obs   =   115,352\n-------------+----------------------------------   F(9, 115342)    =   4039.95\n       Model |  17435.5509         9  1937.28343   Prob &gt; F        =    0.0000\n    Residual |  55310.0589   115,342  .479530951   R-squared       =    0.2397\n-------------+----------------------------------   Adj R-squared   =    0.2396\n       Total |  72745.6098   115,351   .63064568   Root MSE        =    .69248\n\n------------------------------------------------------------------------------\n      lnwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      educat |\n        &lt;HS  |  -.3882762   .0178644   -21.73   0.000    -.4232902   -.3532622\n        HS+  |  -.0839435   .0104229    -8.05   0.000    -.1043722   -.0635149\n         BA  |   .6145964   .0109763    55.99   0.000     .5930831    .6361097\n         PG  |   .9055692   .0141961    63.79   0.000      .877745    .9333933\n             |\n       exper |   .0182621   .0003709    49.24   0.000     .0175351    .0189891\n             |\n      educat#|\n     c.exper |\n        &lt;HS  |    .001037   .0007627     1.36   0.174     -.000458    .0025319\n        HS+  |   .0095156   .0005186    18.35   0.000     .0084991    .0105321\n         BA  |  -.0032067   .0005743    -5.58   0.000    -.0043324   -.0020811\n         PG  |  -.0051215   .0007698    -6.65   0.000    -.0066302   -.0036128\n             |\n       _cons |   6.101809   .0077485   787.48   0.000     6.086622    6.116996\n------------------------------------------------------------------------------\n\n ( 1)  1.educat#c.exper = 0\n ( 2)  3.educat#c.exper = 0\n ( 3)  4.educat#c.exper = 0\n ( 4)  5.educat#c.exper = 0\n\n       F(  4,115342) =  178.34\n            Prob &gt; F =    0.0000\n\n\n12. Compute the relevant Chi-squared distributed test statistic and corresponding p-value for the above test, assuming \\(n\\) is large (enough).\n\nscalar Cstat = Fstat*(DOFr-DOFu)\nscalar pval = chi2tail(DOFr-DOFu,Cstat)\nscalar list Cstat pval\n\n     Cstat =  713.37597\n      pval =  4.42e-153\n\n\n13. Using the data from Problem Set 2, estimate the simple linear regression model using OLS,\n\\[\n  \\ln(Wage_i) = \\beta_0 + \\beta_1 Educ_i + \\beta_2 Female_i + \\varepsilon_i\n\\]\n\nuse \"$rootdir/problem-sets/ps-2/problem-set-2-data.dta\", clear\nreg lwage educ female\nest sto ols\nestadd scalar sigma = e(rmse)\n\n(PSID wage data 1976-82 from Baltagi and Khanti-Akom (1990))\n\n      Source |       SS           df       MS      Number of obs   =     4,165\n-------------+----------------------------------   F(2, 4162)      =    732.99\n       Model |  231.021419         2   115.51071   Prob &gt; F        =    0.0000\n    Residual |  655.883483     4,162  .157588535   R-squared       =    0.2605\n-------------+----------------------------------   Adj R-squared   =    0.2601\n       Total |  886.904902     4,164  .212993492   Root MSE        =    .39697\n\n------------------------------------------------------------------------------\n       lwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        educ |   .0651382   .0022066    29.52   0.000     .0608121    .0694642\n      female |  -.4737645   .0194589   -24.35   0.000    -.5119143   -.4356147\n       _cons |    5.89297   .0290891   202.58   0.000      5.83594        5.95\n------------------------------------------------------------------------------\n\nadded scalar:\n              e(sigma) =  .39697422\n\n\n14. Estimate the Mincer equation using Maximum Likelihood. Take a look at https://www.stata.com/manuals13/rmlexp.pdf, the documentation for the mlexp command. It has a discussion on estimating the CLRM using ML.1\n\nmlexp (ln(normalden(lwage, {xb: educ female _cons}, exp({theta}))))\nereturn list\nnlcom (sigma: exp(_b[/theta]))\nestadd scalar sigma = r(b)[1,1]\neststo ml\n\n\nInitial:      Log likelihood = -97095.356\nAlternative:  Log likelihood = -35297.969\nRescale:      Log likelihood = -12999.606\nRescale eq:   Log likelihood = -7350.6222\nIteration 0:  Log likelihood = -7350.6222  (not concave)\nIteration 1:  Log likelihood =  -3936.054  \nIteration 2:  Log likelihood = -2187.1092  (backed up)\nIteration 3:  Log likelihood = -2073.0429  \nIteration 4:  Log likelihood = -2060.4271  \nIteration 5:  Log likelihood = -2060.4019  \nIteration 6:  Log likelihood = -2060.4019  \n\nMaximum likelihood estimation\n\nLog likelihood = -2060.4019                              Number of obs = 4,165\n\n------------------------------------------------------------------------------\n             | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nxb           |\n        educ |   .0651382   .0022058    29.53   0.000      .060815    .0694614\n      female |  -.4737645   .0194519   -24.36   0.000    -.5118895   -.4356396\n       _cons |    5.89297   .0290786   202.66   0.000     5.835977    5.949963\n-------------+----------------------------------------------------------------\n      /theta |  -.9242442   .0109566   -84.35   0.000    -.9457188   -.9027696\n------------------------------------------------------------------------------\n\nscalars:\n               e(rank) =  4\n                  e(N) =  4165\n                 e(ic) =  6\n                  e(k) =  4\n               e(k_eq) =  2\n          e(converged) =  1\n                 e(rc) =  0\n                 e(ll) =  -2060.40189888505\n              e(k_aux) =  1\n               e(df_m) =  4\n         e(k_eq_model) =  0\n\nmacros:\n            e(cmdline) : \"mlexp (ln(normalden(lwage, {xb: educ female _cons..\"\n                e(cmd) : \"mlexp\"\n            e(predict) : \"mlexp_p\"\n          e(estat_cmd) : \"mlexp_estat\"\n       e(marginsnotok) : \"SCores\"\n          e(marginsok) : \"default xb\"\n        e(marginsprop) : \"nochainrule\"\n               e(lexp) : \"ln(normalden(lwage,{xb:},exp({theta:})))\"\n             e(params) : \"xb:educ xb:female xb:_cons theta:_cons\"\n                e(opt) : \"moptimize\"\n                e(vce) : \"oim\"\n          e(ml_method) : \"lf0\"\n          e(technique) : \"nr\"\n         e(properties) : \"b V\"\n\nmatrices:\n                  e(b) :  1 x 4\n                  e(V) :  4 x 4\n               e(init) :  1 x 4\n               e(ilog) :  1 x 20\n           e(gradient) :  1 x 4\n\nfunctions:\n             e(sample)   \n\n       sigma: exp(_b[/theta])\n\n------------------------------------------------------------------------------\n             | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       sigma |   .3968312   .0043479    91.27   0.000     .3883094     .405353\n------------------------------------------------------------------------------\n\nadded scalar:\n              e(sigma) =  .39683123\n\n\n15. Estimate the Mincer equation using Method of Moments. You can use the gmm command in Stata. Hint: the regressors will be their own instruments and use the onestep option.2\n\ngmm (lwage - {xb: educ female _cons}), instruments(educ female) onestep\neststo mm\n\nesttab ols ml mm, se drop(theta:_cons) scalar(N sigma) mtitle(OLS ML MM)\n\n\nStep 1\nIteration 0:  GMM criterion Q(b) =  44.629069  \nIteration 1:  GMM criterion Q(b) =  2.101e-24  \nIteration 2:  GMM criterion Q(b) =  1.368e-31  \n\nnote: model is exactly identified.\n\nGMM estimation \n\nNumber of parameters =   3\nNumber of moments    =   3\nInitial weight matrix: Unadjusted                 Number of obs   =      4,165\n\n------------------------------------------------------------------------------\n             |               Robust\n             | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        educ |   .0651382   .0023187    28.09   0.000     .0605935    .0696828\n      female |  -.4737645   .0177811   -26.64   0.000    -.5086148   -.4389143\n       _cons |    5.89297   .0300924   195.83   0.000      5.83399     5.95195\n------------------------------------------------------------------------------\nInstruments for equation 1: educ female _cons\n\n------------------------------------------------------------\n                      (1)             (2)             (3)   \n                      OLS              ML              MM   \n------------------------------------------------------------\nmain                                                        \neduc               0.0651***       0.0651***       0.0651***\n                (0.00221)       (0.00221)       (0.00232)   \n\nfemale             -0.474***       -0.474***       -0.474***\n                 (0.0195)        (0.0195)        (0.0178)   \n\n_cons               5.893***        5.893***        5.893***\n                 (0.0291)        (0.0291)        (0.0301)   \n------------------------------------------------------------\nN                    4165            4165            4165   \nsigma               0.397           0.397                   \n------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001"
  },
  {
    "objectID": "problem-set-4-solutions.html#postamble",
    "href": "problem-set-4-solutions.html#postamble",
    "title": "Problem Set 4 (SOLUTIONS)",
    "section": "",
    "text": "log close"
  },
  {
    "objectID": "problem-set-4-solutions.html#footnotes",
    "href": "problem-set-4-solutions.html#footnotes",
    "title": "Problem Set 4 (SOLUTIONS)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYou can also look at the following resource for a more flexible approach to ML estimation in Stata: https://www.stata.com/features/overview/maximum-likelihood-estimation/↩︎\nHere is a resource on GMM in Stata: https://www.stata.com/features/overview/generalized-method-of-moments/↩︎"
  }
]