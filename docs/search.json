[
  {
    "objectID": "revision.html",
    "href": "revision.html",
    "title": "Revision Questions",
    "section": "",
    "text": "Consider two random \\(k\\)-dimension vectors \\(\\{X,Y\\}\\) and non-random \\(k\\times k\\) matrices \\(\\{A,B\\}\\). Show,\n1.1. \\(Var(AX) = A Var(X) A'\\)\n1.2. \\(Var(AX+b) = A Var(X) A'\\) for non-random \\(k\\)-dimension vector \\(b\\)\n1.3. \\(Cov(AX,BY) = A Cov(X,Y) B'\\)\nSuppose \\(X\\sim N(\\mu,\\Sigma)\\), with \\(X\\in \\mathbb{R}^k\\). Find the distribution, \\(Y = AX + b\\), for non-random \\(k\\times k\\) matrix \\(A\\) and non-random \\(k\\)-dimension vector \\(b\\)."
  },
  {
    "objectID": "revision.html#basics",
    "href": "revision.html#basics",
    "title": "Revision Questions",
    "section": "",
    "text": "Consider two random \\(k\\)-dimension vectors \\(\\{X,Y\\}\\) and non-random \\(k\\times k\\) matrices \\(\\{A,B\\}\\). Show,\n1.1. \\(Var(AX) = A Var(X) A'\\)\n1.2. \\(Var(AX+b) = A Var(X) A'\\) for non-random \\(k\\)-dimension vector \\(b\\)\n1.3. \\(Cov(AX,BY) = A Cov(X,Y) B'\\)\nSuppose \\(X\\sim N(\\mu,\\Sigma)\\), with \\(X\\in \\mathbb{R}^k\\). Find the distribution, \\(Y = AX + b\\), for non-random \\(k\\times k\\) matrix \\(A\\) and non-random \\(k\\)-dimension vector \\(b\\)."
  },
  {
    "objectID": "revision.html#clrm",
    "href": "revision.html#clrm",
    "title": "Revision Questions",
    "section": "2 CLRM",
    "text": "2 CLRM\n\nWhich of the CLRM assumptions is required for identification of \\(\\beta\\)? Demonstrate this claim.\nProvide an example of a model that is non-linear in regressors, but linear in parameters. Similarly, provide an example of a model that is linear in regressors, but non-linear in parameters.\nSuppose, the true data generating process was given by,\n\\[\nY = X\\beta + \\epsilon\n\\] where \\(E[\\epsilon|X] = \\alpha\\) and \\(X\\) included a constant term with parameter \\(\\beta_1\\). Is \\(\\beta_1\\) identified?\nGiven the result \\(\\beta = E[X_iX_i']^{-1}E[X_iY_i]\\). With two regressors, \\(X_i = [1\\;X_{2i}]'\\), show\n4.1. \\(\\beta_2 = \\frac{Cov(X_{2i},Y_i)}{Var(X_{2i})}\\)\n4.2. \\(\\beta_1 = E[Y_i]-\\beta_2E[X_i]\\)"
  },
  {
    "objectID": "revision.html#ols",
    "href": "revision.html#ols",
    "title": "Revision Questions",
    "section": "3 OLS",
    "text": "3 OLS\n\nConsider the projection matrices \\(P_X = X(X'X)^{-1}X'\\) and \\(M_X = I_n-P_X\\). Show,\n1.1. \\(P_XX = X\\)\n1.2. \\(M_XX = 0\\)\n1.3. \\(P_XM_X = 0\\)\n1.4. \\(X'P_X=X'\\)\nShow that \\(X'X\\), where \\(X\\) is a \\(n\\times k\\) random matrix, can be expressed as \\(\\sum_{i=1}^nX_iX_i'\\), where \\(X_i\\) is a \\(k\\times 1\\) vector.\nConsider the partitioned regression model, \\[\nY = X_1\\beta_1 + X_2\\beta_2 + u\n\\] Show,\n3.1. \\(E[\\hat{\\beta}_1|X]=\\beta_1\\) where \\(\\hat{\\beta}_1 = (X_1'M_2X_1)^{-1}X_1'M_2Y\\).\n3.2. Write down the conditional variance of the OLS estimator for \\(\\beta_1\\), assuming homoskedasticity: \\(E[uu'|X]=\\sigma^2 I_n\\).\n3.3. Write down the conditional variance of the OLS estimator for \\(\\beta_1\\), assuming heteroskedasticity: \\(E[uu'|X]=\\Omega\\).\nDemonstrate the BLUE result: the OLS estimator (\\(\\hat{\\beta}\\)) is the Best Linear Unbiased Estimator. Consider the alternative unbiased, linear estimator \\(b=AY\\); such that, \\[\n\\begin{aligned}\nE[b|X] = \\beta\n\\end{aligned}\n\\]\n4.1. Show that since \\(b\\) is unbiased, it must be that \\(AX = I_k\\).\n4.2. Using the above result, show that under CLRM 1-6 (i.e., including homoskedasticity), \\[\nCov(\\hat{\\beta},b|X) = Var(\\hat{\\beta}|X)\n\\]\n4.2. Show that \\(Var(b|X)-Var(\\hat{\\beta}|X)\\geq 0\\) (i.e. a positive semi-definite matrix), by solving for \\[\nVar(\\hat{\\beta}-b|X)\n\\]\nConsider the GLS estimator (\\(\\tilde{\\beta}\\)), which solves the problem\n\\[\n\\underset{b}{\\min}\\; (Y-Xb)'\\Omega^{-1}(Y-Xb)\n\\]\nwhere \\(E[\\varepsilon \\varepsilon'|X]=\\Omega\\).\n5.1. Show that \\(Var(\\hat{\\beta}|X)-Var(\\tilde{\\beta}|X)\\geq 0\\), where \\(\\hat{\\beta}\\) is the OLS estimator.\n5.2. Under which assumption are the two estimators equivalent?"
  },
  {
    "objectID": "revision.html#linear-tests",
    "href": "revision.html#linear-tests",
    "title": "Revision Questions",
    "section": "4 Linear Tests",
    "text": "4 Linear Tests\n\nUnder CLRM 1-6, solve for the finite sample distribution of \\(R\\hat{beta}\\), where \\(R\\) is non-random \\(k\\times k\\) matrix.\nConsider the multiple, linear hypotheses:\n\\[\n\\begin{aligned}\nH_0:\\;& \\beta_2 -4\\beta_4 = 1 \\\\\n& \\beta_3 = 3 \\\\\n& \\beta_5 = \\beta_6\n\\end{aligned}\n\\]\n2.1. Write the 4 hypotheses in the form \\(R\\beta = r\\).\n2.2. Write down the F-statistic for the test (assuming homoskedasticity) as well as it’s finite sample distribution.\n2.3. What is the asymptotic distribution of this test statistic.\n2.4. For a linear model with \\(k=6\\), write the restricted model corresponding to the above hypotheses.\nConsider the model of household food expenditure,\n\\[\nfoodexp_i = \\beta_1 + \\beta_2 inc_i + \\beta_3hhsize_i + \\beta_4hhsize^2_i + \\varepsilon_i\n\\]\n3.1. Suppose you wish to test the hypothesis of increasing returns to food consumption in the household: each additional household member is marginally cheaper to feed. Which is a more powerful test:\n\\[\nH_0:\\; \\beta_4=0 \\qquad \\text{against}\\qquad H_1:\\; \\beta_4\\neq0\n\\] or, \\[\nH_0:\\; \\beta_4\\geq 0 \\qquad \\text{against}\\qquad H_1:\\; \\beta_4&lt;0\n\\]\n3.2. Transform the model to test the hypothesis \\(H_0: -\\beta_3/\\beta_4 = 5\\), using just the coefficient on the variable \\(hhsize\\).\n3.3. Transform the model to test the same hypothesis, using just the coefficient on the variable \\(hhsize^2\\)."
  },
  {
    "objectID": "revision.html#panel-data",
    "href": "revision.html#panel-data",
    "title": "Revision Questions",
    "section": "5 Panel Data",
    "text": "5 Panel Data\n\nShow that \\(\\tilde{Y}_i = Y_i - \\bar{Y}_i\\), where \\(\\bar{Y}_i = \\frac{1}{T}\\sum_{t=1}^T Y_{it}\\), can be written as \\(Y = M_\\ell Y_i\\), where \\(M_\\ell = \\ell (\\ell'\\ell)^{-1}\\ell'\\) and \\(\\ell\\) is a \\(T\\times 1\\) vector of 1’s.\nShow that \\(X'X\\), where \\(X\\) is a \\(nT\\times k\\) random matrix, can be expressed as \\(\\sum_{i=1}^nX_i'X_i\\), where \\(X_i\\) is a \\(T\\times k\\) matrix.\nShow that \\(I_n\\otimes M_\\ell\\) is an idempotent matrix and \\(\\sum_{i=1}^n\\tilde{X}_i'\\tilde{X}_i=X'(I_n\\otimes M_\\ell)X\\).\nShow that for T=3, \\(D'(DD')^{-1}D = M_\\ell\\).\nDemonstrate that the OLS estimator of the ‘within-unit’ transformed model is unbiased. Is it efficient?\nConditional variance of random effects model,\n6.1. Solve for \\(Var(\\hat{\\beta}^{OLS}|X)\\), the variance of the (‘pooled’) OLS estimator for the random effects model.\n6.2. Solve for \\(Var(\\hat{\\beta}^{GLS}|X)\\), the variance of the GLS estimator for the random effects model.\n6.3. Verify that \\(Var(\\hat{\\beta}^{OLS}|X)-Var(\\hat{\\beta}^{GLS}|X)\\geq 0\\), is a positive-semidefinite matrix.\nConditional variance of first-differenced transformation\n7.1. Solve for \\(Var(\\hat{\\beta}^{FD-OLS}|X)\\), the variance of the OLS estimator for the FD transformation.\n7.2. Solve for \\(Var(\\hat{\\beta}^{FD-GLS}|X)\\), the variance of the GLS estimator for the FD transformation.\n7.3. Verify that \\(Var(\\hat{\\beta}^{FD-OLS}|X)-Var(\\hat{\\beta}^{FD-GLS}|X)\\geq 0\\), is a positive-semidefinite matrix.\nWhy is it important that the assumed model underlying the ‘pooled’ OLS estimator and ‘within’ OLS estimator are the same in the Hausman test? That is, why cannot we not compare the LSDV estimator of the fixed-effects model with the ‘pooled’ OLS estimator?\nWhat is the purpose of the Mundlack correction?"
  },
  {
    "objectID": "revision.html#binary-outcome-models",
    "href": "revision.html#binary-outcome-models",
    "title": "Revision Questions",
    "section": "6 Binary Outcome Models",
    "text": "6 Binary Outcome Models\n\nConsider the Logit model.\n1.1. Write down the likelihood function.\n1.2. State the maximization problem that solves for ML estimator.\n1.3. Solve for the F.O.C.s of the ML problem.\n1.4. Solve for the asymptotic variance-covariance matrix of \\(\\hat{\\beta}^{ML}\\).\nConsider the Probit model.\n2.1. Write down the likelihood function.\n2.2. State the maximization problem that solves for ML estimator.\n2.3. Solve for the F.O.C.s of the ML problem.\n2.4. Solve for the asymptotic variance-covariance matrix of \\(\\hat{\\beta}^{ML}\\).\nFrom the proof of the asymptotic normality, show\n\\[\nE\\bigg[\\frac{\\partial^2 f(Y_i|X_i;\\beta_0)/\\partial\\beta\\partial\\beta'}{f(Y_i|X_i;\\beta_0)}\\bigg]=0\n\\]\n\n\n\n\n\n\n\n\nCode\nquietly {\n  gen eduyrs = .\n    replace eduyrs = .3 if grade92==31\n    replace eduyrs = 3.2 if grade92==32\n    replace eduyrs = 7.2 if grade92==33\n    replace eduyrs = 7.2 if grade92==34\n    replace eduyrs = 9  if grade92==35\n    replace eduyrs = 10 if grade92==36\n    replace eduyrs = 11 if grade92==37\n    replace eduyrs = 12 if grade92==38\n    replace eduyrs = 12 if grade92==39\n    replace eduyrs = 13 if grade92==40\n    replace eduyrs = 14 if grade92==41\n    replace eduyrs = 14 if grade92==42\n    replace eduyrs = 16 if grade92==43\n    replace eduyrs = 18 if grade92==44\n    replace eduyrs = 18 if grade92==45\n    replace eduyrs = 18 if grade92==46\n    lab var eduyrs \"completed education\"\n  gen exper = age-(eduyrs+6)\n    replace exper=0 if exper&lt;0\n  keep if inrange(age,18,54)\n  gen lnwage = ln(earnwke)\n  gen exper2 = exper^2\n  gen employed = lnwage!=0\n  gen married = marital&lt;=3\n  gen female = sex==2\n  gen child = ownchild&gt;=1\n}\nlogit employed eduyrs exper exper2 i.married i.female##i.child \n\n\n\nIteration 0:  Log likelihood = -209.35624  \nIteration 1:  Log likelihood = -205.66439  \nIteration 2:  Log likelihood = -205.27888  \nIteration 3:  Log likelihood = -205.27756  \nIteration 4:  Log likelihood = -205.27756  \n\nLogistic regression                                    Number of obs = 165,038\n                                                       LR chi2(7)    =    8.16\n                                                       Prob &gt; chi2   =  0.3189\nLog likelihood = -205.27756                            Pseudo R2     =  0.0195\n\n------------------------------------------------------------------------------\n    employed | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      eduyrs |  -.2139028   .0990457    -2.16   0.031    -.4080287   -.0197768\n       exper |   .0408304   .0861167     0.47   0.635    -.1279552    .2096161\n      exper2 |  -.0012522   .0024549    -0.51   0.610    -.0060637    .0035593\n   1.married |  -.1047635   .5283537    -0.20   0.843    -1.140318    .9307907\n    1.female |   .0208576   .5035107     0.04   0.967    -.9660052    1.007721\n     1.child |   .4950251   .7490084     0.66   0.509    -.9730045    1.963055\n             |\nfemale#child |\n        1 1  |   .6419965   1.046185     0.61   0.539    -1.408489    2.692482\n             |\n       _cons |   11.64254   1.476069     7.89   0.000     8.749502    14.53559\n------------------------------------------------------------------------------\n\n\n\nConsider the logit model output above, estimated using the dataset from Problem Set 4. The sample and variables are defined as in PS 4.\n4.1. Compute the marginal effect of an additional year of education on the probability of being employed for a childless, unmarried, female with 15 years of education and 5 years of (potential) experience.\n4.2. Compute the marginal effect of being married for a male with children, 12 years of education and 10 years of experience."
  },
  {
    "objectID": "revision.html#endogenous-selection-models",
    "href": "revision.html#endogenous-selection-models",
    "title": "Revision Questions",
    "section": "7 Endogenous Selection Models",
    "text": "7 Endogenous Selection Models\n\nConsider the endogenous the right-censored Tobit model. The observed distribution is,\n\\[\nf(y) = \\begin{cases} f^*(y) \\qquad \\text{for} \\quad y &lt;0 \\\\\nF^*(0)  \\qquad \\text{for} \\quad y=0 \\\\\n0 \\qquad \\text{for} \\quad y&gt;0\n\\end{cases}\n\\] Suppose, \\(Y^* = X_i'\\beta + \\varepsilon_i\\), where the error term has a normally distributed (conditional on \\(X\\)).\n1.1. Solve for \\(E[Y_i|Y_i&lt;0]\\)\n1.2. Write down the likelihood function of the observed data.\n1.3. Solve for \\(\\frac{\\partial E[Y_i|X_i,Y_i&lt;0]}{\\partial X_i}\\)\n1.4. Solve for \\(\\frac{\\partial E[Y_i|X_i]}{\\partial X_i}\\)\n1.5. How do the answers compare to the case where \\(Y\\) is left-censored at 0?\n\n\n\nCode\nprobit employed eduyrs exper exper2 i.married i.female##i.child \n\n\n\nIteration 0:  Log likelihood = -209.35624  \nIteration 1:  Log likelihood = -205.60572  \nIteration 2:  Log likelihood = -205.33806  \nIteration 3:  Log likelihood = -205.33756  \nIteration 4:  Log likelihood = -205.33756  \n\nProbit regression                                      Number of obs = 165,038\n                                                       LR chi2(7)    =    8.04\n                                                       Prob &gt; chi2   =  0.3293\nLog likelihood = -205.33756                            Pseudo R2     =  0.0192\n\n------------------------------------------------------------------------------\n    employed | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      eduyrs |  -.0544498   .0255841    -2.13   0.033    -.1045937   -.0043059\n       exper |   .0095296   .0223186     0.43   0.669    -.0342139    .0532732\n      exper2 |  -.0002998   .0006319    -0.47   0.635    -.0015382    .0009387\n   1.married |  -.0259376   .1384233    -0.19   0.851    -.2972423    .2453672\n    1.female |   .0049027   .1321027     0.04   0.970    -.2540139    .2638192\n     1.child |   .1297358   .1937444     0.67   0.503    -.2499962    .5094679\n             |\nfemale#child |\n        1 1  |   .1540261   .2634162     0.58   0.559    -.3622602    .6703123\n             |\n       _cons |   4.341448   .3821216    11.36   0.000     3.592503    5.090392\n------------------------------------------------------------------------------\n\n\n\nConsider an endogenous threshold model, for (log of) employee earnings. Included in the model is a linear term in years of education, quadratic terms in years of (potential) experience, a married dummy-variable, and a female dummy-variable.\n2.1. Consider the output above from the selection equation, which includes an additional interaction between gender and presence of children (under 18). Compute the Inverse Mills Ratio for a married women, with children, 15 years of education and 8 years of experience.\n2.2. Write down the likelihood function of the observed outcomes: \\([ln(wage_i),Employed_i]\\). Where the latter is a dummy variable indicating employment (positive earnings).\n2.3. The Heckit output is given below. What can you conclude regarding selection into employment. And, under what assumptions.\n2.4. Does the interaction bewteen gender and parenthood belong in the main equation?\n\n\n\nCode\nheckman lnwage eduyrs exper exper2 i.married i.female, select(employed = exper exper2 i.married i.female##i.child) twostep\n\n\nnote: two-step estimate of rho = 19.806689 is being truncated to 1\n\nHeckman selection model -- two-step estimates   Number of obs     =    115,352\n(regression model with sample selection)              Selected    =    115,331\n                                                      Nonselected =         21\n\n                                                Wald chi2(5)      =       4.66\n                                                Prob &gt; chi2       =     0.4590\n\n------------------------------------------------------------------------------\n             | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nlnwage       |\n      eduyrs |   .1289768   .0827439     1.56   0.119    -.0331983     .291152\n       exper |   .0559093   .0878259     0.64   0.524    -.1162263    .2280449\n      exper2 |  -.0010644   .0023925    -0.44   0.656    -.0057535    .0036248\n   1.married |   .1015441   .4325466     0.23   0.814    -.7462316    .9493198\n    1.female |  -.3315884   .3953545    -0.84   0.402    -1.106469    .4432921\n       _cons |   4.387386   1.504299     2.92   0.004     1.439014    7.335759\n-------------+----------------------------------------------------------------\nemployed     |\n       exper |   .0079269   .0219371     0.36   0.718    -.0350691    .0509228\n      exper2 |  -.0001872   .0006005    -0.31   0.755     -.001364    .0009897\n   1.married |  -.0687001   .1375189    -0.50   0.617    -.3382323    .2008321\n    1.female |   -.023606   .1337126    -0.18   0.860    -.2856778    .2384659\n     1.child |   .1552287   .1938012     0.80   0.423    -.2246146    .5350721\n             |\nfemale#child |\n        1 1  |   .1308613   .2683362     0.49   0.626    -.3950681    .6567907\n             |\n       _cons |   3.482178   .1598692    21.78   0.000      3.16884    3.795516\n-------------+----------------------------------------------------------------\n/mills       |\n      lambda |   66.28671    929.642     0.07   0.943    -1755.778    1888.352\n-------------+----------------------------------------------------------------\n         rho |    1.00000\n       sigma |  66.286706\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "problem-set-6-solutions.html",
    "href": "problem-set-6-solutions.html",
    "title": "Problem Set 6 (SOLUTIONS)",
    "section": "",
    "text": "The purpose of the first part of this problem set is to estimate, interpret the results, and compare the results across different binary dependent variable models. In the second part you will estimate and compare different specifications of an endogenous selection model.\nFirst part will be discussed in week 9 and the second part in week 10 of this term.\nThe data file for this exercise is on Moodle: mus16data.dta. It is a subset of the data used by P. Deb, M. Munkin and P.K. Trivedi (2006): “Bayesian Analysis of Two-Part Model with Endogeneity”, Journal of Applied Econometrics, 21, 1081-1100. Data is for 2001 and comes from the Medical Expenditure Survey. Sample has 3,328 observations.\nThe main outcome variable of interest is ambulatory expenditure (ambexp) and the regressors are given below.\nSince the expenditure data is skewed, we will be using the logged expenditure variable as our dependent variable. You should read Cameron A.C. and Trivedi, P.K. Micro-econometrics using Stata to see the pros and cons regarding whether to log the dependent variable or not.\nNote, there is one individual who has an expenditure=1 and this will get coded as 0 when variable is logged. Since it is only one individual, we will ignore the problem by not doing anything. If there are many individuals like this, you will need to see whether you can say why this might be the case.\nDependent variable\n\nambexp: Ambulatory medical expenditures (excluding dental and outpatient mental). There are 526 individuals with zero expenditure. There is one individual who has expenditure=$1. I am going to assume that this individual did not spend any money.\nlambexp: ln(ambexp) given ambexp &gt; 0 ; missing otherwise\ndambexp: 1 if ambexp &gt; 0 and 0 otherwise (binary indicator)\n\nRegressors\n\nins: health insurance measures, either PPO or HMO type insurance\ntotchr: health status measures: number of chronic diseases\nage: age age in years/10\nfemale: 1 for females, zero otherwise\neduc: years of schooling of decision maker\nblhisp: either black or Hispanic\nincome: income in USD/1000\n\n\n\n\n\n\n\n\nCreate a do-file for this problem set and include a preamble that sets the directory and opens the data. For example,\n\nclear \n//or, to remove all stored values (including macros, matrices, scalars, etc.) \n*clear all\n\n* Replace $rootdir with the relevant path to on your local harddrive.\ncd \"$rootdir/problem-sets/ps-6\"\n\ncap log close\nlog using problem-set-6-log.txt, replace\n\nuse mus16data.dta, clear\n\nC:\\Users\\neil_\\OneDrive - University of Warwick\\Documents\\EC910\\website\\warwick\n&gt; -ec910\\problem-sets\\ps-6\n-------------------------------------------------------------------------------\n      name:  &lt;unnamed&gt;\n       log:  C:\\Users\\neil_\\OneDrive - University of Warwick\\Documents\\EC910\\we\n&gt; bsite\\warwick-ec910\\problem-sets\\ps-6\\problem-set-6-log.txt\n  log type:  smcl\n opened on:  19 Nov 2024, 17:40:40\n\n\n\n\n\n\n\n1.1. Obtain and comment on the descriptive statistics for ambexp, lambexp, age, female, educ, blhisp, totchr, ins, income.\n\nsu dambexp ambexp lambexp age female educ blhisp totchr ins income\n\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n     dambexp |      3,328    .8419471    .3648454          0          1\n      ambexp |      3,328    1386.519    2530.406          0      49960\n     lambexp |      2,802    6.555066     1.41073          0   10.81898\n         age |      3,328    4.056881    1.121212        2.1        6.4\n      female |      3,328    .5084135    .5000043          0          1\n-------------+---------------------------------------------------------\n        educ |      3,328    13.40565    2.574199          0         17\n      blhisp |      3,328    .3085938    .4619824          0          1\n      totchr |      3,328    .4831731    .7720426          0          5\n         ins |      3,328    .3650841    .4815261          0          1\n      income |      3,328    36.80485    26.70121     -90.05    237.301\n\n\n1.2. Estimate a LP, Probit and a Logit model to explain dambexp. Store the \\(\\beta\\) coefficients and report them in a table.\n\nglobal xlist age i.female educ i.blhisp totchr i.ins income  //Define regressor list $xlist\nbys dambexp: su $xlist   \n\n** LPM    \neststo LPM: reg dambexp $xlist, robust // heterosk needs to be corrected\n\n** probit\neststo probit: probit dambexp $xlist\n\n** logit\neststo logit: logit dambexp $xlist\n\nesttab LPM probit logit, se scalar(N r2 ll) mtitle(\"LPM\" \"Probit\" \"Logit\") title(Estimated Coefficients)\n\n\n-------------------------------------------------------------------------------\n-&gt; dambexp = 0\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n         age |        526    3.695627    1.076467        2.1        6.4\n             |\n      female |\n          0  |        526    .7338403    .4423695          0          1\n          1  |        526    .2661597    .4423695          0          1\n             |\n        educ |        526    12.48859    2.697241          0         17\n             |\n      blhisp |\n          0  |        526    .5171103    .5001828          0          1\n-------------+---------------------------------------------------------\n          1  |        526    .4828897    .5001828          0          1\n             |\n      totchr |        526    .0912548    .3074311          0          2\n             |\n         ins |\n          0  |        526    .6977186    .4596837          0          1\n          1  |        526    .3022814    .4596837          0          1\n             |\n      income |        526    31.63409    23.17116          0     166.78\n\n-------------------------------------------------------------------------------\n-&gt; dambexp = 1\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n         age |      2,802    4.124697    1.116641        2.1        6.4\n             |\n      female |\n          0  |      2,802    .4461099    .4971761          0          1\n          1  |      2,802    .5538901    .4971761          0          1\n             |\n        educ |      2,802     13.5778    2.513906          0         17\n             |\n      blhisp |\n          0  |      2,802    .7241256    .4470336          0          1\n-------------+---------------------------------------------------------\n          1  |      2,802    .2758744    .4470336          0          1\n             |\n      totchr |      2,802    .5567452     .809943          0          5\n             |\n         ins |\n          0  |      2,802    .6231263    .4846893          0          1\n          1  |      2,802    .3768737    .4846893          0          1\n             |\n      income |      2,802    37.77552    27.20742     -90.05    237.301\n\n\nLinear regression                               Number of obs     =      3,328\n                                                F(7, 3320)        =      69.43\n                                                Prob &gt; F          =     0.0000\n                                                R-squared         =     0.1276\n                                                Root MSE          =     .34114\n\n------------------------------------------------------------------------------\n             |               Robust\n     dambexp | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         age |   .0216413   .0056048     3.86   0.000     .0106521    .0326304\n    1.female |   .1394928   .0119061    11.72   0.000     .1161487    .1628368\n        educ |   .0143544   .0025774     5.57   0.000      .009301    .0194078\n    1.blhisp |  -.0889738   .0143088    -6.22   0.000    -.1170288   -.0609188\n      totchr |   .0830088   .0057913    14.33   0.000     .0716539    .0943637\n       1.ins |   .0364663   .0119311     3.06   0.002     .0130733    .0598592\n      income |    .000443   .0002215     2.00   0.046     8.70e-06    .0008774\n       _cons |   .4485313   .0430509    10.42   0.000     .3641224    .5329402\n------------------------------------------------------------------------------\n\nIteration 0:  Log likelihood = -1452.4289  \nIteration 1:  Log likelihood = -1218.0426  \nIteration 2:  Log likelihood = -1195.6199  \nIteration 3:  Log likelihood = -1195.5158  \nIteration 4:  Log likelihood = -1195.5158  \n\nProbit regression                                       Number of obs =  3,328\n                                                        LR chi2(7)    = 513.83\n                                                        Prob &gt; chi2   = 0.0000\nLog likelihood = -1195.5158                             Pseudo R2     = 0.1769\n\n------------------------------------------------------------------------------\n     dambexp | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         age |   .0868152   .0274556     3.16   0.002     .0330032    .1406272\n    1.female |   .6635053   .0609648    10.88   0.000     .5440164    .7829941\n        educ |    .061884    .012039     5.14   0.000      .038288    .0854801\n    1.blhisp |  -.3657835   .0619095    -5.91   0.000    -.4871239   -.2444432\n      totchr |   .7957496   .0712174    11.17   0.000      .656166    .9353332\n       1.ins |    .169107   .0629296     2.69   0.007     .0457673    .2924467\n      income |   .0026773   .0013105     2.04   0.041     .0001088    .0052458\n       _cons |  -.6686471   .1941247    -3.44   0.001    -1.049125   -.2881698\n------------------------------------------------------------------------------\n\nIteration 0:  Log likelihood = -1452.4289  \nIteration 1:  Log likelihood =  -1238.914  \nIteration 2:  Log likelihood = -1194.7039  \nIteration 3:  Log likelihood = -1192.8189  \nIteration 4:  Log likelihood = -1192.8089  \nIteration 5:  Log likelihood = -1192.8089  \n\nLogistic regression                                     Number of obs =  3,328\n                                                        LR chi2(7)    = 519.24\n                                                        Prob &gt; chi2   = 0.0000\nLog likelihood = -1192.8089                             Pseudo R2     = 0.1787\n\n------------------------------------------------------------------------------\n     dambexp | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         age |   .1618629   .0496641     3.26   0.001     .0645229    .2592028\n    1.female |   1.226724   .1130182    10.85   0.000     1.005212    1.448235\n        educ |   .1080498   .0210874     5.12   0.000     .0667192    .1493804\n    1.blhisp |  -.6666381   .1093362    -6.10   0.000    -.8809332    -.452343\n      totchr |   1.554664   .1499606    10.37   0.000     1.260746    1.848581\n       1.ins |    .296996   .1133154     2.62   0.009     .0749019    .5190901\n      income |     .00462   .0024332     1.90   0.058     -.000149     .009389\n       _cons |   -1.26832   .3407805    -3.72   0.000    -1.936237    -.600402\n------------------------------------------------------------------------------\n\nEstimated Coefficients\n------------------------------------------------------------\n                      (1)             (2)             (3)   \n                      LPM          Probit           Logit   \n------------------------------------------------------------\nmain                                                        \nage                0.0216***       0.0868**         0.162** \n                (0.00560)        (0.0275)        (0.0497)   \n\n0.female                0               0               0   \n                      (.)             (.)             (.)   \n\n1.female            0.139***        0.664***        1.227***\n                 (0.0119)        (0.0610)         (0.113)   \n\neduc               0.0144***       0.0619***        0.108***\n                (0.00258)        (0.0120)        (0.0211)   \n\n0.blhisp                0               0               0   \n                      (.)             (.)             (.)   \n\n1.blhisp          -0.0890***       -0.366***       -0.667***\n                 (0.0143)        (0.0619)         (0.109)   \n\ntotchr             0.0830***        0.796***        1.555***\n                (0.00579)        (0.0712)         (0.150)   \n\n0.ins                   0               0               0   \n                      (.)             (.)             (.)   \n\n1.ins              0.0365**         0.169**         0.297** \n                 (0.0119)        (0.0629)         (0.113)   \n\nincome           0.000443*        0.00268*        0.00462   \n               (0.000222)       (0.00131)       (0.00243)   \n\n_cons               0.449***       -0.669***       -1.268***\n                 (0.0431)         (0.194)         (0.341)   \n------------------------------------------------------------\nN                    3328            3328            3328   \nr2                  0.128                                   \nll                -1139.1         -1195.5         -1192.8   \n------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001\n\n\n1.3. Estimate the Marginal Effect at the Mean for each model, and report them in a table. You will want to use the estpost margins post-estimation command, with the relevant option for MEM. Pay special attention to the treatment of discrete regressors. Hint: check to see any differences in the estimated MEs based on whether you use factor notation; for example, i.female vs female.\n\nest clear\n\n** LPM   \nqui reg dambexp $xlist, robust \nestpost margins, dydx(*) atmean\nest store LPM\n\n** probit\nqui probit dambexp $xlist\nestpost margins, dydx(*) atmean\nest store probit\n\n** logit\nqui logit dambexp $xlist\nestpost margins, dydx(*) atmean\nest store logit\n\nesttab LPM probit logit, se scalar(N r2 ll) mtitle(\"LPM\" \"Probit\" \"Logit\") title(Marginal Effects at the Mean)\n\n\nConditional marginal effects                             Number of obs = 3,328\nModel VCE: Robust\n\nExpression: Linear prediction, predict()\ndy/dx wrt:  age 1.female educ 1.blhisp totchr 1.ins income\nAt: age      = 4.056881 (mean)\n    0.female = .4915865 (mean)\n    1.female = .5084135 (mean)\n    educ     = 13.40565 (mean)\n    0.blhisp = .6914063 (mean)\n    1.blhisp = .3085938 (mean)\n    totchr   = .4831731 (mean)\n    0.ins    = .6349159 (mean)\n    1.ins    = .3650841 (mean)\n    income   = 36.80485 (mean)\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         age |   .0216413   .0056048     3.86   0.000     .0106521    .0326304\n    1.female |   .1394928   .0119061    11.72   0.000     .1161487    .1628368\n        educ |   .0143544   .0025774     5.57   0.000      .009301    .0194078\n    1.blhisp |  -.0889738   .0143088    -6.22   0.000    -.1170288   -.0609188\n      totchr |   .0830088   .0057913    14.33   0.000     .0716539    .0943637\n       1.ins |   .0364663   .0119311     3.06   0.002     .0130733    .0598592\n      income |    .000443   .0002215     2.00   0.046     8.70e-06    .0008774\n------------------------------------------------------------------------------\nNote: dy/dx for factor levels is the discrete change from the base level.\n\nConditional marginal effects                             Number of obs = 3,328\nModel VCE: OIM\n\nExpression: Pr(dambexp), predict()\ndy/dx wrt:  age 1.female educ 1.blhisp totchr 1.ins income\nAt: age      = 4.056881 (mean)\n    0.female = .4915865 (mean)\n    1.female = .5084135 (mean)\n    educ     = 13.40565 (mean)\n    0.blhisp = .6914063 (mean)\n    1.blhisp = .3085938 (mean)\n    totchr   = .4831731 (mean)\n    0.ins    = .6349159 (mean)\n    1.ins    = .3650841 (mean)\n    income   = 36.80485 (mean)\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         age |   .0152201    .004837     3.15   0.002     .0057396    .0247005\n    1.female |   .1184629   .0112862    10.50   0.000     .0963423    .1405835\n        educ |   .0108492   .0021305     5.09   0.000     .0066736    .0150249\n    1.blhisp |  -.0701607   .0130287    -5.39   0.000    -.0956964   -.0446249\n      totchr |   .1395073   .0102098    13.66   0.000     .1194966    .1595181\n       1.ins |   .0288089   .0104412     2.76   0.006     .0083445    .0492733\n      income |   .0004694   .0002295     2.05   0.041     .0000195    .0009192\n------------------------------------------------------------------------------\nNote: dy/dx for factor levels is the discrete change from the base level.\n\nConditional marginal effects                             Number of obs = 3,328\nModel VCE: OIM\n\nExpression: Pr(dambexp), predict()\ndy/dx wrt:  age 1.female educ 1.blhisp totchr 1.ins income\nAt: age      = 4.056881 (mean)\n    0.female = .4915865 (mean)\n    1.female = .5084135 (mean)\n    educ     = 13.40565 (mean)\n    0.blhisp = .6914063 (mean)\n    1.blhisp = .3085938 (mean)\n    totchr   = .4831731 (mean)\n    0.ins    = .6349159 (mean)\n    1.ins    = .3650841 (mean)\n    income   = 36.80485 (mean)\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         age |   .0135771   .0042044     3.23   0.001     .0053365    .0218176\n    1.female |   .1068865   .0107295     9.96   0.000      .085857    .1279159\n        educ |   .0090632   .0018074     5.01   0.000     .0055208    .0126056\n    1.blhisp |   -.062462   .0116115    -5.38   0.000    -.0852201    -.039704\n      totchr |   .1304052   .0093686    13.92   0.000      .112043    .1487674\n       1.ins |   .0241537   .0089994     2.68   0.007     .0065151    .0417922\n      income |   .0003875   .0002043     1.90   0.058    -.0000128    .0007879\n------------------------------------------------------------------------------\nNote: dy/dx for factor levels is the discrete change from the base level.\n\nMarginal Effects at the Mean\n------------------------------------------------------------\n                      (1)             (2)             (3)   \n                      LPM          Probit           Logit   \n------------------------------------------------------------\nage                0.0216***       0.0152**        0.0136** \n                (0.00560)       (0.00484)       (0.00420)   \n\n0.female                0               0               0   \n                      (.)             (.)             (.)   \n\n1.female            0.139***        0.118***        0.107***\n                 (0.0119)        (0.0113)        (0.0107)   \n\neduc               0.0144***       0.0108***      0.00906***\n                (0.00258)       (0.00213)       (0.00181)   \n\n0.blhisp                0               0               0   \n                      (.)             (.)             (.)   \n\n1.blhisp          -0.0890***      -0.0702***      -0.0625***\n                 (0.0143)        (0.0130)        (0.0116)   \n\ntotchr             0.0830***        0.140***        0.130***\n                (0.00579)        (0.0102)       (0.00937)   \n\n0.ins                   0               0               0   \n                      (.)             (.)             (.)   \n\n1.ins              0.0365**        0.0288**        0.0242** \n                 (0.0119)        (0.0104)       (0.00900)   \n\nincome           0.000443*       0.000469*       0.000388   \n               (0.000222)      (0.000230)      (0.000204)   \n------------------------------------------------------------\nN                    3328            3328            3328   \nr2                  0.128                                   \nll                -1139.1         -1195.5         -1192.8   \n------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001\n\n\n1.4. Estimate the Average Marginal Effect at the Mean for each model, and report them in a table.You will want to use the estpost margins post-estimation command, with the relevant option for AME.\n\nest clear\n\n** LPM    \nqui reg dambexp $xlist, robust \nestpost margins, dydx(*)\nest store LPM\n\n** probit\nqui probit dambexp $xlist\nestpost margins, dydx(*)\nest store probit\n\n** logit\nqui logit dambexp $xlist\nestpost margins, dydx(*)\nest store logit\n\nesttab LPM probit logit, se scalar(N r2 ll) mtitle(\"LPM\" \"Probit\" \"Logit\") title(Average Marginal Effects)\n\n\nAverage marginal effects                                 Number of obs = 3,328\nModel VCE: Robust\n\nExpression: Linear prediction, predict()\ndy/dx wrt:  age 1.female educ 1.blhisp totchr 1.ins income\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         age |   .0216413   .0056048     3.86   0.000     .0106521    .0326304\n    1.female |   .1394928   .0119061    11.72   0.000     .1161487    .1628368\n        educ |   .0143544   .0025774     5.57   0.000      .009301    .0194078\n    1.blhisp |  -.0889738   .0143088    -6.22   0.000    -.1170288   -.0609188\n      totchr |   .0830088   .0057913    14.33   0.000     .0716539    .0943637\n       1.ins |   .0364663   .0119311     3.06   0.002     .0130733    .0598592\n      income |    .000443   .0002215     2.00   0.046     8.70e-06    .0008774\n------------------------------------------------------------------------------\nNote: dy/dx for factor levels is the discrete change from the base level.\n\nAverage marginal effects                                 Number of obs = 3,328\nModel VCE: OIM\n\nExpression: Pr(dambexp), predict()\ndy/dx wrt:  age 1.female educ 1.blhisp totchr 1.ins income\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         age |   .0173895   .0054832     3.17   0.002     .0066426    .0281365\n    1.female |    .132906   .0118133    11.25   0.000     .1097524    .1560596\n        educ |   .0123957   .0023862     5.19   0.000     .0077189    .0170725\n    1.blhisp |  -.0777517   .0137954    -5.64   0.000    -.1047901   -.0507133\n      totchr |   .1593929   .0139062    11.46   0.000     .1321371    .1866486\n       1.ins |    .033324   .0121638     2.74   0.006     .0094834    .0571646\n      income |   .0005363   .0002621     2.05   0.041     .0000225    .0010501\n------------------------------------------------------------------------------\nNote: dy/dx for factor levels is the discrete change from the base level.\n\nAverage marginal effects                                 Number of obs = 3,328\nModel VCE: OIM\n\nExpression: Pr(dambexp), predict()\ndy/dx wrt:  age 1.female educ 1.blhisp totchr 1.ins income\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         age |   .0181007   .0055261     3.28   0.001     .0072697    .0289317\n    1.female |   .1357958   .0117423    11.56   0.000     .1127813    .1588102\n        educ |   .0120829   .0023248     5.20   0.000     .0075264    .0166394\n    1.blhisp |  -.0795676   .0136638    -5.82   0.000    -.1063481   -.0527871\n      totchr |   .1738536   .0164518    10.57   0.000     .1416087    .2060985\n       1.ins |   .0326182   .0121752     2.68   0.007     .0087552    .0564812\n      income |   .0005166   .0002718     1.90   0.057     -.000016    .0010493\n------------------------------------------------------------------------------\nNote: dy/dx for factor levels is the discrete change from the base level.\n\nAverage Marginal Effects\n------------------------------------------------------------\n                      (1)             (2)             (3)   \n                      LPM          Probit           Logit   \n------------------------------------------------------------\nage                0.0216***       0.0174**        0.0181** \n                (0.00560)       (0.00548)       (0.00553)   \n\n0.female                0               0               0   \n                      (.)             (.)             (.)   \n\n1.female            0.139***        0.133***        0.136***\n                 (0.0119)        (0.0118)        (0.0117)   \n\neduc               0.0144***       0.0124***       0.0121***\n                (0.00258)       (0.00239)       (0.00232)   \n\n0.blhisp                0               0               0   \n                      (.)             (.)             (.)   \n\n1.blhisp          -0.0890***      -0.0778***      -0.0796***\n                 (0.0143)        (0.0138)        (0.0137)   \n\ntotchr             0.0830***        0.159***        0.174***\n                (0.00579)        (0.0139)        (0.0165)   \n\n0.ins                   0               0               0   \n                      (.)             (.)             (.)   \n\n1.ins              0.0365**        0.0333**        0.0326** \n                 (0.0119)        (0.0122)        (0.0122)   \n\nincome           0.000443*       0.000536*       0.000517   \n               (0.000222)      (0.000262)      (0.000272)   \n------------------------------------------------------------\nN                    3328            3328            3328   \nr2                  0.128                                   \nll                -1139.1         -1195.5         -1192.8   \n------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001\n\n\n1.5. Check to see how well the prodbit model predicts the outcome using the estat classification post-estimation command.\n\nqui probit dambexp age i.female educ i.blhisp totchr i.ins income\nestat classification\n\n\nProbit model for dambexp\n\n              -------- True --------\nClassified |         D            ~D  |      Total\n-----------+--------------------------+-----------\n     +     |      2768           469  |       3237\n     -     |        34            57  |         91\n-----------+--------------------------+-----------\n   Total   |      2802           526  |       3328\n\nClassified + if predicted Pr(D) &gt;= .5\nTrue D defined as dambexp != 0\n--------------------------------------------------\nSensitivity                     Pr( +| D)   98.79%\nSpecificity                     Pr( -|~D)   10.84%\nPositive predictive value       Pr( D| +)   85.51%\nNegative predictive value       Pr(~D| -)   62.64%\n--------------------------------------------------\nFalse + rate for true ~D        Pr( +|~D)   89.16%\nFalse - rate for true D         Pr( -| D)    1.21%\nFalse + rate for classified +   Pr(~D| +)   14.49%\nFalse - rate for classified -   Pr( D| -)   37.36%\n--------------------------------------------------\nCorrectly classified                        84.89%\n--------------------------------------------------\n\n\n1.6. Construct and interpret the LR test for the omission of income in the probit model. Do this in two ways: (1) using the post estimation lrtest; (2) manually recreate (1)’s results (both test-statistic and p-value).\n\nest clear\n\n** Remove income from xlist\nglobal xlist age i.female educ i.blhisp totchr i.ins \n\neststo modU: qui probit dambexp $xlist income\nscalar logl_U = e(ll)\n\neststo modR: qui probit dambexp $xlist\nscalar logl_R = e(ll)\n\nlrtest modU modR\n\n** Replicate\nscalar stat = 2 * (logl_U - logl_R) \nscalar pval = chi2tail(1,stat)\nscalar list\n\n\nLikelihood-ratio test\nAssumption: modR nested within modU\n\n LR chi2(1) =   4.30\nProb &gt; chi2 = 0.0382\n      pval =  .03817363\n      stat =   4.297269\n    logl_R = -1197.6644\n    logl_U = -1195.5158\n\n\n\n\n\nEstimate the following models for lambexp treating the selection into non-zero lambexp value as endogenous using, both Heckman 2-step method and also MLE.\nIn the main data lambexp is missing for values of ambexp=0. Before proceeding,\n\nreplace lambexp = 0 if ambexp==0\n\n(526 real changes made)\n\n\nThis will correction will also treat observations with ambexp=1 as equivalent to =0; however, this is only a single observation.\n\n** Remove income from xlist\nglobal xlist age i.female educ i.blhisp totchr i.ins\n\n2.1. Estimate the Heckman 2-step estimator and store the results. In addition, store the Mills ratio as a separate variable. Use income as the excluded variable. This means that income appears in the selection equation, but NOT the main equation.\n\neststo heck_2sW: heckman lambexp $xlist, select(dambexp = $xlist income) twostep mills(mills_a)\n\n\nHeckman selection model -- two-step estimates   Number of obs     =      3,328\n(regression model with sample selection)              Selected    =      2,802\n                                                      Nonselected =        526\n\n                                                Wald chi2(6)      =     193.43\n                                                Prob &gt; chi2       =     0.0000\n\n------------------------------------------------------------------------------\n             | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nlambexp      |\n         age |   .2024668   .0242202     8.36   0.000     .1549961    .2499374\n    1.female |   .2921341   .0725756     4.03   0.000     .1498886    .4343796\n        educ |   .0123889   .0115682     1.07   0.284    -.0102844    .0350622\n    1.blhisp |  -.1828659   .0653449    -2.80   0.005    -.3109396   -.0547922\n      totchr |   .5006332   .0485548    10.31   0.000     .4054675    .5957988\n       1.ins |  -.0465097   .0529742    -0.88   0.380    -.1503373    .0573179\n       _cons |   5.288927    .288522    18.33   0.000     4.723435     5.85442\n-------------+----------------------------------------------------------------\ndambexp      |\n         age |   .0868152   .0274556     3.16   0.002     .0330032    .1406272\n    1.female |   .6635053   .0609648    10.88   0.000     .5440165    .7829941\n        educ |    .061884    .012039     5.14   0.000      .038288    .0854801\n    1.blhisp |  -.3657835   .0619095    -5.91   0.000    -.4871239   -.2444432\n      totchr |   .7957496   .0712174    11.17   0.000      .656166    .9353332\n       1.ins |    .169107   .0629296     2.69   0.007     .0457673    .2924467\n      income |   .0026773   .0013105     2.04   0.041     .0001088    .0052458\n       _cons |  -.6686471   .1941247    -3.44   0.001    -1.049125   -.2881698\n-------------+----------------------------------------------------------------\n/mills       |\n      lambda |  -.4637133   .2825997    -1.64   0.101    -1.017598     .090172\n-------------+----------------------------------------------------------------\n         rho |   -0.35907\n       sigma |  1.2914258\n------------------------------------------------------------------------------\n\n\n2.2. Replicate these results by applying the following steps: (1) estimate the selection equation using a probit model; (2) create the mills ratio; (3) compare your mills ratio with the one stored above; (4) estimate the main equation, including the mills ratio.\n\nprobit dambexp $xlist income\npredict index, xb\ngen mills = normalden(index)/normal(index)\ncompare mills mills_a\nreg lambexp $xlist mills\n\n\nIteration 0:  Log likelihood = -1452.4289  \nIteration 1:  Log likelihood = -1218.0426  \nIteration 2:  Log likelihood = -1195.6199  \nIteration 3:  Log likelihood = -1195.5158  \nIteration 4:  Log likelihood = -1195.5158  \n\nProbit regression                                       Number of obs =  3,328\n                                                        LR chi2(7)    = 513.83\n                                                        Prob &gt; chi2   = 0.0000\nLog likelihood = -1195.5158                             Pseudo R2     = 0.1769\n\n------------------------------------------------------------------------------\n     dambexp | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         age |   .0868152   .0274556     3.16   0.002     .0330032    .1406272\n    1.female |   .6635053   .0609648    10.88   0.000     .5440164    .7829941\n        educ |    .061884    .012039     5.14   0.000      .038288    .0854801\n    1.blhisp |  -.3657835   .0619095    -5.91   0.000    -.4871239   -.2444432\n      totchr |   .7957496   .0712174    11.17   0.000      .656166    .9353332\n       1.ins |    .169107   .0629296     2.69   0.007     .0457673    .2924467\n      income |   .0026773   .0013105     2.04   0.041     .0001088    .0052458\n       _cons |  -.6686471   .1941247    -3.44   0.001    -1.049125   -.2881698\n------------------------------------------------------------------------------\n\n                                        ---------- Difference ----------\n                            Count       Minimum      Average     Maximum\n------------------------------------------------------------------------\nmills&lt;mills_a                1660     -5.02e-08    -8.91e-09   -1.75e-14\nmills&gt;mills_a                1668      7.75e-15     8.64e-09    5.45e-08\n                       ----------\nJointly defined              3328     -5.02e-08    -1.10e-10    5.45e-08\n                       ----------\nTotal                        3328\n\n      Source |       SS           df       MS      Number of obs   =     3,328\n-------------+----------------------------------   F(7, 3320)      =    164.14\n       Model |  6325.70678         7  903.672398   Prob &gt; F        =    0.0000\n    Residual |  18278.1125     3,320  5.50545557   R-squared       =    0.2571\n-------------+----------------------------------   Adj R-squared   =    0.2555\n       Total |  24603.8193     3,327  7.39519666   Root MSE        =    2.3464\n\n------------------------------------------------------------------------------\n     lambexp | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         age |   .1628716   .0410937     3.96   0.000     .0823001    .2434431\n    1.female |   .1898127   .1257893     1.51   0.131    -.0568198    .4364451\n        educ |  -.0016555   .0199775    -0.08   0.934    -.0408251     .037514\n    1.blhisp |  -.1086689   .1107824    -0.98   0.327    -.3258776    .1085397\n      totchr |   .4202052   .0839337     5.01   0.000     .2556382    .5847723\n       1.ins |  -.0686172   .0899672    -0.76   0.446    -.2450141    .1077796\n       mills |  -4.592193   .4582359   -10.02   0.000    -5.490646   -3.693739\n       _cons |   5.904903   .5004624    11.80   0.000     4.923657    6.886149\n------------------------------------------------------------------------------\n\n\n2.3 Estimate the marginal effects of the selection equation. You can do this using the margins command, with predict() option psel. This should correspond to a probit model estimation above.\n\nqui heckman lambexp $xlist, select(dambexp = $xlist income) twostep\nmargins, dydx(*) predict(psel)\n\n\nAverage marginal effects                                 Number of obs = 3,328\nModel VCE: Conventional\n\nExpression: Pr(dambexp), predict(psel)\ndy/dx wrt:  age 1.female educ 1.blhisp totchr 1.ins income\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         age |   .0173895   .0054832     3.17   0.002     .0066426    .0281365\n    1.female |    .132906   .0118133    11.25   0.000     .1097524    .1560596\n        educ |   .0123957   .0023862     5.19   0.000     .0077189    .0170725\n    1.blhisp |  -.0777517   .0137954    -5.64   0.000    -.1047901   -.0507133\n      totchr |   .1593929   .0139062    11.46   0.000     .1321371    .1866486\n       1.ins |    .033324   .0121638     2.74   0.006     .0094834    .0571646\n      income |   .0005363   .0002621     2.05   0.041     .0000225    .0010501\n------------------------------------------------------------------------------\nNote: dy/dx for factor levels is the discrete change from the base level.\n\n\n2.4. Estimate the Maximum Likelihood version of the Heckmann correction (with an excluded variable) and store the results.\n\neststo heck_mlW: heckman lambexp $xlist, select(dambexp = $xlist income) nolog mills(mills_a_mle)\n\n\nHeckman selection model                         Number of obs     =      3,328\n(regression model with sample selection)              Selected    =      2,802\n                                                      Nonselected =        526\n\n                                                Wald chi2(6)      =     288.88\nLog likelihood = -5836.219                      Prob &gt; chi2       =     0.0000\n\n------------------------------------------------------------------------------\n             | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nlambexp      |\n         age |   .2119749   .0230072     9.21   0.000     .1668816    .2570682\n    1.female |   .3481441   .0601142     5.79   0.000     .2303223    .4659658\n        educ |    .018716   .0105473     1.77   0.076    -.0019563    .0393883\n    1.blhisp |  -.2185714   .0596687    -3.66   0.000    -.3355199    -.101623\n      totchr |     .53992   .0393324    13.73   0.000     .4628299      .61701\n       1.ins |  -.0299871   .0510882    -0.59   0.557    -.1301182    .0701439\n       _cons |   5.044056   .2281259    22.11   0.000     4.596938    5.491175\n-------------+----------------------------------------------------------------\ndambexp      |\n         age |   .0879359    .027421     3.21   0.001     .0341917      .14168\n    1.female |   .6626649   .0609384    10.87   0.000     .5432278    .7821021\n        educ |   .0619485   .0120295     5.15   0.000     .0383711    .0855258\n    1.blhisp |  -.3639377   .0618734    -5.88   0.000    -.4852073   -.2426682\n      totchr |   .7969518   .0711306    11.20   0.000     .6575383    .9363653\n       1.ins |   .1701367   .0628711     2.71   0.007     .0469117    .2933618\n      income |   .0027078   .0013168     2.06   0.040      .000127    .0052886\n       _cons |  -.6760546   .1940288    -3.48   0.000    -1.056344   -.2957652\n-------------+----------------------------------------------------------------\n     /athrho |  -.1313456   .1496292    -0.88   0.380    -.4246134    .1619222\n    /lnsigma |   .2398173   .0144598    16.59   0.000     .2114767     .268158\n-------------+----------------------------------------------------------------\n         rho |  -.1305955   .1470772                     -.4008098    .1605217\n       sigma |   1.271017   .0183786                      1.235501    1.307554\n      lambda |  -.1659891   .1878698                     -.5342072    .2022291\n------------------------------------------------------------------------------\nLR test of indep. eqns. (rho = 0): chi2(1) = 0.91         Prob &gt; chi2 = 0.3406\n\n\n2.5 Compute the marginal effects of each regressor for: (1) probability of selection; (2) the expected value of the outcome; and (3) the expected value of the outcome, conditional on selection. You will need to use the post-estimation command margins, dydx(*) predict() with predict options: psel, yexpected, and ycond.\n\nqui heckman lambexp $xlist, select(dambexp = $xlist income) nolog\n\nmargins, dydx(*) predict(psel) \nmargins, dydx(*) predict(yexpected) \nmargins, dydx(*) predict(ycond) \n\n\nAverage marginal effects                                 Number of obs = 3,328\nModel VCE: OIM\n\nExpression: Pr(dambexp), predict(psel)\ndy/dx wrt:  age 1.female educ 1.blhisp totchr 1.ins income\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         age |   .0176149   .0054761     3.22   0.001      .006882    .0283479\n    1.female |   .1327517   .0118078    11.24   0.000     .1096089    .1558945\n        educ |   .0124093   .0023845     5.20   0.000     .0077357    .0170828\n    1.blhisp |  -.0773377   .0137795    -5.61   0.000    -.1043449   -.0503305\n      totchr |    .159642    .013898    11.49   0.000     .1324024    .1868817\n       1.ins |    .033526   .0121515     2.76   0.006     .0097095    .0573425\n      income |   .0005424   .0002634     2.06   0.039     .0000262    .0010586\n------------------------------------------------------------------------------\nNote: dy/dx for factor levels is the discrete change from the base level.\n\nAverage marginal effects                                 Number of obs = 3,328\nModel VCE: OIM\n\nExpression: E(lambexp*|Pr(dambexp)), predict(yexpected)\ndy/dx wrt:  age 1.female educ 1.blhisp totchr 1.ins income\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         age |   .2897346   .0381846     7.59   0.000     .2148942     .364575\n    1.female |    1.14081   .0832165    13.71   0.000     .9777089    1.303911\n        educ |   .0941877   .0167238     5.63   0.000     .0614096    .1269658\n    1.blhisp |  -.6692709    .095073    -7.04   0.000    -.8556106   -.4829312\n      totchr |   1.463455   .0890935    16.43   0.000     1.288834    1.638075\n       1.ins |   .1863924   .0856166     2.18   0.029     .0185871    .3541978\n      income |   .0034285   .0016722     2.05   0.040     .0001511    .0067059\n------------------------------------------------------------------------------\nNote: dy/dx for factor levels is the discrete change from the base level.\n\nAverage marginal effects                                 Number of obs = 3,328\nModel VCE: OIM\n\nExpression: E(lambexp|Zg&gt;0), predict(ycond)\ndy/dx wrt:  age 1.female educ 1.blhisp totchr 1.ins income\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         age |   .2165793   .0222037     9.75   0.000     .1730609    .2600977\n    1.female |   .3826391   .0486389     7.87   0.000     .2873087    .4779695\n        educ |   .0219597   .0097532     2.25   0.024     .0028438    .0410755\n    1.blhisp |  -.2385954   .0551014    -4.33   0.000    -.3465921   -.1305986\n      totchr |   .5816493   .0379133    15.34   0.000     .5073406    .6559579\n       1.ins |  -.0212273   .0499484    -0.42   0.671    -.1191243    .0766697\n      income |   .0001418   .0001762     0.80   0.421    -.0002036    .0004872\n------------------------------------------------------------------------------\nNote: dy/dx for factor levels is the discrete change from the base level.\n\n\n2.6. Now re-estimate the two-step and MLE approach without an excluded variable, storing the results each time. This means that the same set of regressors enter both equations. i.e. include income in the outcome equation.\n\nglobal xlist age i.female educ i.blhisp totchr i.ins income\n\neststo heck_2sWO: heckman lambexp $xlist, select(dambexp = $xlist) twostep mills(mills_b)\n\neststo heck_mlWO: heckman lambexp $xlist, select(dambexp = $xlist) nolog mills(mills_b_mle)\n\n\nHeckman selection model -- two-step estimates   Number of obs     =      3,328\n(regression model with sample selection)              Selected    =      2,802\n                                                      Nonselected =        526\n\n                                                Wald chi2(7)      =     192.92\n                                                Prob &gt; chi2       =     0.0000\n\n------------------------------------------------------------------------------\n             | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nlambexp      |\n         age |   .2043022   .0244086     8.37   0.000     .1564622    .2521422\n    1.female |   .2786877   .0750154     3.72   0.000     .1316602    .4257151\n        educ |   .0141631   .0118462     1.20   0.232    -.0090551    .0373812\n    1.blhisp |  -.1797416   .0656337    -2.74   0.006    -.3083812    -.051102\n      totchr |   .4938391    .049539     9.97   0.000     .3967445    .5909337\n       1.ins |  -.0461181    .053113    -0.87   0.385    -.1502176    .0579815\n      income |  -.0007456   .0010158    -0.73   0.463    -.0027367    .0012454\n       _cons |   5.306311   .2901551    18.29   0.000     4.737617    5.875004\n-------------+----------------------------------------------------------------\ndambexp      |\n         age |   .0868152   .0274556     3.16   0.002     .0330032    .1406272\n    1.female |   .6635053   .0609648    10.88   0.000     .5440165    .7829941\n        educ |    .061884    .012039     5.14   0.000      .038288    .0854801\n    1.blhisp |  -.3657835   .0619095    -5.91   0.000    -.4871239   -.2444432\n      totchr |   .7957496   .0712174    11.17   0.000      .656166    .9353332\n       1.ins |    .169107   .0629296     2.69   0.007     .0457673    .2924467\n      income |   .0026773   .0013105     2.04   0.041     .0001088    .0052458\n       _cons |  -.6686471   .1941247    -3.44   0.001    -1.049125   -.2881698\n-------------+----------------------------------------------------------------\n/mills       |\n      lambda |  -.5087361   .2894687    -1.76   0.079    -1.076084    .0586121\n-------------+----------------------------------------------------------------\n         rho |   -0.39250\n       sigma |  1.2961455\n------------------------------------------------------------------------------\n\nHeckman selection model                         Number of obs     =      3,328\n(regression model with sample selection)              Selected    =      2,802\n                                                      Nonselected =        526\n\n                                                Wald chi2(7)      =     285.98\nLog likelihood =  -5836.09                      Prob &gt; chi2       =     0.0000\n\n------------------------------------------------------------------------------\n             | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nlambexp      |\n         age |   .2137594   .0232969     9.18   0.000     .1680983    .2594205\n    1.female |    .342293   .0615522     5.56   0.000     .2216528    .4629332\n        educ |   .0202746   .0110032     1.84   0.065    -.0012913    .0418406\n    1.blhisp |  -.2185104   .0598099    -3.65   0.000    -.3357357   -.1012852\n      totchr |   .5375964   .0398453    13.49   0.000      .459501    .6156918\n       1.ins |  -.0287728   .0511856    -0.56   0.574    -.1290946    .0715491\n      income |  -.0005026    .000989    -0.51   0.611    -.0024411    .0014359\n       _cons |   5.041712    .229726    21.95   0.000     4.591458    5.491967\n-------------+----------------------------------------------------------------\ndambexp      |\n         age |   .0878613   .0274099     3.21   0.001      .034139    .1415837\n    1.female |   .6628035    .060929    10.88   0.000     .5433848    .7822223\n        educ |   .0617998   .0120332     5.14   0.000     .0382152    .0853844\n    1.blhisp |  -.3636885   .0618724    -5.88   0.000    -.4849562   -.2424207\n      totchr |   .7968988   .0711265    11.20   0.000     .6574934    .9363041\n       1.ins |   .1699645   .0628669     2.70   0.007     .0467476    .2931815\n      income |   .0027483   .0013209     2.08   0.037     .0001595    .0053372\n       _cons |   -.675346   .1939739    -3.48   0.000    -1.055528    -.295164\n-------------+----------------------------------------------------------------\n     /athrho |  -.1419126   .1535634    -0.92   0.355    -.4428913    .1590661\n    /lnsigma |    .240186   .0146925    16.35   0.000     .2113892    .2689828\n-------------+----------------------------------------------------------------\n         rho |  -.1409675   .1505118                     -.4160382     .157738\n       sigma |   1.271486   .0186813                      1.235393    1.308633\n      lambda |  -.1792382   .1924853                     -.5565025    .1980261\n------------------------------------------------------------------------------\nLR test of indep. eqns. (rho = 0): chi2(1) = 1.02         Prob &gt; chi2 = 0.3122\n\n\n2.7. Create a table that reports the four models alongside one another and compare the results.\n\nesttab heck_2sW heck_2sWO heck_mlW heck_mlWO, se scalar(N) mtitle(\"2-step,w/\" \"2-step,w/o\" \"ML,w/\" \"ML,w/o\") title(Heckman Selection Models) drop(0.*)\n\n\nHeckman Selection Models\n----------------------------------------------------------------------------\n                      (1)             (2)             (3)             (4)   \n                2-step,w/      2-step,w/o           ML,w/          ML,w/o   \n----------------------------------------------------------------------------\nlambexp                                                                     \nage                 0.202***        0.204***        0.212***        0.214***\n                 (0.0242)        (0.0244)        (0.0230)        (0.0233)   \n\n1.female            0.292***        0.279***        0.348***        0.342***\n                 (0.0726)        (0.0750)        (0.0601)        (0.0616)   \n\neduc               0.0124          0.0142          0.0187          0.0203   \n                 (0.0116)        (0.0118)        (0.0105)        (0.0110)   \n\n1.blhisp           -0.183**        -0.180**        -0.219***       -0.219***\n                 (0.0653)        (0.0656)        (0.0597)        (0.0598)   \n\ntotchr              0.501***        0.494***        0.540***        0.538***\n                 (0.0486)        (0.0495)        (0.0393)        (0.0398)   \n\n1.ins             -0.0465         -0.0461         -0.0300         -0.0288   \n                 (0.0530)        (0.0531)        (0.0511)        (0.0512)   \n\nincome                          -0.000746                       -0.000503   \n                                (0.00102)                      (0.000989)   \n\n_cons               5.289***        5.306***        5.044***        5.042***\n                  (0.289)         (0.290)         (0.228)         (0.230)   \n----------------------------------------------------------------------------\ndambexp                                                                     \nage                0.0868**        0.0868**        0.0879**        0.0879** \n                 (0.0275)        (0.0275)        (0.0274)        (0.0274)   \n\n1.female            0.664***        0.664***        0.663***        0.663***\n                 (0.0610)        (0.0610)        (0.0609)        (0.0609)   \n\neduc               0.0619***       0.0619***       0.0619***       0.0618***\n                 (0.0120)        (0.0120)        (0.0120)        (0.0120)   \n\n1.blhisp           -0.366***       -0.366***       -0.364***       -0.364***\n                 (0.0619)        (0.0619)        (0.0619)        (0.0619)   \n\ntotchr              0.796***        0.796***        0.797***        0.797***\n                 (0.0712)        (0.0712)        (0.0711)        (0.0711)   \n\n1.ins               0.169**         0.169**         0.170**         0.170** \n                 (0.0629)        (0.0629)        (0.0629)        (0.0629)   \n\nincome            0.00268*        0.00268*        0.00271*        0.00275*  \n                (0.00131)       (0.00131)       (0.00132)       (0.00132)   \n\n_cons              -0.669***       -0.669***       -0.676***       -0.675***\n                  (0.194)         (0.194)         (0.194)         (0.194)   \n----------------------------------------------------------------------------\n/mills                                                                      \nlambda             -0.464          -0.509                                   \n                  (0.283)         (0.289)                                   \n----------------------------------------------------------------------------\n/                                                                           \nathrho                                             -0.131          -0.142   \n                                                  (0.150)         (0.154)   \n\nlnsigma                                             0.240***        0.240***\n                                                 (0.0145)        (0.0147)   \n----------------------------------------------------------------------------\nN                    3328            3328            3328            3328   \n----------------------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001\n\n\n\n\n\n\n\nlog close\n\n      name:  &lt;unnamed&gt;\n       log:  C:\\Users\\neil_\\OneDrive - University of Warwick\\Documents\\EC910\\we\n&gt; bsite\\warwick-ec910\\problem-sets\\ps-6\\problem-set-6-log.txt\n  log type:  smcl\n closed on:  19 Nov 2024, 17:40:52\n-------------------------------------------------------------------------------"
  },
  {
    "objectID": "problem-set-6-solutions.html#preamble",
    "href": "problem-set-6-solutions.html#preamble",
    "title": "Problem Set 6 (SOLUTIONS)",
    "section": "",
    "text": "Create a do-file for this problem set and include a preamble that sets the directory and opens the data. For example,\n\nclear \n//or, to remove all stored values (including macros, matrices, scalars, etc.) \n*clear all\n\n* Replace $rootdir with the relevant path to on your local harddrive.\ncd \"$rootdir/problem-sets/ps-6\"\n\ncap log close\nlog using problem-set-6-log.txt, replace\n\nuse mus16data.dta, clear\n\nC:\\Users\\neil_\\OneDrive - University of Warwick\\Documents\\EC910\\website\\warwick\n&gt; -ec910\\problem-sets\\ps-6\n-------------------------------------------------------------------------------\n      name:  &lt;unnamed&gt;\n       log:  C:\\Users\\neil_\\OneDrive - University of Warwick\\Documents\\EC910\\we\n&gt; bsite\\warwick-ec910\\problem-sets\\ps-6\\problem-set-6-log.txt\n  log type:  smcl\n opened on:  19 Nov 2024, 17:40:40"
  },
  {
    "objectID": "problem-set-6-solutions.html#questions",
    "href": "problem-set-6-solutions.html#questions",
    "title": "Problem Set 6 (SOLUTIONS)",
    "section": "",
    "text": "1.1. Obtain and comment on the descriptive statistics for ambexp, lambexp, age, female, educ, blhisp, totchr, ins, income.\n\nsu dambexp ambexp lambexp age female educ blhisp totchr ins income\n\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n     dambexp |      3,328    .8419471    .3648454          0          1\n      ambexp |      3,328    1386.519    2530.406          0      49960\n     lambexp |      2,802    6.555066     1.41073          0   10.81898\n         age |      3,328    4.056881    1.121212        2.1        6.4\n      female |      3,328    .5084135    .5000043          0          1\n-------------+---------------------------------------------------------\n        educ |      3,328    13.40565    2.574199          0         17\n      blhisp |      3,328    .3085938    .4619824          0          1\n      totchr |      3,328    .4831731    .7720426          0          5\n         ins |      3,328    .3650841    .4815261          0          1\n      income |      3,328    36.80485    26.70121     -90.05    237.301\n\n\n1.2. Estimate a LP, Probit and a Logit model to explain dambexp. Store the \\(\\beta\\) coefficients and report them in a table.\n\nglobal xlist age i.female educ i.blhisp totchr i.ins income  //Define regressor list $xlist\nbys dambexp: su $xlist   \n\n** LPM    \neststo LPM: reg dambexp $xlist, robust // heterosk needs to be corrected\n\n** probit\neststo probit: probit dambexp $xlist\n\n** logit\neststo logit: logit dambexp $xlist\n\nesttab LPM probit logit, se scalar(N r2 ll) mtitle(\"LPM\" \"Probit\" \"Logit\") title(Estimated Coefficients)\n\n\n-------------------------------------------------------------------------------\n-&gt; dambexp = 0\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n         age |        526    3.695627    1.076467        2.1        6.4\n             |\n      female |\n          0  |        526    .7338403    .4423695          0          1\n          1  |        526    .2661597    .4423695          0          1\n             |\n        educ |        526    12.48859    2.697241          0         17\n             |\n      blhisp |\n          0  |        526    .5171103    .5001828          0          1\n-------------+---------------------------------------------------------\n          1  |        526    .4828897    .5001828          0          1\n             |\n      totchr |        526    .0912548    .3074311          0          2\n             |\n         ins |\n          0  |        526    .6977186    .4596837          0          1\n          1  |        526    .3022814    .4596837          0          1\n             |\n      income |        526    31.63409    23.17116          0     166.78\n\n-------------------------------------------------------------------------------\n-&gt; dambexp = 1\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n         age |      2,802    4.124697    1.116641        2.1        6.4\n             |\n      female |\n          0  |      2,802    .4461099    .4971761          0          1\n          1  |      2,802    .5538901    .4971761          0          1\n             |\n        educ |      2,802     13.5778    2.513906          0         17\n             |\n      blhisp |\n          0  |      2,802    .7241256    .4470336          0          1\n-------------+---------------------------------------------------------\n          1  |      2,802    .2758744    .4470336          0          1\n             |\n      totchr |      2,802    .5567452     .809943          0          5\n             |\n         ins |\n          0  |      2,802    .6231263    .4846893          0          1\n          1  |      2,802    .3768737    .4846893          0          1\n             |\n      income |      2,802    37.77552    27.20742     -90.05    237.301\n\n\nLinear regression                               Number of obs     =      3,328\n                                                F(7, 3320)        =      69.43\n                                                Prob &gt; F          =     0.0000\n                                                R-squared         =     0.1276\n                                                Root MSE          =     .34114\n\n------------------------------------------------------------------------------\n             |               Robust\n     dambexp | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         age |   .0216413   .0056048     3.86   0.000     .0106521    .0326304\n    1.female |   .1394928   .0119061    11.72   0.000     .1161487    .1628368\n        educ |   .0143544   .0025774     5.57   0.000      .009301    .0194078\n    1.blhisp |  -.0889738   .0143088    -6.22   0.000    -.1170288   -.0609188\n      totchr |   .0830088   .0057913    14.33   0.000     .0716539    .0943637\n       1.ins |   .0364663   .0119311     3.06   0.002     .0130733    .0598592\n      income |    .000443   .0002215     2.00   0.046     8.70e-06    .0008774\n       _cons |   .4485313   .0430509    10.42   0.000     .3641224    .5329402\n------------------------------------------------------------------------------\n\nIteration 0:  Log likelihood = -1452.4289  \nIteration 1:  Log likelihood = -1218.0426  \nIteration 2:  Log likelihood = -1195.6199  \nIteration 3:  Log likelihood = -1195.5158  \nIteration 4:  Log likelihood = -1195.5158  \n\nProbit regression                                       Number of obs =  3,328\n                                                        LR chi2(7)    = 513.83\n                                                        Prob &gt; chi2   = 0.0000\nLog likelihood = -1195.5158                             Pseudo R2     = 0.1769\n\n------------------------------------------------------------------------------\n     dambexp | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         age |   .0868152   .0274556     3.16   0.002     .0330032    .1406272\n    1.female |   .6635053   .0609648    10.88   0.000     .5440164    .7829941\n        educ |    .061884    .012039     5.14   0.000      .038288    .0854801\n    1.blhisp |  -.3657835   .0619095    -5.91   0.000    -.4871239   -.2444432\n      totchr |   .7957496   .0712174    11.17   0.000      .656166    .9353332\n       1.ins |    .169107   .0629296     2.69   0.007     .0457673    .2924467\n      income |   .0026773   .0013105     2.04   0.041     .0001088    .0052458\n       _cons |  -.6686471   .1941247    -3.44   0.001    -1.049125   -.2881698\n------------------------------------------------------------------------------\n\nIteration 0:  Log likelihood = -1452.4289  \nIteration 1:  Log likelihood =  -1238.914  \nIteration 2:  Log likelihood = -1194.7039  \nIteration 3:  Log likelihood = -1192.8189  \nIteration 4:  Log likelihood = -1192.8089  \nIteration 5:  Log likelihood = -1192.8089  \n\nLogistic regression                                     Number of obs =  3,328\n                                                        LR chi2(7)    = 519.24\n                                                        Prob &gt; chi2   = 0.0000\nLog likelihood = -1192.8089                             Pseudo R2     = 0.1787\n\n------------------------------------------------------------------------------\n     dambexp | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         age |   .1618629   .0496641     3.26   0.001     .0645229    .2592028\n    1.female |   1.226724   .1130182    10.85   0.000     1.005212    1.448235\n        educ |   .1080498   .0210874     5.12   0.000     .0667192    .1493804\n    1.blhisp |  -.6666381   .1093362    -6.10   0.000    -.8809332    -.452343\n      totchr |   1.554664   .1499606    10.37   0.000     1.260746    1.848581\n       1.ins |    .296996   .1133154     2.62   0.009     .0749019    .5190901\n      income |     .00462   .0024332     1.90   0.058     -.000149     .009389\n       _cons |   -1.26832   .3407805    -3.72   0.000    -1.936237    -.600402\n------------------------------------------------------------------------------\n\nEstimated Coefficients\n------------------------------------------------------------\n                      (1)             (2)             (3)   \n                      LPM          Probit           Logit   \n------------------------------------------------------------\nmain                                                        \nage                0.0216***       0.0868**         0.162** \n                (0.00560)        (0.0275)        (0.0497)   \n\n0.female                0               0               0   \n                      (.)             (.)             (.)   \n\n1.female            0.139***        0.664***        1.227***\n                 (0.0119)        (0.0610)         (0.113)   \n\neduc               0.0144***       0.0619***        0.108***\n                (0.00258)        (0.0120)        (0.0211)   \n\n0.blhisp                0               0               0   \n                      (.)             (.)             (.)   \n\n1.blhisp          -0.0890***       -0.366***       -0.667***\n                 (0.0143)        (0.0619)         (0.109)   \n\ntotchr             0.0830***        0.796***        1.555***\n                (0.00579)        (0.0712)         (0.150)   \n\n0.ins                   0               0               0   \n                      (.)             (.)             (.)   \n\n1.ins              0.0365**         0.169**         0.297** \n                 (0.0119)        (0.0629)         (0.113)   \n\nincome           0.000443*        0.00268*        0.00462   \n               (0.000222)       (0.00131)       (0.00243)   \n\n_cons               0.449***       -0.669***       -1.268***\n                 (0.0431)         (0.194)         (0.341)   \n------------------------------------------------------------\nN                    3328            3328            3328   \nr2                  0.128                                   \nll                -1139.1         -1195.5         -1192.8   \n------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001\n\n\n1.3. Estimate the Marginal Effect at the Mean for each model, and report them in a table. You will want to use the estpost margins post-estimation command, with the relevant option for MEM. Pay special attention to the treatment of discrete regressors. Hint: check to see any differences in the estimated MEs based on whether you use factor notation; for example, i.female vs female.\n\nest clear\n\n** LPM   \nqui reg dambexp $xlist, robust \nestpost margins, dydx(*) atmean\nest store LPM\n\n** probit\nqui probit dambexp $xlist\nestpost margins, dydx(*) atmean\nest store probit\n\n** logit\nqui logit dambexp $xlist\nestpost margins, dydx(*) atmean\nest store logit\n\nesttab LPM probit logit, se scalar(N r2 ll) mtitle(\"LPM\" \"Probit\" \"Logit\") title(Marginal Effects at the Mean)\n\n\nConditional marginal effects                             Number of obs = 3,328\nModel VCE: Robust\n\nExpression: Linear prediction, predict()\ndy/dx wrt:  age 1.female educ 1.blhisp totchr 1.ins income\nAt: age      = 4.056881 (mean)\n    0.female = .4915865 (mean)\n    1.female = .5084135 (mean)\n    educ     = 13.40565 (mean)\n    0.blhisp = .6914063 (mean)\n    1.blhisp = .3085938 (mean)\n    totchr   = .4831731 (mean)\n    0.ins    = .6349159 (mean)\n    1.ins    = .3650841 (mean)\n    income   = 36.80485 (mean)\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         age |   .0216413   .0056048     3.86   0.000     .0106521    .0326304\n    1.female |   .1394928   .0119061    11.72   0.000     .1161487    .1628368\n        educ |   .0143544   .0025774     5.57   0.000      .009301    .0194078\n    1.blhisp |  -.0889738   .0143088    -6.22   0.000    -.1170288   -.0609188\n      totchr |   .0830088   .0057913    14.33   0.000     .0716539    .0943637\n       1.ins |   .0364663   .0119311     3.06   0.002     .0130733    .0598592\n      income |    .000443   .0002215     2.00   0.046     8.70e-06    .0008774\n------------------------------------------------------------------------------\nNote: dy/dx for factor levels is the discrete change from the base level.\n\nConditional marginal effects                             Number of obs = 3,328\nModel VCE: OIM\n\nExpression: Pr(dambexp), predict()\ndy/dx wrt:  age 1.female educ 1.blhisp totchr 1.ins income\nAt: age      = 4.056881 (mean)\n    0.female = .4915865 (mean)\n    1.female = .5084135 (mean)\n    educ     = 13.40565 (mean)\n    0.blhisp = .6914063 (mean)\n    1.blhisp = .3085938 (mean)\n    totchr   = .4831731 (mean)\n    0.ins    = .6349159 (mean)\n    1.ins    = .3650841 (mean)\n    income   = 36.80485 (mean)\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         age |   .0152201    .004837     3.15   0.002     .0057396    .0247005\n    1.female |   .1184629   .0112862    10.50   0.000     .0963423    .1405835\n        educ |   .0108492   .0021305     5.09   0.000     .0066736    .0150249\n    1.blhisp |  -.0701607   .0130287    -5.39   0.000    -.0956964   -.0446249\n      totchr |   .1395073   .0102098    13.66   0.000     .1194966    .1595181\n       1.ins |   .0288089   .0104412     2.76   0.006     .0083445    .0492733\n      income |   .0004694   .0002295     2.05   0.041     .0000195    .0009192\n------------------------------------------------------------------------------\nNote: dy/dx for factor levels is the discrete change from the base level.\n\nConditional marginal effects                             Number of obs = 3,328\nModel VCE: OIM\n\nExpression: Pr(dambexp), predict()\ndy/dx wrt:  age 1.female educ 1.blhisp totchr 1.ins income\nAt: age      = 4.056881 (mean)\n    0.female = .4915865 (mean)\n    1.female = .5084135 (mean)\n    educ     = 13.40565 (mean)\n    0.blhisp = .6914063 (mean)\n    1.blhisp = .3085938 (mean)\n    totchr   = .4831731 (mean)\n    0.ins    = .6349159 (mean)\n    1.ins    = .3650841 (mean)\n    income   = 36.80485 (mean)\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         age |   .0135771   .0042044     3.23   0.001     .0053365    .0218176\n    1.female |   .1068865   .0107295     9.96   0.000      .085857    .1279159\n        educ |   .0090632   .0018074     5.01   0.000     .0055208    .0126056\n    1.blhisp |   -.062462   .0116115    -5.38   0.000    -.0852201    -.039704\n      totchr |   .1304052   .0093686    13.92   0.000      .112043    .1487674\n       1.ins |   .0241537   .0089994     2.68   0.007     .0065151    .0417922\n      income |   .0003875   .0002043     1.90   0.058    -.0000128    .0007879\n------------------------------------------------------------------------------\nNote: dy/dx for factor levels is the discrete change from the base level.\n\nMarginal Effects at the Mean\n------------------------------------------------------------\n                      (1)             (2)             (3)   \n                      LPM          Probit           Logit   \n------------------------------------------------------------\nage                0.0216***       0.0152**        0.0136** \n                (0.00560)       (0.00484)       (0.00420)   \n\n0.female                0               0               0   \n                      (.)             (.)             (.)   \n\n1.female            0.139***        0.118***        0.107***\n                 (0.0119)        (0.0113)        (0.0107)   \n\neduc               0.0144***       0.0108***      0.00906***\n                (0.00258)       (0.00213)       (0.00181)   \n\n0.blhisp                0               0               0   \n                      (.)             (.)             (.)   \n\n1.blhisp          -0.0890***      -0.0702***      -0.0625***\n                 (0.0143)        (0.0130)        (0.0116)   \n\ntotchr             0.0830***        0.140***        0.130***\n                (0.00579)        (0.0102)       (0.00937)   \n\n0.ins                   0               0               0   \n                      (.)             (.)             (.)   \n\n1.ins              0.0365**        0.0288**        0.0242** \n                 (0.0119)        (0.0104)       (0.00900)   \n\nincome           0.000443*       0.000469*       0.000388   \n               (0.000222)      (0.000230)      (0.000204)   \n------------------------------------------------------------\nN                    3328            3328            3328   \nr2                  0.128                                   \nll                -1139.1         -1195.5         -1192.8   \n------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001\n\n\n1.4. Estimate the Average Marginal Effect at the Mean for each model, and report them in a table.You will want to use the estpost margins post-estimation command, with the relevant option for AME.\n\nest clear\n\n** LPM    \nqui reg dambexp $xlist, robust \nestpost margins, dydx(*)\nest store LPM\n\n** probit\nqui probit dambexp $xlist\nestpost margins, dydx(*)\nest store probit\n\n** logit\nqui logit dambexp $xlist\nestpost margins, dydx(*)\nest store logit\n\nesttab LPM probit logit, se scalar(N r2 ll) mtitle(\"LPM\" \"Probit\" \"Logit\") title(Average Marginal Effects)\n\n\nAverage marginal effects                                 Number of obs = 3,328\nModel VCE: Robust\n\nExpression: Linear prediction, predict()\ndy/dx wrt:  age 1.female educ 1.blhisp totchr 1.ins income\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         age |   .0216413   .0056048     3.86   0.000     .0106521    .0326304\n    1.female |   .1394928   .0119061    11.72   0.000     .1161487    .1628368\n        educ |   .0143544   .0025774     5.57   0.000      .009301    .0194078\n    1.blhisp |  -.0889738   .0143088    -6.22   0.000    -.1170288   -.0609188\n      totchr |   .0830088   .0057913    14.33   0.000     .0716539    .0943637\n       1.ins |   .0364663   .0119311     3.06   0.002     .0130733    .0598592\n      income |    .000443   .0002215     2.00   0.046     8.70e-06    .0008774\n------------------------------------------------------------------------------\nNote: dy/dx for factor levels is the discrete change from the base level.\n\nAverage marginal effects                                 Number of obs = 3,328\nModel VCE: OIM\n\nExpression: Pr(dambexp), predict()\ndy/dx wrt:  age 1.female educ 1.blhisp totchr 1.ins income\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         age |   .0173895   .0054832     3.17   0.002     .0066426    .0281365\n    1.female |    .132906   .0118133    11.25   0.000     .1097524    .1560596\n        educ |   .0123957   .0023862     5.19   0.000     .0077189    .0170725\n    1.blhisp |  -.0777517   .0137954    -5.64   0.000    -.1047901   -.0507133\n      totchr |   .1593929   .0139062    11.46   0.000     .1321371    .1866486\n       1.ins |    .033324   .0121638     2.74   0.006     .0094834    .0571646\n      income |   .0005363   .0002621     2.05   0.041     .0000225    .0010501\n------------------------------------------------------------------------------\nNote: dy/dx for factor levels is the discrete change from the base level.\n\nAverage marginal effects                                 Number of obs = 3,328\nModel VCE: OIM\n\nExpression: Pr(dambexp), predict()\ndy/dx wrt:  age 1.female educ 1.blhisp totchr 1.ins income\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         age |   .0181007   .0055261     3.28   0.001     .0072697    .0289317\n    1.female |   .1357958   .0117423    11.56   0.000     .1127813    .1588102\n        educ |   .0120829   .0023248     5.20   0.000     .0075264    .0166394\n    1.blhisp |  -.0795676   .0136638    -5.82   0.000    -.1063481   -.0527871\n      totchr |   .1738536   .0164518    10.57   0.000     .1416087    .2060985\n       1.ins |   .0326182   .0121752     2.68   0.007     .0087552    .0564812\n      income |   .0005166   .0002718     1.90   0.057     -.000016    .0010493\n------------------------------------------------------------------------------\nNote: dy/dx for factor levels is the discrete change from the base level.\n\nAverage Marginal Effects\n------------------------------------------------------------\n                      (1)             (2)             (3)   \n                      LPM          Probit           Logit   \n------------------------------------------------------------\nage                0.0216***       0.0174**        0.0181** \n                (0.00560)       (0.00548)       (0.00553)   \n\n0.female                0               0               0   \n                      (.)             (.)             (.)   \n\n1.female            0.139***        0.133***        0.136***\n                 (0.0119)        (0.0118)        (0.0117)   \n\neduc               0.0144***       0.0124***       0.0121***\n                (0.00258)       (0.00239)       (0.00232)   \n\n0.blhisp                0               0               0   \n                      (.)             (.)             (.)   \n\n1.blhisp          -0.0890***      -0.0778***      -0.0796***\n                 (0.0143)        (0.0138)        (0.0137)   \n\ntotchr             0.0830***        0.159***        0.174***\n                (0.00579)        (0.0139)        (0.0165)   \n\n0.ins                   0               0               0   \n                      (.)             (.)             (.)   \n\n1.ins              0.0365**        0.0333**        0.0326** \n                 (0.0119)        (0.0122)        (0.0122)   \n\nincome           0.000443*       0.000536*       0.000517   \n               (0.000222)      (0.000262)      (0.000272)   \n------------------------------------------------------------\nN                    3328            3328            3328   \nr2                  0.128                                   \nll                -1139.1         -1195.5         -1192.8   \n------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001\n\n\n1.5. Check to see how well the prodbit model predicts the outcome using the estat classification post-estimation command.\n\nqui probit dambexp age i.female educ i.blhisp totchr i.ins income\nestat classification\n\n\nProbit model for dambexp\n\n              -------- True --------\nClassified |         D            ~D  |      Total\n-----------+--------------------------+-----------\n     +     |      2768           469  |       3237\n     -     |        34            57  |         91\n-----------+--------------------------+-----------\n   Total   |      2802           526  |       3328\n\nClassified + if predicted Pr(D) &gt;= .5\nTrue D defined as dambexp != 0\n--------------------------------------------------\nSensitivity                     Pr( +| D)   98.79%\nSpecificity                     Pr( -|~D)   10.84%\nPositive predictive value       Pr( D| +)   85.51%\nNegative predictive value       Pr(~D| -)   62.64%\n--------------------------------------------------\nFalse + rate for true ~D        Pr( +|~D)   89.16%\nFalse - rate for true D         Pr( -| D)    1.21%\nFalse + rate for classified +   Pr(~D| +)   14.49%\nFalse - rate for classified -   Pr( D| -)   37.36%\n--------------------------------------------------\nCorrectly classified                        84.89%\n--------------------------------------------------\n\n\n1.6. Construct and interpret the LR test for the omission of income in the probit model. Do this in two ways: (1) using the post estimation lrtest; (2) manually recreate (1)’s results (both test-statistic and p-value).\n\nest clear\n\n** Remove income from xlist\nglobal xlist age i.female educ i.blhisp totchr i.ins \n\neststo modU: qui probit dambexp $xlist income\nscalar logl_U = e(ll)\n\neststo modR: qui probit dambexp $xlist\nscalar logl_R = e(ll)\n\nlrtest modU modR\n\n** Replicate\nscalar stat = 2 * (logl_U - logl_R) \nscalar pval = chi2tail(1,stat)\nscalar list\n\n\nLikelihood-ratio test\nAssumption: modR nested within modU\n\n LR chi2(1) =   4.30\nProb &gt; chi2 = 0.0382\n      pval =  .03817363\n      stat =   4.297269\n    logl_R = -1197.6644\n    logl_U = -1195.5158\n\n\n\n\n\nEstimate the following models for lambexp treating the selection into non-zero lambexp value as endogenous using, both Heckman 2-step method and also MLE.\nIn the main data lambexp is missing for values of ambexp=0. Before proceeding,\n\nreplace lambexp = 0 if ambexp==0\n\n(526 real changes made)\n\n\nThis will correction will also treat observations with ambexp=1 as equivalent to =0; however, this is only a single observation.\n\n** Remove income from xlist\nglobal xlist age i.female educ i.blhisp totchr i.ins\n\n2.1. Estimate the Heckman 2-step estimator and store the results. In addition, store the Mills ratio as a separate variable. Use income as the excluded variable. This means that income appears in the selection equation, but NOT the main equation.\n\neststo heck_2sW: heckman lambexp $xlist, select(dambexp = $xlist income) twostep mills(mills_a)\n\n\nHeckman selection model -- two-step estimates   Number of obs     =      3,328\n(regression model with sample selection)              Selected    =      2,802\n                                                      Nonselected =        526\n\n                                                Wald chi2(6)      =     193.43\n                                                Prob &gt; chi2       =     0.0000\n\n------------------------------------------------------------------------------\n             | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nlambexp      |\n         age |   .2024668   .0242202     8.36   0.000     .1549961    .2499374\n    1.female |   .2921341   .0725756     4.03   0.000     .1498886    .4343796\n        educ |   .0123889   .0115682     1.07   0.284    -.0102844    .0350622\n    1.blhisp |  -.1828659   .0653449    -2.80   0.005    -.3109396   -.0547922\n      totchr |   .5006332   .0485548    10.31   0.000     .4054675    .5957988\n       1.ins |  -.0465097   .0529742    -0.88   0.380    -.1503373    .0573179\n       _cons |   5.288927    .288522    18.33   0.000     4.723435     5.85442\n-------------+----------------------------------------------------------------\ndambexp      |\n         age |   .0868152   .0274556     3.16   0.002     .0330032    .1406272\n    1.female |   .6635053   .0609648    10.88   0.000     .5440165    .7829941\n        educ |    .061884    .012039     5.14   0.000      .038288    .0854801\n    1.blhisp |  -.3657835   .0619095    -5.91   0.000    -.4871239   -.2444432\n      totchr |   .7957496   .0712174    11.17   0.000      .656166    .9353332\n       1.ins |    .169107   .0629296     2.69   0.007     .0457673    .2924467\n      income |   .0026773   .0013105     2.04   0.041     .0001088    .0052458\n       _cons |  -.6686471   .1941247    -3.44   0.001    -1.049125   -.2881698\n-------------+----------------------------------------------------------------\n/mills       |\n      lambda |  -.4637133   .2825997    -1.64   0.101    -1.017598     .090172\n-------------+----------------------------------------------------------------\n         rho |   -0.35907\n       sigma |  1.2914258\n------------------------------------------------------------------------------\n\n\n2.2. Replicate these results by applying the following steps: (1) estimate the selection equation using a probit model; (2) create the mills ratio; (3) compare your mills ratio with the one stored above; (4) estimate the main equation, including the mills ratio.\n\nprobit dambexp $xlist income\npredict index, xb\ngen mills = normalden(index)/normal(index)\ncompare mills mills_a\nreg lambexp $xlist mills\n\n\nIteration 0:  Log likelihood = -1452.4289  \nIteration 1:  Log likelihood = -1218.0426  \nIteration 2:  Log likelihood = -1195.6199  \nIteration 3:  Log likelihood = -1195.5158  \nIteration 4:  Log likelihood = -1195.5158  \n\nProbit regression                                       Number of obs =  3,328\n                                                        LR chi2(7)    = 513.83\n                                                        Prob &gt; chi2   = 0.0000\nLog likelihood = -1195.5158                             Pseudo R2     = 0.1769\n\n------------------------------------------------------------------------------\n     dambexp | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         age |   .0868152   .0274556     3.16   0.002     .0330032    .1406272\n    1.female |   .6635053   .0609648    10.88   0.000     .5440164    .7829941\n        educ |    .061884    .012039     5.14   0.000      .038288    .0854801\n    1.blhisp |  -.3657835   .0619095    -5.91   0.000    -.4871239   -.2444432\n      totchr |   .7957496   .0712174    11.17   0.000      .656166    .9353332\n       1.ins |    .169107   .0629296     2.69   0.007     .0457673    .2924467\n      income |   .0026773   .0013105     2.04   0.041     .0001088    .0052458\n       _cons |  -.6686471   .1941247    -3.44   0.001    -1.049125   -.2881698\n------------------------------------------------------------------------------\n\n                                        ---------- Difference ----------\n                            Count       Minimum      Average     Maximum\n------------------------------------------------------------------------\nmills&lt;mills_a                1660     -5.02e-08    -8.91e-09   -1.75e-14\nmills&gt;mills_a                1668      7.75e-15     8.64e-09    5.45e-08\n                       ----------\nJointly defined              3328     -5.02e-08    -1.10e-10    5.45e-08\n                       ----------\nTotal                        3328\n\n      Source |       SS           df       MS      Number of obs   =     3,328\n-------------+----------------------------------   F(7, 3320)      =    164.14\n       Model |  6325.70678         7  903.672398   Prob &gt; F        =    0.0000\n    Residual |  18278.1125     3,320  5.50545557   R-squared       =    0.2571\n-------------+----------------------------------   Adj R-squared   =    0.2555\n       Total |  24603.8193     3,327  7.39519666   Root MSE        =    2.3464\n\n------------------------------------------------------------------------------\n     lambexp | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         age |   .1628716   .0410937     3.96   0.000     .0823001    .2434431\n    1.female |   .1898127   .1257893     1.51   0.131    -.0568198    .4364451\n        educ |  -.0016555   .0199775    -0.08   0.934    -.0408251     .037514\n    1.blhisp |  -.1086689   .1107824    -0.98   0.327    -.3258776    .1085397\n      totchr |   .4202052   .0839337     5.01   0.000     .2556382    .5847723\n       1.ins |  -.0686172   .0899672    -0.76   0.446    -.2450141    .1077796\n       mills |  -4.592193   .4582359   -10.02   0.000    -5.490646   -3.693739\n       _cons |   5.904903   .5004624    11.80   0.000     4.923657    6.886149\n------------------------------------------------------------------------------\n\n\n2.3 Estimate the marginal effects of the selection equation. You can do this using the margins command, with predict() option psel. This should correspond to a probit model estimation above.\n\nqui heckman lambexp $xlist, select(dambexp = $xlist income) twostep\nmargins, dydx(*) predict(psel)\n\n\nAverage marginal effects                                 Number of obs = 3,328\nModel VCE: Conventional\n\nExpression: Pr(dambexp), predict(psel)\ndy/dx wrt:  age 1.female educ 1.blhisp totchr 1.ins income\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         age |   .0173895   .0054832     3.17   0.002     .0066426    .0281365\n    1.female |    .132906   .0118133    11.25   0.000     .1097524    .1560596\n        educ |   .0123957   .0023862     5.19   0.000     .0077189    .0170725\n    1.blhisp |  -.0777517   .0137954    -5.64   0.000    -.1047901   -.0507133\n      totchr |   .1593929   .0139062    11.46   0.000     .1321371    .1866486\n       1.ins |    .033324   .0121638     2.74   0.006     .0094834    .0571646\n      income |   .0005363   .0002621     2.05   0.041     .0000225    .0010501\n------------------------------------------------------------------------------\nNote: dy/dx for factor levels is the discrete change from the base level.\n\n\n2.4. Estimate the Maximum Likelihood version of the Heckmann correction (with an excluded variable) and store the results.\n\neststo heck_mlW: heckman lambexp $xlist, select(dambexp = $xlist income) nolog mills(mills_a_mle)\n\n\nHeckman selection model                         Number of obs     =      3,328\n(regression model with sample selection)              Selected    =      2,802\n                                                      Nonselected =        526\n\n                                                Wald chi2(6)      =     288.88\nLog likelihood = -5836.219                      Prob &gt; chi2       =     0.0000\n\n------------------------------------------------------------------------------\n             | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nlambexp      |\n         age |   .2119749   .0230072     9.21   0.000     .1668816    .2570682\n    1.female |   .3481441   .0601142     5.79   0.000     .2303223    .4659658\n        educ |    .018716   .0105473     1.77   0.076    -.0019563    .0393883\n    1.blhisp |  -.2185714   .0596687    -3.66   0.000    -.3355199    -.101623\n      totchr |     .53992   .0393324    13.73   0.000     .4628299      .61701\n       1.ins |  -.0299871   .0510882    -0.59   0.557    -.1301182    .0701439\n       _cons |   5.044056   .2281259    22.11   0.000     4.596938    5.491175\n-------------+----------------------------------------------------------------\ndambexp      |\n         age |   .0879359    .027421     3.21   0.001     .0341917      .14168\n    1.female |   .6626649   .0609384    10.87   0.000     .5432278    .7821021\n        educ |   .0619485   .0120295     5.15   0.000     .0383711    .0855258\n    1.blhisp |  -.3639377   .0618734    -5.88   0.000    -.4852073   -.2426682\n      totchr |   .7969518   .0711306    11.20   0.000     .6575383    .9363653\n       1.ins |   .1701367   .0628711     2.71   0.007     .0469117    .2933618\n      income |   .0027078   .0013168     2.06   0.040      .000127    .0052886\n       _cons |  -.6760546   .1940288    -3.48   0.000    -1.056344   -.2957652\n-------------+----------------------------------------------------------------\n     /athrho |  -.1313456   .1496292    -0.88   0.380    -.4246134    .1619222\n    /lnsigma |   .2398173   .0144598    16.59   0.000     .2114767     .268158\n-------------+----------------------------------------------------------------\n         rho |  -.1305955   .1470772                     -.4008098    .1605217\n       sigma |   1.271017   .0183786                      1.235501    1.307554\n      lambda |  -.1659891   .1878698                     -.5342072    .2022291\n------------------------------------------------------------------------------\nLR test of indep. eqns. (rho = 0): chi2(1) = 0.91         Prob &gt; chi2 = 0.3406\n\n\n2.5 Compute the marginal effects of each regressor for: (1) probability of selection; (2) the expected value of the outcome; and (3) the expected value of the outcome, conditional on selection. You will need to use the post-estimation command margins, dydx(*) predict() with predict options: psel, yexpected, and ycond.\n\nqui heckman lambexp $xlist, select(dambexp = $xlist income) nolog\n\nmargins, dydx(*) predict(psel) \nmargins, dydx(*) predict(yexpected) \nmargins, dydx(*) predict(ycond) \n\n\nAverage marginal effects                                 Number of obs = 3,328\nModel VCE: OIM\n\nExpression: Pr(dambexp), predict(psel)\ndy/dx wrt:  age 1.female educ 1.blhisp totchr 1.ins income\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         age |   .0176149   .0054761     3.22   0.001      .006882    .0283479\n    1.female |   .1327517   .0118078    11.24   0.000     .1096089    .1558945\n        educ |   .0124093   .0023845     5.20   0.000     .0077357    .0170828\n    1.blhisp |  -.0773377   .0137795    -5.61   0.000    -.1043449   -.0503305\n      totchr |    .159642    .013898    11.49   0.000     .1324024    .1868817\n       1.ins |    .033526   .0121515     2.76   0.006     .0097095    .0573425\n      income |   .0005424   .0002634     2.06   0.039     .0000262    .0010586\n------------------------------------------------------------------------------\nNote: dy/dx for factor levels is the discrete change from the base level.\n\nAverage marginal effects                                 Number of obs = 3,328\nModel VCE: OIM\n\nExpression: E(lambexp*|Pr(dambexp)), predict(yexpected)\ndy/dx wrt:  age 1.female educ 1.blhisp totchr 1.ins income\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         age |   .2897346   .0381846     7.59   0.000     .2148942     .364575\n    1.female |    1.14081   .0832165    13.71   0.000     .9777089    1.303911\n        educ |   .0941877   .0167238     5.63   0.000     .0614096    .1269658\n    1.blhisp |  -.6692709    .095073    -7.04   0.000    -.8556106   -.4829312\n      totchr |   1.463455   .0890935    16.43   0.000     1.288834    1.638075\n       1.ins |   .1863924   .0856166     2.18   0.029     .0185871    .3541978\n      income |   .0034285   .0016722     2.05   0.040     .0001511    .0067059\n------------------------------------------------------------------------------\nNote: dy/dx for factor levels is the discrete change from the base level.\n\nAverage marginal effects                                 Number of obs = 3,328\nModel VCE: OIM\n\nExpression: E(lambexp|Zg&gt;0), predict(ycond)\ndy/dx wrt:  age 1.female educ 1.blhisp totchr 1.ins income\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         age |   .2165793   .0222037     9.75   0.000     .1730609    .2600977\n    1.female |   .3826391   .0486389     7.87   0.000     .2873087    .4779695\n        educ |   .0219597   .0097532     2.25   0.024     .0028438    .0410755\n    1.blhisp |  -.2385954   .0551014    -4.33   0.000    -.3465921   -.1305986\n      totchr |   .5816493   .0379133    15.34   0.000     .5073406    .6559579\n       1.ins |  -.0212273   .0499484    -0.42   0.671    -.1191243    .0766697\n      income |   .0001418   .0001762     0.80   0.421    -.0002036    .0004872\n------------------------------------------------------------------------------\nNote: dy/dx for factor levels is the discrete change from the base level.\n\n\n2.6. Now re-estimate the two-step and MLE approach without an excluded variable, storing the results each time. This means that the same set of regressors enter both equations. i.e. include income in the outcome equation.\n\nglobal xlist age i.female educ i.blhisp totchr i.ins income\n\neststo heck_2sWO: heckman lambexp $xlist, select(dambexp = $xlist) twostep mills(mills_b)\n\neststo heck_mlWO: heckman lambexp $xlist, select(dambexp = $xlist) nolog mills(mills_b_mle)\n\n\nHeckman selection model -- two-step estimates   Number of obs     =      3,328\n(regression model with sample selection)              Selected    =      2,802\n                                                      Nonselected =        526\n\n                                                Wald chi2(7)      =     192.92\n                                                Prob &gt; chi2       =     0.0000\n\n------------------------------------------------------------------------------\n             | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nlambexp      |\n         age |   .2043022   .0244086     8.37   0.000     .1564622    .2521422\n    1.female |   .2786877   .0750154     3.72   0.000     .1316602    .4257151\n        educ |   .0141631   .0118462     1.20   0.232    -.0090551    .0373812\n    1.blhisp |  -.1797416   .0656337    -2.74   0.006    -.3083812    -.051102\n      totchr |   .4938391    .049539     9.97   0.000     .3967445    .5909337\n       1.ins |  -.0461181    .053113    -0.87   0.385    -.1502176    .0579815\n      income |  -.0007456   .0010158    -0.73   0.463    -.0027367    .0012454\n       _cons |   5.306311   .2901551    18.29   0.000     4.737617    5.875004\n-------------+----------------------------------------------------------------\ndambexp      |\n         age |   .0868152   .0274556     3.16   0.002     .0330032    .1406272\n    1.female |   .6635053   .0609648    10.88   0.000     .5440165    .7829941\n        educ |    .061884    .012039     5.14   0.000      .038288    .0854801\n    1.blhisp |  -.3657835   .0619095    -5.91   0.000    -.4871239   -.2444432\n      totchr |   .7957496   .0712174    11.17   0.000      .656166    .9353332\n       1.ins |    .169107   .0629296     2.69   0.007     .0457673    .2924467\n      income |   .0026773   .0013105     2.04   0.041     .0001088    .0052458\n       _cons |  -.6686471   .1941247    -3.44   0.001    -1.049125   -.2881698\n-------------+----------------------------------------------------------------\n/mills       |\n      lambda |  -.5087361   .2894687    -1.76   0.079    -1.076084    .0586121\n-------------+----------------------------------------------------------------\n         rho |   -0.39250\n       sigma |  1.2961455\n------------------------------------------------------------------------------\n\nHeckman selection model                         Number of obs     =      3,328\n(regression model with sample selection)              Selected    =      2,802\n                                                      Nonselected =        526\n\n                                                Wald chi2(7)      =     285.98\nLog likelihood =  -5836.09                      Prob &gt; chi2       =     0.0000\n\n------------------------------------------------------------------------------\n             | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nlambexp      |\n         age |   .2137594   .0232969     9.18   0.000     .1680983    .2594205\n    1.female |    .342293   .0615522     5.56   0.000     .2216528    .4629332\n        educ |   .0202746   .0110032     1.84   0.065    -.0012913    .0418406\n    1.blhisp |  -.2185104   .0598099    -3.65   0.000    -.3357357   -.1012852\n      totchr |   .5375964   .0398453    13.49   0.000      .459501    .6156918\n       1.ins |  -.0287728   .0511856    -0.56   0.574    -.1290946    .0715491\n      income |  -.0005026    .000989    -0.51   0.611    -.0024411    .0014359\n       _cons |   5.041712    .229726    21.95   0.000     4.591458    5.491967\n-------------+----------------------------------------------------------------\ndambexp      |\n         age |   .0878613   .0274099     3.21   0.001      .034139    .1415837\n    1.female |   .6628035    .060929    10.88   0.000     .5433848    .7822223\n        educ |   .0617998   .0120332     5.14   0.000     .0382152    .0853844\n    1.blhisp |  -.3636885   .0618724    -5.88   0.000    -.4849562   -.2424207\n      totchr |   .7968988   .0711265    11.20   0.000     .6574934    .9363041\n       1.ins |   .1699645   .0628669     2.70   0.007     .0467476    .2931815\n      income |   .0027483   .0013209     2.08   0.037     .0001595    .0053372\n       _cons |   -.675346   .1939739    -3.48   0.000    -1.055528    -.295164\n-------------+----------------------------------------------------------------\n     /athrho |  -.1419126   .1535634    -0.92   0.355    -.4428913    .1590661\n    /lnsigma |    .240186   .0146925    16.35   0.000     .2113892    .2689828\n-------------+----------------------------------------------------------------\n         rho |  -.1409675   .1505118                     -.4160382     .157738\n       sigma |   1.271486   .0186813                      1.235393    1.308633\n      lambda |  -.1792382   .1924853                     -.5565025    .1980261\n------------------------------------------------------------------------------\nLR test of indep. eqns. (rho = 0): chi2(1) = 1.02         Prob &gt; chi2 = 0.3122\n\n\n2.7. Create a table that reports the four models alongside one another and compare the results.\n\nesttab heck_2sW heck_2sWO heck_mlW heck_mlWO, se scalar(N) mtitle(\"2-step,w/\" \"2-step,w/o\" \"ML,w/\" \"ML,w/o\") title(Heckman Selection Models) drop(0.*)\n\n\nHeckman Selection Models\n----------------------------------------------------------------------------\n                      (1)             (2)             (3)             (4)   \n                2-step,w/      2-step,w/o           ML,w/          ML,w/o   \n----------------------------------------------------------------------------\nlambexp                                                                     \nage                 0.202***        0.204***        0.212***        0.214***\n                 (0.0242)        (0.0244)        (0.0230)        (0.0233)   \n\n1.female            0.292***        0.279***        0.348***        0.342***\n                 (0.0726)        (0.0750)        (0.0601)        (0.0616)   \n\neduc               0.0124          0.0142          0.0187          0.0203   \n                 (0.0116)        (0.0118)        (0.0105)        (0.0110)   \n\n1.blhisp           -0.183**        -0.180**        -0.219***       -0.219***\n                 (0.0653)        (0.0656)        (0.0597)        (0.0598)   \n\ntotchr              0.501***        0.494***        0.540***        0.538***\n                 (0.0486)        (0.0495)        (0.0393)        (0.0398)   \n\n1.ins             -0.0465         -0.0461         -0.0300         -0.0288   \n                 (0.0530)        (0.0531)        (0.0511)        (0.0512)   \n\nincome                          -0.000746                       -0.000503   \n                                (0.00102)                      (0.000989)   \n\n_cons               5.289***        5.306***        5.044***        5.042***\n                  (0.289)         (0.290)         (0.228)         (0.230)   \n----------------------------------------------------------------------------\ndambexp                                                                     \nage                0.0868**        0.0868**        0.0879**        0.0879** \n                 (0.0275)        (0.0275)        (0.0274)        (0.0274)   \n\n1.female            0.664***        0.664***        0.663***        0.663***\n                 (0.0610)        (0.0610)        (0.0609)        (0.0609)   \n\neduc               0.0619***       0.0619***       0.0619***       0.0618***\n                 (0.0120)        (0.0120)        (0.0120)        (0.0120)   \n\n1.blhisp           -0.366***       -0.366***       -0.364***       -0.364***\n                 (0.0619)        (0.0619)        (0.0619)        (0.0619)   \n\ntotchr              0.796***        0.796***        0.797***        0.797***\n                 (0.0712)        (0.0712)        (0.0711)        (0.0711)   \n\n1.ins               0.169**         0.169**         0.170**         0.170** \n                 (0.0629)        (0.0629)        (0.0629)        (0.0629)   \n\nincome            0.00268*        0.00268*        0.00271*        0.00275*  \n                (0.00131)       (0.00131)       (0.00132)       (0.00132)   \n\n_cons              -0.669***       -0.669***       -0.676***       -0.675***\n                  (0.194)         (0.194)         (0.194)         (0.194)   \n----------------------------------------------------------------------------\n/mills                                                                      \nlambda             -0.464          -0.509                                   \n                  (0.283)         (0.289)                                   \n----------------------------------------------------------------------------\n/                                                                           \nathrho                                             -0.131          -0.142   \n                                                  (0.150)         (0.154)   \n\nlnsigma                                             0.240***        0.240***\n                                                 (0.0145)        (0.0147)   \n----------------------------------------------------------------------------\nN                    3328            3328            3328            3328   \n----------------------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001"
  },
  {
    "objectID": "problem-set-6-solutions.html#postamble",
    "href": "problem-set-6-solutions.html#postamble",
    "title": "Problem Set 6 (SOLUTIONS)",
    "section": "",
    "text": "log close\n\n      name:  &lt;unnamed&gt;\n       log:  C:\\Users\\neil_\\OneDrive - University of Warwick\\Documents\\EC910\\we\n&gt; bsite\\warwick-ec910\\problem-sets\\ps-6\\problem-set-6-log.txt\n  log type:  smcl\n closed on:  19 Nov 2024, 17:40:52\n-------------------------------------------------------------------------------"
  },
  {
    "objectID": "problem-set-5-solutions.html",
    "href": "problem-set-5-solutions.html",
    "title": "Problem Set 5 (SOLUTIONS)",
    "section": "",
    "text": "This problem set will revise some of the material covered in Handout 5 on panel data models. This will require you to familiarize yourself with Stata’s panel-data commands.\n\nhelp xtset\nhelp xttab\nhelp xtreg\n\nYou will be using a dataset that comes with Stata: psidextract.dta. The data is a correct version of the PSID sample in Cornwell and Rupert (1988), found in Baltagi and Khanti-Akom (1990). It includes a sample of 595 individuals observed for the years 1976-82.\n\n\n\n\n\n\n\nCreate a do-file for this problem set and include a preamble that sets the directory and opens the data. For example,\n\nclear \n//or, to remove all stored values (including macros, matrices, scalars, etc.) \n*clear all\n\n* Replace $rootdir with the relevant path to on your local harddrive.\ncd \"$rootdir/problem-sets/ps-5\"\n\ncap log close\nlog using problem-set-5-log.txt, replace\n\nuse problem-set-5-data.dta, clear\n\nC:\\Users\\neil_\\OneDrive - University of Warwick\\Documents\\EC910\\website\\warwick\n&gt; -ec910\\problem-sets\\ps-5\n-------------------------------------------------------------------------------\n      name:  &lt;unnamed&gt;\n       log:  C:\\Users\\neil_\\OneDrive - University of Warwick\\Documents\\EC910\\we\n&gt; bsite\\warwick-ec910\\problem-sets\\ps-5\\problem-set-5-log.txt\n  log type:  smcl\n opened on:  11 Nov 2024, 12:44:56\n(PSID wage data 1976-82 from Baltagi and Khanti-Akom (1990))\n\n\n\n\n\n1. Set the unit identifier and time variable using xtset. Note, you can also use tsset for this task. This will allow you to use xt package commands.\n\nxtset id t\n\n\nPanel variable: id (strongly balanced)\n Time variable: t, 1 to 7\n         Delta: 1 unit\n\n\n2. Describe and summarise the variables in the dataset using the normal describe and summarize commands.\n\ndes \nsum id t lwage ed exper weeks south\n\n\nContains data from problem-set-5-data.dta\n Observations:         4,165                  PSID wage data 1976-82 from\n                                                Baltagi and Khanti-Akom (1990)\n    Variables:            14                  11 Nov 2024 11:19\n                                              (_dta has notes)\n-------------------------------------------------------------------------------\nVariable      Storage   Display    Value\n    name         type    format    label      Variable label\n-------------------------------------------------------------------------------\nexper           float   %9.0g                 years of full-time work\n                                                experience\nweeks           float   %9.0g                 weeks worked\noccup           float   %9.0g                 occupation; occ==1 if in a\n                                                blue-collar occupation\nindustry        float   %9.0g                 industry; ind==1 if working in a\n                                                manufacturing industry\nsouth           float   %9.0g                 residence; south==1 if in the\n                                                South area\nsmsa            float   %9.0g                 smsa==1 if in the Standard\n                                                metropolitan statistical area\nms              float   %9.0g                 marital status\nfemale          float   %9.0g                 female or male\nunion           float   %9.0g                 if wage set be a union contract\neduc            float   %9.0g                 years of education\nblack           float   %9.0g                 black\nlwage           float   %9.0g                 log wage\nid              float   %9.0g                 \nt               float   %9.0g                 \n-------------------------------------------------------------------------------\nSorted by: id  t\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n          id |      4,165         298    171.7821          1        595\n           t |      4,165           4     2.00024          1          7\n       lwage |      4,165    6.676346    .4615122    4.60517      8.537\n        educ |      4,165    12.84538    2.787995          4         17\n       exper |      4,165    19.85378    10.96637          1         51\n-------------+---------------------------------------------------------\n       weeks |      4,165    46.81152    5.129098          5         52\n       south |      4,165    .2902761    .4539442          0          1\n\n\n3. Describe and summarise the variables in the dataset using the panel commands: xtdescribe and xtsummarize. Comment on the information provided.\n\nxtdescribe \nxtsum id t lwage ed exper weeks south\n\n\n      id:  1, 2, ..., 595                                    n =        595\n       t:  1, 2, ..., 7                                      T =          7\n           Delta(t) = 1 unit\n           Span(t)  = 7 periods\n           (id*t uniquely identifies each observation)\n\nDistribution of T_i:   min      5%     25%       50%       75%     95%     max\n                         7       7       7         7         7       7       7\n\n     Freq.  Percent    Cum. |  Pattern\n ---------------------------+---------\n      595    100.00  100.00 |  1111111\n ---------------------------+---------\n      595    100.00         |  XXXXXXX\n\nVariable         |      Mean   Std. dev.       Min        Max |    Observations\n-----------------+--------------------------------------------+----------------\nid       overall |       298   171.7821          1        595 |     N =    4165\n         between |              171.906          1        595 |     n =     595\n         within  |                    0        298        298 |     T =       7\n                 |                                            |\nt        overall |         4    2.00024          1          7 |     N =    4165\n         between |                    0          4          4 |     n =     595\n         within  |              2.00024          1          7 |     T =       7\n                 |                                            |\nlwage    overall |  6.676346   .4615122    4.60517      8.537 |     N =    4165\n         between |             .3942387     5.3364   7.813596 |     n =     595\n         within  |             .2404023   4.781808   8.621092 |     T =       7\n                 |                                            |\neduc     overall |  12.84538   2.787995          4         17 |     N =    4165\n         between |             2.790006          4         17 |     n =     595\n         within  |                    0   12.84538   12.84538 |     T =       7\n                 |                                            |\nexper    overall |  19.85378   10.96637          1         51 |     N =    4165\n         between |             10.79018          4         48 |     n =     595\n         within  |              2.00024   16.85378   22.85378 |     T =       7\n                 |                                            |\nweeks    overall |  46.81152   5.129098          5         52 |     N =    4165\n         between |             3.284016   31.57143   51.57143 |     n =     595\n         within  |             3.941881    12.2401   63.66867 |     T =       7\n                 |                                            |\nsouth    overall |  .2902761   .4539442          0          1 |     N =    4165\n         between |             .4489462          0          1 |     n =     595\n         within  |             .0693042  -.5668667   1.147419 |     T =       7\n\n\n4. Use the command xttab and xtrans, freq to describe transitions over time in the variable south.\n\nxttab south\nxttrans south, freq\n\n\n                  Overall             Between            Within\n    south |    Freq.  Percent      Freq.  Percent        Percent\n----------+-----------------------------------------------------\n        0 |    2956     70.97       428     71.93          98.66\n        1 |    1209     29.03       182     30.59          94.90\n----------+-----------------------------------------------------\n    Total |    4165    100.00       610    102.52          97.54\n                               (n = 595)\n\nresidence; |\n  south==1 |  residence; south==1\n if in the | if in the South area\nSouth area |         0          1 |     Total\n-----------+----------------------+----------\n         0 |     2,527          8 |     2,535 \n           |     99.68       0.32 |    100.00 \n-----------+----------------------+----------\n         1 |         8      1,027 |     1,035 \n           |      0.77      99.23 |    100.00 \n-----------+----------------------+----------\n     Total |     2,535      1,035 |     3,570 \n           |     71.01      28.99 |    100.00 \n\n\n5. Create the variable: expsq=exper^2/1000. Why would you scale the variable in this way?\n\ngen expsq=exp*exp/1000\n\n6. Estimate the following model using pooled OLS, between-group, feasible GLS, within-group, LSDV, and first-difference. For the first-difference estimator, you can define a first-difference in Stata using the time-series operator: D.variable.\n\\[\n\\ln(Wage_{it}) = \\beta_1 + \\beta_2 Exper_{it} + \\beta_3 Exper^2_{it} + \\beta_4 Weeks_{it} + \\beta_5 Eduyrs_{it} + \\varepsilon_{it}\n\\] With each model, store the results using estimates store. For example,\n\n* clear existing stored estimates\nest clear\n\n* Pooled OLS\nregress lwage exper expsq weeks ed\nest store OlS\n\n* alternatively, \neststo OLS: regress lwage exper expsq weeks ed\n\n\nest clear\n* Pooled-OLS\neststo OLS: regress lwage exper expsq weeks ed\n\n* Between-group\neststo BG: xtreg lwage exper expsq weeks ed, be\n\n* Feasible-GLS\neststo FGLS: xtreg lwage exper expsq weeks ed, re theta\n\n* Within-group\neststo WG: xtreg lwage exper expsq weeks ed, fe \n\n* LSDV\neststo LSDV: areg lwage exper expsq weeks ed, absorb(id)\n\n* First-differnce\neststo FD: reg D.(lwage exper expsq weeks), noconst\n\n\n      Source |       SS           df       MS      Number of obs   =     4,165\n-------------+----------------------------------   F(4, 4160)      =    411.62\n       Model |  251.491445         4  62.8728612   Prob &gt; F        =    0.0000\n    Residual |  635.413457     4,160  .152743619   R-squared       =    0.2836\n-------------+----------------------------------   Adj R-squared   =    0.2829\n       Total |  886.904902     4,164  .212993492   Root MSE        =    .39082\n\n------------------------------------------------------------------------------\n       lwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       exper |    .044675   .0023929    18.67   0.000     .0399838    .0493663\n       expsq |   -.715631   .0527938   -13.56   0.000    -.8191351   -.6121268\n       weeks |    .005827   .0011827     4.93   0.000     .0035084    .0081456\n        educ |   .0760407   .0022266    34.15   0.000     .0716754     .080406\n       _cons |   4.907961   .0673297    72.89   0.000     4.775959    5.039963\n------------------------------------------------------------------------------\n\nBetween regression (regression on group means)  Number of obs     =      4,165\nGroup variable: id                              Number of groups  =        595\n\nR-squared:                                      Obs per group:\n     Within  = 0.1357                                         min =          7\n     Between = 0.3264                                         avg =        7.0\n     Overall = 0.2723                                         max =          7\n\n                                                F(4,590)          =      71.48\nsd(u_i + avg(e_i.)) = .324656                   Prob &gt; F          =     0.0000\n\n------------------------------------------------------------------------------\n       lwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       exper |    .038153   .0056967     6.70   0.000     .0269647    .0493412\n       expsq |   -.631272   .1256812    -5.02   0.000    -.8781089    -.384435\n       weeks |   .0130903   .0040659     3.22   0.001     .0051048    .0210757\n        educ |   .0737838   .0048985    15.06   0.000     .0641632    .0834044\n       _cons |   4.683039   .2100989    22.29   0.000     4.270407    5.095672\n------------------------------------------------------------------------------\n\nRandom-effects GLS regression                   Number of obs     =      4,165\nGroup variable: id                              Number of groups  =        595\n\nR-squared:                                      Obs per group:\n     Within  = 0.6340                                         min =          7\n     Between = 0.1716                                         avg =        7.0\n     Overall = 0.1830                                         max =          7\n\n                                                Wald chi2(4)      =    3012.45\ncorr(u_i, X) = 0 (assumed)                      Prob &gt; chi2       =     0.0000\ntheta        = .82280511\n\n------------------------------------------------------------------------------\n       lwage | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       exper |   .0888609   .0028178    31.54   0.000     .0833382    .0943837\n       expsq |   -.772565   .0622619   -12.41   0.000     -.894596   -.6505339\n       weeks |   .0009658   .0007433     1.30   0.194     -.000491    .0024226\n        educ |   .1117099   .0060572    18.44   0.000     .0998381    .1235818\n       _cons |   3.829366   .0936336    40.90   0.000     3.645848    4.012885\n-------------+----------------------------------------------------------------\n     sigma_u |  .31951859\n     sigma_e |  .15220316\n         rho |  .81505521   (fraction of variance due to u_i)\n------------------------------------------------------------------------------\nnote: educ omitted because of collinearity.\n\nFixed-effects (within) regression               Number of obs     =      4,165\nGroup variable: id                              Number of groups  =        595\n\nR-squared:                                      Obs per group:\n     Within  = 0.6566                                         min =          7\n     Between = 0.0276                                         avg =        7.0\n     Overall = 0.0476                                         max =          7\n\n                                                F(3, 3567)        =    2273.74\ncorr(u_i, Xb) = -0.9107                         Prob &gt; F          =     0.0000\n\n------------------------------------------------------------------------------\n       lwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       exper |   .1137879   .0024689    46.09   0.000     .1089473    .1186284\n       expsq |  -.4243693   .0546316    -7.77   0.000    -.5314816    -.317257\n       weeks |   .0008359   .0005997     1.39   0.163    -.0003399    .0020116\n        educ |          0  (omitted)\n       _cons |   4.596396   .0389061   118.14   0.000     4.520116    4.672677\n-------------+----------------------------------------------------------------\n     sigma_u |  1.0362039\n     sigma_e |  .15220316\n         rho |  .97888036   (fraction of variance due to u_i)\n------------------------------------------------------------------------------\nF test that all u_i=0: F(594, 3567) = 53.12                  Prob &gt; F = 0.0000\nnote: educ omitted because of collinearity.\n\nLinear regression, absorbing indicators            Number of obs     =   4,165\nAbsorbed variable: id                              No. of categories =     595\n                                                   F(3, 3567)        = 2273.74\n                                                   Prob &gt; F          =  0.0000\n                                                   R-squared         =  0.9068\n                                                   Adj R-squared     =  0.8912\n                                                   Root MSE          =  0.1522\n\n------------------------------------------------------------------------------\n       lwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       exper |   .1137879   .0024689    46.09   0.000     .1089473    .1186284\n       expsq |  -.4243693   .0546316    -7.77   0.000    -.5314816    -.317257\n       weeks |   .0008359   .0005997     1.39   0.163    -.0003399    .0020116\n        educ |          0  (omitted)\n       _cons |   4.596396   .0389061   118.14   0.000     4.520116    4.672677\n------------------------------------------------------------------------------\nF test of absorbed indicators: F(594, 3567) = 53.118          Prob &gt; F = 0.000\n\n      Source |       SS           df       MS      Number of obs   =     3,570\n-------------+----------------------------------   F(3, 3567)      =    337.12\n       Model |  33.3371458         3  11.1123819   Prob &gt; F        =    0.0000\n    Residual |   117.57812     3,567  .032962747   R-squared       =    0.2209\n-------------+----------------------------------   Adj R-squared   =    0.2202\n       Total |  150.915266     3,570  .042273184   Root MSE        =    .18156\n\n------------------------------------------------------------------------------\n     D.lwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       exper |\n         D1. |   .1170654   .0063106    18.55   0.000     .1046927    .1294381\n             |\n       expsq |\n         D1. |  -.5321208   .1392741    -3.82   0.000    -.8051857    -.259056\n             |\n       weeks |\n         D1. |  -.0002683   .0005648    -0.47   0.635    -.0013757    .0008392\n------------------------------------------------------------------------------\n\n\n7. Using the formula from Handout 5, replicate the value of \\(\\theta\\) reported above by the FGLS estimator. Note, you will need to use the stored values of \\(\\sigma^2_{\\varepsilon}\\) and \\(\\sigma^2_{\\alpha}\\).\n\nqui xtreg lwage exper expsq weeks ed, re theta\ndisplay \"theta = \"  1 - sqrt(e(sigma_e)^2 / (7*e(sigma_u)^2+e(sigma_e)^2))\n\ntheta = .82280511\n\n\n8. Make a table of the computed estimates. You can either use estimates table or esttab. The latter is part of the estout package, which you may need to install: ssc install estout.\n\nesttab OLS BG FGLS, se scalar(N r2 r2_o r2_b r2_w sigma_u sigma_e rho) mtitle(\"OLS\" \"BG\" \"FGLS\")\nesttab WG LSDV FD, se scalar(N r2 r2_o r2_b r2_w sigma_u sigma_e rho) rename(D.exper exper D.expsq expsq D.weeks weeks) mtitle(\"WG\" \"LSDV\" \"FD\")\n\n\n------------------------------------------------------------\n                      (1)             (2)             (3)   \n                      OLS              BG            FGLS   \n------------------------------------------------------------\nexper              0.0447***       0.0382***       0.0889***\n                (0.00239)       (0.00570)       (0.00282)   \n\nexpsq              -0.716***       -0.631***       -0.773***\n                 (0.0528)         (0.126)        (0.0623)   \n\nweeks             0.00583***       0.0131**      0.000966   \n                (0.00118)       (0.00407)      (0.000743)   \n\neduc               0.0760***       0.0738***        0.112***\n                (0.00223)       (0.00490)       (0.00606)   \n\n_cons               4.908***        4.683***        3.829***\n                 (0.0673)         (0.210)        (0.0936)   \n------------------------------------------------------------\nN                    4165            4165            4165   \nr2                  0.284           0.326                   \nr2_o                                0.272           0.183   \nr2_b                                0.326           0.172   \nr2_w                                0.136           0.634   \nsigma_u                                             0.320   \nsigma_e                                             0.152   \nrho                                                 0.815   \n------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001\n\n------------------------------------------------------------\n                      (1)             (2)             (3)   \n                       WG            LSDV              FD   \n------------------------------------------------------------\nexper               0.114***        0.114***        0.117***\n                (0.00247)       (0.00247)       (0.00631)   \n\nexpsq              -0.424***       -0.424***       -0.532***\n                 (0.0546)        (0.0546)         (0.139)   \n\nweeks            0.000836        0.000836       -0.000268   \n               (0.000600)      (0.000600)      (0.000565)   \n\neduc                    0               0                   \n                      (.)             (.)                   \n\n_cons               4.596***        4.596***                \n                 (0.0389)        (0.0389)                   \n------------------------------------------------------------\nN                    4165            4165            3570   \nr2                  0.657           0.907           0.221   \nr2_o               0.0476                                   \nr2_b               0.0276                                   \nr2_w                0.657                                   \nsigma_u             1.036                                   \nsigma_e             0.152                                   \nrho                 0.979                                   \n------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001\n\n\n9. Perform a Hausman test comparing the results of the FLGS and WG estimators. You should use the hausman command, with the option sigmamore. Be sure to get the order of the estimates correct. What do you learn from the test?\n\nhausman WG FGLS, sigmamore\n\n\n                 ---- Coefficients ----\n             |      (b)          (B)            (b-B)     sqrt(diag(V_b-V_B))\n             |       WG          FGLS        Difference       Std. err.\n-------------+----------------------------------------------------------------\n       exper |    .1137879     .0888609        .0249269        .0012778\n       expsq |   -.4243693     -.772565        .3481957        .0284727\n       weeks |    .0008359     .0009658       -.0001299        .0001108\n------------------------------------------------------------------------------\n                          b = Consistent under H0 and Ha; obtained from xtreg.\n           B = Inconsistent under Ha, efficient under H0; obtained from xtreg.\n\nTest of H0: Difference in coefficients not systematic\n\n    chi2(3) = (b-B)'[(V_b-V_B)^(-1)](b-B)\n            = 1513.02\nProb &gt; chi2 =  0.0000\n\n\n10. Estimate FGLS for the model below:\n\\[\n\\begin{aligned}\n\\ln(Wage_{it}) =& \\beta_1 + \\beta_2 Exper_{it} + \\beta_3 Exper^2_{it} + \\beta_4 Weeks_{it} + \\beta_5 Eduyrs_{it} \\\\\n&+ \\gamma_2 \\overline{Exper}_{i} + \\gamma_3 \\overline{Exper^2}_{i} + \\gamma_4 \\overline{Weeks}_{i}+\\varepsilon_{it}\n\\end{aligned}\n\\] You will need to manually create the variables: \\(\\{\\overline{Exper}_{i}, \\overline{Exper^2}_{i},\\overline{Weeks}_{i}\\}\\) - the individual-level averages of each variable. This is referred to as the Mundlack correction. Once you have estimated the model, repeat the Hausman test comparing these results with those of the WG estimator. What is the significance of the Mundlack correction?\n\nforeach var in exper expsq weeks{\n    bys id: egen av`var' = mean(`var')     \n}\n\neststo MUN: xtreg lwage exper expsq weeks ed avexper avexpsq avweeks, re theta\n\nesttab WG LSDV FD MUN, se scalar(N r2 r2_o r2_b r2_w sigma_u sigma_e rho) rename(D.exper exper D.expsq expsq D.weeks weeks) mtitle(\"WG\" \"LSDV\" \"FD\" \"Mundlack\")\n\nhausman MUN FGLS, sigmamore\n\n\nRandom-effects GLS regression                   Number of obs     =      4,165\nGroup variable: id                              Number of groups  =        595\n\nR-squared:                                      Obs per group:\n     Within  = 0.6566                                         min =          7\n     Between = 0.3264                                         avg =        7.0\n     Overall = 0.4160                                         max =          7\n\n                                                Wald chi2(7)      =    7107.12\ncorr(u_i, X) = 0 (assumed)                      Prob &gt; chi2       =     0.0000\ntheta        = .82280511\n\n------------------------------------------------------------------------------\n       lwage | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       exper |   .1137879   .0024689    46.09   0.000     .1089489    .1186268\n       expsq |  -.4243693   .0546316    -7.77   0.000    -.5314452   -.3172934\n       weeks |   .0008359   .0005997     1.39   0.163    -.0003395    .0020112\n        educ |   .0737838   .0048985    15.06   0.000     .0641829    .0833846\n     avexper |  -.0756349   .0062087   -12.18   0.000    -.0878036   -.0634662\n     avexpsq |  -.2069027   .1370415    -1.51   0.131    -.4754991    .0616937\n     avweeks |   .0122544   .0041099     2.98   0.003     .0041991    .0203097\n       _cons |   4.683039   .2100989    22.29   0.000     4.271253    5.094826\n-------------+----------------------------------------------------------------\n     sigma_u |  .31951859\n     sigma_e |  .15220316\n         rho |  .81505521   (fraction of variance due to u_i)\n------------------------------------------------------------------------------\n\n----------------------------------------------------------------------------\n                      (1)             (2)             (3)             (4)   \n                       WG            LSDV              FD        Mundlack   \n----------------------------------------------------------------------------\nexper               0.114***        0.114***        0.117***        0.114***\n                (0.00247)       (0.00247)       (0.00631)       (0.00247)   \n\nexpsq              -0.424***       -0.424***       -0.532***       -0.424***\n                 (0.0546)        (0.0546)         (0.139)        (0.0546)   \n\nweeks            0.000836        0.000836       -0.000268        0.000836   \n               (0.000600)      (0.000600)      (0.000565)      (0.000600)   \n\neduc                    0               0                          0.0738***\n                      (.)             (.)                       (0.00490)   \n\navexper                                                           -0.0756***\n                                                                (0.00621)   \n\navexpsq                                                            -0.207   \n                                                                  (0.137)   \n\navweeks                                                            0.0123** \n                                                                (0.00411)   \n\n_cons               4.596***        4.596***                        4.683***\n                 (0.0389)        (0.0389)                         (0.210)   \n----------------------------------------------------------------------------\nN                    4165            4165            3570            4165   \nr2                  0.657           0.907           0.221                   \nr2_o               0.0476                                           0.416   \nr2_b               0.0276                                           0.326   \nr2_w                0.657                                           0.657   \nsigma_u             1.036                                           0.320   \nsigma_e             0.152                                           0.152   \nrho                 0.979                                           0.815   \n----------------------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001\n\nNote: the rank of the differenced variance matrix (3) does not equal the number\n        of coefficients being tested (4); be sure this is what you expect, or\n        there may be problems computing the test.  Examine the output of your\n        estimators for anything unexpected and possibly consider scaling your\n        variables so that the coefficients are on a similar scale.\n\n                 ---- Coefficients ----\n             |      (b)          (B)            (b-B)     sqrt(diag(V_b-V_B))\n             |      MUN          FGLS        Difference       Std. err.\n-------------+----------------------------------------------------------------\n       exper |    .1137879     .0888609        .0249269        .0012778\n       expsq |   -.4243693     -.772565        .3481957        .0284727\n       weeks |    .0008359     .0009658       -.0001299        .0001108\n        educ |    .0737838     .1117099       -.0379262        .0009972\n------------------------------------------------------------------------------\n                          b = Consistent under H0 and Ha; obtained from xtreg.\n           B = Inconsistent under Ha, efficient under H0; obtained from xtreg.\n\nTest of H0: Difference in coefficients not systematic\n\n    chi2(3) = (b-B)'[(V_b-V_B)^(-1)](b-B)\n            = 1513.02\nProb &gt; chi2 =  0.0000\n(V_b-V_B is not positive definite)\n\n\n11. Export the results as a single CSV/Excel file. You can use esttab for .csv or outreg2 for .xlsx.\n\nesttab using \"problem-set-5-results.csv\", replace se scalar(N r2 r2_o r2_b r2_w sigma_u sigma_e rho) rename(D.exper exper D.expsq expsq D.weeks weeks) mtitle(\"OLS\" \"BG\" \"FGLS\" \"WG\" \"LSDV\" \"FD\" \"Mundlack\")\n\n(output written to problem-set-5-results.csv)\n\n\n\n\n\n\nlog close\n\n      name:  &lt;unnamed&gt;\n       log:  C:\\Users\\neil_\\OneDrive - University of Warwick\\Documents\\EC910\\we\n&gt; bsite\\warwick-ec910\\problem-sets\\ps-5\\problem-set-5-log.txt\n  log type:  smcl\n closed on:  11 Nov 2024, 12:44:59\n-------------------------------------------------------------------------------"
  },
  {
    "objectID": "problem-set-5-solutions.html#preamble",
    "href": "problem-set-5-solutions.html#preamble",
    "title": "Problem Set 5 (SOLUTIONS)",
    "section": "",
    "text": "Create a do-file for this problem set and include a preamble that sets the directory and opens the data. For example,\n\nclear \n//or, to remove all stored values (including macros, matrices, scalars, etc.) \n*clear all\n\n* Replace $rootdir with the relevant path to on your local harddrive.\ncd \"$rootdir/problem-sets/ps-5\"\n\ncap log close\nlog using problem-set-5-log.txt, replace\n\nuse problem-set-5-data.dta, clear\n\nC:\\Users\\neil_\\OneDrive - University of Warwick\\Documents\\EC910\\website\\warwick\n&gt; -ec910\\problem-sets\\ps-5\n-------------------------------------------------------------------------------\n      name:  &lt;unnamed&gt;\n       log:  C:\\Users\\neil_\\OneDrive - University of Warwick\\Documents\\EC910\\we\n&gt; bsite\\warwick-ec910\\problem-sets\\ps-5\\problem-set-5-log.txt\n  log type:  smcl\n opened on:  11 Nov 2024, 12:44:56\n(PSID wage data 1976-82 from Baltagi and Khanti-Akom (1990))"
  },
  {
    "objectID": "problem-set-5-solutions.html#questions",
    "href": "problem-set-5-solutions.html#questions",
    "title": "Problem Set 5 (SOLUTIONS)",
    "section": "",
    "text": "1. Set the unit identifier and time variable using xtset. Note, you can also use tsset for this task. This will allow you to use xt package commands.\n\nxtset id t\n\n\nPanel variable: id (strongly balanced)\n Time variable: t, 1 to 7\n         Delta: 1 unit\n\n\n2. Describe and summarise the variables in the dataset using the normal describe and summarize commands.\n\ndes \nsum id t lwage ed exper weeks south\n\n\nContains data from problem-set-5-data.dta\n Observations:         4,165                  PSID wage data 1976-82 from\n                                                Baltagi and Khanti-Akom (1990)\n    Variables:            14                  11 Nov 2024 11:19\n                                              (_dta has notes)\n-------------------------------------------------------------------------------\nVariable      Storage   Display    Value\n    name         type    format    label      Variable label\n-------------------------------------------------------------------------------\nexper           float   %9.0g                 years of full-time work\n                                                experience\nweeks           float   %9.0g                 weeks worked\noccup           float   %9.0g                 occupation; occ==1 if in a\n                                                blue-collar occupation\nindustry        float   %9.0g                 industry; ind==1 if working in a\n                                                manufacturing industry\nsouth           float   %9.0g                 residence; south==1 if in the\n                                                South area\nsmsa            float   %9.0g                 smsa==1 if in the Standard\n                                                metropolitan statistical area\nms              float   %9.0g                 marital status\nfemale          float   %9.0g                 female or male\nunion           float   %9.0g                 if wage set be a union contract\neduc            float   %9.0g                 years of education\nblack           float   %9.0g                 black\nlwage           float   %9.0g                 log wage\nid              float   %9.0g                 \nt               float   %9.0g                 \n-------------------------------------------------------------------------------\nSorted by: id  t\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n          id |      4,165         298    171.7821          1        595\n           t |      4,165           4     2.00024          1          7\n       lwage |      4,165    6.676346    .4615122    4.60517      8.537\n        educ |      4,165    12.84538    2.787995          4         17\n       exper |      4,165    19.85378    10.96637          1         51\n-------------+---------------------------------------------------------\n       weeks |      4,165    46.81152    5.129098          5         52\n       south |      4,165    .2902761    .4539442          0          1\n\n\n3. Describe and summarise the variables in the dataset using the panel commands: xtdescribe and xtsummarize. Comment on the information provided.\n\nxtdescribe \nxtsum id t lwage ed exper weeks south\n\n\n      id:  1, 2, ..., 595                                    n =        595\n       t:  1, 2, ..., 7                                      T =          7\n           Delta(t) = 1 unit\n           Span(t)  = 7 periods\n           (id*t uniquely identifies each observation)\n\nDistribution of T_i:   min      5%     25%       50%       75%     95%     max\n                         7       7       7         7         7       7       7\n\n     Freq.  Percent    Cum. |  Pattern\n ---------------------------+---------\n      595    100.00  100.00 |  1111111\n ---------------------------+---------\n      595    100.00         |  XXXXXXX\n\nVariable         |      Mean   Std. dev.       Min        Max |    Observations\n-----------------+--------------------------------------------+----------------\nid       overall |       298   171.7821          1        595 |     N =    4165\n         between |              171.906          1        595 |     n =     595\n         within  |                    0        298        298 |     T =       7\n                 |                                            |\nt        overall |         4    2.00024          1          7 |     N =    4165\n         between |                    0          4          4 |     n =     595\n         within  |              2.00024          1          7 |     T =       7\n                 |                                            |\nlwage    overall |  6.676346   .4615122    4.60517      8.537 |     N =    4165\n         between |             .3942387     5.3364   7.813596 |     n =     595\n         within  |             .2404023   4.781808   8.621092 |     T =       7\n                 |                                            |\neduc     overall |  12.84538   2.787995          4         17 |     N =    4165\n         between |             2.790006          4         17 |     n =     595\n         within  |                    0   12.84538   12.84538 |     T =       7\n                 |                                            |\nexper    overall |  19.85378   10.96637          1         51 |     N =    4165\n         between |             10.79018          4         48 |     n =     595\n         within  |              2.00024   16.85378   22.85378 |     T =       7\n                 |                                            |\nweeks    overall |  46.81152   5.129098          5         52 |     N =    4165\n         between |             3.284016   31.57143   51.57143 |     n =     595\n         within  |             3.941881    12.2401   63.66867 |     T =       7\n                 |                                            |\nsouth    overall |  .2902761   .4539442          0          1 |     N =    4165\n         between |             .4489462          0          1 |     n =     595\n         within  |             .0693042  -.5668667   1.147419 |     T =       7\n\n\n4. Use the command xttab and xtrans, freq to describe transitions over time in the variable south.\n\nxttab south\nxttrans south, freq\n\n\n                  Overall             Between            Within\n    south |    Freq.  Percent      Freq.  Percent        Percent\n----------+-----------------------------------------------------\n        0 |    2956     70.97       428     71.93          98.66\n        1 |    1209     29.03       182     30.59          94.90\n----------+-----------------------------------------------------\n    Total |    4165    100.00       610    102.52          97.54\n                               (n = 595)\n\nresidence; |\n  south==1 |  residence; south==1\n if in the | if in the South area\nSouth area |         0          1 |     Total\n-----------+----------------------+----------\n         0 |     2,527          8 |     2,535 \n           |     99.68       0.32 |    100.00 \n-----------+----------------------+----------\n         1 |         8      1,027 |     1,035 \n           |      0.77      99.23 |    100.00 \n-----------+----------------------+----------\n     Total |     2,535      1,035 |     3,570 \n           |     71.01      28.99 |    100.00 \n\n\n5. Create the variable: expsq=exper^2/1000. Why would you scale the variable in this way?\n\ngen expsq=exp*exp/1000\n\n6. Estimate the following model using pooled OLS, between-group, feasible GLS, within-group, LSDV, and first-difference. For the first-difference estimator, you can define a first-difference in Stata using the time-series operator: D.variable.\n\\[\n\\ln(Wage_{it}) = \\beta_1 + \\beta_2 Exper_{it} + \\beta_3 Exper^2_{it} + \\beta_4 Weeks_{it} + \\beta_5 Eduyrs_{it} + \\varepsilon_{it}\n\\] With each model, store the results using estimates store. For example,\n\n* clear existing stored estimates\nest clear\n\n* Pooled OLS\nregress lwage exper expsq weeks ed\nest store OlS\n\n* alternatively, \neststo OLS: regress lwage exper expsq weeks ed\n\n\nest clear\n* Pooled-OLS\neststo OLS: regress lwage exper expsq weeks ed\n\n* Between-group\neststo BG: xtreg lwage exper expsq weeks ed, be\n\n* Feasible-GLS\neststo FGLS: xtreg lwage exper expsq weeks ed, re theta\n\n* Within-group\neststo WG: xtreg lwage exper expsq weeks ed, fe \n\n* LSDV\neststo LSDV: areg lwage exper expsq weeks ed, absorb(id)\n\n* First-differnce\neststo FD: reg D.(lwage exper expsq weeks), noconst\n\n\n      Source |       SS           df       MS      Number of obs   =     4,165\n-------------+----------------------------------   F(4, 4160)      =    411.62\n       Model |  251.491445         4  62.8728612   Prob &gt; F        =    0.0000\n    Residual |  635.413457     4,160  .152743619   R-squared       =    0.2836\n-------------+----------------------------------   Adj R-squared   =    0.2829\n       Total |  886.904902     4,164  .212993492   Root MSE        =    .39082\n\n------------------------------------------------------------------------------\n       lwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       exper |    .044675   .0023929    18.67   0.000     .0399838    .0493663\n       expsq |   -.715631   .0527938   -13.56   0.000    -.8191351   -.6121268\n       weeks |    .005827   .0011827     4.93   0.000     .0035084    .0081456\n        educ |   .0760407   .0022266    34.15   0.000     .0716754     .080406\n       _cons |   4.907961   .0673297    72.89   0.000     4.775959    5.039963\n------------------------------------------------------------------------------\n\nBetween regression (regression on group means)  Number of obs     =      4,165\nGroup variable: id                              Number of groups  =        595\n\nR-squared:                                      Obs per group:\n     Within  = 0.1357                                         min =          7\n     Between = 0.3264                                         avg =        7.0\n     Overall = 0.2723                                         max =          7\n\n                                                F(4,590)          =      71.48\nsd(u_i + avg(e_i.)) = .324656                   Prob &gt; F          =     0.0000\n\n------------------------------------------------------------------------------\n       lwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       exper |    .038153   .0056967     6.70   0.000     .0269647    .0493412\n       expsq |   -.631272   .1256812    -5.02   0.000    -.8781089    -.384435\n       weeks |   .0130903   .0040659     3.22   0.001     .0051048    .0210757\n        educ |   .0737838   .0048985    15.06   0.000     .0641632    .0834044\n       _cons |   4.683039   .2100989    22.29   0.000     4.270407    5.095672\n------------------------------------------------------------------------------\n\nRandom-effects GLS regression                   Number of obs     =      4,165\nGroup variable: id                              Number of groups  =        595\n\nR-squared:                                      Obs per group:\n     Within  = 0.6340                                         min =          7\n     Between = 0.1716                                         avg =        7.0\n     Overall = 0.1830                                         max =          7\n\n                                                Wald chi2(4)      =    3012.45\ncorr(u_i, X) = 0 (assumed)                      Prob &gt; chi2       =     0.0000\ntheta        = .82280511\n\n------------------------------------------------------------------------------\n       lwage | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       exper |   .0888609   .0028178    31.54   0.000     .0833382    .0943837\n       expsq |   -.772565   .0622619   -12.41   0.000     -.894596   -.6505339\n       weeks |   .0009658   .0007433     1.30   0.194     -.000491    .0024226\n        educ |   .1117099   .0060572    18.44   0.000     .0998381    .1235818\n       _cons |   3.829366   .0936336    40.90   0.000     3.645848    4.012885\n-------------+----------------------------------------------------------------\n     sigma_u |  .31951859\n     sigma_e |  .15220316\n         rho |  .81505521   (fraction of variance due to u_i)\n------------------------------------------------------------------------------\nnote: educ omitted because of collinearity.\n\nFixed-effects (within) regression               Number of obs     =      4,165\nGroup variable: id                              Number of groups  =        595\n\nR-squared:                                      Obs per group:\n     Within  = 0.6566                                         min =          7\n     Between = 0.0276                                         avg =        7.0\n     Overall = 0.0476                                         max =          7\n\n                                                F(3, 3567)        =    2273.74\ncorr(u_i, Xb) = -0.9107                         Prob &gt; F          =     0.0000\n\n------------------------------------------------------------------------------\n       lwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       exper |   .1137879   .0024689    46.09   0.000     .1089473    .1186284\n       expsq |  -.4243693   .0546316    -7.77   0.000    -.5314816    -.317257\n       weeks |   .0008359   .0005997     1.39   0.163    -.0003399    .0020116\n        educ |          0  (omitted)\n       _cons |   4.596396   .0389061   118.14   0.000     4.520116    4.672677\n-------------+----------------------------------------------------------------\n     sigma_u |  1.0362039\n     sigma_e |  .15220316\n         rho |  .97888036   (fraction of variance due to u_i)\n------------------------------------------------------------------------------\nF test that all u_i=0: F(594, 3567) = 53.12                  Prob &gt; F = 0.0000\nnote: educ omitted because of collinearity.\n\nLinear regression, absorbing indicators            Number of obs     =   4,165\nAbsorbed variable: id                              No. of categories =     595\n                                                   F(3, 3567)        = 2273.74\n                                                   Prob &gt; F          =  0.0000\n                                                   R-squared         =  0.9068\n                                                   Adj R-squared     =  0.8912\n                                                   Root MSE          =  0.1522\n\n------------------------------------------------------------------------------\n       lwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       exper |   .1137879   .0024689    46.09   0.000     .1089473    .1186284\n       expsq |  -.4243693   .0546316    -7.77   0.000    -.5314816    -.317257\n       weeks |   .0008359   .0005997     1.39   0.163    -.0003399    .0020116\n        educ |          0  (omitted)\n       _cons |   4.596396   .0389061   118.14   0.000     4.520116    4.672677\n------------------------------------------------------------------------------\nF test of absorbed indicators: F(594, 3567) = 53.118          Prob &gt; F = 0.000\n\n      Source |       SS           df       MS      Number of obs   =     3,570\n-------------+----------------------------------   F(3, 3567)      =    337.12\n       Model |  33.3371458         3  11.1123819   Prob &gt; F        =    0.0000\n    Residual |   117.57812     3,567  .032962747   R-squared       =    0.2209\n-------------+----------------------------------   Adj R-squared   =    0.2202\n       Total |  150.915266     3,570  .042273184   Root MSE        =    .18156\n\n------------------------------------------------------------------------------\n     D.lwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       exper |\n         D1. |   .1170654   .0063106    18.55   0.000     .1046927    .1294381\n             |\n       expsq |\n         D1. |  -.5321208   .1392741    -3.82   0.000    -.8051857    -.259056\n             |\n       weeks |\n         D1. |  -.0002683   .0005648    -0.47   0.635    -.0013757    .0008392\n------------------------------------------------------------------------------\n\n\n7. Using the formula from Handout 5, replicate the value of \\(\\theta\\) reported above by the FGLS estimator. Note, you will need to use the stored values of \\(\\sigma^2_{\\varepsilon}\\) and \\(\\sigma^2_{\\alpha}\\).\n\nqui xtreg lwage exper expsq weeks ed, re theta\ndisplay \"theta = \"  1 - sqrt(e(sigma_e)^2 / (7*e(sigma_u)^2+e(sigma_e)^2))\n\ntheta = .82280511\n\n\n8. Make a table of the computed estimates. You can either use estimates table or esttab. The latter is part of the estout package, which you may need to install: ssc install estout.\n\nesttab OLS BG FGLS, se scalar(N r2 r2_o r2_b r2_w sigma_u sigma_e rho) mtitle(\"OLS\" \"BG\" \"FGLS\")\nesttab WG LSDV FD, se scalar(N r2 r2_o r2_b r2_w sigma_u sigma_e rho) rename(D.exper exper D.expsq expsq D.weeks weeks) mtitle(\"WG\" \"LSDV\" \"FD\")\n\n\n------------------------------------------------------------\n                      (1)             (2)             (3)   \n                      OLS              BG            FGLS   \n------------------------------------------------------------\nexper              0.0447***       0.0382***       0.0889***\n                (0.00239)       (0.00570)       (0.00282)   \n\nexpsq              -0.716***       -0.631***       -0.773***\n                 (0.0528)         (0.126)        (0.0623)   \n\nweeks             0.00583***       0.0131**      0.000966   \n                (0.00118)       (0.00407)      (0.000743)   \n\neduc               0.0760***       0.0738***        0.112***\n                (0.00223)       (0.00490)       (0.00606)   \n\n_cons               4.908***        4.683***        3.829***\n                 (0.0673)         (0.210)        (0.0936)   \n------------------------------------------------------------\nN                    4165            4165            4165   \nr2                  0.284           0.326                   \nr2_o                                0.272           0.183   \nr2_b                                0.326           0.172   \nr2_w                                0.136           0.634   \nsigma_u                                             0.320   \nsigma_e                                             0.152   \nrho                                                 0.815   \n------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001\n\n------------------------------------------------------------\n                      (1)             (2)             (3)   \n                       WG            LSDV              FD   \n------------------------------------------------------------\nexper               0.114***        0.114***        0.117***\n                (0.00247)       (0.00247)       (0.00631)   \n\nexpsq              -0.424***       -0.424***       -0.532***\n                 (0.0546)        (0.0546)         (0.139)   \n\nweeks            0.000836        0.000836       -0.000268   \n               (0.000600)      (0.000600)      (0.000565)   \n\neduc                    0               0                   \n                      (.)             (.)                   \n\n_cons               4.596***        4.596***                \n                 (0.0389)        (0.0389)                   \n------------------------------------------------------------\nN                    4165            4165            3570   \nr2                  0.657           0.907           0.221   \nr2_o               0.0476                                   \nr2_b               0.0276                                   \nr2_w                0.657                                   \nsigma_u             1.036                                   \nsigma_e             0.152                                   \nrho                 0.979                                   \n------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001\n\n\n9. Perform a Hausman test comparing the results of the FLGS and WG estimators. You should use the hausman command, with the option sigmamore. Be sure to get the order of the estimates correct. What do you learn from the test?\n\nhausman WG FGLS, sigmamore\n\n\n                 ---- Coefficients ----\n             |      (b)          (B)            (b-B)     sqrt(diag(V_b-V_B))\n             |       WG          FGLS        Difference       Std. err.\n-------------+----------------------------------------------------------------\n       exper |    .1137879     .0888609        .0249269        .0012778\n       expsq |   -.4243693     -.772565        .3481957        .0284727\n       weeks |    .0008359     .0009658       -.0001299        .0001108\n------------------------------------------------------------------------------\n                          b = Consistent under H0 and Ha; obtained from xtreg.\n           B = Inconsistent under Ha, efficient under H0; obtained from xtreg.\n\nTest of H0: Difference in coefficients not systematic\n\n    chi2(3) = (b-B)'[(V_b-V_B)^(-1)](b-B)\n            = 1513.02\nProb &gt; chi2 =  0.0000\n\n\n10. Estimate FGLS for the model below:\n\\[\n\\begin{aligned}\n\\ln(Wage_{it}) =& \\beta_1 + \\beta_2 Exper_{it} + \\beta_3 Exper^2_{it} + \\beta_4 Weeks_{it} + \\beta_5 Eduyrs_{it} \\\\\n&+ \\gamma_2 \\overline{Exper}_{i} + \\gamma_3 \\overline{Exper^2}_{i} + \\gamma_4 \\overline{Weeks}_{i}+\\varepsilon_{it}\n\\end{aligned}\n\\] You will need to manually create the variables: \\(\\{\\overline{Exper}_{i}, \\overline{Exper^2}_{i},\\overline{Weeks}_{i}\\}\\) - the individual-level averages of each variable. This is referred to as the Mundlack correction. Once you have estimated the model, repeat the Hausman test comparing these results with those of the WG estimator. What is the significance of the Mundlack correction?\n\nforeach var in exper expsq weeks{\n    bys id: egen av`var' = mean(`var')     \n}\n\neststo MUN: xtreg lwage exper expsq weeks ed avexper avexpsq avweeks, re theta\n\nesttab WG LSDV FD MUN, se scalar(N r2 r2_o r2_b r2_w sigma_u sigma_e rho) rename(D.exper exper D.expsq expsq D.weeks weeks) mtitle(\"WG\" \"LSDV\" \"FD\" \"Mundlack\")\n\nhausman MUN FGLS, sigmamore\n\n\nRandom-effects GLS regression                   Number of obs     =      4,165\nGroup variable: id                              Number of groups  =        595\n\nR-squared:                                      Obs per group:\n     Within  = 0.6566                                         min =          7\n     Between = 0.3264                                         avg =        7.0\n     Overall = 0.4160                                         max =          7\n\n                                                Wald chi2(7)      =    7107.12\ncorr(u_i, X) = 0 (assumed)                      Prob &gt; chi2       =     0.0000\ntheta        = .82280511\n\n------------------------------------------------------------------------------\n       lwage | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       exper |   .1137879   .0024689    46.09   0.000     .1089489    .1186268\n       expsq |  -.4243693   .0546316    -7.77   0.000    -.5314452   -.3172934\n       weeks |   .0008359   .0005997     1.39   0.163    -.0003395    .0020112\n        educ |   .0737838   .0048985    15.06   0.000     .0641829    .0833846\n     avexper |  -.0756349   .0062087   -12.18   0.000    -.0878036   -.0634662\n     avexpsq |  -.2069027   .1370415    -1.51   0.131    -.4754991    .0616937\n     avweeks |   .0122544   .0041099     2.98   0.003     .0041991    .0203097\n       _cons |   4.683039   .2100989    22.29   0.000     4.271253    5.094826\n-------------+----------------------------------------------------------------\n     sigma_u |  .31951859\n     sigma_e |  .15220316\n         rho |  .81505521   (fraction of variance due to u_i)\n------------------------------------------------------------------------------\n\n----------------------------------------------------------------------------\n                      (1)             (2)             (3)             (4)   \n                       WG            LSDV              FD        Mundlack   \n----------------------------------------------------------------------------\nexper               0.114***        0.114***        0.117***        0.114***\n                (0.00247)       (0.00247)       (0.00631)       (0.00247)   \n\nexpsq              -0.424***       -0.424***       -0.532***       -0.424***\n                 (0.0546)        (0.0546)         (0.139)        (0.0546)   \n\nweeks            0.000836        0.000836       -0.000268        0.000836   \n               (0.000600)      (0.000600)      (0.000565)      (0.000600)   \n\neduc                    0               0                          0.0738***\n                      (.)             (.)                       (0.00490)   \n\navexper                                                           -0.0756***\n                                                                (0.00621)   \n\navexpsq                                                            -0.207   \n                                                                  (0.137)   \n\navweeks                                                            0.0123** \n                                                                (0.00411)   \n\n_cons               4.596***        4.596***                        4.683***\n                 (0.0389)        (0.0389)                         (0.210)   \n----------------------------------------------------------------------------\nN                    4165            4165            3570            4165   \nr2                  0.657           0.907           0.221                   \nr2_o               0.0476                                           0.416   \nr2_b               0.0276                                           0.326   \nr2_w                0.657                                           0.657   \nsigma_u             1.036                                           0.320   \nsigma_e             0.152                                           0.152   \nrho                 0.979                                           0.815   \n----------------------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001\n\nNote: the rank of the differenced variance matrix (3) does not equal the number\n        of coefficients being tested (4); be sure this is what you expect, or\n        there may be problems computing the test.  Examine the output of your\n        estimators for anything unexpected and possibly consider scaling your\n        variables so that the coefficients are on a similar scale.\n\n                 ---- Coefficients ----\n             |      (b)          (B)            (b-B)     sqrt(diag(V_b-V_B))\n             |      MUN          FGLS        Difference       Std. err.\n-------------+----------------------------------------------------------------\n       exper |    .1137879     .0888609        .0249269        .0012778\n       expsq |   -.4243693     -.772565        .3481957        .0284727\n       weeks |    .0008359     .0009658       -.0001299        .0001108\n        educ |    .0737838     .1117099       -.0379262        .0009972\n------------------------------------------------------------------------------\n                          b = Consistent under H0 and Ha; obtained from xtreg.\n           B = Inconsistent under Ha, efficient under H0; obtained from xtreg.\n\nTest of H0: Difference in coefficients not systematic\n\n    chi2(3) = (b-B)'[(V_b-V_B)^(-1)](b-B)\n            = 1513.02\nProb &gt; chi2 =  0.0000\n(V_b-V_B is not positive definite)\n\n\n11. Export the results as a single CSV/Excel file. You can use esttab for .csv or outreg2 for .xlsx.\n\nesttab using \"problem-set-5-results.csv\", replace se scalar(N r2 r2_o r2_b r2_w sigma_u sigma_e rho) rename(D.exper exper D.expsq expsq D.weeks weeks) mtitle(\"OLS\" \"BG\" \"FGLS\" \"WG\" \"LSDV\" \"FD\" \"Mundlack\")\n\n(output written to problem-set-5-results.csv)"
  },
  {
    "objectID": "problem-set-5-solutions.html#postamble",
    "href": "problem-set-5-solutions.html#postamble",
    "title": "Problem Set 5 (SOLUTIONS)",
    "section": "",
    "text": "log close\n\n      name:  &lt;unnamed&gt;\n       log:  C:\\Users\\neil_\\OneDrive - University of Warwick\\Documents\\EC910\\we\n&gt; bsite\\warwick-ec910\\problem-sets\\ps-5\\problem-set-5-log.txt\n  log type:  smcl\n closed on:  11 Nov 2024, 12:44:59\n-------------------------------------------------------------------------------"
  },
  {
    "objectID": "problem-set-4-solutions.html",
    "href": "problem-set-4-solutions.html",
    "title": "Problem Set 4 (SOLUTIONS)",
    "section": "",
    "text": "This problem set will revisit some of the material covered in Handouts 3 and 4. You will be required to work with a ‘raw’ dataset, downloaded from an online repository. For this reason, you should take care to check how the data is coded.\nYou will be using a version of the US Current Population Survey (CPS) called the Merged Outgoing Rotation Group (MORG). This data is compiled by the National Bureau of Economic Research (NBER) and has been used in many famous studies of the US economy. The CPS has a rather unique rotating panel design: “The monthly CPS is a rotating panel design; households are interviewed for four consecutive months, are not in the sample for the next eight months, and then are interviewed for four more consecutive months.” (source: IPUMS). The NBER’s MORG keeps only the outgoing rotation group’s observations.\nThe MORG .dta files can be found at: https://data.nber.org/morg/annual/.\n\n\n\n\n\n\n\nCreate a do-file for this problem set and include a preamble that sets the directory and opens the data directly from the NBER website. Of course, this requires a good internet connection. For example,\n\nclear \n//or, to remove all stored values (including macros, matrices, scalars, etc.) \n*clear all\n\n* Replace $rootdir with the relevant path to on your local harddrive.\ncd \"$rootdir/problem-sets/ps-4\"\n\ncap log close\nlog using problem-set-4-log.txt, replace\n\nuse \"https://data.nber.org/morg/annual/morg19.dta\", clear\n\nYou can, of course, download the data and open it locally on your computer.\n\n\n\n1. Create a new variable exper equal to age minus (years of education + 6). This is referred to as potential years of experience. Check how each variable defines missing values before proceeding. You will need to create a years of education variable for this. Here is he the suggested code:\n\ntab grade92, m\ngen eduyrs = .\n    replace eduyrs = .3 if grade92==31\n    replace eduyrs = 3.2 if grade92==32\n    replace eduyrs = 7.2 if grade92==33\n    replace eduyrs = 7.2 if grade92==34\n    replace eduyrs = 9  if grade92==35\n    replace eduyrs = 10 if grade92==36\n    replace eduyrs = 11 if grade92==37\n    replace eduyrs = 12 if grade92==38\n    replace eduyrs = 12 if grade92==39\n    replace eduyrs = 13 if grade92==40\n    replace eduyrs = 14 if grade92==41\n    replace eduyrs = 14 if grade92==42\n    replace eduyrs = 16 if grade92==43\n    replace eduyrs = 18 if grade92==44\n    replace eduyrs = 18 if grade92==45\n    replace eduyrs = 18 if grade92==46\n    lab var eduyrs \"completed education\"\ntab grade92, sum(eduyrs)\n\n\n    Highest |\n      grade |\n  completed |      Freq.     Percent        Cum.\n------------+-----------------------------------\n         31 |        814        0.28        0.28\n         32 |      1,495        0.51        0.79\n         33 |      3,071        1.05        1.85\n         34 |      4,123        1.41        3.26\n         35 |      5,244        1.80        5.06\n         36 |      7,824        2.69        7.75\n         37 |      9,271        3.18       10.93\n         38 |      4,226        1.45       12.38\n         39 |     82,795       28.41       40.79\n         40 |     50,112       17.20       57.99\n         41 |     12,392        4.25       62.24\n         42 |     16,161        5.55       67.79\n         43 |     59,438       20.40       88.19\n         44 |     25,374        8.71       96.89\n         45 |      3,785        1.30       98.19\n         46 |      5,265        1.81      100.00\n------------+-----------------------------------\n      Total |    291,390      100.00\n(291,390 missing values generated)\n(814 real changes made)\n(1,495 real changes made)\n(3,071 real changes made)\n(4,123 real changes made)\n(5,244 real changes made)\n(7,824 real changes made)\n(9,271 real changes made)\n(4,226 real changes made)\n(82,795 real changes made)\n(50,112 real changes made)\n(12,392 real changes made)\n(16,161 real changes made)\n(59,438 real changes made)\n(25,374 real changes made)\n(3,785 real changes made)\n(5,265 real changes made)\n\n    Highest |\n      grade |   Summary of completed education\n  completed |        Mean   Std. dev.       Freq.\n------------+------------------------------------\n         31 |   .30000001           0         814\n         32 |         3.2           0       1,495\n         33 |   7.1999998           0       3,071\n         34 |   7.1999998           0       4,123\n         35 |           9           0       5,244\n         36 |          10           0       7,824\n         37 |          11           0       9,271\n         38 |          12           0       4,226\n         39 |          12           0      82,795\n         40 |          13           0      50,112\n         41 |          14           0      12,392\n         42 |          14           0      16,161\n         43 |          16           0      59,438\n         44 |          18           0      25,374\n         45 |          18           0       3,785\n         46 |          18           0       5,265\n------------+------------------------------------\n      Total |   13.556855   2.7030576     291,390\n\n\n\ntab age, m\ngen exper = age-(eduyrs+6)\n\n\n        Age |      Freq.     Percent        Cum.\n------------+-----------------------------------\n         16 |      4,661        1.60        1.60\n         17 |      4,630        1.59        3.19\n         18 |      4,417        1.52        4.70\n         19 |      4,039        1.39        6.09\n         20 |      3,915        1.34        7.43\n         21 |      3,996        1.37        8.81\n         22 |      3,918        1.34       10.15\n         23 |      3,950        1.36       11.51\n         24 |      4,194        1.44       12.94\n         25 |      4,185        1.44       14.38\n         26 |      4,325        1.48       15.87\n         27 |      4,476        1.54       17.40\n         28 |      4,600        1.58       18.98\n         29 |      4,633        1.59       20.57\n         30 |      4,829        1.66       22.23\n         31 |      4,735        1.62       23.85\n         32 |      4,601        1.58       25.43\n         33 |      4,748        1.63       27.06\n         34 |      4,646        1.59       28.66\n         35 |      4,730        1.62       30.28\n         36 |      4,742        1.63       31.91\n         37 |      4,848        1.66       33.57\n         38 |      4,550        1.56       35.13\n         39 |      4,735        1.62       36.76\n         40 |      4,667        1.60       38.36\n         41 |      4,503        1.55       39.90\n         42 |      4,390        1.51       41.41\n         43 |      4,309        1.48       42.89\n         44 |      4,193        1.44       44.33\n         45 |      4,253        1.46       45.79\n         46 |      4,266        1.46       47.25\n         47 |      4,447        1.53       48.78\n         48 |      4,563        1.57       50.34\n         49 |      4,698        1.61       51.96\n         50 |      4,646        1.59       53.55\n         51 |      4,477        1.54       55.09\n         52 |      4,555        1.56       56.65\n         53 |      4,523        1.55       58.20\n         54 |      4,736        1.63       59.83\n         55 |      5,010        1.72       61.55\n         56 |      5,035        1.73       63.27\n         57 |      4,976        1.71       64.98\n         58 |      5,030        1.73       66.71\n         59 |      5,066        1.74       68.45\n         60 |      5,124        1.76       70.20\n         61 |      5,067        1.74       71.94\n         62 |      5,035        1.73       73.67\n         63 |      4,927        1.69       75.36\n         64 |      4,892        1.68       77.04\n         65 |      4,554        1.56       78.60\n         66 |      4,526        1.55       80.16\n         67 |      4,344        1.49       81.65\n         68 |      4,328        1.49       83.13\n         69 |      4,100        1.41       84.54\n         70 |      4,058        1.39       85.93\n         71 |      4,008        1.38       87.31\n         72 |      3,897        1.34       88.65\n         73 |      3,147        1.08       89.73\n         74 |      2,815        0.97       90.69\n         75 |      2,809        0.96       91.66\n         76 |      2,623        0.90       92.56\n         77 |      2,373        0.81       93.37\n         78 |      2,201        0.76       94.13\n         79 |      1,977        0.68       94.80\n         80 |      7,799        2.68       97.48\n         85 |      7,340        2.52      100.00\n------------+-----------------------------------\n      Total |    291,390      100.00\n\n\n2. Keep only those between the ages of 18 and 54. Check the distribution of `exper’ and replace any negative values to 0.\n\nkeep if inrange(age,18,54)\nsum exper, det\nreplace exper=0 if exper&lt;0\n\n(126,352 observations deleted)\n\n                            exper\n-------------------------------------------------------------\n      Percentiles      Smallest\n 1%            0             -6\n 5%            1             -5\n10%            2             -4       Obs             165,038\n25%            7             -4       Sum of wgt.     165,038\n\n50%           16                      Mean           16.50623\n                        Largest       Std. dev.      10.57401\n75%           25           47.7\n90%           31           47.7       Variance       111.8097\n95%           34           47.7       Skewness       .1296908\n99%           36           47.7       Kurtosis       1.887811\n(1,078 real changes made)\n\n\n3. Create a categorical variable that takes on 4 values: 1 “less than High School”; 2 “High School Diploma”; 3 “some Higher Education”; 4 “Bachelors”; 5 “Postgraduate”. This variable should be based on the the grade92 variable. You can find the value labels for this variable in this document: https://data.nber.org/morg/docs/cpsx.pdf. I suggest using the recode command, which allows you to create value labels while assigning values. Check the distributio of exper by education category.\n\nrecode grade92 (31/38 = 1 \"&lt;HS\") (39 = 2 \"HS\") (40/42 = 3 \"HS+\") (43 = 4 \"BA\") (44/46 = 5 \"PG\"), gen(educat)\ntab grade92 educat, m\n\ntab educat, sum(exper)\n\n(165,038 differences between grade92 and educat)\n\n   Highest |\n     grade |      RECODE of grade92 (Highest grade completed)\n completed |       &lt;HS         HS        HS+         BA         PG |     Total\n-----------+-------------------------------------------------------+----------\n        31 |       398          0          0          0          0 |       398 \n        32 |       578          0          0          0          0 |       578 \n        33 |     1,515          0          0          0          0 |     1,515 \n        34 |     1,571          0          0          0          0 |     1,571 \n        35 |     1,971          0          0          0          0 |     1,971 \n        36 |     2,198          0          0          0          0 |     2,198 \n        37 |     4,373          0          0          0          0 |     4,373 \n        38 |     2,440          0          0          0          0 |     2,440 \n        39 |         0     45,013          0          0          0 |    45,013 \n        40 |         0          0     30,934          0          0 |    30,934 \n        41 |         0          0      7,154          0          0 |     7,154 \n        42 |         0          0      9,708          0          0 |     9,708 \n        43 |         0          0          0     37,557          0 |    37,557 \n        44 |         0          0          0          0     14,804 |    14,804 \n        45 |         0          0          0          0      2,021 |     2,021 \n        46 |         0          0          0          0      2,803 |     2,803 \n-----------+-------------------------------------------------------+----------\n     Total |    15,044     45,013     47,796     37,557     19,628 |   165,038 \n\n  RECODE of |\n    grade92 |\n   (Highest |\n      grade |          Summary of exper\n completed) |        Mean   Std. dev.       Freq.\n------------+------------------------------------\n        &lt;HS |    19.29665    12.82301      15,044\n         HS |   17.726435   11.044222      45,013\n        HS+ |   15.643589   10.806949      47,796\n         BA |   15.376841   9.3440749      37,557\n         PG |   15.900092   8.1569821      19,628\n------------+------------------------------------\n      Total |   16.514468   10.560511     165,038\n\n\n4. Create the variable lnwage equal to the (natural) log of weekly earnings. Create a figure that shows the predicted linear fit of lwage against exper, by educat. Try to place all 5 fitted lines in the same graph.\n\ngen lnwage = ln(earnwke)\ntwoway (lfit lnwage exper if educat==1) (lfit lnwage exper if educat==2)  (lfit lnwage exper if educat==3) (lfit lnwage exper if educat==4) (lfit lnwage exper if educat==5) , legend(order(1 \"&lt;HS\" 2 \"HS\" 3 \"HS+\" 4 \"BA\" 5 \"PG\") pos(6) r(1)) xtitle(Years of experience) \n\n(49,686 missing values generated)\n\n\n\n\n\n\n\n\n\n5. Estimate a linear regression model that allows the slope coefficient on exper and constant term to vary by education category (educat). Let the base (excluded) education category be 2 “High School diploma”.\n\\[\n  \\ln(Wage_i) = \\alpha + \\sum_{j\\neq2}\\psi_j \\mathbf{1}\\{Educat_i=j\\} + \\beta Exper_i + \\sum_{j\\neq2}\\gamma_j Exper_i\\times\\mathbf{1}\\{Educat_i=j\\}+\\upsilon_i\n\\]\n\nreg lnwage ib2.educat##c.exper\n\n\n      Source |       SS           df       MS      Number of obs   =   115,352\n-------------+----------------------------------   F(9, 115342)    =   4039.95\n       Model |  17435.5509         9  1937.28343   Prob &gt; F        =    0.0000\n    Residual |  55310.0589   115,342  .479530951   R-squared       =    0.2397\n-------------+----------------------------------   Adj R-squared   =    0.2396\n       Total |  72745.6098   115,351   .63064568   Root MSE        =    .69248\n\n------------------------------------------------------------------------------\n      lnwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      educat |\n        &lt;HS  |  -.3882762   .0178644   -21.73   0.000    -.4232902   -.3532622\n        HS+  |  -.0839435   .0104229    -8.05   0.000    -.1043722   -.0635149\n         BA  |   .6145964   .0109763    55.99   0.000     .5930831    .6361097\n         PG  |   .9055692   .0141961    63.79   0.000      .877745    .9333933\n             |\n       exper |   .0182621   .0003709    49.24   0.000     .0175351    .0189891\n             |\n      educat#|\n     c.exper |\n        &lt;HS  |    .001037   .0007627     1.36   0.174     -.000458    .0025319\n        HS+  |   .0095156   .0005186    18.35   0.000     .0084991    .0105321\n         BA  |  -.0032067   .0005743    -5.58   0.000    -.0043324   -.0020811\n         PG  |  -.0051215   .0007698    -6.65   0.000    -.0066302   -.0036128\n             |\n       _cons |   6.101809   .0077485   787.48   0.000     6.086622    6.116996\n------------------------------------------------------------------------------\n\n\n6. Show that after 13 years of experience, those with some Higer Education (but no Bachelors), out earn those with just a high school diploma. You can assume that there are is a 2 year difference between the experience (education).\n\ndis _b[exper]*14\ndis (_b[exper] + _b[3.educat#exper])*12 + _b[3.educat]\n\n\ndis _b[exper]*15\ndis (_b[exper] + _b[3.educat#exper])*13 + _b[3.educat]\n\n.25566943\n.24938901\n.27393153\n.27716672\n\n\n7. Use the post-estimation test command to test the null hypothesis: \\(H_0: 15\\beta = 13(\\beta+\\gamma_3)+\\psi_3\\).\n\ntest exper*15 = (exper+3.educat#exper)*13+3.educat\n\n\n ( 1)  - 3.educat + 2*exper - 13*3.educat#c.exper = 0\n\n       F(  1,115342) =    0.32\n            Prob &gt; F =    0.5734\n\n\n8. Estimate a transformed version of the above model allowing you to test the above hypothesis using the coefficient from a single regressor. That is, the resulting test should be a simple t-test of \\(H_0: \\phi=0\\), where \\(\\phi\\) is the coefficient on the interaction of exper and a dummy variable for educat=3. This will be easier to do if you estimate the model using only the relevant sample: those with High School diplomas and some Higher Education. I suggest avoiding the use of factor notation to create the dummy variables and interaction terms for this exercise. For example, the following should replicate the relevant coefficients from Q5.\n\ngen hasHE = educat==3 if inlist(educat,2,3)\ngen hasHEexp = hasHE*exper\n\nreg lnwage exper hasHE hasHEexp\n\n(72,229 missing values generated)\n(72,229 missing values generated)\n\n      Source |       SS           df       MS      Number of obs   =    62,811\n-------------+----------------------------------   F(3, 62807)     =   2860.46\n       Model |  4000.16053         3  1333.38684   Prob &gt; F        =    0.0000\n    Residual |  29277.0845    62,807  .466143655   R-squared       =    0.1202\n-------------+----------------------------------   Adj R-squared   =    0.1202\n       Total |  33277.2451    62,810  .529808073   Root MSE        =    .68275\n\n------------------------------------------------------------------------------\n      lnwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       exper |   .0182621   .0003657    49.94   0.000     .0175453    .0189789\n       hasHE |  -.0839435   .0102764    -8.17   0.000    -.1040852   -.0638019\n    hasHEexp |   .0095156   .0005113    18.61   0.000     .0085134    .0105178\n       _cons |   6.101809   .0076396   798.71   0.000     6.086835    6.116782\n------------------------------------------------------------------------------\n\n\n\ngen experR = exper+hasHEexp*2/13\ngen hasHER = hasHE-hasHEexp/13\nreg lnwage experR hasHER hasHEexp\n\n(72,229 missing values generated)\n(72,229 missing values generated)\n\n      Source |       SS           df       MS      Number of obs   =    62,811\n-------------+----------------------------------   F(3, 62807)     =   2860.46\n       Model |  4000.16052         3  1333.38684   Prob &gt; F        =    0.0000\n    Residual |  29277.0845    62,807  .466143655   R-squared       =    0.1202\n-------------+----------------------------------   Adj R-squared   =    0.1202\n       Total |  33277.2451    62,810  .529808073   Root MSE        =    .68275\n\n------------------------------------------------------------------------------\n      lnwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      experR |   .0182621   .0003657    49.94   0.000     .0175453    .0189789\n      hasHER |  -.0839435   .0102764    -8.17   0.000    -.1040852   -.0638019\n    hasHEexp |   .0002489   .0004358     0.57   0.568    -.0006053     .001103\n       _cons |   6.101809   .0076396   798.71   0.000     6.086835    6.116782\n------------------------------------------------------------------------------\n\n\n9. Verify that the F-statistic from Q7 is the square of the above T-statistic.\n\ndis (_b[hasHEexp]/_se[hasHEexp])^2\n\n.32610143\n\n\n10. Use the restricted OLS approach to replicate the F-statistic and p-value from Q7.\n\nreg lnwage exper hasHE hasHEexp\nscalar RSSu = e(rss)\nscalar DOFu = e(df_r)\nreg lnwage experR hasHER \nscalar RSSr = e(rss)\nscalar DOFr = e(df_r)\n\nscalar Fstat = ((RSSr-RSSu)/(DOFr-DOFu))/(RSSu/DOFu)\nscalar pval = Ftail(1,DOFu,Fstat)\n\nscalar list Fstat pval\n\n\n      Source |       SS           df       MS      Number of obs   =    62,811\n-------------+----------------------------------   F(3, 62807)     =   2860.46\n       Model |  4000.16053         3  1333.38684   Prob &gt; F        =    0.0000\n    Residual |  29277.0845    62,807  .466143655   R-squared       =    0.1202\n-------------+----------------------------------   Adj R-squared   =    0.1202\n       Total |  33277.2451    62,810  .529808073   Root MSE        =    .68275\n\n------------------------------------------------------------------------------\n      lnwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       exper |   .0182621   .0003657    49.94   0.000     .0175453    .0189789\n       hasHE |  -.0839435   .0102764    -8.17   0.000    -.1040852   -.0638019\n    hasHEexp |   .0095156   .0005113    18.61   0.000     .0085134    .0105178\n       _cons |   6.101809   .0076396   798.71   0.000     6.086835    6.116782\n------------------------------------------------------------------------------\n\n      Source |       SS           df       MS      Number of obs   =    62,811\n-------------+----------------------------------   F(2, 62808)     =   4290.58\n       Model |  4000.00851         2  2000.00425   Prob &gt; F        =    0.0000\n    Residual |  29277.2366    62,808  .466138654   R-squared       =    0.1202\n-------------+----------------------------------   Adj R-squared   =    0.1202\n       Total |  33277.2451    62,810  .529808073   Root MSE        =    .68274\n\n------------------------------------------------------------------------------\n      lnwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      experR |   .0182233   .0003593    50.71   0.000      .017519    .0189277\n      hasHER |  -.0882792   .0069253   -12.75   0.000    -.1018527   -.0747056\n       _cons |   6.104077   .0065255   935.42   0.000     6.091287    6.116867\n------------------------------------------------------------------------------\n     Fstat =  .32612066\n      pval =   .5679544\n\n\n11. Use the restricted OLS approach to test the following hypothesis corresponding to the model in Q5:\n\\[\nH_0: \\gamma_j = 0\\qquad \\text{for}\\quad j=1,3,4,5\n\\] Compute the F-statistic and p-value. Verify your result using the post-estimation test command.\n\nreg lnwage ib2.educat##c.exper\nscalar RSSu = e(rss)\nscalar DOFu = e(df_r)\nreg lnwage ib2.educat exper \nscalar RSSr = e(rss)\nscalar DOFr = e(df_r)\n\nscalar Fstat = ((RSSr-RSSu)/(DOFr-DOFu))/(RSSu/DOFu)\nscalar pval = Ftail(DOFr-DOFu,DOFu,Fstat)\n\nscalar list Fstat pval\n\n** verify\nreg lnwage ib2.educat##c.exper\ntest 1.educat#exper 3.educat#exper 4.educat#exper 5.educat#exper\n\n\n      Source |       SS           df       MS      Number of obs   =   115,352\n-------------+----------------------------------   F(9, 115342)    =   4039.95\n       Model |  17435.5509         9  1937.28343   Prob &gt; F        =    0.0000\n    Residual |  55310.0589   115,342  .479530951   R-squared       =    0.2397\n-------------+----------------------------------   Adj R-squared   =    0.2396\n       Total |  72745.6098   115,351   .63064568   Root MSE        =    .69248\n\n------------------------------------------------------------------------------\n      lnwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      educat |\n        &lt;HS  |  -.3882762   .0178644   -21.73   0.000    -.4232902   -.3532622\n        HS+  |  -.0839435   .0104229    -8.05   0.000    -.1043722   -.0635149\n         BA  |   .6145964   .0109763    55.99   0.000     .5930831    .6361097\n         PG  |   .9055692   .0141961    63.79   0.000      .877745    .9333933\n             |\n       exper |   .0182621   .0003709    49.24   0.000     .0175351    .0189891\n             |\n      educat#|\n     c.exper |\n        &lt;HS  |    .001037   .0007627     1.36   0.174     -.000458    .0025319\n        HS+  |   .0095156   .0005186    18.35   0.000     .0084991    .0105321\n         BA  |  -.0032067   .0005743    -5.58   0.000    -.0043324   -.0020811\n         PG  |  -.0051215   .0007698    -6.65   0.000    -.0066302   -.0036128\n             |\n       _cons |   6.101809   .0077485   787.48   0.000     6.086622    6.116996\n------------------------------------------------------------------------------\n\n      Source |       SS           df       MS      Number of obs   =   115,352\n-------------+----------------------------------   F(5, 115346)    =   7085.67\n       Model |  17093.4651         5  3418.69301   Prob &gt; F        =    0.0000\n    Residual |  55652.1448   115,346  .482480058   R-squared       =    0.2350\n-------------+----------------------------------   Adj R-squared   =    0.2349\n       Total |  72745.6098   115,351   .63064568   Root MSE        =    .69461\n\n------------------------------------------------------------------------------\n      lnwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      educat |\n        &lt;HS  |  -.3724213   .0090073   -41.35   0.000    -.3900754   -.3547671\n        HS+  |   .0726614   .0055618    13.06   0.000     .0617604    .0835624\n         BA  |   .5714202   .0057563    99.27   0.000     .5601379    .5827025\n         PG  |   .8295498   .0068114   121.79   0.000     .8161996    .8428999\n             |\n       exper |   .0201711   .0002025    99.60   0.000     .0197742    .0205681\n       _cons |   6.067685   .0054116  1121.24   0.000     6.057079    6.078292\n------------------------------------------------------------------------------\n     Fstat =  178.34399\n      pval =  1.32e-152\n\n      Source |       SS           df       MS      Number of obs   =   115,352\n-------------+----------------------------------   F(9, 115342)    =   4039.95\n       Model |  17435.5509         9  1937.28343   Prob &gt; F        =    0.0000\n    Residual |  55310.0589   115,342  .479530951   R-squared       =    0.2397\n-------------+----------------------------------   Adj R-squared   =    0.2396\n       Total |  72745.6098   115,351   .63064568   Root MSE        =    .69248\n\n------------------------------------------------------------------------------\n      lnwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      educat |\n        &lt;HS  |  -.3882762   .0178644   -21.73   0.000    -.4232902   -.3532622\n        HS+  |  -.0839435   .0104229    -8.05   0.000    -.1043722   -.0635149\n         BA  |   .6145964   .0109763    55.99   0.000     .5930831    .6361097\n         PG  |   .9055692   .0141961    63.79   0.000      .877745    .9333933\n             |\n       exper |   .0182621   .0003709    49.24   0.000     .0175351    .0189891\n             |\n      educat#|\n     c.exper |\n        &lt;HS  |    .001037   .0007627     1.36   0.174     -.000458    .0025319\n        HS+  |   .0095156   .0005186    18.35   0.000     .0084991    .0105321\n         BA  |  -.0032067   .0005743    -5.58   0.000    -.0043324   -.0020811\n         PG  |  -.0051215   .0007698    -6.65   0.000    -.0066302   -.0036128\n             |\n       _cons |   6.101809   .0077485   787.48   0.000     6.086622    6.116996\n------------------------------------------------------------------------------\n\n ( 1)  1.educat#c.exper = 0\n ( 2)  3.educat#c.exper = 0\n ( 3)  4.educat#c.exper = 0\n ( 4)  5.educat#c.exper = 0\n\n       F(  4,115342) =  178.34\n            Prob &gt; F =    0.0000\n\n\n12. Compute the relevant Chi-squared distributed test statistic and corresponding p-value for the above test, assuming \\(n\\) is large (enough).\n\nscalar Cstat = Fstat*(DOFr-DOFu)\nscalar pval = chi2tail(DOFr-DOFu,Cstat)\nscalar list Cstat pval\n\n     Cstat =  713.37597\n      pval =  4.42e-153\n\n\n13. Using the data from Problem Set 2, estimate the simple linear regression model using OLS,\n\\[\n  \\ln(Wage_i) = \\beta_0 + \\beta_1 Educ_i + \\beta_2 Female_i + \\varepsilon_i\n\\]\n\nuse \"$rootdir/problem-sets/ps-2/problem-set-2-data.dta\", clear\nreg lwage educ female\nest sto ols\nestadd scalar sigma = e(rmse)\n\n(PSID wage data 1976-82 from Baltagi and Khanti-Akom (1990))\n\n      Source |       SS           df       MS      Number of obs   =     4,165\n-------------+----------------------------------   F(2, 4162)      =    732.99\n       Model |  231.021419         2   115.51071   Prob &gt; F        =    0.0000\n    Residual |  655.883483     4,162  .157588535   R-squared       =    0.2605\n-------------+----------------------------------   Adj R-squared   =    0.2601\n       Total |  886.904902     4,164  .212993492   Root MSE        =    .39697\n\n------------------------------------------------------------------------------\n       lwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        educ |   .0651382   .0022066    29.52   0.000     .0608121    .0694642\n      female |  -.4737645   .0194589   -24.35   0.000    -.5119143   -.4356147\n       _cons |    5.89297   .0290891   202.58   0.000      5.83594        5.95\n------------------------------------------------------------------------------\n\nadded scalar:\n              e(sigma) =  .39697422\n\n\n14. Estimate the Mincer equation using Maximum Likelihood. Take a look at https://www.stata.com/manuals13/rmlexp.pdf, the documentation for the mlexp command. It has a discussion on estimating the CLRM using ML.1\n\nmlexp (ln(normalden(lwage, {xb: educ female _cons}, exp({theta}))))\nereturn list\nnlcom (sigma: exp(_b[/theta]))\nestadd scalar sigma = r(b)[1,1]\neststo ml\n\n\nInitial:      Log likelihood = -97095.356\nAlternative:  Log likelihood = -35297.969\nRescale:      Log likelihood = -12999.606\nRescale eq:   Log likelihood = -7350.6222\nIteration 0:  Log likelihood = -7350.6222  (not concave)\nIteration 1:  Log likelihood =  -3936.054  \nIteration 2:  Log likelihood = -2187.1092  (backed up)\nIteration 3:  Log likelihood = -2073.0429  \nIteration 4:  Log likelihood = -2060.4271  \nIteration 5:  Log likelihood = -2060.4019  \nIteration 6:  Log likelihood = -2060.4019  \n\nMaximum likelihood estimation\n\nLog likelihood = -2060.4019                              Number of obs = 4,165\n\n------------------------------------------------------------------------------\n             | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nxb           |\n        educ |   .0651382   .0022058    29.53   0.000      .060815    .0694614\n      female |  -.4737645   .0194519   -24.36   0.000    -.5118895   -.4356396\n       _cons |    5.89297   .0290786   202.66   0.000     5.835977    5.949963\n-------------+----------------------------------------------------------------\n      /theta |  -.9242442   .0109566   -84.35   0.000    -.9457188   -.9027696\n------------------------------------------------------------------------------\n\nscalars:\n               e(rank) =  4\n                  e(N) =  4165\n                 e(ic) =  6\n                  e(k) =  4\n               e(k_eq) =  2\n          e(converged) =  1\n                 e(rc) =  0\n                 e(ll) =  -2060.40189888505\n              e(k_aux) =  1\n               e(df_m) =  4\n         e(k_eq_model) =  0\n\nmacros:\n            e(cmdline) : \"mlexp (ln(normalden(lwage, {xb: educ female _cons..\"\n                e(cmd) : \"mlexp\"\n            e(predict) : \"mlexp_p\"\n          e(estat_cmd) : \"mlexp_estat\"\n       e(marginsnotok) : \"SCores\"\n          e(marginsok) : \"default xb\"\n        e(marginsprop) : \"nochainrule\"\n               e(lexp) : \"ln(normalden(lwage,{xb:},exp({theta:})))\"\n             e(params) : \"xb:educ xb:female xb:_cons theta:_cons\"\n                e(opt) : \"moptimize\"\n                e(vce) : \"oim\"\n          e(ml_method) : \"lf0\"\n          e(technique) : \"nr\"\n         e(properties) : \"b V\"\n\nmatrices:\n                  e(b) :  1 x 4\n                  e(V) :  4 x 4\n               e(init) :  1 x 4\n               e(ilog) :  1 x 20\n           e(gradient) :  1 x 4\n\nfunctions:\n             e(sample)   \n\n       sigma: exp(_b[/theta])\n\n------------------------------------------------------------------------------\n             | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       sigma |   .3968312   .0043479    91.27   0.000     .3883094     .405353\n------------------------------------------------------------------------------\n\nadded scalar:\n              e(sigma) =  .39683123\n\n\n15. Estimate the Mincer equation using Method of Moments. You can use the gmm command in Stata. Hint: the regressors will be their own instruments and use the onestep option.2\n\ngmm (lwage - {xb: educ female _cons}), instruments(educ female) onestep\neststo mm\n\nesttab ols ml mm, se drop(theta:_cons) scalar(N sigma) mtitle(OLS ML MM)\n\n\nStep 1\nIteration 0:  GMM criterion Q(b) =  44.629069  \nIteration 1:  GMM criterion Q(b) =  2.101e-24  \nIteration 2:  GMM criterion Q(b) =  1.368e-31  \n\nnote: model is exactly identified.\n\nGMM estimation \n\nNumber of parameters =   3\nNumber of moments    =   3\nInitial weight matrix: Unadjusted                 Number of obs   =      4,165\n\n------------------------------------------------------------------------------\n             |               Robust\n             | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        educ |   .0651382   .0023187    28.09   0.000     .0605935    .0696828\n      female |  -.4737645   .0177811   -26.64   0.000    -.5086148   -.4389143\n       _cons |    5.89297   .0300924   195.83   0.000      5.83399     5.95195\n------------------------------------------------------------------------------\nInstruments for equation 1: educ female _cons\n\n------------------------------------------------------------\n                      (1)             (2)             (3)   \n                      OLS              ML              MM   \n------------------------------------------------------------\nmain                                                        \neduc               0.0651***       0.0651***       0.0651***\n                (0.00221)       (0.00221)       (0.00232)   \n\nfemale             -0.474***       -0.474***       -0.474***\n                 (0.0195)        (0.0195)        (0.0178)   \n\n_cons               5.893***        5.893***        5.893***\n                 (0.0291)        (0.0291)        (0.0301)   \n------------------------------------------------------------\nN                    4165            4165            4165   \nsigma               0.397           0.397                   \n------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001\n\n\n\n\n\n\nlog close"
  },
  {
    "objectID": "problem-set-4-solutions.html#preamble",
    "href": "problem-set-4-solutions.html#preamble",
    "title": "Problem Set 4 (SOLUTIONS)",
    "section": "",
    "text": "Create a do-file for this problem set and include a preamble that sets the directory and opens the data directly from the NBER website. Of course, this requires a good internet connection. For example,\n\nclear \n//or, to remove all stored values (including macros, matrices, scalars, etc.) \n*clear all\n\n* Replace $rootdir with the relevant path to on your local harddrive.\ncd \"$rootdir/problem-sets/ps-4\"\n\ncap log close\nlog using problem-set-4-log.txt, replace\n\nuse \"https://data.nber.org/morg/annual/morg19.dta\", clear\n\nYou can, of course, download the data and open it locally on your computer."
  },
  {
    "objectID": "problem-set-4-solutions.html#questions",
    "href": "problem-set-4-solutions.html#questions",
    "title": "Problem Set 4 (SOLUTIONS)",
    "section": "",
    "text": "1. Create a new variable exper equal to age minus (years of education + 6). This is referred to as potential years of experience. Check how each variable defines missing values before proceeding. You will need to create a years of education variable for this. Here is he the suggested code:\n\ntab grade92, m\ngen eduyrs = .\n    replace eduyrs = .3 if grade92==31\n    replace eduyrs = 3.2 if grade92==32\n    replace eduyrs = 7.2 if grade92==33\n    replace eduyrs = 7.2 if grade92==34\n    replace eduyrs = 9  if grade92==35\n    replace eduyrs = 10 if grade92==36\n    replace eduyrs = 11 if grade92==37\n    replace eduyrs = 12 if grade92==38\n    replace eduyrs = 12 if grade92==39\n    replace eduyrs = 13 if grade92==40\n    replace eduyrs = 14 if grade92==41\n    replace eduyrs = 14 if grade92==42\n    replace eduyrs = 16 if grade92==43\n    replace eduyrs = 18 if grade92==44\n    replace eduyrs = 18 if grade92==45\n    replace eduyrs = 18 if grade92==46\n    lab var eduyrs \"completed education\"\ntab grade92, sum(eduyrs)\n\n\n    Highest |\n      grade |\n  completed |      Freq.     Percent        Cum.\n------------+-----------------------------------\n         31 |        814        0.28        0.28\n         32 |      1,495        0.51        0.79\n         33 |      3,071        1.05        1.85\n         34 |      4,123        1.41        3.26\n         35 |      5,244        1.80        5.06\n         36 |      7,824        2.69        7.75\n         37 |      9,271        3.18       10.93\n         38 |      4,226        1.45       12.38\n         39 |     82,795       28.41       40.79\n         40 |     50,112       17.20       57.99\n         41 |     12,392        4.25       62.24\n         42 |     16,161        5.55       67.79\n         43 |     59,438       20.40       88.19\n         44 |     25,374        8.71       96.89\n         45 |      3,785        1.30       98.19\n         46 |      5,265        1.81      100.00\n------------+-----------------------------------\n      Total |    291,390      100.00\n(291,390 missing values generated)\n(814 real changes made)\n(1,495 real changes made)\n(3,071 real changes made)\n(4,123 real changes made)\n(5,244 real changes made)\n(7,824 real changes made)\n(9,271 real changes made)\n(4,226 real changes made)\n(82,795 real changes made)\n(50,112 real changes made)\n(12,392 real changes made)\n(16,161 real changes made)\n(59,438 real changes made)\n(25,374 real changes made)\n(3,785 real changes made)\n(5,265 real changes made)\n\n    Highest |\n      grade |   Summary of completed education\n  completed |        Mean   Std. dev.       Freq.\n------------+------------------------------------\n         31 |   .30000001           0         814\n         32 |         3.2           0       1,495\n         33 |   7.1999998           0       3,071\n         34 |   7.1999998           0       4,123\n         35 |           9           0       5,244\n         36 |          10           0       7,824\n         37 |          11           0       9,271\n         38 |          12           0       4,226\n         39 |          12           0      82,795\n         40 |          13           0      50,112\n         41 |          14           0      12,392\n         42 |          14           0      16,161\n         43 |          16           0      59,438\n         44 |          18           0      25,374\n         45 |          18           0       3,785\n         46 |          18           0       5,265\n------------+------------------------------------\n      Total |   13.556855   2.7030576     291,390\n\n\n\ntab age, m\ngen exper = age-(eduyrs+6)\n\n\n        Age |      Freq.     Percent        Cum.\n------------+-----------------------------------\n         16 |      4,661        1.60        1.60\n         17 |      4,630        1.59        3.19\n         18 |      4,417        1.52        4.70\n         19 |      4,039        1.39        6.09\n         20 |      3,915        1.34        7.43\n         21 |      3,996        1.37        8.81\n         22 |      3,918        1.34       10.15\n         23 |      3,950        1.36       11.51\n         24 |      4,194        1.44       12.94\n         25 |      4,185        1.44       14.38\n         26 |      4,325        1.48       15.87\n         27 |      4,476        1.54       17.40\n         28 |      4,600        1.58       18.98\n         29 |      4,633        1.59       20.57\n         30 |      4,829        1.66       22.23\n         31 |      4,735        1.62       23.85\n         32 |      4,601        1.58       25.43\n         33 |      4,748        1.63       27.06\n         34 |      4,646        1.59       28.66\n         35 |      4,730        1.62       30.28\n         36 |      4,742        1.63       31.91\n         37 |      4,848        1.66       33.57\n         38 |      4,550        1.56       35.13\n         39 |      4,735        1.62       36.76\n         40 |      4,667        1.60       38.36\n         41 |      4,503        1.55       39.90\n         42 |      4,390        1.51       41.41\n         43 |      4,309        1.48       42.89\n         44 |      4,193        1.44       44.33\n         45 |      4,253        1.46       45.79\n         46 |      4,266        1.46       47.25\n         47 |      4,447        1.53       48.78\n         48 |      4,563        1.57       50.34\n         49 |      4,698        1.61       51.96\n         50 |      4,646        1.59       53.55\n         51 |      4,477        1.54       55.09\n         52 |      4,555        1.56       56.65\n         53 |      4,523        1.55       58.20\n         54 |      4,736        1.63       59.83\n         55 |      5,010        1.72       61.55\n         56 |      5,035        1.73       63.27\n         57 |      4,976        1.71       64.98\n         58 |      5,030        1.73       66.71\n         59 |      5,066        1.74       68.45\n         60 |      5,124        1.76       70.20\n         61 |      5,067        1.74       71.94\n         62 |      5,035        1.73       73.67\n         63 |      4,927        1.69       75.36\n         64 |      4,892        1.68       77.04\n         65 |      4,554        1.56       78.60\n         66 |      4,526        1.55       80.16\n         67 |      4,344        1.49       81.65\n         68 |      4,328        1.49       83.13\n         69 |      4,100        1.41       84.54\n         70 |      4,058        1.39       85.93\n         71 |      4,008        1.38       87.31\n         72 |      3,897        1.34       88.65\n         73 |      3,147        1.08       89.73\n         74 |      2,815        0.97       90.69\n         75 |      2,809        0.96       91.66\n         76 |      2,623        0.90       92.56\n         77 |      2,373        0.81       93.37\n         78 |      2,201        0.76       94.13\n         79 |      1,977        0.68       94.80\n         80 |      7,799        2.68       97.48\n         85 |      7,340        2.52      100.00\n------------+-----------------------------------\n      Total |    291,390      100.00\n\n\n2. Keep only those between the ages of 18 and 54. Check the distribution of `exper’ and replace any negative values to 0.\n\nkeep if inrange(age,18,54)\nsum exper, det\nreplace exper=0 if exper&lt;0\n\n(126,352 observations deleted)\n\n                            exper\n-------------------------------------------------------------\n      Percentiles      Smallest\n 1%            0             -6\n 5%            1             -5\n10%            2             -4       Obs             165,038\n25%            7             -4       Sum of wgt.     165,038\n\n50%           16                      Mean           16.50623\n                        Largest       Std. dev.      10.57401\n75%           25           47.7\n90%           31           47.7       Variance       111.8097\n95%           34           47.7       Skewness       .1296908\n99%           36           47.7       Kurtosis       1.887811\n(1,078 real changes made)\n\n\n3. Create a categorical variable that takes on 4 values: 1 “less than High School”; 2 “High School Diploma”; 3 “some Higher Education”; 4 “Bachelors”; 5 “Postgraduate”. This variable should be based on the the grade92 variable. You can find the value labels for this variable in this document: https://data.nber.org/morg/docs/cpsx.pdf. I suggest using the recode command, which allows you to create value labels while assigning values. Check the distributio of exper by education category.\n\nrecode grade92 (31/38 = 1 \"&lt;HS\") (39 = 2 \"HS\") (40/42 = 3 \"HS+\") (43 = 4 \"BA\") (44/46 = 5 \"PG\"), gen(educat)\ntab grade92 educat, m\n\ntab educat, sum(exper)\n\n(165,038 differences between grade92 and educat)\n\n   Highest |\n     grade |      RECODE of grade92 (Highest grade completed)\n completed |       &lt;HS         HS        HS+         BA         PG |     Total\n-----------+-------------------------------------------------------+----------\n        31 |       398          0          0          0          0 |       398 \n        32 |       578          0          0          0          0 |       578 \n        33 |     1,515          0          0          0          0 |     1,515 \n        34 |     1,571          0          0          0          0 |     1,571 \n        35 |     1,971          0          0          0          0 |     1,971 \n        36 |     2,198          0          0          0          0 |     2,198 \n        37 |     4,373          0          0          0          0 |     4,373 \n        38 |     2,440          0          0          0          0 |     2,440 \n        39 |         0     45,013          0          0          0 |    45,013 \n        40 |         0          0     30,934          0          0 |    30,934 \n        41 |         0          0      7,154          0          0 |     7,154 \n        42 |         0          0      9,708          0          0 |     9,708 \n        43 |         0          0          0     37,557          0 |    37,557 \n        44 |         0          0          0          0     14,804 |    14,804 \n        45 |         0          0          0          0      2,021 |     2,021 \n        46 |         0          0          0          0      2,803 |     2,803 \n-----------+-------------------------------------------------------+----------\n     Total |    15,044     45,013     47,796     37,557     19,628 |   165,038 \n\n  RECODE of |\n    grade92 |\n   (Highest |\n      grade |          Summary of exper\n completed) |        Mean   Std. dev.       Freq.\n------------+------------------------------------\n        &lt;HS |    19.29665    12.82301      15,044\n         HS |   17.726435   11.044222      45,013\n        HS+ |   15.643589   10.806949      47,796\n         BA |   15.376841   9.3440749      37,557\n         PG |   15.900092   8.1569821      19,628\n------------+------------------------------------\n      Total |   16.514468   10.560511     165,038\n\n\n4. Create the variable lnwage equal to the (natural) log of weekly earnings. Create a figure that shows the predicted linear fit of lwage against exper, by educat. Try to place all 5 fitted lines in the same graph.\n\ngen lnwage = ln(earnwke)\ntwoway (lfit lnwage exper if educat==1) (lfit lnwage exper if educat==2)  (lfit lnwage exper if educat==3) (lfit lnwage exper if educat==4) (lfit lnwage exper if educat==5) , legend(order(1 \"&lt;HS\" 2 \"HS\" 3 \"HS+\" 4 \"BA\" 5 \"PG\") pos(6) r(1)) xtitle(Years of experience) \n\n(49,686 missing values generated)\n\n\n\n\n\n\n\n\n\n5. Estimate a linear regression model that allows the slope coefficient on exper and constant term to vary by education category (educat). Let the base (excluded) education category be 2 “High School diploma”.\n\\[\n  \\ln(Wage_i) = \\alpha + \\sum_{j\\neq2}\\psi_j \\mathbf{1}\\{Educat_i=j\\} + \\beta Exper_i + \\sum_{j\\neq2}\\gamma_j Exper_i\\times\\mathbf{1}\\{Educat_i=j\\}+\\upsilon_i\n\\]\n\nreg lnwage ib2.educat##c.exper\n\n\n      Source |       SS           df       MS      Number of obs   =   115,352\n-------------+----------------------------------   F(9, 115342)    =   4039.95\n       Model |  17435.5509         9  1937.28343   Prob &gt; F        =    0.0000\n    Residual |  55310.0589   115,342  .479530951   R-squared       =    0.2397\n-------------+----------------------------------   Adj R-squared   =    0.2396\n       Total |  72745.6098   115,351   .63064568   Root MSE        =    .69248\n\n------------------------------------------------------------------------------\n      lnwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      educat |\n        &lt;HS  |  -.3882762   .0178644   -21.73   0.000    -.4232902   -.3532622\n        HS+  |  -.0839435   .0104229    -8.05   0.000    -.1043722   -.0635149\n         BA  |   .6145964   .0109763    55.99   0.000     .5930831    .6361097\n         PG  |   .9055692   .0141961    63.79   0.000      .877745    .9333933\n             |\n       exper |   .0182621   .0003709    49.24   0.000     .0175351    .0189891\n             |\n      educat#|\n     c.exper |\n        &lt;HS  |    .001037   .0007627     1.36   0.174     -.000458    .0025319\n        HS+  |   .0095156   .0005186    18.35   0.000     .0084991    .0105321\n         BA  |  -.0032067   .0005743    -5.58   0.000    -.0043324   -.0020811\n         PG  |  -.0051215   .0007698    -6.65   0.000    -.0066302   -.0036128\n             |\n       _cons |   6.101809   .0077485   787.48   0.000     6.086622    6.116996\n------------------------------------------------------------------------------\n\n\n6. Show that after 13 years of experience, those with some Higer Education (but no Bachelors), out earn those with just a high school diploma. You can assume that there are is a 2 year difference between the experience (education).\n\ndis _b[exper]*14\ndis (_b[exper] + _b[3.educat#exper])*12 + _b[3.educat]\n\n\ndis _b[exper]*15\ndis (_b[exper] + _b[3.educat#exper])*13 + _b[3.educat]\n\n.25566943\n.24938901\n.27393153\n.27716672\n\n\n7. Use the post-estimation test command to test the null hypothesis: \\(H_0: 15\\beta = 13(\\beta+\\gamma_3)+\\psi_3\\).\n\ntest exper*15 = (exper+3.educat#exper)*13+3.educat\n\n\n ( 1)  - 3.educat + 2*exper - 13*3.educat#c.exper = 0\n\n       F(  1,115342) =    0.32\n            Prob &gt; F =    0.5734\n\n\n8. Estimate a transformed version of the above model allowing you to test the above hypothesis using the coefficient from a single regressor. That is, the resulting test should be a simple t-test of \\(H_0: \\phi=0\\), where \\(\\phi\\) is the coefficient on the interaction of exper and a dummy variable for educat=3. This will be easier to do if you estimate the model using only the relevant sample: those with High School diplomas and some Higher Education. I suggest avoiding the use of factor notation to create the dummy variables and interaction terms for this exercise. For example, the following should replicate the relevant coefficients from Q5.\n\ngen hasHE = educat==3 if inlist(educat,2,3)\ngen hasHEexp = hasHE*exper\n\nreg lnwage exper hasHE hasHEexp\n\n(72,229 missing values generated)\n(72,229 missing values generated)\n\n      Source |       SS           df       MS      Number of obs   =    62,811\n-------------+----------------------------------   F(3, 62807)     =   2860.46\n       Model |  4000.16053         3  1333.38684   Prob &gt; F        =    0.0000\n    Residual |  29277.0845    62,807  .466143655   R-squared       =    0.1202\n-------------+----------------------------------   Adj R-squared   =    0.1202\n       Total |  33277.2451    62,810  .529808073   Root MSE        =    .68275\n\n------------------------------------------------------------------------------\n      lnwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       exper |   .0182621   .0003657    49.94   0.000     .0175453    .0189789\n       hasHE |  -.0839435   .0102764    -8.17   0.000    -.1040852   -.0638019\n    hasHEexp |   .0095156   .0005113    18.61   0.000     .0085134    .0105178\n       _cons |   6.101809   .0076396   798.71   0.000     6.086835    6.116782\n------------------------------------------------------------------------------\n\n\n\ngen experR = exper+hasHEexp*2/13\ngen hasHER = hasHE-hasHEexp/13\nreg lnwage experR hasHER hasHEexp\n\n(72,229 missing values generated)\n(72,229 missing values generated)\n\n      Source |       SS           df       MS      Number of obs   =    62,811\n-------------+----------------------------------   F(3, 62807)     =   2860.46\n       Model |  4000.16052         3  1333.38684   Prob &gt; F        =    0.0000\n    Residual |  29277.0845    62,807  .466143655   R-squared       =    0.1202\n-------------+----------------------------------   Adj R-squared   =    0.1202\n       Total |  33277.2451    62,810  .529808073   Root MSE        =    .68275\n\n------------------------------------------------------------------------------\n      lnwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      experR |   .0182621   .0003657    49.94   0.000     .0175453    .0189789\n      hasHER |  -.0839435   .0102764    -8.17   0.000    -.1040852   -.0638019\n    hasHEexp |   .0002489   .0004358     0.57   0.568    -.0006053     .001103\n       _cons |   6.101809   .0076396   798.71   0.000     6.086835    6.116782\n------------------------------------------------------------------------------\n\n\n9. Verify that the F-statistic from Q7 is the square of the above T-statistic.\n\ndis (_b[hasHEexp]/_se[hasHEexp])^2\n\n.32610143\n\n\n10. Use the restricted OLS approach to replicate the F-statistic and p-value from Q7.\n\nreg lnwage exper hasHE hasHEexp\nscalar RSSu = e(rss)\nscalar DOFu = e(df_r)\nreg lnwage experR hasHER \nscalar RSSr = e(rss)\nscalar DOFr = e(df_r)\n\nscalar Fstat = ((RSSr-RSSu)/(DOFr-DOFu))/(RSSu/DOFu)\nscalar pval = Ftail(1,DOFu,Fstat)\n\nscalar list Fstat pval\n\n\n      Source |       SS           df       MS      Number of obs   =    62,811\n-------------+----------------------------------   F(3, 62807)     =   2860.46\n       Model |  4000.16053         3  1333.38684   Prob &gt; F        =    0.0000\n    Residual |  29277.0845    62,807  .466143655   R-squared       =    0.1202\n-------------+----------------------------------   Adj R-squared   =    0.1202\n       Total |  33277.2451    62,810  .529808073   Root MSE        =    .68275\n\n------------------------------------------------------------------------------\n      lnwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       exper |   .0182621   .0003657    49.94   0.000     .0175453    .0189789\n       hasHE |  -.0839435   .0102764    -8.17   0.000    -.1040852   -.0638019\n    hasHEexp |   .0095156   .0005113    18.61   0.000     .0085134    .0105178\n       _cons |   6.101809   .0076396   798.71   0.000     6.086835    6.116782\n------------------------------------------------------------------------------\n\n      Source |       SS           df       MS      Number of obs   =    62,811\n-------------+----------------------------------   F(2, 62808)     =   4290.58\n       Model |  4000.00851         2  2000.00425   Prob &gt; F        =    0.0000\n    Residual |  29277.2366    62,808  .466138654   R-squared       =    0.1202\n-------------+----------------------------------   Adj R-squared   =    0.1202\n       Total |  33277.2451    62,810  .529808073   Root MSE        =    .68274\n\n------------------------------------------------------------------------------\n      lnwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      experR |   .0182233   .0003593    50.71   0.000      .017519    .0189277\n      hasHER |  -.0882792   .0069253   -12.75   0.000    -.1018527   -.0747056\n       _cons |   6.104077   .0065255   935.42   0.000     6.091287    6.116867\n------------------------------------------------------------------------------\n     Fstat =  .32612066\n      pval =   .5679544\n\n\n11. Use the restricted OLS approach to test the following hypothesis corresponding to the model in Q5:\n\\[\nH_0: \\gamma_j = 0\\qquad \\text{for}\\quad j=1,3,4,5\n\\] Compute the F-statistic and p-value. Verify your result using the post-estimation test command.\n\nreg lnwage ib2.educat##c.exper\nscalar RSSu = e(rss)\nscalar DOFu = e(df_r)\nreg lnwage ib2.educat exper \nscalar RSSr = e(rss)\nscalar DOFr = e(df_r)\n\nscalar Fstat = ((RSSr-RSSu)/(DOFr-DOFu))/(RSSu/DOFu)\nscalar pval = Ftail(DOFr-DOFu,DOFu,Fstat)\n\nscalar list Fstat pval\n\n** verify\nreg lnwage ib2.educat##c.exper\ntest 1.educat#exper 3.educat#exper 4.educat#exper 5.educat#exper\n\n\n      Source |       SS           df       MS      Number of obs   =   115,352\n-------------+----------------------------------   F(9, 115342)    =   4039.95\n       Model |  17435.5509         9  1937.28343   Prob &gt; F        =    0.0000\n    Residual |  55310.0589   115,342  .479530951   R-squared       =    0.2397\n-------------+----------------------------------   Adj R-squared   =    0.2396\n       Total |  72745.6098   115,351   .63064568   Root MSE        =    .69248\n\n------------------------------------------------------------------------------\n      lnwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      educat |\n        &lt;HS  |  -.3882762   .0178644   -21.73   0.000    -.4232902   -.3532622\n        HS+  |  -.0839435   .0104229    -8.05   0.000    -.1043722   -.0635149\n         BA  |   .6145964   .0109763    55.99   0.000     .5930831    .6361097\n         PG  |   .9055692   .0141961    63.79   0.000      .877745    .9333933\n             |\n       exper |   .0182621   .0003709    49.24   0.000     .0175351    .0189891\n             |\n      educat#|\n     c.exper |\n        &lt;HS  |    .001037   .0007627     1.36   0.174     -.000458    .0025319\n        HS+  |   .0095156   .0005186    18.35   0.000     .0084991    .0105321\n         BA  |  -.0032067   .0005743    -5.58   0.000    -.0043324   -.0020811\n         PG  |  -.0051215   .0007698    -6.65   0.000    -.0066302   -.0036128\n             |\n       _cons |   6.101809   .0077485   787.48   0.000     6.086622    6.116996\n------------------------------------------------------------------------------\n\n      Source |       SS           df       MS      Number of obs   =   115,352\n-------------+----------------------------------   F(5, 115346)    =   7085.67\n       Model |  17093.4651         5  3418.69301   Prob &gt; F        =    0.0000\n    Residual |  55652.1448   115,346  .482480058   R-squared       =    0.2350\n-------------+----------------------------------   Adj R-squared   =    0.2349\n       Total |  72745.6098   115,351   .63064568   Root MSE        =    .69461\n\n------------------------------------------------------------------------------\n      lnwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      educat |\n        &lt;HS  |  -.3724213   .0090073   -41.35   0.000    -.3900754   -.3547671\n        HS+  |   .0726614   .0055618    13.06   0.000     .0617604    .0835624\n         BA  |   .5714202   .0057563    99.27   0.000     .5601379    .5827025\n         PG  |   .8295498   .0068114   121.79   0.000     .8161996    .8428999\n             |\n       exper |   .0201711   .0002025    99.60   0.000     .0197742    .0205681\n       _cons |   6.067685   .0054116  1121.24   0.000     6.057079    6.078292\n------------------------------------------------------------------------------\n     Fstat =  178.34399\n      pval =  1.32e-152\n\n      Source |       SS           df       MS      Number of obs   =   115,352\n-------------+----------------------------------   F(9, 115342)    =   4039.95\n       Model |  17435.5509         9  1937.28343   Prob &gt; F        =    0.0000\n    Residual |  55310.0589   115,342  .479530951   R-squared       =    0.2397\n-------------+----------------------------------   Adj R-squared   =    0.2396\n       Total |  72745.6098   115,351   .63064568   Root MSE        =    .69248\n\n------------------------------------------------------------------------------\n      lnwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      educat |\n        &lt;HS  |  -.3882762   .0178644   -21.73   0.000    -.4232902   -.3532622\n        HS+  |  -.0839435   .0104229    -8.05   0.000    -.1043722   -.0635149\n         BA  |   .6145964   .0109763    55.99   0.000     .5930831    .6361097\n         PG  |   .9055692   .0141961    63.79   0.000      .877745    .9333933\n             |\n       exper |   .0182621   .0003709    49.24   0.000     .0175351    .0189891\n             |\n      educat#|\n     c.exper |\n        &lt;HS  |    .001037   .0007627     1.36   0.174     -.000458    .0025319\n        HS+  |   .0095156   .0005186    18.35   0.000     .0084991    .0105321\n         BA  |  -.0032067   .0005743    -5.58   0.000    -.0043324   -.0020811\n         PG  |  -.0051215   .0007698    -6.65   0.000    -.0066302   -.0036128\n             |\n       _cons |   6.101809   .0077485   787.48   0.000     6.086622    6.116996\n------------------------------------------------------------------------------\n\n ( 1)  1.educat#c.exper = 0\n ( 2)  3.educat#c.exper = 0\n ( 3)  4.educat#c.exper = 0\n ( 4)  5.educat#c.exper = 0\n\n       F(  4,115342) =  178.34\n            Prob &gt; F =    0.0000\n\n\n12. Compute the relevant Chi-squared distributed test statistic and corresponding p-value for the above test, assuming \\(n\\) is large (enough).\n\nscalar Cstat = Fstat*(DOFr-DOFu)\nscalar pval = chi2tail(DOFr-DOFu,Cstat)\nscalar list Cstat pval\n\n     Cstat =  713.37597\n      pval =  4.42e-153\n\n\n13. Using the data from Problem Set 2, estimate the simple linear regression model using OLS,\n\\[\n  \\ln(Wage_i) = \\beta_0 + \\beta_1 Educ_i + \\beta_2 Female_i + \\varepsilon_i\n\\]\n\nuse \"$rootdir/problem-sets/ps-2/problem-set-2-data.dta\", clear\nreg lwage educ female\nest sto ols\nestadd scalar sigma = e(rmse)\n\n(PSID wage data 1976-82 from Baltagi and Khanti-Akom (1990))\n\n      Source |       SS           df       MS      Number of obs   =     4,165\n-------------+----------------------------------   F(2, 4162)      =    732.99\n       Model |  231.021419         2   115.51071   Prob &gt; F        =    0.0000\n    Residual |  655.883483     4,162  .157588535   R-squared       =    0.2605\n-------------+----------------------------------   Adj R-squared   =    0.2601\n       Total |  886.904902     4,164  .212993492   Root MSE        =    .39697\n\n------------------------------------------------------------------------------\n       lwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        educ |   .0651382   .0022066    29.52   0.000     .0608121    .0694642\n      female |  -.4737645   .0194589   -24.35   0.000    -.5119143   -.4356147\n       _cons |    5.89297   .0290891   202.58   0.000      5.83594        5.95\n------------------------------------------------------------------------------\n\nadded scalar:\n              e(sigma) =  .39697422\n\n\n14. Estimate the Mincer equation using Maximum Likelihood. Take a look at https://www.stata.com/manuals13/rmlexp.pdf, the documentation for the mlexp command. It has a discussion on estimating the CLRM using ML.1\n\nmlexp (ln(normalden(lwage, {xb: educ female _cons}, exp({theta}))))\nereturn list\nnlcom (sigma: exp(_b[/theta]))\nestadd scalar sigma = r(b)[1,1]\neststo ml\n\n\nInitial:      Log likelihood = -97095.356\nAlternative:  Log likelihood = -35297.969\nRescale:      Log likelihood = -12999.606\nRescale eq:   Log likelihood = -7350.6222\nIteration 0:  Log likelihood = -7350.6222  (not concave)\nIteration 1:  Log likelihood =  -3936.054  \nIteration 2:  Log likelihood = -2187.1092  (backed up)\nIteration 3:  Log likelihood = -2073.0429  \nIteration 4:  Log likelihood = -2060.4271  \nIteration 5:  Log likelihood = -2060.4019  \nIteration 6:  Log likelihood = -2060.4019  \n\nMaximum likelihood estimation\n\nLog likelihood = -2060.4019                              Number of obs = 4,165\n\n------------------------------------------------------------------------------\n             | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nxb           |\n        educ |   .0651382   .0022058    29.53   0.000      .060815    .0694614\n      female |  -.4737645   .0194519   -24.36   0.000    -.5118895   -.4356396\n       _cons |    5.89297   .0290786   202.66   0.000     5.835977    5.949963\n-------------+----------------------------------------------------------------\n      /theta |  -.9242442   .0109566   -84.35   0.000    -.9457188   -.9027696\n------------------------------------------------------------------------------\n\nscalars:\n               e(rank) =  4\n                  e(N) =  4165\n                 e(ic) =  6\n                  e(k) =  4\n               e(k_eq) =  2\n          e(converged) =  1\n                 e(rc) =  0\n                 e(ll) =  -2060.40189888505\n              e(k_aux) =  1\n               e(df_m) =  4\n         e(k_eq_model) =  0\n\nmacros:\n            e(cmdline) : \"mlexp (ln(normalden(lwage, {xb: educ female _cons..\"\n                e(cmd) : \"mlexp\"\n            e(predict) : \"mlexp_p\"\n          e(estat_cmd) : \"mlexp_estat\"\n       e(marginsnotok) : \"SCores\"\n          e(marginsok) : \"default xb\"\n        e(marginsprop) : \"nochainrule\"\n               e(lexp) : \"ln(normalden(lwage,{xb:},exp({theta:})))\"\n             e(params) : \"xb:educ xb:female xb:_cons theta:_cons\"\n                e(opt) : \"moptimize\"\n                e(vce) : \"oim\"\n          e(ml_method) : \"lf0\"\n          e(technique) : \"nr\"\n         e(properties) : \"b V\"\n\nmatrices:\n                  e(b) :  1 x 4\n                  e(V) :  4 x 4\n               e(init) :  1 x 4\n               e(ilog) :  1 x 20\n           e(gradient) :  1 x 4\n\nfunctions:\n             e(sample)   \n\n       sigma: exp(_b[/theta])\n\n------------------------------------------------------------------------------\n             | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       sigma |   .3968312   .0043479    91.27   0.000     .3883094     .405353\n------------------------------------------------------------------------------\n\nadded scalar:\n              e(sigma) =  .39683123\n\n\n15. Estimate the Mincer equation using Method of Moments. You can use the gmm command in Stata. Hint: the regressors will be their own instruments and use the onestep option.2\n\ngmm (lwage - {xb: educ female _cons}), instruments(educ female) onestep\neststo mm\n\nesttab ols ml mm, se drop(theta:_cons) scalar(N sigma) mtitle(OLS ML MM)\n\n\nStep 1\nIteration 0:  GMM criterion Q(b) =  44.629069  \nIteration 1:  GMM criterion Q(b) =  2.101e-24  \nIteration 2:  GMM criterion Q(b) =  1.368e-31  \n\nnote: model is exactly identified.\n\nGMM estimation \n\nNumber of parameters =   3\nNumber of moments    =   3\nInitial weight matrix: Unadjusted                 Number of obs   =      4,165\n\n------------------------------------------------------------------------------\n             |               Robust\n             | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        educ |   .0651382   .0023187    28.09   0.000     .0605935    .0696828\n      female |  -.4737645   .0177811   -26.64   0.000    -.5086148   -.4389143\n       _cons |    5.89297   .0300924   195.83   0.000      5.83399     5.95195\n------------------------------------------------------------------------------\nInstruments for equation 1: educ female _cons\n\n------------------------------------------------------------\n                      (1)             (2)             (3)   \n                      OLS              ML              MM   \n------------------------------------------------------------\nmain                                                        \neduc               0.0651***       0.0651***       0.0651***\n                (0.00221)       (0.00221)       (0.00232)   \n\nfemale             -0.474***       -0.474***       -0.474***\n                 (0.0195)        (0.0195)        (0.0178)   \n\n_cons               5.893***        5.893***        5.893***\n                 (0.0291)        (0.0291)        (0.0301)   \n------------------------------------------------------------\nN                    4165            4165            4165   \nsigma               0.397           0.397                   \n------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001"
  },
  {
    "objectID": "problem-set-4-solutions.html#postamble",
    "href": "problem-set-4-solutions.html#postamble",
    "title": "Problem Set 4 (SOLUTIONS)",
    "section": "",
    "text": "log close"
  },
  {
    "objectID": "problem-set-4-solutions.html#footnotes",
    "href": "problem-set-4-solutions.html#footnotes",
    "title": "Problem Set 4 (SOLUTIONS)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYou can also look at the following resource for a more flexible approach to ML estimation in Stata: https://www.stata.com/features/overview/maximum-likelihood-estimation/↩︎\nHere is a resource on GMM in Stata: https://www.stata.com/features/overview/generalized-method-of-moments/↩︎"
  },
  {
    "objectID": "problem-set-3-solutions.html",
    "href": "problem-set-3-solutions.html",
    "title": "Problem Set 3 (SOLUTIONS)",
    "section": "",
    "text": "The purpose of this problem set is for you to see how the ordinary least squares (OLS) estimator behaves under various assumptions in a linear regression model where you know what the model is – since you are going to be generating the data from a known data generating process (DGP).\nThe models estimated are simple bivariate regressions but the properties of the OLS estimator with vary with each case. This is demonstrated by changing the (a) distributional properties of the error term (variance-covariance structure), and (b) inducing correlation between the regressor and the error term. Any resulting bias and/or inconsistency will depend on the DGP.\nTo achieve certain results we will have to use a serially-correlated error structure, which is only appropriate in a time-series setting. For this reason, the models will be written with subscript \\(t\\) and not \\(i\\).\nThe code has been provided for model 1. You can then modify the code for models 2-4."
  },
  {
    "objectID": "problem-set-3-solutions.html#preamble",
    "href": "problem-set-3-solutions.html#preamble",
    "title": "Problem Set 3 (SOLUTIONS)",
    "section": "Preamble",
    "text": "Preamble\n\n\n\n\n\nYou do not need to load data for this problem set.\n\nclear \n//or, to remove all stored values (including macros, matrices, scalars, etc.) \n*clear all\n\n* Replace $rootdir with the relevant path to on your local harddrive.\ncd \"$rootdir/problem-sets/ps-3\"\n\ncap log close\nlog using problem-set-3-log.txt, replace\n\nHowever, since we are going to generate random variables, we should set a seed. This ensures replicability of the exercise. The number you choose is arbitrary, it simply ensures that any algorithms used to generate (pseudo) random variables start at the same place.\n\nset seed 981836"
  },
  {
    "objectID": "problem-set-3-solutions.html#model-1-clrm",
    "href": "problem-set-3-solutions.html#model-1-clrm",
    "title": "Problem Set 3 (SOLUTIONS)",
    "section": "Model 1: CLRM",
    "text": "Model 1: CLRM\nThis is your classical linear regression model. OLS estimator is unbiased and consistent.\n\\[\nY_t = \\beta_1 + \\beta_2X_t + \\upsilon_t \\qquad \\text{with}\\quad \\upsilon_t\\sim N(0,\\sigma^2)\n\\]\nWe know that the OLS estimator for \\(\\beta_2\\) is given by,\n\\[\n\\begin{aligned}\n  \\hat{\\beta}_2 =& \\frac{\\sum_t \\big[(X_t-\\bar{X})(Y_t-\\bar{Y})\\big]}{\\sum_t (X_t-\\bar{X})^2} \\\\\n  =& \\beta_2 + \\frac{\\sum_t \\big[(X_t-\\bar{X})(\\upsilon_t-\\bar{\\upsilon})\\big]}{\\sum_t (X_t-\\bar{X})^2} \\\\\n  =& \\beta_2 + \\frac{\\sum_t \\tilde{X}_t\\tilde{\\upsilon}_t}{\\sum_t \\tilde{X}_t^2}\n\\end{aligned}  \n\\] where \\(\\tilde{X}_t\\) and \\(\\tilde{\\upsilon}_t\\) represent the demeaned counterparts of these variables. Alternatively, using linear algebra notation:\n\\[\n\\begin{aligned}\n  \\hat{\\beta}_2 =& \\frac{X'M_{\\ell}Y}{X'M_{\\ell}X} \\\\\n  =& \\beta_2 + \\frac{X'M_{\\ell}\\upsilon}{X'M_{\\ell}X} \\\\\n  =& \\beta_2 + \\frac{\\tilde{X}'\\tilde{\\upsilon}}{\\tilde{X}'\\tilde{X}}\n\\end{aligned}  \n\\] where \\(\\tilde{X} = M_{\\ell}X\\), \\(\\tilde{\\upsilon}= M_{\\ell}\\upsilon\\), and \\(M_{\\ell} = I_n-\\ell(\\ell'\\ell)^{-1}\\ell'\\) (the orthogonal projection of the constant regressor).\nWe know from Handouts 2 & 3,\n\n\\(E[\\hat{\\beta}_2] = \\beta_2\\) (i.e., unbiased)\n\\(p \\lim \\hat{\\beta}_2 = \\beta_2\\) (i.e., consistent)\n\nCan you demonstrate these results?\n\nSimulation\nBegin by designing a programme that takes the parameters of the model as arguments, generates the data, estimates the model, and then returns the stored values.\n\ncap prog drop mc1\nprogram define mc1, rclass\n    syntax [, obs(integer 1) s(real 1) b1(real 0) b2(real 0)  sigma(real 1)]\n    drop _all\n    set obs `obs'\n    gen u = rnormal(0,`sigma')            // sigma is the std deviation of the error distribution\n    gen x=uniform()*`s'                   // s is the std devation of the x distribution\n    gen y=`b1'+`b2'*x + u                   // this generates the dep variable y\n    reg y x\n    return scalar b1=_b[_cons]            // intercept estimate\n    return scalar b2=_b[x]                  // coeff on the x variable\n    return scalar se2 = _se[x]            // std error\n    return scalar t2 = _b[x]/_se[x]     // t ratio\nend\n\n\n\n\nUse the the simulate command in Stata to estimate the model 100 times:\n\nsimulate b1=r(b1) b2=r(b2) se2=r(se2) t2=r(t2), reps(100): mc1, obs(50) s(6) b1(4) b2(2) sigma(3)\n\n\n      Command: mc1, obs(50) s(6) b1(4) b2(2) sigma(3)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\n\n\nCalculate the bias and plot the distribution of the bias.\n\ngen bias2=b2-2\nsu b1 b2 se2 t2\nsu bias2\nhistogram bias2, normal xline(`r(mean)')\n\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n          b1 |        100    3.880226    .9977415   1.080851   6.090028\n          b2 |        100    2.041985     .271704   1.365155   2.673303\n         se2 |        100    .2520885    .0291596   .1814694   .3255497\n          t2 |        100    8.216185      1.4968   5.484826   12.60699\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n       bias2 |        100    .0419852     .271704  -.6348448   .6733029\n(bin=10, start=-.63484478, width=.13081477)\n\n\n\n\n\n\n\n\n\nThe above simulation is a for a fixed sample size. To demonstrate consistency we need to repeat the exercise for larger and larger \\(n\\)’s.\n\ntempfile simdata\nforvalues n = 100(100)1000 {\n    simulate b1=r(b1) b2=r(b2) se2=r(se2) t2=r(t2), reps(100): mc1, obs(`n') s(6) b1(4) b2(2) sigma(3) \n    gen size = `n'\n    if `n'==100 save `simdata', replace\n    else {\n        append using `simdata'\n        save `simdata', replace\n    }\n}\ngen bias2=b2-2\nhistogram bias2, normal xline(0) by(size)\n\n\n      Command: mc1, obs(100) s(6) b1(4) b2(2) sigma(3)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\n(file C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_000005.tmp not found)\nfile C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_000005.tmp saved as .dta\n    format\n\n      Command: mc1, obs(200) s(6) b1(4) b2(2) sigma(3)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\nfile C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_000005.tmp saved as .dta\n    format\n\n      Command: mc1, obs(300) s(6) b1(4) b2(2) sigma(3)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\nfile C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_000005.tmp saved as .dta\n    format\n\n      Command: mc1, obs(400) s(6) b1(4) b2(2) sigma(3)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\nfile C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_000005.tmp saved as .dta\n    format\n\n      Command: mc1, obs(500) s(6) b1(4) b2(2) sigma(3)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\nfile C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_000005.tmp saved as .dta\n    format\n\n      Command: mc1, obs(600) s(6) b1(4) b2(2) sigma(3)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\nfile C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_000005.tmp saved as .dta\n    format\n\n      Command: mc1, obs(700) s(6) b1(4) b2(2) sigma(3)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\nfile C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_000005.tmp saved as .dta\n    format\n\n      Command: mc1, obs(800) s(6) b1(4) b2(2) sigma(3)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\nfile C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_000005.tmp saved as .dta\n    format\n\n      Command: mc1, obs(900) s(6) b1(4) b2(2) sigma(3)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\nfile C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_000005.tmp saved as .dta\n    format\n\n      Command: mc1, obs(1000) s(6) b1(4) b2(2) sigma(3)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\nfile C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_000005.tmp saved as .dta\n    format"
  },
  {
    "objectID": "problem-set-3-solutions.html#model-2-serial-correlation",
    "href": "problem-set-3-solutions.html#model-2-serial-correlation",
    "title": "Problem Set 3 (SOLUTIONS)",
    "section": "Model 2: Serial Correlation",
    "text": "Model 2: Serial Correlation\nRelax the assumption of an iid error term and allow for serial correlation. The OLS estimator is unbiased and consistent. However, the std errors are wrong since the software does not know that you have serially correlated errors and you are not taking this into account in the estimation.\n\\[\nY_t = \\beta_1 + \\beta_2X_t + \\upsilon_t \\qquad \\text{where}\\quad \\upsilon_t = \\rho \\upsilon_{t-1}+\\varepsilon_t \\quad\\text{and}\\quad \\varepsilon_t\\sim N(0,\\sigma^2)\n\\] We say that \\(U_t\\) follows an AR(1) process. You can show that \\(\\hat{\\beta}_2\\) remains unbiased and consistant. However, the standard homoskedastic-variance estimator is incorrect:\n\\[\nVar(\\hat{\\beta}_2) \\neq \\frac{\\sigma^2}{Var(X_t)}\n\\]\n\nSimulation\n\ncap prog drop mc2\nprogram define mc2, rclass\n    syntax [, obs(integer 1) s(real 1) b1(real 0) b2(real 0) bias2(real 0) sigma(real 1) rho(real 0)]\n    drop _all\n    set obs `obs'\n    gen u=0 \n    gen time=_n\n    tsset time\n    gen e = rnormal(0,`sigma')  \n    forvalues i=2/`obs'  {\n    replace u=`rho'*u[_n-1] + e if _n==`i'\n    }\n    gen x=uniform()*`s'\n    gen y=`b1'+`b2'*x + u\n    reg y x     \n    return scalar b1=_b[_cons]\n    return scalar b2=_b[x]\n    return scalar se2 = _se[x]\n    return scalar t2 = _b[x]/_se[x]\nend\n\nsimulate b1=r(b1) b2=r(b2) se2=r(se2) t2=r(t2), reps(100): mc2, obs(50) s(6) b1(4) b2(2) sigma(3) rho(0.2) \ngen bias2=b2-2\nsu b2 t2 se2\nsu bias2\nhistogram bias2, normal xline(`r(mean)')\n\n\n\n      Command: mc2, obs(50) s(6) b1(4) b2(2) sigma(3) rho(0.2)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n          b2 |        100    1.975448    .2406981   1.466939   2.656462\n          t2 |        100    8.010763      1.3815   4.681076   10.78957\n         se2 |        100    .2500731    .0286469   .2042868   .3313515\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n       bias2 |        100   -.0245523    .2406981  -.5330607   .6564617\n(bin=10, start=-.53306067, width=.11895224)\n\n\n\n\n\n\n\n\n\nTo demonstrate consistency we need to repeat the exercise for larger and larger \\(n\\)’s.\n\ntempfile simdata\nforvalues n = 100(100)1000 {\n    simulate b1=r(b1) b2=r(b2) se2=r(se2) t2=r(t2), reps(100): mc2, obs(`n') s(6) b1(4) b2(2) sigma(3) rho(0.2)\n    gen size = `n'\n    if `n'==100 save `simdata', replace\n    else {\n        append using `simdata'\n        save `simdata', replace\n    }\n}\ngen bias2=b2-2\nhistogram bias2, normal xline(0) by(size)\n\n\n      Command: mc2, obs(100) s(6) b1(4) b2(2) sigma(3) rho(0.2)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\n(file C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_000009.tmp not found)\nfile C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_000009.tmp saved as .dta\n    format\n\n      Command: mc2, obs(200) s(6) b1(4) b2(2) sigma(3) rho(0.2)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\nfile C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_000009.tmp saved as .dta\n    format\n\n      Command: mc2, obs(300) s(6) b1(4) b2(2) sigma(3) rho(0.2)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\nfile C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_000009.tmp saved as .dta\n    format\n\n      Command: mc2, obs(400) s(6) b1(4) b2(2) sigma(3) rho(0.2)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\nfile C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_000009.tmp saved as .dta\n    format\n\n      Command: mc2, obs(500) s(6) b1(4) b2(2) sigma(3) rho(0.2)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\nfile C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_000009.tmp saved as .dta\n    format\n\n      Command: mc2, obs(600) s(6) b1(4) b2(2) sigma(3) rho(0.2)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\nfile C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_000009.tmp saved as .dta\n    format\n\n      Command: mc2, obs(700) s(6) b1(4) b2(2) sigma(3) rho(0.2)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\nfile C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_000009.tmp saved as .dta\n    format\n\n      Command: mc2, obs(800) s(6) b1(4) b2(2) sigma(3) rho(0.2)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\nfile C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_000009.tmp saved as .dta\n    format\n\n      Command: mc2, obs(900) s(6) b1(4) b2(2) sigma(3) rho(0.2)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\nfile C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_000009.tmp saved as .dta\n    format\n\n      Command: mc2, obs(1000) s(6) b1(4) b2(2) sigma(3) rho(0.2)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\nfile C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_000009.tmp saved as .dta\n    format"
  },
  {
    "objectID": "problem-set-3-solutions.html#model-3-dynamic-model-without-serial-correlation",
    "href": "problem-set-3-solutions.html#model-3-dynamic-model-without-serial-correlation",
    "title": "Problem Set 3 (SOLUTIONS)",
    "section": "Model 3: Dynamic model without serial correlation",
    "text": "Model 3: Dynamic model without serial correlation\nConsider a version of Model 1, where the regressor is the lag of the dependent variable. \\[\nY_t = \\beta_1 + \\beta_2Y_{t-1} + \\upsilon_t \\qquad \\text{with}\\quad \\upsilon_t\\sim N(0,\\sigma^2)\n\\] The OLS estimator is now, \\[\n  \\hat{\\beta}_2 = \\beta_2 + \\frac{\\sum_t \\tilde{Y}_{t-1}\\tilde{\\upsilon}_t}{\\sum_t \\tilde{Y}_{t-1}^2}\n\\] This model is biased, since\n\\[\nE\\bigg[\\frac{\\sum_t \\tilde{Y}_{t-1}\\tilde{\\upsilon}_t}{\\sum_t \\tilde{Y}_{t-1}^2}\\bigg] \\neq \\frac{E\\big[\\sum_t \\tilde{Y}_{t-1}\\tilde{\\upsilon}_t\\big]}{E\\big[\\sum_t \\tilde{Y}_{t-1}^2\\big]}\n\\] When the regressor was \\(X_t\\), the above statement was true given the Law of Iterated Expectations. However, you can use Slutsky’s theorem and the WLLN to show that \\(\\hat{\\beta}_2\\rightarrow_p \\beta_2\\). This result relies on the fact that \\(Y_{t-1}\\) is realized before \\(\\upsilon_t\\) which is iid. Thus, the bias goes to 0 as \\(n\\rightarrow \\infty\\).\n\nSimulation\n\ncap prog drop mc3\nprogram define mc3, rclass\n    syntax [, obs(integer 1)  b1(real 0)  b2(real 0)  sigma(real 1)]\n    drop _all\n    set obs `obs'\n    gen y=0\n    gen u = rnormal(0,`sigma') \n    gen time=_n\n    tsset time\n    forvalues i=2/`obs'  {\n    replace y=`b1'+ `b2'* y[_n-1] + u  if _n==`i'\n    }\n    reg y  L.y\n    return scalar b1=_b[_cons]\n    return scalar b2=_b[L.y]\n    return scalar se2 = _se[L.y]\n  return scalar t2 = _b[L.y]/_se[L.y]\nend\n\n\nsimulate b1=r(b1) b2=r(b2) se2=r(se2) t2=r(t2), reps(100): mc3, obs(50) b1(4)  b2(0.2) sigma(3)  \ngen bias2=b2-0.2\nsum b2 t2 se2 \nsum bias2\nhistogram bias2, normal xline(`r(mean)')\n\n\n\n      Command: mc3, obs(50) b1(4) b2(0.2) sigma(3)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n          b2 |        100     .167145    .1275283  -.1321701   .4612297\n          t2 |        100    1.213486    .9468185  -.9446563   3.532131\n         se2 |        100    .1400517    .0042463   .1278178   .1539907\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n       bias2 |        100    -.032855    .1275283  -.3321701   .2612297\n(bin=10, start=-.33217007, width=.05933998)\n\n\n\n\n\n\n\n\n\nTo demonstrate consistency we need to repeat the exercise for larger and larger \\(n\\)’s.\n\ntempfile simdata\nforvalues n = 100(100)1000 {\n    simulate b1=r(b1) b2=r(b2) se2=r(se2) t2=r(t2), reps(100): mc3, obs(`n') b1(4) b2(0.2) sigma(3) \n    gen size = `n'\n    if `n'==100 save `simdata', replace\n    else {\n        append using `simdata'\n        save `simdata', replace\n    }\n}\ngen bias2=b2-0.2\nhistogram bias2, normal xline(0) by(size)\n\n\n      Command: mc3, obs(100) b1(4) b2(0.2) sigma(3)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\n(file C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_00000d.tmp not found)\nfile C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_00000d.tmp saved as .dta\n    format\n\n      Command: mc3, obs(200) b1(4) b2(0.2) sigma(3)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\nfile C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_00000d.tmp saved as .dta\n    format\n\n      Command: mc3, obs(300) b1(4) b2(0.2) sigma(3)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\nfile C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_00000d.tmp saved as .dta\n    format\n\n      Command: mc3, obs(400) b1(4) b2(0.2) sigma(3)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\nfile C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_00000d.tmp saved as .dta\n    format\n\n      Command: mc3, obs(500) b1(4) b2(0.2) sigma(3)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\nfile C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_00000d.tmp saved as .dta\n    format\n\n      Command: mc3, obs(600) b1(4) b2(0.2) sigma(3)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\nfile C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_00000d.tmp saved as .dta\n    format\n\n      Command: mc3, obs(700) b1(4) b2(0.2) sigma(3)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\nfile C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_00000d.tmp saved as .dta\n    format\n\n      Command: mc3, obs(800) b1(4) b2(0.2) sigma(3)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\nfile C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_00000d.tmp saved as .dta\n    format\n\n      Command: mc3, obs(900) b1(4) b2(0.2) sigma(3)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\nfile C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_00000d.tmp saved as .dta\n    format\n\n      Command: mc3, obs(1000) b1(4) b2(0.2) sigma(3)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\nfile C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_00000d.tmp saved as .dta\n    format"
  },
  {
    "objectID": "problem-set-3-solutions.html#model-4-dynamic-model-with-serial-correlation",
    "href": "problem-set-3-solutions.html#model-4-dynamic-model-with-serial-correlation",
    "title": "Problem Set 3 (SOLUTIONS)",
    "section": "Model 4: Dynamic model with serial correlation",
    "text": "Model 4: Dynamic model with serial correlation\nConsider a version of Model 2, where the regressor is the lag of the dependent variable. \\[\nY_t = \\beta_1 + \\beta_2Y_{t-1} + \\upsilon_t \\text{where}\\quad \\upsilon_t = \\rho \\upsilon_{t-1}+\\varepsilon_t \\quad\\text{and}\\quad \\varepsilon_t\\sim N(0,\\sigma^2)\n\\] As with model 3, the OLS estimator will be biased. In addition, since \\(Cov(\\upsilon_t,\\upsilon_{t-1})\\neq0\\) and \\(Cov(Y_t,\\upsilon_{t})\\neq 0\\) (for any \\(t\\)), \\[\n\\Rightarrow Cov(Y_{t-1},\\upsilon_{t})\\neq 0\n\\] As a result \\(\\hat{\\beta}_2\\) is inconsistent.\n\nSimulation\n\ncap prog drop mc4\nprogram define mc4, rclass\n    syntax [, obs(integer 1)  b1(real 0)  b2(real 0)  sigma(real 1) rho(real 0) ]\n    drop _all\n    set obs `obs'\n    gen y=0\n    gen u=0\n    gen e = rnormal(0,`sigma') \n    gen time=_n\n    tsset time\n    \n    forvalues i=2/`obs'  {\n    replace u=`rho'*u[_n-1] + e if _n==`i'\n    replace y=`b1'+ `b2'* y[_n-1] + u  if _n==`i'\n    }\n    reg y  L.y\n    return scalar b1=_b[_cons]\n    return scalar b2=_b[L.y]\n    return scalar se2 = _se[L.y]\n  return scalar t2 = _b[L.y]/_se[L.y]\nend\n\nsimulate b1=r(b1) b2=r(b2) se2=r(se2) t2=r(t2), reps(100): mc4, obs(50) b1(4) b2(0.2) sigma(3) rho(0.2)\ngen bias2=b2-0.2\nsum b2 t2 se2\nsum bias2\nhistogram bias2, normal xline(`r(mean)')\n\n\n\n      Command: mc4, obs(50) b1(4) b2(0.2) sigma(3) rho(0.2)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n          b2 |        100    .3424452    .1554043  -.1707231   .6607195\n          t2 |        100    2.666883    1.363437  -1.216084   6.210446\n         se2 |        100    .1323436    .0083629   .1063884   .1438691\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n       bias2 |        100    .1424452    .1554043  -.3707231   .4607195\n(bin=10, start=-.37072313, width=.08314427)\n\n\n\n\n\n\n\n\n\nTo demonstrate consistency we need to repeat the exercise for larger and larger \\(n\\)’s.\n\ntempfile simdata\nforvalues n = 100(100)1000 {\n    simulate b1=r(b1) b2=r(b2) se2=r(se2) t2=r(t2), reps(100): mc4, obs(`n') b1(4) b2(0.2) sigma(3) rho(0.2)\n    gen size = `n'\n    if `n'==100 save `simdata', replace\n    else {\n        append using `simdata'\n        save `simdata', replace\n    }\n}\ngen bias2=b2-0.2\nhistogram bias2, normal xline(0) by(size)\n\n\n      Command: mc4, obs(100) b1(4) b2(0.2) sigma(3) rho(0.2)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\n(file C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_00000h.tmp not found)\nfile C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_00000h.tmp saved as .dta\n    format\n\n      Command: mc4, obs(200) b1(4) b2(0.2) sigma(3) rho(0.2)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\nfile C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_00000h.tmp saved as .dta\n    format\n\n      Command: mc4, obs(300) b1(4) b2(0.2) sigma(3) rho(0.2)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\nfile C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_00000h.tmp saved as .dta\n    format\n\n      Command: mc4, obs(400) b1(4) b2(0.2) sigma(3) rho(0.2)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\nfile C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_00000h.tmp saved as .dta\n    format\n\n      Command: mc4, obs(500) b1(4) b2(0.2) sigma(3) rho(0.2)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\nfile C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_00000h.tmp saved as .dta\n    format\n\n      Command: mc4, obs(600) b1(4) b2(0.2) sigma(3) rho(0.2)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\nfile C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_00000h.tmp saved as .dta\n    format\n\n      Command: mc4, obs(700) b1(4) b2(0.2) sigma(3) rho(0.2)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\nfile C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_00000h.tmp saved as .dta\n    format\n\n      Command: mc4, obs(800) b1(4) b2(0.2) sigma(3) rho(0.2)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\nfile C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_00000h.tmp saved as .dta\n    format\n\n      Command: mc4, obs(900) b1(4) b2(0.2) sigma(3) rho(0.2)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\nfile C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_00000h.tmp saved as .dta\n    format\n\n      Command: mc4, obs(1000) b1(4) b2(0.2) sigma(3) rho(0.2)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\nfile C:\\Users\\neil_\\AppData\\Local\\Temp\\ST_1d48_00000h.tmp saved as .dta\n    format"
  },
  {
    "objectID": "problem-set-3-solutions.html#postamble",
    "href": "problem-set-3-solutions.html#postamble",
    "title": "Problem Set 3 (SOLUTIONS)",
    "section": "Postamble",
    "text": "Postamble\n\nlog close"
  },
  {
    "objectID": "problem-set-2-solutions.html",
    "href": "problem-set-2-solutions.html",
    "title": "Problem Set 2 (SOLUTIONS)",
    "section": "",
    "text": "This problem set will take you through some Stata commands to estimate simple regression equations with dummy variables. You will learn how to interpret the estimated coefficients and test some linear hypotheses. Interpretation of these coefficients will be useful when we do treatment evaluation models later in term 1.\nThe hypothesis tests discussed in this problem set include standard T-tests and F-tests, which is assumed undergraduate knowledge for this module.\nYou will need to download the dataset problemset2.dta, which is available on Moodle."
  },
  {
    "objectID": "problem-set-2-solutions.html#conditional-expectation-function",
    "href": "problem-set-2-solutions.html#conditional-expectation-function",
    "title": "Problem Set 2 (SOLUTIONS)",
    "section": "Conditional Expectation Function",
    "text": "Conditional Expectation Function\nConsider the Conditional Expectation Function (CEF), \\(E[Y_i|X_i]\\). If \\(X\\) takes on discrete values: \\(X_i\\in\\{x_1,x_2,...,x_m\\}\\), then\n\\[\n    E[Y_i|X_i] =  E[Y_i|X_i=x_1]\\cdot\\mathbf{1}\\{X_i = x_1\\}+...+E[Y_i|X_i=x_m]\\cdot\\mathbf{1}\\{X_i = x_m\\}\n\\] where \\(\\mathbf{1}\\{X_i = x_m\\}\\) is a dummy variable, \\(=1\\) when \\(X_i=x_m\\). Since the values of \\(X_i\\) are mutually exclusive there is no overlap of these dummy variables.\nNote, we do not need to assume that \\(X\\) is a single random variable. It can be a vector of random variables that takes on discrete values.\nWe can re-arrange this expression using anyone of the values of \\(X\\). The natural option is to choose the first, but this is arbitrary.\n\\[\n\\begin{aligned}\n    E[Y_i|X_i] =& E[Y_i|X_i=x_1]+ (E[Y_i|X_i=x_2]-E[Y_i|X_i=x_1])\\cdot\\mathbf{1}\\{X_i = x_2\\}+... \\\\\n&+(E[Y_i|X_i=x_m]-E[Y_i|X_i=x_1])\\cdot\\mathbf{1}\\{X_i = x_m\\}\n\\end{aligned}\n\\]\nSince, \\(E[Y_i|X_i = x_m]\\) is a constant (\\(X_i\\) is set to a specific value), we can express the CEF as a function that is linear in parameters.\n\\[\n    E[Y_i|X_i] = \\beta_1 + \\beta_2D_{i2} + ... + \\beta_m D_{im}\n\\]\nwhere \\(D_{im}=\\mathbf{1}\\{X_i = x_m\\}\\)."
  },
  {
    "objectID": "problem-set-2-solutions.html#preamble",
    "href": "problem-set-2-solutions.html#preamble",
    "title": "Problem Set 2 (SOLUTIONS)",
    "section": "Preamble",
    "text": "Preamble\n\n\n\n\n\nCreate a do-file for this problem set and include a preamble that sets the directory and opens the data. For example,\n\nclear \n//or, to remove all stored values (including macros, matrices, scalars, etc.) \n*clear all\n\n* Replace $rootdir with the relevant path to on your local harddrive.\ncd \"$rootdir/problem-sets/ps-2\"\n\ncap log close\nlog using problem-set-2-log.txt, replace\n\nuse problem-set-2-data.dta"
  },
  {
    "objectID": "problem-set-2-solutions.html#questions",
    "href": "problem-set-2-solutions.html#questions",
    "title": "Problem Set 2 (SOLUTIONS)",
    "section": "Questions",
    "text": "Questions\n1. Consider the \\(E[ln(Wage_i)|Gender_i]\\), where \\(Gender_i\\in\\{1 ``Male'', 2 ``Female''\\}\\). Show that this CEF implies a linear model,\n\\[\n    ln(Wage_i) = \\beta_1 + \\beta_2 D_{i2} + \\varepsilon_i\n\\]\nWhat do the parameters \\(\\beta_1\\) and \\(\\beta_2\\) imply?\n\\[\nE[ln(Wage_i)|G_i] = E[ln(Wage_i)|G_i=1]+ (E[ln(Wage_i)|G_i=2]-E[ln(Wage_i)|G_i=1])\\cdot\\mathbf{1}\\{G_i = 2\\}\n\\] \\[\n= \\beta_1+ \\beta_2D_{i2}\n\\] \\(\\Rightarrow E[\\varepsilon_i|D_{i2}]=0\\).\n2. Regress lwage (log wage) on just a set of binary indicators that will enable you to test the hypothesis that males and females are on average, paid the same wage, ceteris paribus. Test this hypothesis.\n\nreg lwage female\n\n* or\n\ngen male=1-female\nreg lwage male\n\n\n      Source |       SS           df       MS      Number of obs   =     4,165\n-------------+----------------------------------   F(1, 4163)      =    491.72\n       Model |  93.6914807         1  93.6914807   Prob &gt; F        =    0.0000\n    Residual |  793.213421     4,163  .190538895   R-squared       =    0.1056\n-------------+----------------------------------   Adj R-squared   =    0.1054\n       Total |  886.904902     4,164  .212993492   Root MSE        =    .43651\n\n------------------------------------------------------------------------------\n       lwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      female |  -.4744661   .0213967   -22.17   0.000     -.516415   -.4325171\n       _cons |   6.729774     .00718   937.29   0.000     6.715697     6.74385\n------------------------------------------------------------------------------\n\n      Source |       SS           df       MS      Number of obs   =     4,165\n-------------+----------------------------------   F(1, 4163)      =    491.72\n       Model |  93.6914807         1  93.6914807   Prob &gt; F        =    0.0000\n    Residual |  793.213421     4,163  .190538895   R-squared       =    0.1056\n-------------+----------------------------------   Adj R-squared   =    0.1054\n       Total |  886.904902     4,164  .212993492   Root MSE        =    .43651\n\n------------------------------------------------------------------------------\n       lwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        male |   .4744661   .0213967    22.17   0.000     .4325171     .516415\n       _cons |   6.255308    .020156   310.34   0.000     6.215791    6.294824\n------------------------------------------------------------------------------\n\n\nAlternatively, you could use Stata’s factor notation:\n\nreg lwage i.female\n\n//note: defaults to smallest value as base category. This can be changed as follows.\n\nreg lwage ib1.female\n\n\n      Source |       SS           df       MS      Number of obs   =     4,165\n-------------+----------------------------------   F(1, 4163)      =    491.72\n       Model |  93.6914807         1  93.6914807   Prob &gt; F        =    0.0000\n    Residual |  793.213421     4,163  .190538895   R-squared       =    0.1056\n-------------+----------------------------------   Adj R-squared   =    0.1054\n       Total |  886.904902     4,164  .212993492   Root MSE        =    .43651\n\n------------------------------------------------------------------------------\n       lwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n    1.female |  -.4744661   .0213967   -22.17   0.000     -.516415   -.4325171\n       _cons |   6.729774     .00718   937.29   0.000     6.715697     6.74385\n------------------------------------------------------------------------------\n\n      Source |       SS           df       MS      Number of obs   =     4,165\n-------------+----------------------------------   F(1, 4163)      =    491.72\n       Model |  93.6914807         1  93.6914807   Prob &gt; F        =    0.0000\n    Residual |  793.213421     4,163  .190538895   R-squared       =    0.1056\n-------------+----------------------------------   Adj R-squared   =    0.1054\n       Total |  886.904902     4,164  .212993492   Root MSE        =    .43651\n\n------------------------------------------------------------------------------\n       lwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n    0.female |   .4744661   .0213967    22.17   0.000     .4325171     .516415\n       _cons |   6.255308    .020156   310.34   0.000     6.215791    6.294824\n------------------------------------------------------------------------------\n\n\nIt is evident from the test p-value that the difference is statistically significantly. Note, the standard reg command assume homoskedastic SEs. If we believe that the variance varies of (log of) wages varies with gender, we should estimate heteroskedastic SEs. However, in this instance it will not make a difference to the conclusion.\n\nreg lwage female, r\n\n\nLinear regression                               Number of obs     =      4,165\n                                                F(1, 4163)        =     520.64\n                                                Prob &gt; F          =     0.0000\n                                                R-squared         =     0.1056\n                                                Root MSE          =     .43651\n\n------------------------------------------------------------------------------\n             |               Robust\n       lwage | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      female |  -.4744661   .0207939   -22.82   0.000    -.5152332   -.4336989\n       _cons |   6.729774   .0072089   933.53   0.000      6.71564    6.743907\n------------------------------------------------------------------------------\n\n\n3. Extend the specification in (2) that will enable you to test the hypothesis that there is no difference in the wages between the following gender-ethnicity groups. Begin by defining the following dummy variables:\n\nfemale_black = female\\(\\times\\)black\nmale_black = (1-female)\\(\\times\\)black\nfemale_nonblack = female\\(\\times\\)(1-black)\nmale_nonblack = (1-female)\\(\\times\\)(1-black)\n\n\ngen female_black=female*black\ngen female_nonblack=female*(1-black)\ngen male_black=(1-female)*black\ngen male_nonblack=(1-female)*(1-black)\n\nThen estimate the following regressions:\n\nlwage on female_black, female_nonblack, male_black, male_nonblack (without a constant: option nocons)\nlwage on female, black, female_black\nlwage on female_black, female_nonblack, male_black\n\nFor some of these exercises you may be able to use Stata’s factor notation. However, in some instances you will need to manually create the above dummy-variable interactions.\nIn each case, identify the base category and write down the parameters of the (implied) model in terms of conditional expectations.\n\n* (a)\nreg lwage female_black female_nonblack male_black male_nonblack\n\n// note, Stata has dropped one variable due to perfect collinearity\n\nreg lwage female_black female_nonblack male_black male_nonblack, nocons\n\nnote: male_black omitted because of collinearity.\n\n      Source |       SS           df       MS      Number of obs   =     4,165\n-------------+----------------------------------   F(3, 4161)      =    191.17\n       Model |  107.436063         3  35.8120209   Prob &gt; F        =    0.0000\n    Residual |  779.468839     4,161  .187327287   R-squared       =    0.1211\n-------------+----------------------------------   Adj R-squared   =    0.1205\n       Total |  886.904902     4,164  .212993492   Root MSE        =    .43281\n\n------------------------------------------------------------------------------\n       lwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nfemale_black |  -.4434409   .0523433    -8.47   0.000    -.5460617     -.34082\nfemale_non~k |  -.2101553   .0383456    -5.48   0.000    -.2853332   -.1349774\n  male_black |          0  (omitted)\nmale_nonbl~k |   .2239593   .0317691     7.05   0.000     .1616749    .2862436\n       _cons |   6.517691   .0309152   210.82   0.000     6.457081    6.578301\n------------------------------------------------------------------------------\n\n      Source |       SS           df       MS      Number of obs   =     4,165\n-------------+----------------------------------   F(4, 4161)      &gt;  99999.00\n       Model |  185756.485         4  46439.1213   Prob &gt; F        =    0.0000\n    Residual |  779.468839     4,161  .187327287   R-squared       =    0.9958\n-------------+----------------------------------   Adj R-squared   =    0.9958\n       Total |  186535.954     4,165  44.7865436   Root MSE        =    .43281\n\n------------------------------------------------------------------------------\n       lwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nfemale_black |    6.07425   .0422382   143.81   0.000     5.991441     6.15706\nfemale_non~k |   6.307536   .0226856   278.04   0.000      6.26306    6.352012\n  male_black |   6.517691   .0309152   210.82   0.000     6.457081    6.578301\nmale_nonbl~k |    6.74165   .0073159   921.51   0.000     6.727307    6.755993\n------------------------------------------------------------------------------\n\n\nThis model returns four parameter-estimates, each corresponding to the four gender-ethnicity groups. These are essentially conditional mean estimates.\n\n* (b)\nreg lwage female black female_black\n\n// or, using factor notation:\n\nreg lwage i.female##i.black\n\n\n      Source |       SS           df       MS      Number of obs   =     4,165\n-------------+----------------------------------   F(3, 4161)      =    191.17\n       Model |  107.436063         3  35.8120209   Prob &gt; F        =    0.0000\n    Residual |  779.468839     4,161  .187327287   R-squared       =    0.1211\n-------------+----------------------------------   Adj R-squared   =    0.1205\n       Total |  886.904902     4,164  .212993492   Root MSE        =    .43281\n\n------------------------------------------------------------------------------\n       lwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      female |  -.4341146   .0238361   -18.21   0.000     -.480846   -.3873832\n       black |  -.2239593   .0317691    -7.05   0.000    -.2862436   -.1616749\nfemale_black |  -.0093263    .057515    -0.16   0.871    -.1220865    .1034339\n       _cons |    6.74165   .0073159   921.51   0.000     6.727307    6.755993\n------------------------------------------------------------------------------\n\n      Source |       SS           df       MS      Number of obs   =     4,165\n-------------+----------------------------------   F(3, 4161)      =    191.17\n       Model |  107.436063         3  35.8120209   Prob &gt; F        =    0.0000\n    Residual |  779.468839     4,161  .187327287   R-squared       =    0.1211\n-------------+----------------------------------   Adj R-squared   =    0.1205\n       Total |  886.904902     4,164  .212993492   Root MSE        =    .43281\n\n------------------------------------------------------------------------------\n       lwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n    1.female |  -.4341146   .0238361   -18.21   0.000     -.480846   -.3873832\n     1.black |  -.2239593   .0317691    -7.05   0.000    -.2862436   -.1616749\n             |\nfemale#black |\n        1 1  |  -.0093263    .057515    -0.16   0.871    -.1220865    .1034339\n             |\n       _cons |    6.74165   .0073159   921.51   0.000     6.727307    6.755993\n------------------------------------------------------------------------------\n\n\nIn this model, we have the following:\n\n\\(\\beta_1 = E[ln(Wage_i)|F = 0,B = 0]\\)\n\\(\\beta_2 = E[ln(Wage_i)|F = 1,B = 0]-E[ln(Wage_i)|F = 0,B = 0]\\)\n\\(\\beta_3 = E[ln(Wage_i)|F = 0,B = 1]-E[ln(Wage_i)|F = 0,B = 0]\\)\n\\(\\beta_4 = (E[ln(Wage_i)|F = 1,B = 1]-E[ln(Wage_i)|F = 0,B = 1])-(E[ln(Wage_i)|F = 1,B = 0]-E[ln(Wage_i)|F = 0,B = 0])\\)\n\n\n* (c) \nreg lwage female_black female_nonblack male_black  \n\n// note, this one is harder to replicate using factor notation. \n\n\n      Source |       SS           df       MS      Number of obs   =     4,165\n-------------+----------------------------------   F(3, 4161)      =    191.17\n       Model |  107.436063         3  35.8120209   Prob &gt; F        =    0.0000\n    Residual |  779.468839     4,161  .187327287   R-squared       =    0.1211\n-------------+----------------------------------   Adj R-squared   =    0.1205\n       Total |  886.904902     4,164  .212993492   Root MSE        =    .43281\n\n------------------------------------------------------------------------------\n       lwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nfemale_black |  -.6674001   .0428671   -15.57   0.000    -.7514426   -.5833576\nfemale_non~k |  -.4341146   .0238361   -18.21   0.000     -.480846   -.3873832\n  male_black |  -.2239593   .0317691    -7.05   0.000    -.2862436   -.1616749\n       _cons |    6.74165   .0073159   921.51   0.000     6.727307    6.755993\n------------------------------------------------------------------------------\n\n\nIn this model, we have the following:\n\n\\(\\beta_1 = E[ln(Wage_i)|F = 0,B = 0]\\)\n\\(\\beta_2 = E[ln(Wage_i)|F = 1,B = 1]-E[ln(Wage_i)|F = 0,B = 0]\\)\n\\(\\beta_3 = E[ln(Wage_i)|F = 1,B = 0]-E[ln(Wage_i)|F = 0,B = 0]\\)\n\\(\\beta_4 = E[ln(Wage_i)|F = 0,B = 1]-E[ln(Wage_i)|F = 0,B = 0]\\)\n\n4. Compare the estimated coefficients with the sample average values for the lwage for the four subgroups. What do you see?\n\ntable female black, stat(mean lwage)\n\n\n------------------------------------------------\n               |               black            \n               |         0          1      Total\n---------------+--------------------------------\nfemale or male |                                \n  0            |   6.74165   6.517691   6.729774\n  1            |  6.307536    6.07425   6.255308\n  Total        |  6.700755   6.363002   6.676346\n------------------------------------------------\n\n\nYou can compute the coefficients from each model simply using these averages.\n5. In each of the above models, describe the null hypothesis you would test to evaluate whether there is a significant earnings difference between the earnings of black and non-black females.\n\n\\(H_0: \\beta_1 = \\beta_2\\)\n\\(H_0: \\beta_3 + \\beta_4 = 0\\)\n\\(H_0: \\beta_2 - \\beta_3 = 0\\)\n\n6. Verify your solution to 4 by performing a test using the three set of regression output. You can use the post-estimation test command.\n\nreg lwage female_black female_nonblack male_black male_nonblack, nocons\n\ntest female_black = female_nonblack\n\nreg lwage female black female_black\n\ntest female_black + black = 0\n\nreg lwage female_black female_nonblack male_black  \n\ntest female_black - female_nonblack = 0\n\n\n      Source |       SS           df       MS      Number of obs   =     4,165\n-------------+----------------------------------   F(4, 4161)      &gt;  99999.00\n       Model |  185756.485         4  46439.1213   Prob &gt; F        =    0.0000\n    Residual |  779.468839     4,161  .187327287   R-squared       =    0.9958\n-------------+----------------------------------   Adj R-squared   =    0.9958\n       Total |  186535.954     4,165  44.7865436   Root MSE        =    .43281\n\n------------------------------------------------------------------------------\n       lwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nfemale_black |    6.07425   .0422382   143.81   0.000     5.991441     6.15706\nfemale_non~k |   6.307536   .0226856   278.04   0.000      6.26306    6.352012\n  male_black |   6.517691   .0309152   210.82   0.000     6.457081    6.578301\nmale_nonbl~k |    6.74165   .0073159   921.51   0.000     6.727307    6.755993\n------------------------------------------------------------------------------\n\n ( 1)  female_black - female_nonblack = 0\n\n       F(  1,  4161) =   23.68\n            Prob &gt; F =    0.0000\n\n      Source |       SS           df       MS      Number of obs   =     4,165\n-------------+----------------------------------   F(3, 4161)      =    191.17\n       Model |  107.436063         3  35.8120209   Prob &gt; F        =    0.0000\n    Residual |  779.468839     4,161  .187327287   R-squared       =    0.1211\n-------------+----------------------------------   Adj R-squared   =    0.1205\n       Total |  886.904902     4,164  .212993492   Root MSE        =    .43281\n\n------------------------------------------------------------------------------\n       lwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      female |  -.4341146   .0238361   -18.21   0.000     -.480846   -.3873832\n       black |  -.2239593   .0317691    -7.05   0.000    -.2862436   -.1616749\nfemale_black |  -.0093263    .057515    -0.16   0.871    -.1220865    .1034339\n       _cons |    6.74165   .0073159   921.51   0.000     6.727307    6.755993\n------------------------------------------------------------------------------\n\n ( 1)  black + female_black = 0\n\n       F(  1,  4161) =   23.68\n            Prob &gt; F =    0.0000\n\n      Source |       SS           df       MS      Number of obs   =     4,165\n-------------+----------------------------------   F(3, 4161)      =    191.17\n       Model |  107.436063         3  35.8120209   Prob &gt; F        =    0.0000\n    Residual |  779.468839     4,161  .187327287   R-squared       =    0.1211\n-------------+----------------------------------   Adj R-squared   =    0.1205\n       Total |  886.904902     4,164  .212993492   Root MSE        =    .43281\n\n------------------------------------------------------------------------------\n       lwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nfemale_black |  -.6674001   .0428671   -15.57   0.000    -.7514426   -.5833576\nfemale_non~k |  -.4341146   .0238361   -18.21   0.000     -.480846   -.3873832\n  male_black |  -.2239593   .0317691    -7.05   0.000    -.2862436   -.1616749\n       _cons |    6.74165   .0073159   921.51   0.000     6.727307    6.755993\n------------------------------------------------------------------------------\n\n ( 1)  female_black - female_nonblack = 0\n\n       F(  1,  4161) =   23.68\n            Prob &gt; F =    0.0000\n\n\n7. In each case, test equality across all four gender-ethnicity groups. Again, you should get the same result.\n\nreg lwage female_black female_nonblack male_black male_nonblack, nocons\n\ntest female_black = female_nonblack = male_black = male_nonblack\n\nreg lwage female black female_black\n\ntest female_black = black = female = 0\n\nreg lwage female_black female_nonblack male_black  \n\ntest female_black = female_nonblack = male_black = 0\n\n\n      Source |       SS           df       MS      Number of obs   =     4,165\n-------------+----------------------------------   F(4, 4161)      &gt;  99999.00\n       Model |  185756.485         4  46439.1213   Prob &gt; F        =    0.0000\n    Residual |  779.468839     4,161  .187327287   R-squared       =    0.9958\n-------------+----------------------------------   Adj R-squared   =    0.9958\n       Total |  186535.954     4,165  44.7865436   Root MSE        =    .43281\n\n------------------------------------------------------------------------------\n       lwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nfemale_black |    6.07425   .0422382   143.81   0.000     5.991441     6.15706\nfemale_non~k |   6.307536   .0226856   278.04   0.000      6.26306    6.352012\n  male_black |   6.517691   .0309152   210.82   0.000     6.457081    6.578301\nmale_nonbl~k |    6.74165   .0073159   921.51   0.000     6.727307    6.755993\n------------------------------------------------------------------------------\n\n ( 1)  female_black - female_nonblack = 0\n ( 2)  female_black - male_black = 0\n ( 3)  female_black - male_nonblack = 0\n\n       F(  3,  4161) =  191.17\n            Prob &gt; F =    0.0000\n\n      Source |       SS           df       MS      Number of obs   =     4,165\n-------------+----------------------------------   F(3, 4161)      =    191.17\n       Model |  107.436063         3  35.8120209   Prob &gt; F        =    0.0000\n    Residual |  779.468839     4,161  .187327287   R-squared       =    0.1211\n-------------+----------------------------------   Adj R-squared   =    0.1205\n       Total |  886.904902     4,164  .212993492   Root MSE        =    .43281\n\n------------------------------------------------------------------------------\n       lwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      female |  -.4341146   .0238361   -18.21   0.000     -.480846   -.3873832\n       black |  -.2239593   .0317691    -7.05   0.000    -.2862436   -.1616749\nfemale_black |  -.0093263    .057515    -0.16   0.871    -.1220865    .1034339\n       _cons |    6.74165   .0073159   921.51   0.000     6.727307    6.755993\n------------------------------------------------------------------------------\n\n ( 1)  - black + female_black = 0\n ( 2)  - female + female_black = 0\n ( 3)  female_black = 0\n\n       F(  3,  4161) =  191.17\n            Prob &gt; F =    0.0000\n\n      Source |       SS           df       MS      Number of obs   =     4,165\n-------------+----------------------------------   F(3, 4161)      =    191.17\n       Model |  107.436063         3  35.8120209   Prob &gt; F        =    0.0000\n    Residual |  779.468839     4,161  .187327287   R-squared       =    0.1211\n-------------+----------------------------------   Adj R-squared   =    0.1205\n       Total |  886.904902     4,164  .212993492   Root MSE        =    .43281\n\n------------------------------------------------------------------------------\n       lwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nfemale_black |  -.6674001   .0428671   -15.57   0.000    -.7514426   -.5833576\nfemale_non~k |  -.4341146   .0238361   -18.21   0.000     -.480846   -.3873832\n  male_black |  -.2239593   .0317691    -7.05   0.000    -.2862436   -.1616749\n       _cons |    6.74165   .0073159   921.51   0.000     6.727307    6.755993\n------------------------------------------------------------------------------\n\n ( 1)  female_black - female_nonblack = 0\n ( 2)  female_black - male_black = 0\n ( 3)  female_black = 0\n\n       F(  3,  4161) =  191.17\n            Prob &gt; F =    0.0000\n\n\n8. Try to replicate the F-statistic for one of the above models. Hint, the F-stat for these models is the same as that of the whole model.\n\nreg lwage female black female_black\nereturn list\nscalar fstat = (e(r2)*e(df_r))/((1-e(r2))*e(df_m))\nscalar list fstat\n\n\n      Source |       SS           df       MS      Number of obs   =     4,165\n-------------+----------------------------------   F(3, 4161)      =    191.17\n       Model |  107.436063         3  35.8120209   Prob &gt; F        =    0.0000\n    Residual |  779.468839     4,161  .187327287   R-squared       =    0.1211\n-------------+----------------------------------   Adj R-squared   =    0.1205\n       Total |  886.904902     4,164  .212993492   Root MSE        =    .43281\n\n------------------------------------------------------------------------------\n       lwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      female |  -.4341146   .0238361   -18.21   0.000     -.480846   -.3873832\n       black |  -.2239593   .0317691    -7.05   0.000    -.2862436   -.1616749\nfemale_black |  -.0093263    .057515    -0.16   0.871    -.1220865    .1034339\n       _cons |    6.74165   .0073159   921.51   0.000     6.727307    6.755993\n------------------------------------------------------------------------------\n\nscalars:\n                  e(N) =  4165\n               e(df_m) =  3\n               e(df_r) =  4161\n                  e(F) =  191.1735422330181\n                 e(r2) =  .1211359442526956\n               e(rmse) =  .4328132235793649\n                e(mss) =  107.4360627542734\n                e(rss) =  779.4688391479763\n               e(r2_a) =  .1205023003768864\n                 e(ll) =  -2419.902951629166\n               e(ll_0) =  -2688.805870567022\n               e(rank) =  4\n\nmacros:\n            e(cmdline) : \"regress lwage female black female_black\"\n              e(title) : \"Linear regression\"\n          e(marginsok) : \"XB default\"\n                e(vce) : \"ols\"\n             e(depvar) : \"lwage\"\n                e(cmd) : \"regress\"\n         e(properties) : \"b V\"\n            e(predict) : \"regres_p\"\n              e(model) : \"ols\"\n          e(estat_cmd) : \"regress_estat\"\n\nmatrices:\n                  e(b) :  1 x 4\n                  e(V) :  4 x 4\n               e(beta) :  1 x 3\n\nfunctions:\n             e(sample)   \n     fstat =  191.17354\n\n\nIn the case where the F-test corresponds to the test of the entire model, you can write the F-statistic in terms of \\(R^2\\).\n9. Estimate the following model:\n\\[\n    lwage = \\beta_1 + \\beta_2F + \\beta_3B + \\beta_4F\\times B + \\beta_5exp + \\beta_6exp^2 + \\beta_7educ + \\varepsilon\n\\]\n\nInterpret the estimated coeffiecients \\(\\hat{\\beta}_7\\).\nInterpret the effect of experience variable exp. Use the median level of experience to make your calculation.\n\n\nsum educ, det\nreg lwage i.female##i.black exper expsq educ\n\ndi (exp(_b[educ])-1)*100\n\n\n                     years of education\n-------------------------------------------------------------\n      Percentiles      Smallest\n 1%            6              4\n 5%            8              4\n10%            9              4       Obs               4,165\n25%           12              4       Sum of wgt.       4,165\n\n50%           12                      Mean           12.84538\n                        Largest       Std. dev.      2.787995\n75%           16             17\n90%           17             17       Variance       7.772916\n95%           17             17       Skewness      -.2581161\n99%           17             17       Kurtosis        2.71273\n\n      Source |       SS           df       MS      Number of obs   =     4,165\n-------------+----------------------------------   F(6, 4158)      =    411.81\n       Model |     330.586         6  55.0976667   Prob &gt; F        =    0.0000\n    Residual |  556.318902     4,158   .13379483   R-squared       =    0.3727\n-------------+----------------------------------   Adj R-squared   =    0.3718\n       Total |  886.904902     4,164  .212993492   Root MSE        =    .36578\n\n------------------------------------------------------------------------------\n       lwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n    1.female |  -.4032047   .0203208   -19.84   0.000    -.4430444    -.363365\n     1.black |  -.1551546   .0269249    -5.76   0.000    -.2079417   -.1023674\n             |\nfemale#black |\n        1 1  |   -.002071   .0488405    -0.04   0.966    -.0978246    .0936826\n             |\n       exper |   .0427346   .0022404    19.07   0.000     .0383422     .047127\n       expsq |  -.0006982   .0000494   -14.14   0.000    -.0007951   -.0006014\n        educ |   .0731837   .0020983    34.88   0.000     .0690698    .0772975\n       _cons |   5.303667   .0362462   146.32   0.000     5.232606    5.374729\n------------------------------------------------------------------------------\n7.5928119\n\n\n\nA one unit increase in years of educ is associated with an increase of 7.59% in expected wages, holding other regressors fixed.\n\n\nsum exper, det\nreturn list\ndi (exp(_b[exper]+2*r(p50)*_b[expsq])-1)*100\n\n\n             years of full-time work experience\n-------------------------------------------------------------\n      Percentiles      Smallest\n 1%            3              1\n 5%            5              1\n10%            7              1       Obs               4,165\n25%           11              1       Sum of wgt.       4,165\n\n50%           18                      Mean           19.85378\n                        Largest       Std. dev.      10.96637\n75%           29             50\n90%           36             50       Variance       120.2613\n95%           39             50       Skewness       .4000014\n99%           44             51       Kurtosis       2.072064\n\nscalars:\n                  r(N) =  4165\n              r(sum_w) =  4165\n               r(mean) =  19.85378151260504\n                r(Var) =  120.2612759224727\n                 r(sd) =  10.96637022548814\n           r(skewness) =  .4000013893781186\n           r(kurtosis) =  2.072064120792274\n                r(sum) =  82691\n                r(min) =  1\n                r(max) =  51\n                 r(p1) =  3\n                 r(p5) =  5\n                r(p10) =  7\n                r(p25) =  11\n                r(p50) =  18\n                r(p75) =  29\n                r(p90) =  36\n                r(p95) =  39\n                r(p99) =  44\n1.7754427\n\n\n\nA one unit incease in years of experience is associated with an increase of 1.78% in expected wages, holding other regressors fixed.\n\n10. Theoretically, how would you test the following restrictions for the model below?\n\n\\(\\beta_2 = \\beta_3\\)\n\\(\\beta_4 + \\beta_5 = 1\\)\n\\(\\beta_2 = \\beta_3\\) and \\(\\beta_4 + \\beta_5 = 1\\)\n\n\\[\n    Y = \\beta_1 + \\beta_2X_2 + \\beta_3X_3 + \\beta_4X_4 + \\beta_5X_5 + \\varepsilon\n\\]\n\nRestrictions 1: \\(H_0: \\beta_2 = \\beta_3\\Rightarrow \\beta_2-\\beta_3 = 0\\). This can be written as a simple T-test (or F-tests),\n\n\\[\n\\text{T-stat}  = \\frac{\\hat{\\beta}_2-\\hat{\\beta}_3}{\\sqrt{\\hat{Var}(\\hat{\\beta}_2-\\hat{\\beta}_3)}}\n\\]\nwhere,\n\\[\nVar(\\hat{\\beta}_2-\\hat{\\beta}_3) = Var(\\hat{\\beta}_2) + Var(\\hat{\\beta}_3)-2\\cdot Cov(\\hat{\\beta}_2,\\hat{\\beta}_3)\n\\]\nAlternatively, rewrite the model, adding and subtracting \\(\\beta_3X_2\\) (or \\(\\beta_2X_3\\)):\n\\[\n    Y = \\beta_1 + (\\beta_2-\\beta_3)X_2 + \\beta_3(X_2+X_3) + \\beta_4X_4 + \\beta_5X_5 + \\varepsilon\n\\]\nThen test the hypothese that the coefficient on \\(X_2\\) is \\(=0\\).\n\nRestrictions 1: \\(H_0: \\beta_4 + \\beta_5 = 1\\). This can be written as a simple T-test,\n\n\\[\n\\text{T-stat} = \\frac{\\hat{\\beta}_4+\\hat{\\beta}_5-1}{\\sqrt{\\hat{Var}(\\hat{\\beta}_4+\\hat{\\beta}_5})}\n\\]\nwhere,\n\\[\nVar(\\hat{\\beta}_4+\\hat{\\beta}_5) = Var(\\hat{\\beta}_4) + Var(\\hat{\\beta}_5)+2\\cdot Cov(\\hat{\\beta}_4,\\hat{\\beta}_5)\n\\]\nAlternatively, rewrite the model, adding and subtracting \\(\\beta_5X_4-X_5\\) (or \\(\\beta_2X_3\\)):\n\\[\n    Y-X_4 = \\beta_1 + \\beta_2X_2 + \\beta_3X_3 + (\\beta_4+\\beta_5-1)X_4 + (\\beta_5)(X_5-X_4) + \\varepsilon\n\\]\nThen test the hypothese that the coefficient on \\(X_4\\) is \\(=0\\).\n\nTo test both of the linear restrictions simultaneously, we would use an F-test.\n\nStep 1: estimate the unrestricted model\n\\[\n    Y = \\beta_1 + \\beta_2X_2 + \\beta_3X_3 + \\beta_4X_4 + \\beta_5X_5 + \\varepsilon\n\\]\nStore the \\(SSR_U\\)\nStep 2: estimate the restricted model\n\\[\n    (Y-X_4) = \\gamma_1 + \\gamma_2(X_2+X_3) + \\gamma_5(X_5-X_4) + \\varepsilon\n\\]\nStore the \\(SSR_R\\).\nStep 3: Compute the F-statistic\n\\[\n\\text{F-stat} = \\frac{(SSR_R-SSR_U)/(df_R-df_U)}{SSR_U/df_U}\n\\]"
  },
  {
    "objectID": "problem-set-2-solutions.html#postamble",
    "href": "problem-set-2-solutions.html#postamble",
    "title": "Problem Set 2 (SOLUTIONS)",
    "section": "Postamble",
    "text": "Postamble\n\nlog close"
  },
  {
    "objectID": "material-interpretation.html",
    "href": "material-interpretation.html",
    "title": "Interpretation of Linear Models",
    "section": "",
    "text": "In this short handout we will consider the interpretation of linear regression model coefficients in models with different combinations of outcome and regressor variables:\n\ncontinuous level-level\ncontinuous-discrete\ndiscrete-continuous\ndiscrete-discrete\nlog-level\nlevel-log\nlog-log\n\nIn all instances, we will work on the CLRM model assumptions 1 & 2, which tell us that the conditional expectation function is linear in parameters:\n\\[\nE[Y_i|X_i] = X_i'\\beta\n\\]\n\n\nIf \\(Y_i\\) and \\(X_i\\) are both continuously distributed random variables then,\n\\[\n\\beta_j = \\frac{\\partial E[Y_i|X_i]}{\\partial X_{ij}}\n\\] or, as a vector,\n\\[\n\\beta = \\frac{\\partial E[Y_i|X_i]}{\\partial X_{i}} = \\begin{bmatrix}\\frac{\\partial E[Y_i|X_i]}{\\partial X_{i1}}\\\\ \\vdots \\\\ \\frac{\\partial E[Y_i|X_i]}{\\partial X_{ik}}\\end{bmatrix} = \\begin{bmatrix}\\beta_1\\\\ \\vdots \\\\ \\beta_k\\end{bmatrix}\n\\]\nThe regression parameter has a partial derivative interpretation with respect to the CEF. As discussed in Handout 1, this is often used to motivate the experimental language of ceteris paribus: “holding all else fixed.\n\n\n\nConsider a case where there is a single discrete regressor: \\(D_i \\in \\{0,1\\}\\). For example,\n\\[\n  Y_i = \\beta_1 + \\beta_2 D_i + \\varepsilon_i\n\\] We cannot apply the partial derivative interpretation since \\(D\\) is not continuous. Instead, we will look at differences:\n\\[\n\\begin{aligned}\n    E[Y_i|D_i=1] =& \\beta_1 + \\beta_2 \\\\\n    E[Y_i|D_i=0] =& \\beta_1 \\\\   \n    \\Rightarrow \\beta_2 =& E[Y_i|D_i=1] - E[Y_i|D_i=0]\n\\end{aligned}\n\\]\nWe can easily extend this the case where the model includes additional (discrete or continuous) covariates, as well as case where the variable takes on multiple discrete values.\n\n\n\nIf the outcome is discrete (\\(Y_i\\in\\{0,1\\}\\)) while the regressors are continuous, the resulting linear model is referred to as a linear probability model.\n\\[\nE[Y_i|X_i] = Pr(Y_i = 1|X_i) = X_i'\\beta\n\\] This is differentiable, since \\(X\\) is continuous and the same partial derivative interpretation follows.\n\\[\n\\beta_j = \\frac{\\partial Pr(Y_i=1|X_i)}{\\partial X_{ij}}\n\\]\nNote, the unit of \\(Y\\) is probability-points (\\(\\in[0,1]\\)), not %-points (\\(\\in[0,100]\\)). Of course, the conversion of units can be made by \\(\\times 100\\) to measure in %-points.\n\n\n\nIf both the outcome and regressor(s) are discrete, then the parameter identifies a difference in conditional probabilities, \\[\n\\beta_2 = Pr(Y_i|D_i=1) - Pr(Y_i=1|D_i=0)\n\\] Note, the unit of \\(Y\\) is probability-points (\\(\\in[0,1]\\)), not %-points (\\(\\in[0,100]\\)).\n\n\n\nConsider the model,\n\\[\n  \\ln(Y_i) = X_i'\\beta + \\varepsilon_i\n\\] Then,\n\\[\n  X_i'\\beta = E[\\ln(Y_i)|X_i]\n\\]\n\\[\n\\beta_j = \\frac{\\partial E[\\ln(Y_i)|X_i]}{\\partial X_{ij}}\n\\]\nThe coefficient is therefore measured in log-units of \\(Y\\). The relation to a change in the (expected) level of \\(Y\\) is given by,\n\\[\n\\%\\Delta E[Y_i|X_i] = (exp(\\beta)-1)\\times 100\n\\] For reasonably small values of \\(\\beta\\) (i.e. within the range \\([-0.1,0.1]\\)) this can be approximated by,\n\\[\n\\%\\Delta E[Y_i|X_i] = \\beta\\times 100\n\\] A 1-unit change in \\(X_{i1}\\) is associated with a \\(\\beta_1\\) percent change in the expected value of \\(Y\\).\nThis referred to as a semi-elasticity.\n\n\n\nIf the regressor(s) is measure in log-units; for example,\n\\[\n  Y_i = \\beta_1 + \\beta_2 \\ln(X_i)_i + \\varepsilon_i\n\\] Then,\n\\[\n\\beta_2 = \\frac{\\partial E[Y_i|X_i]}{\\partial \\ln(X_{i})}\n\\]\nA 1 percent increase in \\(X\\) is given by \\(X\\times1.01\\). This is equivalent to a change in \\(\\ln(X)\\) of,\n\\[\n\\ln(X_i\\times1.01) - \\ln(X_i) = \\ln(1.01) \\approx 0.01\n\\] Thus, a 1 percent increase in the level of \\(X\\) is associated with a \\(\\beta_2/100\\) increase in the expected value of \\(Y\\). Or, more accurate\n\\[\n  \\Delta E[Y_i|X_i] = \\beta_2\\times \\ln(1.01)\n\\] This is also a semi-elasticity.\n\n\n\nIn models where both the outcome and regressor are log-transformed with an elasticity interpretation.\n\\[\n    \\ln(Y_i) = \\beta_1 + \\beta_2 \\ln(X_i)_i + \\varepsilon_i\n\\] \\[\n\\beta_2 = \\frac{\\partial E[\\ln(Y_i)|X_i]}{\\partial \\ln(X_{i})}\n\\] \\(\\beta_2\\) is a the % change in the expected value of \\(Y\\) from a 1 % change in \\(X\\)."
  },
  {
    "objectID": "material-interpretation.html#continuous-level-level-models",
    "href": "material-interpretation.html#continuous-level-level-models",
    "title": "Interpretation of Linear Models",
    "section": "",
    "text": "If \\(Y_i\\) and \\(X_i\\) are both continuously distributed random variables then,\n\\[\n\\beta_j = \\frac{\\partial E[Y_i|X_i]}{\\partial X_{ij}}\n\\] or, as a vector,\n\\[\n\\beta = \\frac{\\partial E[Y_i|X_i]}{\\partial X_{i}} = \\begin{bmatrix}\\frac{\\partial E[Y_i|X_i]}{\\partial X_{i1}}\\\\ \\vdots \\\\ \\frac{\\partial E[Y_i|X_i]}{\\partial X_{ik}}\\end{bmatrix} = \\begin{bmatrix}\\beta_1\\\\ \\vdots \\\\ \\beta_k\\end{bmatrix}\n\\]\nThe regression parameter has a partial derivative interpretation with respect to the CEF. As discussed in Handout 1, this is often used to motivate the experimental language of ceteris paribus: “holding all else fixed."
  },
  {
    "objectID": "material-interpretation.html#continuous-discrete-models",
    "href": "material-interpretation.html#continuous-discrete-models",
    "title": "Interpretation of Linear Models",
    "section": "",
    "text": "Consider a case where there is a single discrete regressor: \\(D_i \\in \\{0,1\\}\\). For example,\n\\[\n  Y_i = \\beta_1 + \\beta_2 D_i + \\varepsilon_i\n\\] We cannot apply the partial derivative interpretation since \\(D\\) is not continuous. Instead, we will look at differences:\n\\[\n\\begin{aligned}\n    E[Y_i|D_i=1] =& \\beta_1 + \\beta_2 \\\\\n    E[Y_i|D_i=0] =& \\beta_1 \\\\   \n    \\Rightarrow \\beta_2 =& E[Y_i|D_i=1] - E[Y_i|D_i=0]\n\\end{aligned}\n\\]\nWe can easily extend this the case where the model includes additional (discrete or continuous) covariates, as well as case where the variable takes on multiple discrete values."
  },
  {
    "objectID": "material-interpretation.html#discrete-continuous-models",
    "href": "material-interpretation.html#discrete-continuous-models",
    "title": "Interpretation of Linear Models",
    "section": "",
    "text": "If the outcome is discrete (\\(Y_i\\in\\{0,1\\}\\)) while the regressors are continuous, the resulting linear model is referred to as a linear probability model.\n\\[\nE[Y_i|X_i] = Pr(Y_i = 1|X_i) = X_i'\\beta\n\\] This is differentiable, since \\(X\\) is continuous and the same partial derivative interpretation follows.\n\\[\n\\beta_j = \\frac{\\partial Pr(Y_i=1|X_i)}{\\partial X_{ij}}\n\\]\nNote, the unit of \\(Y\\) is probability-points (\\(\\in[0,1]\\)), not %-points (\\(\\in[0,100]\\)). Of course, the conversion of units can be made by \\(\\times 100\\) to measure in %-points."
  },
  {
    "objectID": "material-interpretation.html#discrete-discrete-models",
    "href": "material-interpretation.html#discrete-discrete-models",
    "title": "Interpretation of Linear Models",
    "section": "",
    "text": "If both the outcome and regressor(s) are discrete, then the parameter identifies a difference in conditional probabilities, \\[\n\\beta_2 = Pr(Y_i|D_i=1) - Pr(Y_i=1|D_i=0)\n\\] Note, the unit of \\(Y\\) is probability-points (\\(\\in[0,1]\\)), not %-points (\\(\\in[0,100]\\))."
  },
  {
    "objectID": "material-interpretation.html#log-level-models",
    "href": "material-interpretation.html#log-level-models",
    "title": "Interpretation of Linear Models",
    "section": "",
    "text": "Consider the model,\n\\[\n  \\ln(Y_i) = X_i'\\beta + \\varepsilon_i\n\\] Then,\n\\[\n  X_i'\\beta = E[\\ln(Y_i)|X_i]\n\\]\n\\[\n\\beta_j = \\frac{\\partial E[\\ln(Y_i)|X_i]}{\\partial X_{ij}}\n\\]\nThe coefficient is therefore measured in log-units of \\(Y\\). The relation to a change in the (expected) level of \\(Y\\) is given by,\n\\[\n\\%\\Delta E[Y_i|X_i] = (exp(\\beta)-1)\\times 100\n\\] For reasonably small values of \\(\\beta\\) (i.e. within the range \\([-0.1,0.1]\\)) this can be approximated by,\n\\[\n\\%\\Delta E[Y_i|X_i] = \\beta\\times 100\n\\] A 1-unit change in \\(X_{i1}\\) is associated with a \\(\\beta_1\\) percent change in the expected value of \\(Y\\).\nThis referred to as a semi-elasticity."
  },
  {
    "objectID": "material-interpretation.html#level-log-models",
    "href": "material-interpretation.html#level-log-models",
    "title": "Interpretation of Linear Models",
    "section": "",
    "text": "If the regressor(s) is measure in log-units; for example,\n\\[\n  Y_i = \\beta_1 + \\beta_2 \\ln(X_i)_i + \\varepsilon_i\n\\] Then,\n\\[\n\\beta_2 = \\frac{\\partial E[Y_i|X_i]}{\\partial \\ln(X_{i})}\n\\]\nA 1 percent increase in \\(X\\) is given by \\(X\\times1.01\\). This is equivalent to a change in \\(\\ln(X)\\) of,\n\\[\n\\ln(X_i\\times1.01) - \\ln(X_i) = \\ln(1.01) \\approx 0.01\n\\] Thus, a 1 percent increase in the level of \\(X\\) is associated with a \\(\\beta_2/100\\) increase in the expected value of \\(Y\\). Or, more accurate\n\\[\n  \\Delta E[Y_i|X_i] = \\beta_2\\times \\ln(1.01)\n\\] This is also a semi-elasticity."
  },
  {
    "objectID": "material-interpretation.html#log-log-models",
    "href": "material-interpretation.html#log-log-models",
    "title": "Interpretation of Linear Models",
    "section": "",
    "text": "In models where both the outcome and regressor are log-transformed with an elasticity interpretation.\n\\[\n    \\ln(Y_i) = \\beta_1 + \\beta_2 \\ln(X_i)_i + \\varepsilon_i\n\\] \\[\n\\beta_2 = \\frac{\\partial E[\\ln(Y_i)|X_i]}{\\partial \\ln(X_{i})}\n\\] \\(\\beta_2\\) is a the % change in the expected value of \\(Y\\) from a 1 % change in \\(X\\)."
  },
  {
    "objectID": "material-dummy.html",
    "href": "material-dummy.html",
    "title": "Dummy Variables",
    "section": "",
    "text": "Dummy variables are used extensively in Econometrics and applied research. They are used to model the impact (treatment effect) of public policies, randomized control trials, and ‘natural’ experiments. They are also used to model categorical variables and fixed-effects (used to explain unobserved heterogeneity in cross-section and panel models). Given their proclivity, it is important that we understand their mechanics.\nYou might be surprised by how much ‘real-world’ data is not continuous. For example, if you downloaded a survey dataset you will likely find that most variables are in fact categorical. This is the result of a number of factors, including accuracy (avoiding reporting bias) and just the nature of real-world variables. For example, sureys often ask people to report their income on a binned scale (e.g., $0-$10,000; $10,000-$20,000; …) as a lot of people do not know their exact annual or monthly salary. Variables about a households structure (e.g. “2 parents, with children”; “Single parent, without children”; …) are naturally categorical.\nEven if a variable may have (close to) continuous measure (e.g. age in months/days; years of education), you might choose to model the variable as categorical since it may not be cardinal. For example, a bachelors degree may correspond to a 15 years of education, compared to 12 years of education for a high school diploma. However, is this 3-year difference equivalent to the difference in education of individuals with 5 and 2 years of education? You could say this about income (measured in $’s), height/weight/BMI, and employment tenure, as these measures are cardinal.\n\n\nAny dummy variable is a discrete random variable (often denoted \\(D\\)) that takes on two values: \\(D_i\\in\\{0,1\\}\\). It corresponds to a true-false statement:\n\\[\nD_i = \\mathbf{1}\\{\\text{``true\"}\\}\n\\] This simple function returns the value 1 if the statement inside is true (for unit \\(i\\)), and 0 otherwise. The For example, suppose there was a categorical variable that took on \\(m\\) distinct values, each with their own label,\n\\[\nX_{i}\\in\\{x_1\\;\\text{``label 1\"},x_2\\;\\text{``label 2\"},...,x_m\\;\\text{``label m\"}\\}\n\\] For example, \\(favouritecolour_i \\in\\{1\\;\\text{``red\"}, 2\\;\\text{``blue\"}, 3\\;\\text{``yellow\"}, 4\\;\\text{``green\"}\\}\\). Then for each value of \\(X_i\\), you can define a dummy variable:\n\\[\n  D_{im} = \\mathbf{1}\\{X_i = x_m\\}\n\\]\nA common usage of dummy variables is in the evaluation of randomized and ‘natural’ experiments. Researchers will typically define a single dummy variable to denote treatment status: \\(D_i = \\mathbf{1}\\{\\text{``treated\"}\\}\\). The dummy variable splits the sample into two groups: treated (\\(D_i=1\\)) and control (\\(D_i=0\\)).\n\n\n\nIn a basic setting the use a of dummy variable might be relatively straight forward. For example, consider the above exmple of a single treatment-status dummy variable. We can include this in a univariate regression model (including a constant term),\n\\[\nY_i = \\beta_{1}+\\beta_{2} D_{i} + \\varepsilon_i\n\\] We can show that \\(\\beta_2\\) can be interpretted as the difference between the mean of \\(Y\\) for the treated and control group (see material on interpretting linear models),\n\\[\n  \\beta_2 = E[Y_i|D_i=1] - E[Y_i|D_i=0]\n\\] Unfortunately, this notation is not going to help us move forward, so let us be a little more formal/detailed. Suppose, a categorical regressor takes on two values \\(X\\in\\{1,2\\}\\). We can then define two dummy variables,\n\\[\n\\begin{aligned}\n  D_{i1} =& \\mathbf{1}\\{X_i = 1\\} \\\\\n  D_{i2} =& \\mathbf{1}\\{X_i = 2\\}\n\\end{aligned}\n\\]\nSince the values of \\(X\\) are exhaustive and mutually exclusive, it must be that,\n\\[\n  D_{i1} + D_{i2} = 1\n\\]\nFor every unit, their value of \\(X\\) is either 1 or 2.\nNow, let us consider the linear model that includes these dummy variables as regressors:\n\\[\nY_i = \\beta_1 + \\beta_2D_{i1} + \\beta_3D_{i2} + \\varepsilon_i\n\\] We immediately have a problem. This model is not identified. This is due to a rank violation (perfect colinearity between regressors).\nSuppose the data was sorted on \\(X\\). Then consider the matrix of regressors in this model:\n\\[\nX=[\\ell,D_1,D_2]=\\begin{bmatrix}1 & 0 &1 \\\\ \\vdots & \\vdots & \\vdots \\\\ 1 & 0 &1 \\\\ 1 & 1 &0\\\\ \\vdots & \\vdots & \\vdots \\\\ 1 & 1 & 0 \\end{bmatrix}\n\\] This matrix has 3 columns, but its rank is 2. Column 3 (\\(D_2\\)) is given by column 1 (\\(\\ell\\)) minus column 2 (\\(D_1\\)).\n\\[\n\\begin{bmatrix}1 \\\\ \\vdots \\\\ 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{bmatrix}=\\begin{bmatrix}0 \\\\ \\vdots \\\\ 0 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{bmatrix}+\\begin{bmatrix}1 \\\\ \\vdots \\\\ 1 \\\\ 0\\\\  \\vdots \\\\  0 \\end{bmatrix}\n\\]\nIf \\(X\\) is not full rank, then the \\(k\\times k\\) matrix \\(X'X\\) is not invertible. And neither is \\(E[X_iX_i']\\), implying a failure of identification. We cannot seperately identify all three \\(\\beta\\)-parameters \\([\\beta_1,\\beta_2,\\beta_3]\\).\nWe can demonstrate the result in another way. Given that \\(D_{i1} + D_{i2} = 1\\) (where 1 represents the constant regressor), we can substitute \\(D_{i2}\\) with \\(1-D_{i1}\\).\n\\[\n\\begin{aligned}\n  Y_i =& \\beta_1 + \\beta_2D_{i1} + \\beta_3(1-D_{i1}) + \\varepsilon_i \\\\\n  =& (\\beta_1 + \\beta_3) + (\\beta_2-\\beta_3)D_{i1} +\\varepsilon_i\n\\end{aligned}\n\\] which we can rewrite as,\n\\[\n  Y_i = \\beta_{1(2)} + \\beta_{2(2)} D_{i1} + \\varepsilon_i\n\\] I’m using the notation of an additional subscript-\\((2)\\) to denote the parameters corresponding to the model where we exclude the dummy variable \\(D_{i2}\\).\nNow, the \\(X_2\\) matrix (denoted as such because it excludes \\(D_2\\)) has two columns and is full rank. Thus, \\([\\beta_{1(2)}, \\beta_{2(2)}]\\) are both identified.\n\\[\nX_{(2)}=[\\ell,D_1]=\\begin{bmatrix}1 & 0 \\\\ \\vdots & \\vdots \\\\ 1 & 0 \\\\ 1 & 1 \\\\ \\vdots & \\vdots \\\\ 1 & 1 \\end{bmatrix}\n\\] Thus, the model is given by,\n\\[\n  Y = X_{(2)}\\beta_{(2)} + \\varepsilon\n\\]\nWe can have also substituted \\(D_{i1}\\) with \\(1-D_{i2}\\). This would give us the model,\n\\[\n  Y_i = \\beta_{1(1)} + \\beta_{2(1)} D_{i2} + \\varepsilon_i\n\\] where \\(\\beta_{1(1)} = \\beta_1 + \\beta_2\\) and \\(\\beta_{2(1)} = \\beta_3-\\beta_2\\).\nor,\n\\[\n  Y = X_{(1)}\\beta_{(1)} + \\varepsilon\n\\] Note, the vectors \\(\\beta_{(1)}\\) and \\(\\beta_{(2)}\\) are not the same. However, you can recover the one from the other. For example,\n\\[\n\\begin{aligned}\n\\beta_{1(1)} =& \\beta_1 + \\beta_2 = \\beta_1 + \\beta_3 + (\\beta_2-\\beta_3) = \\beta_{1(2)}-\\beta_{2(2)} \\\\\n\\beta_{2(1)} =& \\beta_3-\\beta_2 = -\\beta_{2(2)}\n\\end{aligned}\n\\]\nFinally, we can substitute in the constant with \\(D_{i1}+D_{i2}\\). This gives us the model,\n\\[\n  Y_i = \\beta_{1(0)}D_{i1} + \\beta_{2(0)} D_{i2} + \\varepsilon_i\n\\] where \\(\\beta_{1(0)} = \\beta_1 + \\beta_2\\) and \\(\\beta_{2(0)} = \\beta_1+\\beta_3\\).\nor,\n\\[\n  Y = X_{(0)}\\beta_{(0)} + \\varepsilon\n\\] While it might seem strange to exclude the constant term, it is implicitly included. This is because the regressors add up to one.\n\n\n\nWe have considered three different models, each with a different excluded variable:\n\n\\(Y = X_{(2)}\\beta_{(2)} + \\varepsilon\\)\n\\(Y = X_{(1)}\\beta_{(1)} + \\varepsilon\\)\n\\(Y = X_{(0)}\\beta_{(0)} + \\varepsilon\\)\n\nEach model corresponds to a different projection matrix: \\(P_X = X(X'X)^{-1}X'\\). I will denote these \\(P_{(2)}\\), \\(P_{(1)}\\), and \\(P_{(0)}\\). It turns out that,\n\\[\nP_{(2)} = P_{(1)} = P_{(0)}\n\\] Implying the same result for the orthogonal projections: \\(M_{(2)}\\), \\(M_{(1)}\\), and \\(M_{(0)}\\). It implies that the predicted values and residuals are the same for all three models.\n\n\n\nThe above result has important implications for discrete/categorical covariates. Suppose you had a regression with a single a continuous regressors \\(X_{1}\\) and then a single categorical variable \\(X_{2}\\in\\{x_1,...,x_m\\}\\). There will be \\(k = m+1\\) parameters in this model. You would define a set of dummy variables for each category and include \\(m-1\\) in the model, assuming a constant is also included.1 \\[\nY = \\beta_1 X_{1} + \\beta_{21(1)} + \\beta_{22(1)} D_{2} + ... + \\beta_{2m(1)}D_m + \\upsilon\n\\] This can be written as,\n\\[\n  Y = X_1\\beta_1 + X_{2(1)}\\beta_{2(1)} + \\upsilon\n\\] where, \\[\n  X_{2(1)} = [1,D_2,D_3,...,D_m]\n\\]\nThe partitioned regression result tells us that the OLS-estimator for \\(\\beta_1\\) is given by,\n\\[\n  (M_{2(1)}Y) = (M_{2(1)}X_1)\\beta_1 + \\xi\n\\] where \\(M_{2(1)}\\) is the orthogonal projection matrix of all other regressors in the model. In this case, that is the orthogonal projection matrix of the matrix,\nIn this matrix, \\(D_1\\) has been excluded as the base category. Above, we showed that,\n\\[\nM_{2(0)} = M_{2(1)} = M_{2(2)} = ... = M_{2(m)}\n\\] Thus, the choice of base category for the covariate regressors has no impact on our estimate of \\(\\beta_1\\), the parameter of interest. We could have written the same model, excluding \\(D_2\\)\n\\[\nY = \\beta_1 X_{1} + \\beta_{21(2)} + \\beta_{22(2)} D_{1} + \\beta_{23(2)} D_{3} + ... + \\beta_{2m(2)}D_m + \\upsilon\n\\]\nOr even excluding the constant,\n\\[\nY = \\beta_1 X_{1} + \\beta_{21(0)} D_{1} + \\beta_{22(0)} D_{2} +  ... + \\beta_{2m(0)}D_m + \\upsilon\n\\]\nRegardless, of the base category, the estimator for \\(\\beta_1\\) remains unchanged.\n\n\n\nThe above result helps rationalize a fairly common notation in applied econometrics. Suppose you were studying the relationship between (log of) wages and education. We know that there is a lot of spatial heteroegeneity in wages: some occupations in certain cities pay a lot more than the same occupation in a different city. This could be for a whole range of reasons - some individual (differences in education/training, etc.) and some environmental (local ammenities, labour competition, number of employers etc.). Some of these location factors may be obserable, but some not. If you are willing to assume that these characteristics relate to the local area and not the specific individual, then you could model them as,\n\\[\n  A_{l} = [A_{l1},...,A_{ls}]'\n\\] Where \\(A_l\\) is a \\(s\\times 1\\) vector of \\(l\\)-location’s characteristics. You can use the notation \\(A_{l(i)}\\) to link the individual to a specific location. Recall, these are common to all individuals in this location. We can think of this linear combination of location specific variables as a single location-specific parameter \\(\\alpha_l\\).\n\\[\n  \\alpha_l = A_{l(i)}'\\gamma\n\\] By doing this, we remove the need to observe all the variables in \\(A_l\\). However, we must observe multiple people from the same location.\nSuppose the model you have in mind is given by,\n\\[\n\\ln(w_i) = \\alpha + \\beta edu_i + A_{l(i)}'\\gamma + \\varepsilon_{i}\n\\] We can’t estimate this model because we can’t observe all variables in \\(A\\). However, we can estimate the model,\n\\[\n\\ln(w_i) = \\alpha_1 + \\beta edu_i + \\sum_{j=2}^m \\alpha_{2j}\\mathbf{1}\\{location_i = j\\} + \\varepsilon_i\n\\] where we exclude the first (arbitrary) location. Or, we can drop the constant and estimate,\n\\[\n\\ln(w_i) = \\beta edu_i + \\sum_{j=1}^m \\alpha_{j}\\mathbf{1}\\{location_i = j\\} + \\varepsilon_i\n\\] Since, an individual will have only one location (in a given period of time), unit \\(i\\) in location \\(j\\) will just return,\n\\[\n\\sum_{j=1}^m \\alpha_{j}\\mathbf{1}\\{location_i = j\\} = \\alpha_j\n\\] So, the model can be written as,\n\\[\n\\ln(w_{ij}) = \\beta edu_{ij} + \\alpha_j + \\varepsilon_{ij}\n\\] You can think of \\(\\alpha_j\\) as a location-specific constant. This is also why it makes sense to drop the constant whe using fixed-effects notation. This constant explains all the unobserved spatial heterogeneity in the earnings of individuals.\nFrom a modelling perspective, it is important to remember that this notation essentially hides a whole set of dummy variables that are implicitly included in the model. The number of regressors in this model will be \\(k = 1 + m\\) where \\(m\\) is the number of locations."
  },
  {
    "objectID": "material-dummy.html#notation",
    "href": "material-dummy.html#notation",
    "title": "Dummy Variables",
    "section": "",
    "text": "Any dummy variable is a discrete random variable (often denoted \\(D\\)) that takes on two values: \\(D_i\\in\\{0,1\\}\\). It corresponds to a true-false statement:\n\\[\nD_i = \\mathbf{1}\\{\\text{``true\"}\\}\n\\] This simple function returns the value 1 if the statement inside is true (for unit \\(i\\)), and 0 otherwise. The For example, suppose there was a categorical variable that took on \\(m\\) distinct values, each with their own label,\n\\[\nX_{i}\\in\\{x_1\\;\\text{``label 1\"},x_2\\;\\text{``label 2\"},...,x_m\\;\\text{``label m\"}\\}\n\\] For example, \\(favouritecolour_i \\in\\{1\\;\\text{``red\"}, 2\\;\\text{``blue\"}, 3\\;\\text{``yellow\"}, 4\\;\\text{``green\"}\\}\\). Then for each value of \\(X_i\\), you can define a dummy variable:\n\\[\n  D_{im} = \\mathbf{1}\\{X_i = x_m\\}\n\\]\nA common usage of dummy variables is in the evaluation of randomized and ‘natural’ experiments. Researchers will typically define a single dummy variable to denote treatment status: \\(D_i = \\mathbf{1}\\{\\text{``treated\"}\\}\\). The dummy variable splits the sample into two groups: treated (\\(D_i=1\\)) and control (\\(D_i=0\\))."
  },
  {
    "objectID": "material-dummy.html#dummy-regressors",
    "href": "material-dummy.html#dummy-regressors",
    "title": "Dummy Variables",
    "section": "",
    "text": "In a basic setting the use a of dummy variable might be relatively straight forward. For example, consider the above exmple of a single treatment-status dummy variable. We can include this in a univariate regression model (including a constant term),\n\\[\nY_i = \\beta_{1}+\\beta_{2} D_{i} + \\varepsilon_i\n\\] We can show that \\(\\beta_2\\) can be interpretted as the difference between the mean of \\(Y\\) for the treated and control group (see material on interpretting linear models),\n\\[\n  \\beta_2 = E[Y_i|D_i=1] - E[Y_i|D_i=0]\n\\] Unfortunately, this notation is not going to help us move forward, so let us be a little more formal/detailed. Suppose, a categorical regressor takes on two values \\(X\\in\\{1,2\\}\\). We can then define two dummy variables,\n\\[\n\\begin{aligned}\n  D_{i1} =& \\mathbf{1}\\{X_i = 1\\} \\\\\n  D_{i2} =& \\mathbf{1}\\{X_i = 2\\}\n\\end{aligned}\n\\]\nSince the values of \\(X\\) are exhaustive and mutually exclusive, it must be that,\n\\[\n  D_{i1} + D_{i2} = 1\n\\]\nFor every unit, their value of \\(X\\) is either 1 or 2.\nNow, let us consider the linear model that includes these dummy variables as regressors:\n\\[\nY_i = \\beta_1 + \\beta_2D_{i1} + \\beta_3D_{i2} + \\varepsilon_i\n\\] We immediately have a problem. This model is not identified. This is due to a rank violation (perfect colinearity between regressors).\nSuppose the data was sorted on \\(X\\). Then consider the matrix of regressors in this model:\n\\[\nX=[\\ell,D_1,D_2]=\\begin{bmatrix}1 & 0 &1 \\\\ \\vdots & \\vdots & \\vdots \\\\ 1 & 0 &1 \\\\ 1 & 1 &0\\\\ \\vdots & \\vdots & \\vdots \\\\ 1 & 1 & 0 \\end{bmatrix}\n\\] This matrix has 3 columns, but its rank is 2. Column 3 (\\(D_2\\)) is given by column 1 (\\(\\ell\\)) minus column 2 (\\(D_1\\)).\n\\[\n\\begin{bmatrix}1 \\\\ \\vdots \\\\ 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{bmatrix}=\\begin{bmatrix}0 \\\\ \\vdots \\\\ 0 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{bmatrix}+\\begin{bmatrix}1 \\\\ \\vdots \\\\ 1 \\\\ 0\\\\  \\vdots \\\\  0 \\end{bmatrix}\n\\]\nIf \\(X\\) is not full rank, then the \\(k\\times k\\) matrix \\(X'X\\) is not invertible. And neither is \\(E[X_iX_i']\\), implying a failure of identification. We cannot seperately identify all three \\(\\beta\\)-parameters \\([\\beta_1,\\beta_2,\\beta_3]\\).\nWe can demonstrate the result in another way. Given that \\(D_{i1} + D_{i2} = 1\\) (where 1 represents the constant regressor), we can substitute \\(D_{i2}\\) with \\(1-D_{i1}\\).\n\\[\n\\begin{aligned}\n  Y_i =& \\beta_1 + \\beta_2D_{i1} + \\beta_3(1-D_{i1}) + \\varepsilon_i \\\\\n  =& (\\beta_1 + \\beta_3) + (\\beta_2-\\beta_3)D_{i1} +\\varepsilon_i\n\\end{aligned}\n\\] which we can rewrite as,\n\\[\n  Y_i = \\beta_{1(2)} + \\beta_{2(2)} D_{i1} + \\varepsilon_i\n\\] I’m using the notation of an additional subscript-\\((2)\\) to denote the parameters corresponding to the model where we exclude the dummy variable \\(D_{i2}\\).\nNow, the \\(X_2\\) matrix (denoted as such because it excludes \\(D_2\\)) has two columns and is full rank. Thus, \\([\\beta_{1(2)}, \\beta_{2(2)}]\\) are both identified.\n\\[\nX_{(2)}=[\\ell,D_1]=\\begin{bmatrix}1 & 0 \\\\ \\vdots & \\vdots \\\\ 1 & 0 \\\\ 1 & 1 \\\\ \\vdots & \\vdots \\\\ 1 & 1 \\end{bmatrix}\n\\] Thus, the model is given by,\n\\[\n  Y = X_{(2)}\\beta_{(2)} + \\varepsilon\n\\]\nWe can have also substituted \\(D_{i1}\\) with \\(1-D_{i2}\\). This would give us the model,\n\\[\n  Y_i = \\beta_{1(1)} + \\beta_{2(1)} D_{i2} + \\varepsilon_i\n\\] where \\(\\beta_{1(1)} = \\beta_1 + \\beta_2\\) and \\(\\beta_{2(1)} = \\beta_3-\\beta_2\\).\nor,\n\\[\n  Y = X_{(1)}\\beta_{(1)} + \\varepsilon\n\\] Note, the vectors \\(\\beta_{(1)}\\) and \\(\\beta_{(2)}\\) are not the same. However, you can recover the one from the other. For example,\n\\[\n\\begin{aligned}\n\\beta_{1(1)} =& \\beta_1 + \\beta_2 = \\beta_1 + \\beta_3 + (\\beta_2-\\beta_3) = \\beta_{1(2)}-\\beta_{2(2)} \\\\\n\\beta_{2(1)} =& \\beta_3-\\beta_2 = -\\beta_{2(2)}\n\\end{aligned}\n\\]\nFinally, we can substitute in the constant with \\(D_{i1}+D_{i2}\\). This gives us the model,\n\\[\n  Y_i = \\beta_{1(0)}D_{i1} + \\beta_{2(0)} D_{i2} + \\varepsilon_i\n\\] where \\(\\beta_{1(0)} = \\beta_1 + \\beta_2\\) and \\(\\beta_{2(0)} = \\beta_1+\\beta_3\\).\nor,\n\\[\n  Y = X_{(0)}\\beta_{(0)} + \\varepsilon\n\\] While it might seem strange to exclude the constant term, it is implicitly included. This is because the regressors add up to one."
  },
  {
    "objectID": "material-dummy.html#dummy-projections",
    "href": "material-dummy.html#dummy-projections",
    "title": "Dummy Variables",
    "section": "",
    "text": "We have considered three different models, each with a different excluded variable:\n\n\\(Y = X_{(2)}\\beta_{(2)} + \\varepsilon\\)\n\\(Y = X_{(1)}\\beta_{(1)} + \\varepsilon\\)\n\\(Y = X_{(0)}\\beta_{(0)} + \\varepsilon\\)\n\nEach model corresponds to a different projection matrix: \\(P_X = X(X'X)^{-1}X'\\). I will denote these \\(P_{(2)}\\), \\(P_{(1)}\\), and \\(P_{(0)}\\). It turns out that,\n\\[\nP_{(2)} = P_{(1)} = P_{(0)}\n\\] Implying the same result for the orthogonal projections: \\(M_{(2)}\\), \\(M_{(1)}\\), and \\(M_{(0)}\\). It implies that the predicted values and residuals are the same for all three models."
  },
  {
    "objectID": "material-dummy.html#dummy-covariates",
    "href": "material-dummy.html#dummy-covariates",
    "title": "Dummy Variables",
    "section": "",
    "text": "The above result has important implications for discrete/categorical covariates. Suppose you had a regression with a single a continuous regressors \\(X_{1}\\) and then a single categorical variable \\(X_{2}\\in\\{x_1,...,x_m\\}\\). There will be \\(k = m+1\\) parameters in this model. You would define a set of dummy variables for each category and include \\(m-1\\) in the model, assuming a constant is also included.1 \\[\nY = \\beta_1 X_{1} + \\beta_{21(1)} + \\beta_{22(1)} D_{2} + ... + \\beta_{2m(1)}D_m + \\upsilon\n\\] This can be written as,\n\\[\n  Y = X_1\\beta_1 + X_{2(1)}\\beta_{2(1)} + \\upsilon\n\\] where, \\[\n  X_{2(1)} = [1,D_2,D_3,...,D_m]\n\\]\nThe partitioned regression result tells us that the OLS-estimator for \\(\\beta_1\\) is given by,\n\\[\n  (M_{2(1)}Y) = (M_{2(1)}X_1)\\beta_1 + \\xi\n\\] where \\(M_{2(1)}\\) is the orthogonal projection matrix of all other regressors in the model. In this case, that is the orthogonal projection matrix of the matrix,\nIn this matrix, \\(D_1\\) has been excluded as the base category. Above, we showed that,\n\\[\nM_{2(0)} = M_{2(1)} = M_{2(2)} = ... = M_{2(m)}\n\\] Thus, the choice of base category for the covariate regressors has no impact on our estimate of \\(\\beta_1\\), the parameter of interest. We could have written the same model, excluding \\(D_2\\)\n\\[\nY = \\beta_1 X_{1} + \\beta_{21(2)} + \\beta_{22(2)} D_{1} + \\beta_{23(2)} D_{3} + ... + \\beta_{2m(2)}D_m + \\upsilon\n\\]\nOr even excluding the constant,\n\\[\nY = \\beta_1 X_{1} + \\beta_{21(0)} D_{1} + \\beta_{22(0)} D_{2} +  ... + \\beta_{2m(0)}D_m + \\upsilon\n\\]\nRegardless, of the base category, the estimator for \\(\\beta_1\\) remains unchanged."
  },
  {
    "objectID": "material-dummy.html#fixed-effects",
    "href": "material-dummy.html#fixed-effects",
    "title": "Dummy Variables",
    "section": "",
    "text": "The above result helps rationalize a fairly common notation in applied econometrics. Suppose you were studying the relationship between (log of) wages and education. We know that there is a lot of spatial heteroegeneity in wages: some occupations in certain cities pay a lot more than the same occupation in a different city. This could be for a whole range of reasons - some individual (differences in education/training, etc.) and some environmental (local ammenities, labour competition, number of employers etc.). Some of these location factors may be obserable, but some not. If you are willing to assume that these characteristics relate to the local area and not the specific individual, then you could model them as,\n\\[\n  A_{l} = [A_{l1},...,A_{ls}]'\n\\] Where \\(A_l\\) is a \\(s\\times 1\\) vector of \\(l\\)-location’s characteristics. You can use the notation \\(A_{l(i)}\\) to link the individual to a specific location. Recall, these are common to all individuals in this location. We can think of this linear combination of location specific variables as a single location-specific parameter \\(\\alpha_l\\).\n\\[\n  \\alpha_l = A_{l(i)}'\\gamma\n\\] By doing this, we remove the need to observe all the variables in \\(A_l\\). However, we must observe multiple people from the same location.\nSuppose the model you have in mind is given by,\n\\[\n\\ln(w_i) = \\alpha + \\beta edu_i + A_{l(i)}'\\gamma + \\varepsilon_{i}\n\\] We can’t estimate this model because we can’t observe all variables in \\(A\\). However, we can estimate the model,\n\\[\n\\ln(w_i) = \\alpha_1 + \\beta edu_i + \\sum_{j=2}^m \\alpha_{2j}\\mathbf{1}\\{location_i = j\\} + \\varepsilon_i\n\\] where we exclude the first (arbitrary) location. Or, we can drop the constant and estimate,\n\\[\n\\ln(w_i) = \\beta edu_i + \\sum_{j=1}^m \\alpha_{j}\\mathbf{1}\\{location_i = j\\} + \\varepsilon_i\n\\] Since, an individual will have only one location (in a given period of time), unit \\(i\\) in location \\(j\\) will just return,\n\\[\n\\sum_{j=1}^m \\alpha_{j}\\mathbf{1}\\{location_i = j\\} = \\alpha_j\n\\] So, the model can be written as,\n\\[\n\\ln(w_{ij}) = \\beta edu_{ij} + \\alpha_j + \\varepsilon_{ij}\n\\] You can think of \\(\\alpha_j\\) as a location-specific constant. This is also why it makes sense to drop the constant whe using fixed-effects notation. This constant explains all the unobserved spatial heterogeneity in the earnings of individuals.\nFrom a modelling perspective, it is important to remember that this notation essentially hides a whole set of dummy variables that are implicitly included in the model. The number of regressors in this model will be \\(k = 1 + m\\) where \\(m\\) is the number of locations."
  },
  {
    "objectID": "material-dummy.html#footnotes",
    "href": "material-dummy.html#footnotes",
    "title": "Dummy Variables",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI have deliberately moved the order of the variables so that the constant and dummy variables can be grouped together as a single set of regressors \\(X_2\\). In this way, the model follows the notation from Handout 1, in which \\(Y = X_1\\beta_1 + X_2\\beta_2 + \\varepsilon\\).↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Quantitative Methods: Econometrics B",
    "section": "",
    "text": "This is not the official module website. The EC910 (or EC987, 2024/25) Moodle-page is the primary source of communication and resources for this module. All material available on this website can be found there.\nI designed this website as a means of testing out the Quarto package, developed by RStudio.1 This was partly as a way to develop my own skills and better incorporate the R language into the module. However, there are also accessibility benefits to this approach. Mathematical notation published in pdf’s using LaTeX cannot be read by a screen reader, while LaTeX published in html can. In addition, the website has built-in dark-mode option (see top-right toggle). For those who prefer pdf (for printing and notetaking), each page contains a unique downloadable link.\nPlease let me know if you find any mistakes in my notes or code. In addition, I welcome suggestions on how to improve the R (or Stata) code provided in this module. I will endeavor to do my best to acknowledge the many people who have contributed to this material and my wider knowledge of this subject."
  },
  {
    "objectID": "index.html#welcome-to-metrics-b",
    "href": "index.html#welcome-to-metrics-b",
    "title": "Quantitative Methods: Econometrics B",
    "section": "",
    "text": "This is not the official module website. The EC910 (or EC987, 2024/25) Moodle-page is the primary source of communication and resources for this module. All material available on this website can be found there.\nI designed this website as a means of testing out the Quarto package, developed by RStudio.1 This was partly as a way to develop my own skills and better incorporate the R language into the module. However, there are also accessibility benefits to this approach. Mathematical notation published in pdf’s using LaTeX cannot be read by a screen reader, while LaTeX published in html can. In addition, the website has built-in dark-mode option (see top-right toggle). For those who prefer pdf (for printing and notetaking), each page contains a unique downloadable link.\nPlease let me know if you find any mistakes in my notes or code. In addition, I welcome suggestions on how to improve the R (or Stata) code provided in this module. I will endeavor to do my best to acknowledge the many people who have contributed to this material and my wider knowledge of this subject."
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Quantitative Methods: Econometrics B",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nI am building off material developed by Prof. Wiji Arulampalam, who taught this module from 2011-2024. Since arriving at Warwick in 2021, I have learned so much from Prof. Arulampalam’s expertise and years of research and professional experience. More importantly, I have so enjoyed working alongside her as both colleague and friend. I wish her well in her retirement.\nI have also borrowed from material developed by Prof. Vadim Marmer (Vancouver School of Economics, UBC) who taught the Econometrics module I attended during my own Master’s degree. I was the teaching assistant for this module from 2017-2019 and developed some additional notes found here. I am hugely indebted to Prof. Marmer, whose teaching and professional guidance have been foundational to my academic career.\nI hope you enjoy this module!\nNeil Lloyd"
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Quantitative Methods: Econometrics B",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.↩︎"
  },
  {
    "objectID": "handout-7.html",
    "href": "handout-7.html",
    "title": "Endogenous Selection Models",
    "section": "",
    "text": "Code\nlibrary(ggplot2)\nlibrary(latex2exp)"
  },
  {
    "objectID": "handout-7.html#overview",
    "href": "handout-7.html#overview",
    "title": "Endogenous Selection Models",
    "section": "1 Overview",
    "text": "1 Overview\nIn this handout we will review models that allow us to relax two important assumptions\n\nrandom sampling in the cross-section dimension;\nand unrestricted values of the dependent variable.\n\nIn doing so, we will discuss two types of distributions:\n\ntruncated distribution\ncensored distribution\n\nFurther reading can be found in:\n\nChapter 16 of Cameron and Trivedi (2005)\nSection 7.4-7.6 of Verbeek (2017)\nHeckman, J.J. (1979) Sample selction bias as a specification error, Econometrica"
  },
  {
    "objectID": "handout-7.html#censored-truncated-distributions",
    "href": "handout-7.html#censored-truncated-distributions",
    "title": "Endogenous Selection Models",
    "section": "2 Censored & Truncated Distributions",
    "text": "2 Censored & Truncated Distributions\n\n2.1 Censored data\nIn the case of censored data, there is some loss of information for the dependent variable but the explanatory variables are still observed. A common-case of censored data is top-coded income data in surveys; i.e. “Income great than $100,000”.\nIn the extreme, you can think of a discrete-choice binary data as censored, since we don’t observe the latent variable.\nThe following is an example of a left-censored distribution. For individuals with values of the outcome \\(Y&lt;c\\), we only observe the threshold \\(c\\). The distribution is given by,\n\\[\nf(y) = \\begin{cases} 0 \\qquad \\text{for} \\quad y &lt;c \\\\\nF^*(c)  \\qquad \\text{for} \\quad y=c \\\\\nf^*(y) \\qquad \\text{for} \\quad y&gt;c\n\\end{cases}\n\\]\n\n\nCode\n# Threshold\nsd &lt;- 2\nc &lt;- -1\n\n# Define the censored normal density function: x &gt; c\nx1 &lt;- seq(from=c, to=5, by = 0.05)\ncens_data &lt;- data.frame(x=x1, pdf = dnorm(x1,sd=sd))\n\n# Define the uncensored normal density function\nx2 &lt;- seq(from=-5, to=5, by = 0.05)\ndens_data &lt;- data.frame(x=x2, pdf = dnorm(x2,sd=sd))\n\n# Generate data for shading the area under the curve for x&lt;=c\nx3 &lt;- seq(-5, c, by = 0.05)\nshade_data &lt;- data.frame(x =x3 ,ymin = 0,ymax = dnorm(x3,sd=sd))\n\n# Plot\nggplot() +\n  geom_line(data = cens_data, aes(x=x,y=pdf), color = \"blue\", linewidth = 1) +\n  geom_line(data = shade_data, aes(x=x, y=ymin), color = \"blue\", linewidth = 1) +\n  geom_point(aes(x = c, y = 0), color = \"blue\", size = 2, shape=1) +\n  geom_point(aes(x = c, y = dnorm(c,sd=sd)), color = \"blue\", size = 2,shape=1) +\n  geom_point(aes(x = c, y = pnorm(c,sd=sd)), color = \"blue\", size = 2) +\n  geom_line(data = dens_data, aes(x=x,y=pdf), color = \"skyblue\", linewidth = 0.5) +\n  geom_ribbon(data = shade_data, aes(x = x, ymin = ymin, ymax = ymax), fill = \"skyblue\", alpha = 0.5) +\n  labs(\n    title = TeX(\"Censored distribution\"),\n    x = NULL,\n    y = \"Density\"\n  ) +\n  scale_x_continuous(\n    breaks = c(c),            \n    labels = c(\"c\")  \n  ) +\n  theme_minimal() +\n  coord_cartesian(xlim = c(-5, 5)) +\n  annotate(\"text\", x = c*3, y = 0.1, label = \"Censored region\", vjust = -1, hjust = 0.5, size = 5, color = \"blue\")\n\n\n\n\n\n\n\n\n\n\n\n2.2 Truncated data\nIn truncated samples, both the dependent and explanatory variables are missing for some observations where he outcome value is one side of a threshold. For example, you might only observe the test score (and characteristics) of students who passed the test.\nAnother famous example is the labour market: we only observe the wages of those who are employed. Consider a consider where workers are observed working when they receive a wage offer that is as least as good as their reservation wage. According to such a model, those who are not employed must have received wage offers less than their reservation wage. It suggests, that the distribution of accepted wage offers will be truncated.\nThe following, is an example of a distribution that is left-truncated. We only observe values of \\(Y\\) for \\(Y\\geq c\\). The density of a left-truncated random variable is,\n\\[\nf(y) = f^*(y|y&gt;c) = \\frac{f^*(y)}{1-F^*(c)}  \n\\] where \\(1-F^*(c) = Pr(y|y&gt;c)\\). In this way, truncation reduces the range of values the outcome variable can take. In this case, the mean of the truncated variable is greater than the unconditional mean.\n\n\nCode\n# Threshold\nsd &lt;- 2\nc &lt;- -1\n\n# Define the truncated normal density function: x &gt; c\nx1 &lt;- seq(from=c, to=5, by = 0.05)\ntrun_data &lt;- data.frame(x=x1, pdf = dnorm(x1,sd=sd)/(1-pnorm(c,sd=sd)))\n\n# Define the untruncated normal density function\nx2 &lt;- seq(from=-5, to=5, by = 0.05)\ndens_data &lt;- data.frame(x=x2, pdf = dnorm(x2,sd=sd))\n\n# Generate data for shading the area under the curve for x&lt;=c\nx3 &lt;- seq(-5, c, by = 0.05)\nshade_data &lt;- data.frame(x =x3 ,ymin = 0,ymax = dnorm(x3,sd=sd))\n\n# Plot\nggplot() +\n  geom_line(data = trun_data, aes(x=x,y=pdf), color = \"red\", linewidth = 1) +\n  geom_line(data = shade_data, aes(x=x, y=ymin), color = \"red\", linewidth = 1) +\n  geom_point(aes(x = c, y = 0), color = \"red\", size = 2, shape=1) +\n  geom_point(aes(x = c, y = dnorm(c,sd=sd)/(1-pnorm(c,sd=sd))), color = \"red\", size = 2) +\n  geom_line(data = dens_data, aes(x=x,y=pdf), color = \"pink\", linewidth = 0.5) +\n  geom_ribbon(data = shade_data, aes(x = x, ymin = ymin, ymax = ymax), fill = \"pink\", alpha = 0.5) +\n  labs(\n    title = TeX(\"Truncated distribution\"),\n    x = NULL,\n    y = \"Density\"\n  ) +\n  scale_x_continuous(\n    breaks = c(c),            \n    labels = c(\"c\")  \n  ) +\n  theme_minimal() +\n  coord_cartesian(xlim = c(-5, 5)) +\n  annotate(\"text\", x = c*3, y = 0.1, label = \"Truncated region\", vjust = -1, hjust = 0.5, size = 5, color = \"red\")\n\n\n\n\n\n\n\n\n\n\n\n2.3 Truncated normal distribution\nBefore we proceed, it is worth revising some useful traits of (joint) normal distributions.\nWe know that if \\(X\\sim N(\\mu,\\sigma^2)\\), then \\(Z = \\frac{X-\\mu}{\\sigma}\\sim N(0,1)\\) (standard normal distribution). Moreover, as discussed in Handout 6, the pdf and cdf of the standard normal distribution are given by,\n\\[\n\\phi(z) = \\frac{1}{\\sqrt{2\\pi}}exp(-z^2/2)\n\\] and the CDF by, \\[\n\\Phi(z) = \\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^z exp(-u^2/2)du\n\\]\nThe pdf of the \\(X\\) is given by,\n\\[\nf_X(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}exp\\bigg(-\\frac{1}{2}\\bigg(\\frac{x-\\mu}{\\sigma}\\bigg)^2\\bigg) = \\frac{1}{\\sigma}\\phi(z)\n\\] and the cdf by, \\[\nF_X(x) = \\Phi(z) \\qquad \\text{where}\\quad z = \\frac{x-\\mu}{\\sigma}\n\\] We can evaluate the truncated mean of \\(Z\\sim N(0,1)\\). First from the left:\n\\[\n\\begin{aligned}\nE[Z|Z&gt;c] =& \\int_{c}^{+\\infty}z\\frac{\\phi(z)}{1-\\Phi(c)}dz \\\\\n=& \\frac{1}{1-\\Phi(c)}\\int_{c}^{+\\infty}z\\phi(z)dz \\\\\n=& \\frac{1}{1-\\Phi(c)}\\int_{c}^{+\\infty}-\\phi'(z)dz \\\\\n=& \\frac{\\phi(c)}{1-\\Phi(c)}\n\\end{aligned}\n\\] Next from the left: \\[\n\\begin{aligned}\nE[Z|Z&lt;c] =& \\int_{-\\infty}^{c}z\\frac{\\phi(z)}{\\Phi(c)}dz \\\\\n=& \\frac{1}{\\Phi(c)}\\int_{-\\infty}^{c}z\\phi(z)dz \\\\\n=& \\frac{1}{\\Phi(c)}\\int_{-\\infty}^{c}-\\phi'(z)dz \\\\\n=& \\frac{-\\phi(c)}{\\Phi(c)} \\\\\n=&-E[Z|Z&gt;-c]\n\\end{aligned}\n\\] where the last line is given by the symmetry of the standard normal distribution: \\(\\phi(c)=\\phi(-c)\\) and \\(\\Phi(c)=1-\\Phi(-c)\\). The function \\(\\frac{\\phi(c)}{\\Phi(c)} = E[Z|Z&gt;-c]\\) is referred to as the inverse mills ratio.\n\n\n2.4 Conditional normal distribution\nLet \\(\\begin{bmatrix}Y & X\\end{bmatrix}'\\) be joint normal:\n\\[\n\\begin{bmatrix}Y \\\\ X\\end{bmatrix}\\sim N\\bigg(\\begin{bmatrix}\\mu_X \\\\ \\mu_Y\\end{bmatrix},\\begin{bmatrix}\\sigma^2_{Y} & \\sigma_{YX} \\\\ \\sigma_{XY} & \\sigma^2_{X}\\end{bmatrix}\\bigg)\n\\] where \\(\\sigma_{YX}=\\sigma_{XY}\\). Then,\n\\[\nY|X\\sim N\\bigg(\\mu_Y + \\frac{\\sigma_{YX}}{\\sigma_X^2}(X-\\mu_X),\\sigma^2_{Y}-\\sigma_{YX}^2/\\sigma^2_{X}\\bigg)\n\\] Note, the conditional mean of \\(E[Y|X]=\\mu_Y + \\frac{\\sigma_{YX}}{\\sigma_X^2}(X-\\mu_X)\\) is a linear function of X.\n\n\n2.5 Inverse Mills Ratio\nUsing the above result characteristic, we can show that if \\(\\begin{bmatrix}\\varepsilon_i & \\upsilon_i\\end{bmatrix}'\\) are jointly normal, with mean zero, then\n\\[\n\\begin{aligned}\nE[\\varepsilon_i|\\upsilon_i&gt;-c] =& E\\big[E[\\varepsilon_i|\\upsilon_i,\\upsilon_i&gt;-c]\\big|\\upsilon_i&gt;-c\\big] \\\\\n=& E\\bigg[\\frac{\\sigma_{\\varepsilon\\upsilon}}{\\sigma_\\upsilon^2}\\upsilon_i\\bigg|\\upsilon_i&gt;-c\\bigg] \\\\\n=&\\frac{\\sigma_{\\varepsilon\\upsilon}}{\\sigma_\\upsilon^2}E[\\upsilon_i|\\upsilon_i&gt;-c] \\\\\n=&\\frac{\\sigma_{\\varepsilon\\upsilon}}{\\sigma_\\upsilon^2}\\sigma_\\upsilon E\\bigg[\\frac{\\upsilon_i}{\\sigma_\\upsilon}\\bigg|\\frac{\\upsilon_i}{\\sigma_\\upsilon}&gt;-\\frac{c}{\\sigma_\\upsilon}\\bigg] \\\\\n=&\\frac{\\sigma_{\\varepsilon\\upsilon}}{\\sigma_\\upsilon}\\frac{\\phi(c/\\sigma_\\upsilon)}{\\Phi(c/\\sigma_\\upsilon)} \\\\\n=& \\frac{\\sigma_{\\varepsilon\\upsilon}}{\\sigma_\\upsilon}\\lambda(c')\n\\end{aligned}\n\\] where \\(c' = c/\\sigma_\\upsilon\\). In models where \\(\\upsilon_i\\) is the error from the selection equation, \\(\\sigma_\\upsilon\\) is not identified and must be normalized to 1. Thus, \\(c' = c\\)."
  },
  {
    "objectID": "handout-7.html#models",
    "href": "handout-7.html#models",
    "title": "Endogenous Selection Models",
    "section": "3 Models",
    "text": "3 Models\nIn this section we will review a series of models that build on the latent variable model in Handout 6.\n\\[\nY_i^* = X_i'\\beta + \\varepsilon_i\n\\] We want to learn about \\(\\beta\\), but \\(Y^*_i\\) is only partially observed due to some selection process. We will first review the Tobit model, where the outcome is censored. For example, top-coded earnings data. Next, we will look at selection/threshold models, where the outcome is observed based on a selection decision. For example, non-response in a survey. Finally, will look at endogenous switching models, where you observe the outcome under different potential states, and a selection decision determines the state of observation. For example, wages earned in different sectors of the economy.\nIn all cases, the regressors (covariates) will be observed regardless of selection. This is important as we would otherwise not be able to identify the selection decision. Thus, we will not be able to evaluate truncated samples, where covariates are not observed when the outcome is not observed. For example, data on political donations.\n\n3.1 Tobit model\nIn a Tobit model, the outcome is censored. You observe \\(Y_i^*\\) only above (or below) a given threshold \\(c\\), otherwise you observe the threshold value. However, you do observe the regressors (covariates) for all observations (regardless of censoring).\n\\[\nY_i = \\begin{cases}Y_i^* \\qquad \\text{if} \\quad D_i=1 \\\\\nc \\qquad \\text{if} \\quad D_i=0 \\end{cases}\n\\] where,\n\\[\nD_i = \\begin{cases}1 \\qquad \\text{if} \\quad Y^*&gt;c \\\\\n0 \\qquad \\text{if} \\quad Y^*\\leq c \\end{cases}\n\\] Given the latent variable model, you observe, \\[\nY_i = \\begin{cases}X_i'\\beta + \\varepsilon_i \\qquad \\text{if} \\quad X_i'\\beta + \\varepsilon_i&gt;c \\\\\nc \\qquad \\text{if} \\quad X_i'\\beta + \\varepsilon_i\\leq c \\end{cases}\n\\] This is an example of left-censoring, but we could equally allow for right-censoring or both left- and right-censoring.\nThe conditional mean of the observed outcome (above the threshold) is given by:\n\\[\n\\begin{aligned}\n&E[Y_{i}|D_i=1,X_i] \\\\\n=& E[Y_{i}^*|Y_i^*&gt;c,X_i] \\\\\n=&X_i'\\beta + E[\\varepsilon_{i}|\\varepsilon_i&gt;c-X_i'\\beta,X_i]\n\\end{aligned}\n\\] A defining feature of the Tobit model is that observation, or selection, depends on the (latent) outcome \\(Y^*\\) alone. The following two selection models introduce a separate latent variable that determines selection.\n\n\n3.2 Non-stochastic threshold model\nThe outcome \\(Y^*\\) is missing for some units, as in the case of truncated data. However, we do do observe the vector of characteristics \\(X_i\\) and \\(Z_i\\) (which may overlap) for all units. This is referred to as a selected sample. Note, some texts refer to the threshold model as the Heckman selection model, or Heckit for short.\nLet \\(D^*_i\\) be a second continuous latent outcome, that determines observation/selection,\n\\[\nD_i^* = Z_i'\\gamma + \\upsilon_i\n\\] As in a binary choice model (see Handout 6), the observable dummy-variable, \\(D_i\\), which denotes selection into the observed sample, can be defined as,\n\\[\nD_i = \\begin{cases}1 \\qquad \\text{if} \\quad D_i^*&gt;0 \\\\\n0 \\qquad \\text{otherwise}\\end{cases}\n\\] For example, \\(D_i\\) may identify employment in a wage model where you only observe wages of employed individuals.\nThe observed outcome \\(Y_i\\) is then,\n\\[\nY_i = \\begin{cases}Y_i^* \\qquad \\text{if} \\quad D_i=1 \\\\\n\\text{missing} \\qquad \\text{otherwise} \\end{cases}\n\\] Given the latent variable models for \\(Y_i^*\\) and \\(D_i^*\\), this can be written as,\n\\[\nY_i = \\begin{cases}X_i'\\beta + \\varepsilon_i \\qquad \\text{if} \\quad Z_i'\\gamma +\\upsilon_i&gt;0 \\\\\n\\text{missing} \\qquad \\text{otherwise} \\end{cases}\n\\]\nThe model has two error terms which means that we need to make an assumption about their joint distribution. If the two errors are independent, then we can ignore the missing observations; i.e., there is no selection bias.\nConsider the conditional mean of the (observed) outcome:\n\\[\n\\begin{aligned}\n&E[Y_i|D_i=1,X_i] \\\\\n=& E[Y_i^*|D_i=1,X_i] \\\\\n=&X_i'\\beta + E[\\varepsilon_i|D_i^*&gt;0,X_i] \\\\\n=&X_i'\\beta + E[\\varepsilon_i|\\upsilon_i&gt;-Z_i'\\gamma,X_i] \\\\\n\\end{aligned}\n\\]\nIn general, \\(E[\\varepsilon_i|\\upsilon_i&gt;-Z_i'\\gamma,X_i]\\neq 0\\). This means that the OLS estimator of,\n\\[\nY_i = X_i'\\beta + \\varepsilon_i \\qquad \\text{for}\\quad i:D_i = 1\n\\]\nis biased. However, if we know the (conditional) joint distribution of \\(\\begin{bmatrix}\\varepsilon_i & \\upsilon_i\\end{bmatrix}'\\) then we may be able to compute this bias and explicitly correct for it in the model. We will shortly see that the assumption of joint normality provides a relatively simple solution to the problem.\n\n\n3.3 Stochastic threshold model\nIn the above model, selection (into observation of \\(Y_i\\)) was determined by a separate latent variable to the main outcome. As a result, the correlation between \\(Y_i^*\\) and \\(D_i\\) (or \\(D_i^*\\)) depended on the unobserved joint error-term distribution. In a stochastic threshold model, the selection depends directly on \\(Y_i^*\\). \\[\nY_i = \\begin{cases}Y_i^* \\qquad \\text{if} \\quad Y_i^*&gt;S_i^* \\\\\n\\text{missing} \\qquad \\text{otherwise} \\end{cases}\n\\] where, \\[\nS_i^* = W_i'\\eta + \\nu_i\n\\] Thus, the indicator of observation is given by, \\[\nD_i = \\begin{cases}1 \\qquad \\text{if} \\quad Y_i^*&gt;S_i^* \\\\\n0 \\qquad \\text{otherwise}\\end{cases}\n\\] Selection depends on the realization of \\(\\varepsilon_i\\) and not just on \\(Cov(\\varepsilon_i,\\upsilon_i)\\) (where \\(\\upsilon_i\\) is the error term from the selection equation in the non-stochastic model).\nThe observed \\(Y_i\\) is given by, \\[\nY_i = \\begin{cases}X_i'\\beta + \\varepsilon_i \\qquad \\text{if} \\quad X_i'\\beta + \\varepsilon_i\\geq W_i'\\eta + \\nu_i \\\\\n\\text{missing} \\qquad \\text{otherwise} \\end{cases}\n\\] As before, consider the conditional mean of the observed outcome: \\[\n\\begin{aligned}\n&E[Y_i|D_i=1,X_i] \\\\\n=& E[Y_i^*|D_i=1,X_i] \\\\\n=&X_i'\\beta + E[\\varepsilon_i|D_i^*&gt;0,X_i] \\\\\n=&X_i'\\beta + E[\\varepsilon_i|(Y_i^*-S_i^*)&gt;0,X_i] \\\\\n=&X_i'\\beta + E[\\varepsilon_i|\\upsilon_i&gt;-Z_i'\\gamma,X_i]\n\\end{aligned}\n\\] where, \\[\n\\upsilon_i = \\varepsilon_i-\\nu_i \\qquad \\text{and}\\qquad Z_i'\\gamma = X_i'\\beta - W_i'\\eta\n\\] This result shows that the two models - non-stochastic and stochastic threshold models - are equivalent. This equivalence will have implications for the interpretation of the parameters.\nConsider, in the non-stochastic model, we have to consider \\(Cov(\\varepsilon_i,\\upsilon_i)\\). In the stochastic model, this is, \\[\nCov(\\varepsilon_i,\\upsilon_i) = Cov(\\varepsilon_i,\\varepsilon_i-\\nu_i) = Var(\\varepsilon_i)-Cov(\\varepsilon_i,\\nu_i)\n\\] If the \\(Cov(\\varepsilon_i,\\nu_i)=0\\) (i.e., no selection in the stochastic model), then \\(Cov(\\varepsilon_i,\\upsilon_i)&gt;0\\).\nThe equivalence of these models also implies that \\(Z_i\\) must contain all variables in either \\(X_i\\) or \\(W_i\\). However, some variables in \\(W_i\\) need not appear in \\(X_i\\). There may be some variables in the selection equation that do not appear in the main equation. These are referred to as excluded variables.\nIn both threshold models, selection is determined by both observables (\\(W_i\\)) and unobservables (\\(\\upsilon_i\\)). Other methods, like propensity score matching assume that selection is determined by observables alone. Instrumental variable approaches allow for selection on both, but do require an excluded instrument.\n\n\n3.4 Endogenous Switching Model\nConsider a model where there are two latent outcomes \\(\\{Y_{1i}^*,Y_{2i}^*\\}\\):\n\\[\n\\begin{aligned}\nY_{1i}^* =& X_i'\\beta_1 + \\varepsilon_{1i} \\\\\nY_{2i}^* =& X_i'\\beta_2 + \\varepsilon_{2i}\n\\end{aligned}\n\\]\nFor example, these could be wage equations from two sectors. Observation within either state is then determined by another latent variable \\(D_i^*\\):\n\\[\nY_i = \\begin{cases}Y_{1i}^* \\qquad \\text{if} \\quad D_i^*&gt;0 \\\\\nY_{2i}^* \\qquad \\text{otherwise} \\end{cases}\n\\]\nwhere,\n\\[\nD_i^* = Z_i\\gamma + \\theta (Y_{1i}^*-Y_{2i}^*) + \\zeta_i\n\\] You could extent this to more than 2 states; but that makes modelling selection a little more complex. In this simple set-up, the observed outcome is given by, \\[\nY_i = \\begin{cases}X_i'\\beta_1 + \\varepsilon_{1i} \\qquad \\text{if} \\quad Z_i\\gamma + \\theta (Y_{1i}^*-Y_{2i}^*) + \\zeta_i&gt;0 \\\\\nX_i'\\beta_2 + \\varepsilon_{2i} \\qquad \\text{otherwise} \\end{cases}\n\\]\nAn important limitation within this model is the absence of any state-specific covariates in either the \\(\\{Y_{1i}^*,Y_{2i}^*\\}\\) latent models or \\(D_i^*\\) selection model. Selection can only depend on the difference in the latent outcomes.\nFor each individual, we only observe one of the outcomes, which is to say there is a “missing counterfactual”. This is a similar structure then to the potential outcomes framework discussed in Handout 8. Although, with important differences. Here, each covariate has a state-specific vector \\(\\beta_m\\).\nThe conditional mean of the observed outcome in each state is given by:\n\\[\n\\begin{aligned}\n&E[Y_{1i}|D_i=1,X_i] \\\\\n=& E[Y_{1i}^*|D_i=1,X_i] \\\\\n=&X_i'\\beta_1 + E[\\varepsilon_{1i}|D_i^*&gt;0,X_i] \\\\\n=&X_i'\\beta_1 + E[\\varepsilon_{1i}|\\zeta_i&gt;-Z_i'\\gamma-\\theta (Y_{1i}^*-Y_{2i}^*),X_i]\n\\end{aligned}\n\\] for state 1, and for state 2: \\[\n\\begin{aligned}\n&E[Y_{2i}|D_i=0,X_i] \\\\\n=& E[Y_{2i}^*|D_i=0,X_i] \\\\\n=&X_i'\\beta_2 + E[\\varepsilon_{2i}|D_i^*\\leq0,X_i] \\\\\n=&X_i'\\beta_2 + E[\\varepsilon_{2i}|\\zeta_i\\leq -Z_i'\\gamma-\\theta (Y_{1i}^*-Y_{2i}^*),X_i]\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "handout-7.html#estimation",
    "href": "handout-7.html#estimation",
    "title": "Endogenous Selection Models",
    "section": "4 Estimation",
    "text": "4 Estimation\nThe goal is to estimate the \\(\\beta\\) parameters from the latent model under various observation mechanisms.\n\\[\nY_i^* = X_i^*\\beta + \\varepsilon_i\n\\]\nHowever, we can only ever use the observed outcome \\(Y_i\\) and \\(D_i\\) (indicator of selection/observation).\n\n4.1 Tobit model\nRecall, the conditional mean of the observed (non-censored) outcome variable. Applying the definition of the IMR, we have,\n\\[\n\\begin{aligned}\nE[Y_{i}|Y_i^*&gt;c,X_i] =&X_i'\\beta + E[\\varepsilon_{i}|\\varepsilon_i&gt;c-X_i'\\beta,X_i] \\\\\n=&X_i'\\beta + \\sigma_\\varepsilon E\\bigg[\\frac{\\varepsilon_i}{\\sigma_\\varepsilon}\\bigg|\\frac{\\varepsilon_i}{\\sigma_\\varepsilon}&gt;\\frac{c-X_i'\\beta}{\\sigma_\\varepsilon},X_i\\bigg] \\\\\n=&X_i'\\beta + \\sigma_\\varepsilon \\lambda\\bigg(\\frac{X_i'\\beta-c}{\\sigma_\\varepsilon}\\bigg)\n\\end{aligned}\n\\] This equation can be estimated by OLS using a two-step estimator (see of Heckman correction below). However, the more efficient estimator is Maximum Likelihood. The joint likelihood is given by,\n\\[\n\\begin{aligned}\nL_n(\\theta) =&\\prod_{i:D_i=1}f^*(Y_i|X_i;\\theta)\\prod_{i:D_i=0}F^*(c_i|X_i;\\theta) \\\\\n  =&\\prod_{i=1}^n\\bigg(\\frac{1}{\\sigma_\\varepsilon}\\phi\\bigg(\\frac{Y_i-X_i'\\beta}{\\sigma_\\varepsilon}\\bigg)\\bigg)^{D_i}\\Phi\\bigg(\\frac{c-X_i'\\beta}{\\sigma_\\varepsilon}\\bigg)^{1-D_i}\n  \\end{aligned}\n\\] where \\(\\theta = [\\beta,\\sigma_\\varepsilon]\\) and \\(F^*(c_i|X_i;\\theta) = Pr(Y_i\\leq c|X_i)\\).\nNotice, the first part of the likelihood function looks like the likelihood of the CLRM (from Handout 3) while the second part is similar to a probit model likelihood function for \\(D_i=0\\) (from Handout 6). One issue with the likelihood function is that it is not globally concave (see Tobit II model).\n\n\n4.2 Threshold model\nHeckman (1979) put forward a novel solution for the estimation of sample selection models. The approach adds a generated-regressor to the (linear) estimating equation that corrects for the endogenous selection. Conditional on observation, the error term in the observed model will not be mean zero. The approach is called a control function approach and means that the bias corrected model can be estimated using OLS and not ML (which was computationally demanding at the time).\nRecall from the endogenous selection models (stochastic or non-stochatic), that the conditional mean of the observed outcome was,\n\\[\nE[Y_i|D_i=1,X_i] = X_i'\\beta + E[\\varepsilon_i|\\upsilon_i&gt;-Z_i'\\gamma,X_i]\n\\] Suppose, \\[\n\\begin{bmatrix}\\varepsilon_i \\\\ \\upsilon_i\\end{bmatrix}|X_i\\sim N\\bigg(\\begin{bmatrix}0 \\\\ 0\\end{bmatrix},\\begin{bmatrix}\\sigma^2_{\\varepsilon} & \\sigma_{\\varepsilon\\upsilon} \\\\ \\sigma_{\\varepsilon\\upsilon} & 1\\end{bmatrix}\\bigg)\n\\] We need to normalize the variance of \\(\\upsilon_i\\) as it is not identified. Recall, \\(\\upsilon_i\\) is associated with the discrete outcome \\(D_i\\) that indicates observation. Just as in a probit model, the variance of the latent variable model error term is not identified.\nGiven, the joint normality assumption: \\[\nE[Y_i|D_i=1,X_i] = X_i'\\beta + \\sigma_{\\varepsilon\\upsilon}\\lambda(Z_i'\\gamma)\n\\]\nThe Heckman two-step estimator estimates the adjusted model,\n\\[\nY_i = X_i'\\beta + \\sigma_{\\varepsilon\\upsilon}\\hat{\\lambda}_i + \\varepsilon_i \\qquad \\text{for}\\quad i:D_i = 1\n\\] where,\n\nEstimate a probit model of \\(D_i\\), using variables \\(Z\\) (those variables in \\(X_i\\) and \\(W_i\\)) and construct the IMR:\n\n\\[\n\\hat{\\lambda}_i = \\lambda(Z_i'\\hat{\\gamma}) = \\frac{\\phi(Z_i'\\hat{\\gamma})}{\\Phi(Z_i'\\hat{\\gamma})}\n\\]\n\nEstimate the linear model with the added IMR using the observed (selected) sample.\n\nRecall, if \\(\\sigma_{\\varepsilon\\upsilon}=0\\) in the non-stochastic model, selection into observation is unrelated to the latent outcome. Hence, \\(H_0:\\sigma_{\\varepsilon\\upsilon}=0\\) is a valid test for the selection, provided the model is non-stochastic.\nSince the second step includes a generated-regressor, the default vairance estimator will be incorrect. This is standard problem with two-step estimators, like two-stage-least-squares for instrumental variables.\nIdentifcation relies on the non-linearity of the IMR. The IMR can be approximately linear for some values which means that the identification depends on there being a significant share of observations in the tail where the IMR is non-linear. In practice, many researchers will include squared terms of the IMR. Identification does NOT rely on an excluded variable: a variable in the selection equation that does not appear in the main equation. However, it will help to have an excluded variable.\nHeckman’s control function approach removed the need to use a Maximum Likelihood. However, the ML estimator is more efficient. The joint likelihood is given by,\n\\[\n\\begin{aligned}\nL_n(\\theta) =&\\prod_{i:D_i=1}f^*(Y_i|D_i=1,Z_i;\\theta)\\cdot Pr(D_i=1|Z_i)\\prod_{i:D_i=0}Pr(D_i=0|Z_i) \\\\\n=& \\prod_{i:D_i=1} f^*(Y_i|Z_i;\\theta)\\cdot Pr(D_i=1|Y_i,Z_i)\\prod_{i:D_i=0}Pr(D_i=0|Z_i)\\\\\n=&\\prod_{i=1}^n\\bigg[\\frac{1}{\\sigma_\\varepsilon}\\phi\\bigg(\\frac{Y_i-X_i'\\beta}{\\sigma_\\varepsilon}\\bigg)\\cdot\\Phi\\bigg(\\frac{Z_i'\\gamma+\\sigma_{\\varepsilon\\upsilon}\\sigma_{\\varepsilon}^{-2} (Y_i-X_i'\\beta)}{\\sqrt{1-\\sigma_{\\varepsilon\\upsilon}^2\\sigma_{\\varepsilon}^{-2}}}\\bigg)\\bigg]^{D_i}(1-\\Phi(Z_i'\\gamma)\\big)^{1-D_i}\n\\end{aligned}\n\\] where \\(\\theta=[\\beta,\\gamma,\\sigma_\\varepsilon,\\sigma_{\\varepsilon\\upsilon}]\\). Line two follows from Bayes’ rule. The second term in the third row derives from:\n\\[\n\\begin{aligned}\nPr(D_i=1|Y_i,Z_i) =& Pr(\\upsilon_i&gt;-Z_i'\\gamma|Y_i,Z_i) \\\\\n=& Pr(\\sigma_{\\varepsilon\\upsilon}/\\sigma_{\\varepsilon}^2\\varepsilon_i+ \\zeta_i &gt;-Z_i'\\gamma|Y_i,Z_i) \\\\\n=& Pr(\\zeta_i &gt;-Z_i'\\gamma-\\sigma_{\\varepsilon\\upsilon}/\\sigma_{\\varepsilon}^2\\varepsilon_i|Y_i,Z_i) \\\\\n=& \\Phi\\bigg(\\frac{Z_i'\\gamma+\\sigma_{\\varepsilon\\upsilon}\\sigma_{\\varepsilon}^{-2} (Y_i-X_i'\\beta)}{\\sqrt{1-\\sigma_{\\varepsilon\\upsilon}^2\\sigma_{\\varepsilon}^{-2}}}\\bigg)\n\\end{aligned}\n\\] This is because of the joint normality of the errors, which allows us to write,\n\\[\n\\upsilon_i = \\sigma_{\\varepsilon\\upsilon}/\\sigma_{\\varepsilon}^2\\varepsilon_i+ \\zeta_i \\qquad \\text{where}\\quad \\zeta_i\\sim N(0,1-\\sigma^2_{\\varepsilon\\upsilon}/\\sigma_{\\varepsilon}^2)\n\\] Conditional on \\(Y_i^*\\), \\(\\varepsilon_i\\) is not random, which is why is moved to the right-handside and is substituted with \\(\\varepsilon_i=Y_i-X_i'\\beta\\).\nNotice, if \\(\\sigma_{\\varepsilon\\upsilon}=0\\), then likelihood of selection and observing \\(Y_i\\) are independent of one another.\n\n\n4.3 Observational equivalence\nIn the non-stochastic threshold model, \\(Cov(\\varepsilon_i,\\upsilon_i)=\\sigma_{\\varepsilon\\upsilon}=0\\) implies there is not selection-bias in the main equation. However, in the stochastic model \\(\\sigma_{\\varepsilon\\upsilon}&gt;0\\) cannot be zero; since \\(\\upsilon_i = \\varepsilon_i-\\nu_i\\)\n\\[\nCov(\\varepsilon_i,\\upsilon_i) = Cov(\\varepsilon_i,\\varepsilon_i-\\nu_i) = Var(\\varepsilon_i)-Cov(\\varepsilon_i,\\nu_i)\n\\] In the stochastic model, when there is no selection if \\(Cov(\\varepsilon_i,\\nu_i)=0\\). However, this implies that \\(\\sigma_{\\varepsilon\\upsilon}=\\sigma_\\varepsilon\\). As a result,\n\\[\nE[Y_i|D_i=1,X_i] = X_i'\\beta + \\sigma_{\\varepsilon\\upsilon}\\lambda(Z_i'\\gamma) = X'\\beta + \\sigma_\\varepsilon\\lambda(X_i'\\beta)\n\\]\nThe stochastic threshold model returns a Tobit model when there is no selection.\nWe have a puzzle! You cannot distinguish the stochastic and non-stochastic threshold models. The test for selection - \\(H_0:\\sigma_{\\varepsilon\\upsilon}=0\\) - using the coefficient on the IMR is only valid under the non-stochastic case, since \\(\\sigma_{\\varepsilon\\upsilon}&gt;0\\) in the stochastic case. This means that if the stochastic model is valid, there is no test for selection. And if there is no selection, in the stochastic model, the model is essentially a tobit model.\nThis puzzle suggests that all three models are empirically indistinguishable. You need to make an economic argument for why one model is valid."
  },
  {
    "objectID": "handout-7.html#interpretation",
    "href": "handout-7.html#interpretation",
    "title": "Endogenous Selection Models",
    "section": "5 Interpretation",
    "text": "5 Interpretation\nIn the above models, any regressor affects expected value of the observed outcome through two potential channels: (1) the direct effect; (2) the selection effect.\nConsider, there is the effect of \\(X_i\\) on the latent outcome:\n\\[\n\\frac{\\partial E[Y_i^*|X_i]}{\\partial X_i} = \\beta\n\\]\nThen, there is the effect of \\(Z_i\\) (which includes \\(X_i\\), but also potential excluded variables) on selection (into observation):\n\\[\n\\frac{\\partial Pr(D_i=1|Z_i)}{\\partial Z_i}\n\\]\nThen, there is the effect of \\(X_i\\) conditional on selection (observation),\n\\[\n\\frac{\\partial E[Y_i|X_i,D_i=1]}{\\partial X_i}\n\\] and the total effect of \\(X_i\\) on the observed outcome:\n\\[\n\\frac{\\partial E[Y_i|X_i]}{\\partial X_i}\n\\]\n\n5.1 Tobit model\nWe can use the Law of Iterated Expectations to expand the conditional mean of \\(E[Y_i|X_i]\\). Let where \\(\\omega_i= \\frac{X_i'\\beta-c}{\\sigma_\\varepsilon}\\), then\n\\[\n\\begin{aligned}\nE[Y_i|X_i] =& E[Y_i|X_i,D_i=1]\\cdot Pr(D_i=1|X_i)+E[Y_i|X_i,D_i=0]\\cdot Pr(D_i=0|X_i) \\\\\n=&E[Y_i|X_i,D_i=1]\\cdot Pr(D_i=1|X_i) + c\\cdot Pr(D_i=0|X_i) \\\\\n=&\\big[X_i'\\beta + \\sigma_\\varepsilon \\lambda(\\omega_i)\\big]\\cdot \\Phi(\\omega_i)+c\\cdot \\big[1-\\Phi(\\omega_i)\\big]\n\\end{aligned}\n\\] Recall, in this case \\(Pr(D_i=1|X_i)=Pr(Y_i^*&gt;c|X_i)\\). Naturally, \\(c=0\\) simplifies the expression.\nThe relevant marginal effects are:\n\nconditional on observation/selection:\n\n\\[\n\\begin{aligned}\n\\frac{\\partial E[Y_i|D_i=1,X_i]}{\\partial X_i} =& \\big[1 + \\lambda'(\\omega_i)\\big]\\beta \\\\\n=& \\big[1 -\\omega_i\\cdot\\lambda(\\omega_i)-\\lambda(\\omega_i)^2\\big]\\beta\n\\end{aligned}\n\\] You can verify this result using the definition of the IMR.\n\nobservation/selection:\n\n\\[\n\\frac{\\partial Pr(Y_i=1|X_i)}{\\partial X_i} = \\phi(\\omega_i)\\beta/\\sigma_\\varepsilon\n\\]\n\n\n5.2 Threshold model\nLet us assume that all \\(X_i\\) regressors are included in \\(Z_i\\), which we know should be the case given the observational equivalence of the stochastic and non-stochastic models. The above marginal effects for these selection models is given by,\n\nconditional on selection: \\[\n\\begin{aligned}\n\\frac{\\partial E[Y_i|D_i=1,Z_i]}{\\partial X_i} =& \\beta + \\sigma_{\\varepsilon\\upsilon}\\lambda'(Z_i'\\gamma)\\tilde{\\gamma} \\\\\n=&\\beta -\\sigma_{\\varepsilon\\upsilon}\\big[Z_i'\\gamma\\cdot\\lambda(Z_i'\\gamma) + \\lambda(Z_i'\\gamma)^2\\big]\\tilde{\\gamma}\n\\end{aligned}\n\\] where \\(\\tilde{\\gamma}\\) refers to the subset of \\(\\gamma\\)-vector corresponding to the \\(X_i\\) regressors that appear in \\(Z_i\\). That is, it excludes the parameters on any excluded variables.\nselection:\n\n\\[\n\\frac{\\partial Pr(D_i=1|Z_i)}{\\partial Z_i} = \\phi(Z_i'\\gamma)\\gamma\n\\]\nRecall, in the threshold model the variance of the selection equation is not identified. Hence, the selection margin effect resembles that of a probit model. In the Tobit model, selection depends on the outcome itself and the variance can be identified from the part of the data where \\(Y\\) is observed."
  },
  {
    "objectID": "handout-5.html",
    "href": "handout-5.html",
    "title": "Linear Panel Data Models",
    "section": "",
    "text": "In this handout we will see how to test static linear panel data regression models. We will review a number of estimators for these models, including a range of potential estimators:\n\npooled OLS;\nbetween-group;\nfeasible GLS;\nwithin-group;\nand first differences.\n\nFurther reading can be found in:\n\nSection 21 of Cameron and Trivedi (2005)\nSection 10.1-10.3 of Verbeek (2017)"
  },
  {
    "objectID": "handout-5.html#overview",
    "href": "handout-5.html#overview",
    "title": "Linear Panel Data Models",
    "section": "",
    "text": "In this handout we will see how to test static linear panel data regression models. We will review a number of estimators for these models, including a range of potential estimators:\n\npooled OLS;\nbetween-group;\nfeasible GLS;\nwithin-group;\nand first differences.\n\nFurther reading can be found in:\n\nSection 21 of Cameron and Trivedi (2005)\nSection 10.1-10.3 of Verbeek (2017)"
  },
  {
    "objectID": "handout-5.html#panel-data-regression-model",
    "href": "handout-5.html#panel-data-regression-model",
    "title": "Linear Panel Data Models",
    "section": "2 Panel Data Regression Model",
    "text": "2 Panel Data Regression Model\nThe basic, linear panel-data-regression model is given by,\n\\[\n  Y_{it} = X_{it}'\\beta + \\alpha_i + \\varepsilon_{it}\n\\]\nfor \\(i=1,..,n\\) and \\(t = 1,...,T\\). By basic, we mean static. The contrast would be dynamic panel data models, which include lag(s) (and/or leads) of the outcome variable on the right-hand side. In dynamic models, the long-run (equilibrium) relationship between outcome and regressors differs from the short-run (or contemporaneous) relationship. This is the result of including a lagged dependent variable. This static model, can incorporate lags (and/or leads) of regressors; although, you may then need to consider carefully the exogeneity assumption if strict exogeneity does not apply.\nFor the purposes of this discussion, we will treat \\(T\\) as fixed. As \\(n\\) increases, \\(T\\) remains fixed, implying that the asymptotics concern only \\(n\\).\nIf collect all \\(T\\) observations of unit \\(i\\), we can describe them by the model,\n\\[\n  Y_{i} = X_{i}\\beta + \\alpha_i\\ell + \\varepsilon_i\n\\] where \\(Y_i\\) is a \\(T\\times 1\\) random vector; \\(X_i\\) a \\(T\\times k\\) random matrix; and \\(\\ell\\) a \\(T\\times 1\\) vector of \\(1\\)’s.\nThis model has placed no restriction on the values of the outcome variable, \\(Y_i\\), and regressors, \\(X_i\\). In particular, the regressors may include time-varying as well as time-invariant variables.\nWe can extend this specification to include a linear or non-linear trend in time; for example, \\(\\phi t\\). However, the more common option is to include a very flexible time trend using fixed effects:\n\\[\n  \\delta_t = \\sum_{j=1}^T \\delta_j \\mathbf{1}\\{t = j\\}\n\\] Time fixed-effects are essentially a dummy variable for each time-period. This flexible function - sometimes referred to as saturated - can approximate any functional form of the underlying time trend. Models that include both \\(\\alpha_i\\) and \\(\\delta_t\\) are referred to as two-way fixed-effects models.\n\n2.1 Unobserved heterogeneity\nThe term \\(\\alpha_i\\) is particularly important. It represents an unobserved individual effect or unobserved heterogeneity. I strongly recommend you read the discussion on pages 285-286 of Wooldridge (2010). You should not refer to \\(\\alpha_i\\) as individual fixed-effects.\nThe language of fixed-effects comes from models where \\(\\alpha_i\\) is a unit-specific (population) parameter. As a parameter, it is by definition non-random. This would imply that the correlation between \\(\\alpha_i\\) and \\(X_i\\) is by definition 0. Here \\(\\alpha_i\\) is an unobserved random variable. Wooldridge (2010) refers to it as an unobserved effects model (UEM). The term random effects model is often used to describe UEM models where the individual effect is (mean) independent.\nThe \\(\\alpha_i\\) is unobserved, and therefore is a component of the composite error term \\(\\upsilon_{it} = \\alpha_i + \\varepsilon_{it}\\). The time-invariant component (\\(\\alpha_i\\)) of the error is permanent, while the time-varying component (\\(\\varepsilon_{it}\\)) is transient; also referred to as the idiosyncratic error.\nGiven the model, we must consider all three components on the right-hand side - \\(\\{X_i,\\alpha_i,\\varepsilon_{it}\\}\\) - when making assumptions regarding exogeneity. We will make all assumptions conditional on \\(\\alpha_i\\). This is consistent with the idea that \\(\\alpha_i\\) is a permanent shock, and is therefore realized before \\(\\varepsilon_it\\).\nIf we do not condition on \\(\\alpha_i\\), then when we evaluate \\(E[Y_i|X_i]\\), we also need to consider both \\(E[\\varepsilon_i|X_i]\\) and \\(E[\\alpha_i|X_i]\\); not just \\(E[\\varepsilon_i|X_i,\\alpha_i]\\). Moreover, in general it will be that case that \\(E[\\alpha_i|X_i]\\neq E[\\alpha_i]\\) (i.e., not mean independent). For example, \\(\\alpha_i\\) may represent unobserved ability in a wage equation. This will correlated with regressors like education."
  },
  {
    "objectID": "handout-5.html#exogeneity",
    "href": "handout-5.html#exogeneity",
    "title": "Linear Panel Data Models",
    "section": "3 Exogeneity",
    "text": "3 Exogeneity\nHaving assumed that the samples are independent across \\(i\\), we can define mean independence of the transient error term component for unit \\(i\\). There are three potential assumptions we can make:\n\nStrict exogeneity:\n\n\\[\n  E[\\varepsilon_i|X_i,\\alpha_i] = 0\n\\] or, \\[\n  E[\\varepsilon_{it}|X_{i1},X_{i2},...,X_{iT},\\alpha_i] = 0\\qquad \\forall\\;t\n\\] 2. Weak or sequential exogeneity:\n\\[\n  E[\\varepsilon_{it}|X_{i1},X_{i2},...,X_{it},\\alpha_i] = 0\\qquad \\forall\\;t\n\\] Exogeneity with respect to the past sequence of regressors (or predetermined regressors).\n\nContemporaneous exogeneity:\n\n\\[\n  E[\\varepsilon_{it}|X_{it},\\alpha_i] = 0\\qquad \\forall\\;t\n\\] Exogeneity only with respect to the contemporaneous value of \\(X_i\\).\n\n3.1 Strict exogeneity\nStrict exogeneity is a very strong assumption. It implies that \\(X\\) is uncorrelated with past, current, and future values of the transient error term (conditional on \\(\\alpha_i\\)): \\(E[X_{it},\\varepsilon_{is}|\\alpha_i] = 0\\;\\forall\\;t,s\\). Crucially, \\(X_{it}\\) cannot respond to the history of idiosyncratic shocks \\(\\varepsilon_{i1},\\varepsilon_{i2},...,\\varepsilon_{it}\\).\nUnder strict exogeneity,\n\\[\nE[Y_{it}|X_i,\\alpha_i] = E[Y_{it}|X_{it},\\alpha_i] = X_{it}'\\beta + \\alpha_i\n\\] The first equality implies that once you control for \\(X_{it}\\), there is no additional partial effect of \\(X_{is}\\) (\\(\\forall\\;s\\neq t\\)) on (the mean of) \\(Y_{it}\\). This assumption relates to the assumed static nature of the model: the model includes not lags (or leads) of the dependent variable.\n\n\n3.2 Weak exogeneity\nWeak exogeneity, also referred to as sequential exogeneity, implies that the error term is uncorrelated with past and contemporaneous values of the regressors:\n\\[\nE[X_{is}\\varepsilon_{it}] = 0 \\qquad\\forall\\;s=1,...,t\n\\] In the static linear model, it also implies that,\n\\[\nE[Y_{it}|X_i,\\alpha_i] = E[Y_{it}|X_{it},\\alpha_i] = X_{it}'\\beta + \\alpha_i\n\\]\nThis structure can permit a lagged dependent variable amongst the regressors (assuming there is no serial correlation of the error term). However, the assumption remains violated if the regressors are endogenous.\n\n\n3.3 Contemporaneous exogeneity\nThis assumption implies that the error term is only uncorrelated with regressors in the same time period,\n\\[\nE[X_{it}\\varepsilon_{it}] = 0\n\\] Regardless, it still implies that, \\[\nE[Y_{it}|X_i,\\alpha_i] = E[Y_{it}|X_{it},\\alpha_i] = X_{it}'\\beta + \\alpha_i\n\\]"
  },
  {
    "objectID": "handout-5.html#static-linear-panel-model",
    "href": "handout-5.html#static-linear-panel-model",
    "title": "Linear Panel Data Models",
    "section": "4 Static Linear Panel Model",
    "text": "4 Static Linear Panel Model\nWe begin by describing the core assumptions underlying the static (or basic) panel data regression model (SLPM) assumptions. Many of these will be familiar to you\nSLPM 1: The model is static and linear in parameters with a composite error term made up of a time-invariant and time-varying component:\n\\[\nY_{it} = X_{it}'\\beta + \\alpha_i + \\varepsilon_{it}\n\\]\nSLPM 2: Strict exogeneity: \\(E[\\varepsilon_{it}|X_i,\\alpha_i] = 0\\; \\forall\\;t\\)\nSLPM 3: Conditional homoskedasticity and serial uncorrelatedness of the transient error term component.\n\\[\nVar(\\varepsilon_i|X_i,\\alpha_i) = \\begin{bmatrix}\\sigma^2_\\varepsilon & 0 & \\cdots & 0 \\\\ 0 & \\sigma^2_\\varepsilon & & \\\\ \\vdots & & \\ddots & \\\\ 0 & & & \\sigma^2_\\varepsilon \\end{bmatrix} = \\sigma^2_\\varepsilon I_T\n\\]\nTogether, assumptions 2 & 3 are referred to as a ‘classical error term’ structure. Of course, since the model is linear, we need a rank condition. Without placing any additional assumptions on the variation of \\(X_{it}\\), we need to assume that:\nSLPM 4: \\(rank(X) = k\\)\nNext, we need to consider sampling.\nSLPM 5: Independent sampling along the cross-sectional dimension (\\(i\\)).\nSLPM 6: Balanced panel: we observe each \\(i\\) in all \\(T\\) time periods.\nUnder assumptions 1-6:\n\n\\(E[Y_i|X_i,\\alpha_i]  = X_i\\beta + \\alpha_i\\ell\\)\n\\(Var(Y_i|X_i,\\alpha_i) = \\sigma^2_{\\varepsilon}I_T\\)\n\nAt this stage, the unanswered question is how to deal with the unobservables \\(\\alpha_i\\) in the equation. Generally, there are two approaches:\n\nassume away the relationship between \\(\\alpha_i\\) and \\(X_i\\);\nor remove \\(\\alpha_i\\) from the equation prior to estimator.\n\nAdopting approach (1), we will review the pooled OLS, between-group, and (feasible) Generalized Least Squares (GLS) estimators. Under approach (2), will review the within-group, fixed effects, and first-difference estimators."
  },
  {
    "objectID": "handout-5.html#pooled-ols",
    "href": "handout-5.html#pooled-ols",
    "title": "Linear Panel Data Models",
    "section": "5 Pooled OLS",
    "text": "5 Pooled OLS\nWe can ‘assume away’ the relationship between \\(\\alpha_i\\) and \\(X_i\\). Specifically, we will assume conditional mean independence:\n\nSLPM 7 \\(E[\\alpha_i|X_i] = E[\\alpha_i|X_{i1},X_{i2},...,X_{iT}] = E[\\alpha_i]=0\\)\n\nAs in the CLRM, the assumption that the unconditional mean of \\(\\alpha_i\\) is zero is not binding given the inclusion of a constant among the regressors. This also implies uncorrelatedness: \\(E[X_i\\alpha_i]=0\\). Under this assumptions, the model can be written as,\n\\[\n  Y_{it} = X_{it}'\\beta + \\upsilon_{it}\n\\] where \\(E[\\upsilon_{i}|X_i]=0\\). Alternatively, we can stack all \\(nT\\) observations together,\n\\[\n  Y = X\\beta + \\upsilon\n\\]\nSecond, we need to make an assumption regarding the variance of the unobserved heterogeneity:\n\nSLPM 8 \\(Var(\\alpha_i|X_i) = \\sigma^2_\\alpha\\)\n\nUnder these assumptions, the error term \\(\\upsilon_i = \\alpha_i\\ell + \\varepsilon_i\\) has the variance,\n\\[\nVar(\\upsilon_i |X_i) = E[\\upsilon_i\\upsilon_i' |X_i] = \\sigma^2_\\alpha\\ell\\ell' + \\sigma^2_\\varepsilon I_T = \\Sigma\n\\] For all \\(s\\neq t\\) the \\(E[\\upsilon_{it},\\upsilon_{is}] = \\sigma^2_\\alpha\\). And the diagonal elements are given by \\(\\sigma^2_\\alpha + \\sigma^2_\\varepsilon\\).\nUnder these assumptions, the OLS estimator,\n\\[\n\\begin{aligned}\n\\hat{\\beta}^{OLS} =& (X'X)^{-1}X'Y \\\\\n=& \\beta + (X'X)^{-1}X'\\upsilon \\\\\n=& \\beta + \\big(\\sum_iX_i'X_i\\big)^{-1}\\sum_iX_i'\\upsilon_i\n\\end{aligned}\n\\] is both unbiased and consistent. This is because,\n\\[\np \\lim\\frac{1}{n}\\sum_iX_i'\\upsilon_i = \\sum_{t=1}^Tp \\lim\\frac{1}{n}\\sum_{i=1}^nX_{it}\\upsilon_{it} = \\sum_{t=1}^TE[X_{it}\\upsilon_{it}]=0\n\\] The asymptotic distribution is given by,\n\\[\n\\begin{aligned}\n\\sqrt{n}(\\hat{\\beta}^{OLS}-\\beta) =& \\bigg(\\frac{1}{n}\\sum_iX_i'X_i\\bigg)^{-1}\\frac{1}{\\sqrt{n}}\\sum_iX_i'\\upsilon_i \\\\\n\\rightarrow_d& N(0,V^{-1}\\Omega V^{-1})\n\\end{aligned}\n\\] where,\n\n\\(V = E[X_i'X_i]\\)\n\\(\\Omega = E[X_i'\\Sigma X_i]\\)\n\nWe say that the approximate distribution of the pooled OLS estimator is given by,\n\\[\n\\hat{\\beta}^{OLS} \\overset{a}{\\sim} N\\bigg(\\beta, \\big(\\sum_iX_i'X_i\\big)^{-1}\\sum_iX_i'\\Sigma X_i\\big(\\sum_iX_i'X_i\\big)^{-1}\\bigg)\n\\]\nNote, the variance is not homoskedastic. You must therefore estimate heteroskedastic (or clustered) standard errors. The usual homoskedastic estimator for the variance will be biased and inconsistent. Unobserved heterogeneity will result in a serial correlation across the error terms that is not accountant for by the standard estimator.\nFor this reason, pooled OLS is NOT efficient."
  },
  {
    "objectID": "handout-5.html#between-group",
    "href": "handout-5.html#between-group",
    "title": "Linear Panel Data Models",
    "section": "6 Between Group",
    "text": "6 Between Group\nAn alternative to pooled OLS, is to collapse the multiple observations of unit \\(i\\) into a single cross-section aggregate. This transforms the model into,\n\\[\n\\bar{Y}_{i} = \\bar{X}_{i}'\\beta+\\bar{\\upsilon}_{i}\n\\] where \\(\\bar{\\upsilon}_{i} = \\alpha_i + \\bar{\\varepsilon}_i\\). The variance of this error term is,\n\\[\nE[\\bar{\\upsilon}_i^2|X_i] = \\sigma^2_\\alpha + \\frac{\\sigma^2_\\varepsilon}{T}\n\\] The OLS estimator for \\(\\beta\\) is given by,\n\\[\n\\hat{\\beta}^{BG} = (\\bar{X}'\\bar{X})^{-1}\\bar{X}'\\bar{Y} = \\big(\\sum_i\\bar{X}_i\\bar{X}_i'\\big)^{-1}\\sum_i\\bar{X}_i\\bar{Y}_i\n\\]\nSince the variance term is now homoskedastic, the standard homoskedastic variance estimator will unbiased and consistent. This approach removes the problem of serially correlated error terms across repeated observations of \\(i\\) in pooled OLS by collapsing all observations to a single observation. However, it also reduces the information in the data and is therefore less efficient."
  },
  {
    "objectID": "handout-5.html#generalized-least-squares",
    "href": "handout-5.html#generalized-least-squares",
    "title": "Linear Panel Data Models",
    "section": "7 Generalized Least Squares",
    "text": "7 Generalized Least Squares\nThe efficient solution is to account for the error term structure in the estimation using Generalized Least Squares. The structure of the composite error-term variance-covariance matrix is,\n\\[\n\\Sigma = \\begin{bmatrix}\n\\sigma^2_\\alpha+\\sigma^2_\\varepsilon & \\sigma^2_\\alpha & \\cdots & \\sigma^2_\\alpha \\\\ \\sigma^2_\\alpha & \\sigma^2_\\alpha+\\sigma^2_\\varepsilon &  & \\\\\n\\vdots & & \\ddots & \\\\\n\\sigma^2_\\alpha &  &  & \\sigma^2_\\alpha+\\sigma^2_\\varepsilon\n\\end{bmatrix}\n\\] The \\(nT\\times nT\\) matrix, \\(E[\\upsilon\\upsilon' |X]\\), is a block-diagonal matrix in which the off-diagonal values are \\(E[\\upsilon_{it}\\upsilon_{js} |X]=\\sigma^2_\\alpha\\) only for \\(i=j\\) and \\(s\\neq t\\); and zero otherwise. We can describe this matrix using a Kronecker-product operator:\n\\[\nE[\\upsilon\\upsilon' |X] = \\begin{bmatrix}\n\\Sigma & 0 & \\cdots & 0 \\\\\n0 & \\Sigma &  & \\\\\n\\vdots & & \\ddots & \\\\\n0 &  &  & \\Sigma\n\\end{bmatrix} = I_{n}\\otimes \\Sigma  \n\\] In this \\(nT\\times nT\\) matrix, eacg off-diagonal element is a \\(T\\times T\\) matrix of 0’s, representing the cross-unit (\\(i\\)) covariance terms.\nThe Generalized Least Squares solution solves the following least squares problem:\n\\[\n\\hat{\\beta}^{GLS} = \\underset{b}{\\arg \\min} \\quad (Y-Xb)'(I_n\\otimes\\Sigma^{-1})(Y-Xb)\n\\] which can be written as,\n\\[\n\\hat{\\beta}^{GLS} = \\underset{b}{\\arg \\min} \\quad (Y^+-X^+b)'(Y^+-X^+b)\n\\] where \\([Y^+,X^+]=[\\Sigma^{-1/2}Y,\\Sigma^{-1/2}X]\\). In this instance, the net result of this linear transform of the model is give by,\n\\[\n\\underbrace{Y_{it}-\\theta \\bar{Y}_i}_{Y_{it}^+} = \\underbrace{(X_{it}-\\theta\\bar{X}_i)'}_{X_{it}^{+'}}\\beta + \\upsilon_{it}^+\n\\] where,\n\\[\n\\theta = 1- \\frac{\\sigma_\\varepsilon}{\\sqrt{T\\sigma^2_\\alpha+\\sigma^2_\\varepsilon}}\n\\]\nConsider the transformed error term \\(\\nu\\),\n\\[\n\\upsilon_{it}^+ = \\upsilon_{it}-\\theta\\bar{\\upsilon}_i = (1-\\theta)\\alpha_i + \\varepsilon_{it}-\\frac{\\theta}{T}\\sum_t\\varepsilon_{it}\n\\]\nThe serial correlation of this error term is 0. Consider, for \\(t\\neq s\\) \\[\n\\begin{aligned}\nE[\\upsilon_{it}^+\\upsilon_{is}^+|X_i] =& E\\big[\\big((1-\\theta)\\alpha_i + \\varepsilon_{it}-\\frac{\\theta}{T}\\sum_{t'}\\varepsilon_{it'}\\big)\\big((1-\\theta)\\alpha_i + \\varepsilon_{is}-\\frac{\\theta}{T}\\sum_{t'}\\varepsilon_{it'}\\big)|X_i\\big] \\\\\n=&(1-\\theta)^2\\sigma^2_\\alpha  -2\\frac{\\theta}{T}\\sigma^2_\\varepsilon+\\frac{\\theta^2}{T^2}\\sum_{t'}\\sigma^2_\\varepsilon \\\\\n=&\\frac{\\sigma^2_\\varepsilon \\sigma^2_\\alpha}{T\\sigma^2_\\alpha+\\sigma^2_\\varepsilon} + \\frac{\\theta(\\theta-2)}{T}\\sigma^2_\\varepsilon \\\\\n=&\\frac{\\sigma^2_\\varepsilon \\sigma^2_\\alpha}{T\\sigma^2_\\alpha+\\sigma^2_\\varepsilon} - \\frac{\\sigma^2_\\varepsilon}{T}\\bigg(1-\\frac{\\sigma_\\varepsilon}{\\sqrt{T\\sigma^2_\\alpha+\\sigma^2_\\varepsilon}}\\bigg)\\bigg(1+\\frac{\\sigma_\\varepsilon}{\\sqrt{T\\sigma^2_\\alpha+\\sigma^2_\\varepsilon}}\\bigg) \\\\\n=& \\frac{\\sigma^2_\\varepsilon \\sigma^2_\\alpha}{T\\sigma^2_\\alpha+\\sigma^2_\\varepsilon}-\\frac{\\sigma^2_\\varepsilon}{T}\\bigg(1-\\frac{\\sigma^2_\\varepsilon}{T\\sigma^2_\\alpha+\\sigma^2_\\varepsilon}\\bigg) \\\\\n=&\\frac{\\sigma^2_\\varepsilon \\sigma^2_\\alpha}{T\\sigma^2_\\alpha+\\sigma^2_\\varepsilon}-\\frac{\\sigma^2_\\varepsilon \\sigma^2_\\alpha}{T\\sigma^2_\\alpha+\\sigma^2_\\varepsilon} \\\\\n=&0\n\\end{aligned}\n\\] The GLS estimator is then given by,\n\\[\n  \\hat{\\beta}^{GLS} = \\big[\\sum_iX_i^{+'}X_i^+\\big]^{-1}\\sum_iX_i^{+'}Y_i^+\n\\]\nYou can show that \\(\\hat{\\beta}^{GLS}\\) is a weighted average of \\(\\hat{\\beta}^{BG}\\) and \\(\\hat{\\beta}^{WG}\\) (see below). Also, take note of the fact that as \\(T\\rightarrow\\infty\\), \\(\\theta\\rightarrow 1\\). Thus, \\(\\hat{\\beta}_{GLS}\\rightarrow \\hat{\\beta}^{WG}\\) as \\(T\\rightarrow\\infty\\). In addition, if if \\(\\sigma^2_\\alpha=0\\), then \\(\\theta = 0\\) and \\(\\hat{\\beta}^{GLS}=\\hat{\\beta}^{OLS}\\), the pooled OLS estimator.\nHowever, this estimator is NOT feasible. This is because we do not observe \\(\\{\\sigma^2_\\alpha,\\sigma^2_\\varepsilon\\}\\).\n\n7.1 Feasible GLS\nA feasible version of the GLS estimator is given by the following steps:\n\nEstimate \\(\\sigma^2_\\varepsilon\\) using the WG estimator (see below).\nUse the pooled OLS or BG estimator then estimate \\(\\sigma^2_\\alpha\\), using the value of \\(\\sigma^2_\\varepsilon\\) from step 1. For example, the RSS from pooled OLS (divided by \\(nT-k\\)) is a consistent estimator for \\(\\sigma^2_\\varepsilon+\\sigma^2_\\alpha\\). Similar, the RSS from BG estimator (divided by \\(n-k\\)) is a consistent estimator for \\(\\sigma^2_\\varepsilon/T+\\sigma^2_\\alpha\\).\nUsing the estimated \\(\\{\\hat{\\sigma}^2_\\alpha,\\hat{\\sigma}^2_\\varepsilon\\}\\), compute the transformed model (using \\(\\hat{\\theta}\\)) and estimate using \\(\\hat{\\beta}^{FGLS}\\) using OLS.\n\nIn Stata, this estimator is referred to as the random effects estimtor within the xtreg package: xtreg , re."
  },
  {
    "objectID": "handout-5.html#within-group",
    "href": "handout-5.html#within-group",
    "title": "Linear Panel Data Models",
    "section": "8 Within Group",
    "text": "8 Within Group\nThe second approach to dealing with unobserved heterogeneity is to transform the model in such a way that \\(\\alpha_i\\) is eliminated. Having done so, we do not need to make any assumption regarding \\(E[\\alpha_i|X_i]\\).\nHere we will exploit the fact that \\(\\alpha_i\\) is time-invariant. We begin by computing the unit-level average of the model. For the left-hand side,\n\\[\n\\bar{Y}_i = \\frac{1}{T}\\sum_{t=1}^T Y_{it}\n\\] and the right-hand side, \\[\n\\frac{1}{T}\\sum_{t=1}^T \\big(X_{it}'\\beta + \\alpha_i + \\varepsilon_{it}\\big) = \\bar{X}_{i}'\\beta + \\alpha_i + \\bar{\\varepsilon}_{i}\n\\] Next, subtract this from each value, to create a demeaned expression\n\\[\n\\underbrace{Y_{it}-\\bar{Y}_i}_{\\tilde{Y}_{it}} = \\underbrace{(X_{it}-\\bar{X}_i)'\\beta}_{\\tilde{X}_{it}'\\beta}+\\underbrace{\\alpha_{i}-\\alpha_i}_{=0}+ \\underbrace{\\varepsilon_{it}-\\bar{\\varepsilon}_i}_{\\tilde{\\varepsilon}_{it}}\n\\] The permanent error-term component drops out precisely because it is time-invariant. The transformed model is given by,\n\\[\n\\tilde{Y}_{it} = \\tilde{X}_{it}'\\beta+\\tilde{\\varepsilon}_{it}\n\\]\nThis model can be estimated by OLS. The solution is given by,\n\\[\n\\hat{\\beta}^{WG} = (\\tilde{X}'\\tilde{X})^{-1}\\tilde{X}'\\tilde{Y} = (\\sum_{i=1}^n\\tilde{X}_i'\\tilde{X}_i)^{-1}\\sum_{i=1}^n\\tilde{X}_i'\\tilde{Y}_i\n\\] where \\(\\tilde{X}_i\\) is a \\(T\\times k\\) matrix. Note, this is different to the standard cross-section expression.\nThis solution assumes that the \\(k\\times k\\) matrix \\(\\sum_{i=1}^n\\tilde{X}_i'\\tilde{X}_i\\) is invertible. We must therefore make a modified rank assumption,\n\nSLPM 4’: \\(rank(\\tilde{X})=k\\)\n\nGiven that \\(\\tilde{X}\\) is the within-group demeaned vale of \\(X\\), this implies the regressors must all be time-varying. In addition, if the model includes a time trend (including time-FEs), the included variables cannot vary uniformly with time. An example of this is age. If all units’ age values increase by the same amount each period, then\n\\[\n\\tilde{age}_{it} = age_{it}-\\bar{age}_i = t - \\bar{t}\n\\] This is because an individuals age can be expressed as a time-invariant value of their year of birth \\(yob_i\\) plus a linear time-trend that has a unit-specific intercept. The demeaned value of age is perfectly colinear with a demeaned linear time-trend. It would also be perfectly colinear with a higher-order polynomial time-trend or year FEs.\n\\[\n  \\tilde{X}_i = \\begin{bmatrix}\n  \\tilde{X}_{i11} & \\tilde{X}_{i12} & \\cdots & \\tilde{X}_{i1k} \\\\\n  \\tilde{X}_{i21} & \\tilde{X}_{i22} &  & \\\\\n  \\vdots &  & \\ddots & \\\\\n  \\tilde{X}_{iT1} &  &  & \\tilde{X}_{iTk}\n  \\end{bmatrix} = \\begin{bmatrix}\\tilde{X}_{i1}' \\\\ \\tilde{X}_{i2}' \\\\ \\vdots \\\\ \\tilde{X}_{iT}'\\end{bmatrix}\n\\]\n\\(\\tilde{X}_i'\\tilde{X}_i\\) is therefore a \\(k\\times k\\) matrix, which can be expressed as,\n\\[\n\\tilde{X}_i'\\tilde{X}_i = \\sum_{t=1}^T \\tilde{X}_{it}\\tilde{X}_{it}' = \\begin{bmatrix} \\sum_{t}\\tilde{X}_{it1}^2 & \\sum_{t}\\tilde{X}_{it1}\\tilde{X}_{it2} & \\cdots & \\sum_{t}\\tilde{X}_{it1}\\tilde{X}_{itk} \\\\ \\sum_{t}\\tilde{X}_{it2}\\tilde{X}_{it1} & \\sum_{t}\\tilde{X}_{it2}^2 &  & \\\\ \\vdots & & \\ddots & \\\\ \\sum_{t}\\tilde{X}_{itk}\\tilde{X}_{it1} & & & \\sum_{t}\\tilde{X}_{itk}^2 \\end{bmatrix}\n\\]\nFinally, we can express \\(\\tilde{X}'\\tilde{X}\\) as,\n\\[\n\\tilde{X}'\\tilde{X} = \\sum_{i=1}^n \\tilde{X}_i'\\tilde{X}_i = \\begin{bmatrix} \\sum_i\\big(\\sum_{t}\\tilde{X}_{it1}^2\\big) & \\sum_i\\big(\\sum_{t}\\tilde{X}_{it1}\\tilde{X}_{it2}\\big) & \\cdots & \\sum_i\\big(\\sum_{t}\\tilde{X}_{it1}\\tilde{X}_{itk}\\big) \\\\ \\sum_i\\big(\\sum_{t}\\tilde{X}_{it2}\\tilde{X}_{it1}\\big) & \\sum_i\\big(\\sum_{t}\\tilde{X}_{it2}^2\\big) &  & \\\\ \\vdots & & \\ddots & \\\\ \\sum_i\\big(\\sum_{t}\\tilde{X}_{itk}\\tilde{X}_{it1}\\big) & & & \\sum_i\\big(\\sum_{t}\\tilde{X}_{itk}^2\\big) \\end{bmatrix}\n\\]\nWe can use the same method to describe \\(\\sum_{i=1}^n\\tilde{X}_i'\\tilde{Y}_i\\), a \\(k\\times 1\\) vector. Substituting in the definition of \\(\\tilde{Y}_i\\) from the transformed model, we get that,\n\\[\n\\hat{\\beta}^{WG} = \\beta +  (\\sum_{i=1}^n\\tilde{X}_i'\\tilde{X}_i)^{-1}\\sum_{i=1}^n\\tilde{X}_i'\\tilde{\\varepsilon}_i\n\\]\n\n8.1 Conditional variance\nGiven the demeaning of the model, the error term is no longer uncorrelated across \\(t\\) for the same \\(i\\):\n\\[\n\\begin{aligned}\nE[\\tilde{\\varepsilon}_{is}\\tilde{\\varepsilon}_{it}|X] =&E[(\\varepsilon_{is}-\\bar{\\varepsilon}_{i})(\\varepsilon_{it}-\\bar{\\varepsilon}_{i})|X] \\\\\n=&E\\bigg[\\bigg(\\varepsilon_{is}-\\frac{1}{T}\\sum_{s'}\\varepsilon_{is'}\\bigg)\\bigg(\\varepsilon_{it}-\\frac{1}{T}\\sum_{s'}\\varepsilon_{it'}\\bigg)\\bigg| X_i\\bigg] \\\\\n=&\\begin{cases} \\sigma^2_{\\varepsilon}(1-1/T) \\qquad \\text{for}\\quad s=t \\\\\n-\\sigma^2_{\\varepsilon}/T \\qquad \\text{for}\\quad s\\neq t\n\\end{cases}\n\\end{aligned}\n\\] The off-diagonal elements for the same \\(i\\) are the same for all time-periods. Together, this gives us the \\(T\\times T\\) matrix,\n\\[\n\\begin{aligned}\n\\Sigma=& E[\\tilde{\\varepsilon}_{i}\\tilde{\\varepsilon}_{i}'|X_i] \\\\\n=&\n\\begin{bmatrix}\n\\sigma^2_{\\varepsilon}(1-1/T) & -\\sigma^2_{\\varepsilon}/T & \\cdots & -\\sigma^2_{\\varepsilon}/T \\\\\n-\\sigma^2_{\\varepsilon}/T & \\sigma^2_{\\varepsilon}(1-1/T) & & \\\\\n\\vdots & & \\ddots & \\\\\n-\\sigma^2_{\\varepsilon}/T & & & \\sigma^2_{\\varepsilon}(1-1/T)\n\\end{bmatrix} \\\\\n=&\\sigma^{2}_\\varepsilon M_\\ell\n\\end{aligned}\n\\] where \\(M_\\ell = I_T-\\frac{\\ell\\ell'}{T}\\) for the \\(T\\times 1\\) vector of ones \\(\\ell\\). This follows from the fact that \\(\\tilde{\\varepsilon}_{i} = M_\\ell\\varepsilon_{i}\\). Which implies that,\n\\[\n\\begin{aligned}\nE[\\tilde{\\varepsilon}_{i}\\tilde{\\varepsilon}_{i}'|X_i] =& E[M_\\ell\\varepsilon_{i}\\varepsilon_{i}'M_\\ell'|X_i] \\\\\n=&M_\\ell E[\\varepsilon_{i}\\varepsilon_{i}'|X_i]M_\\ell \\\\\n=& M_\\ell\\sigma^2_\\varepsilon I_T M_\\ell \\\\\n=&\\sigma^{2}_\\varepsilon M_\\ell\n\\end{aligned}\n\\]\nThus, \\[\nE[\\tilde{\\varepsilon}\\tilde{\\varepsilon}'|X]=I_{n}\\otimes \\Sigma = \\sigma^{2}_\\varepsilon I_{n}\\otimes M_\\ell\n\\] where the \\(rank(I_{n}\\otimes M_\\ell)=nT-n\\). This follows from the fact that each \\(M_\\ell\\) has \\(rank(M_\\ell)=T-1\\). The matrix \\(M_{I_{n}\\otimes M_\\ell}\\) is a \\(nT\\times nT\\) matrix: a block diagonal matrix of \\(n\\) \\(M_\\ell\\) matrices. With this we can now solve for the conditional variance of the WG estimator.\n\\[\n\\begin{aligned}\nVar(\\hat{\\beta}^{WG}|X) =& (\\tilde{X}'\\tilde{X})^{-1}\\tilde{X}'E[\\tilde{\\varepsilon}\\tilde{\\varepsilon}'|X]\\tilde{X}(\\tilde{X}'\\tilde{X})^{-1} \\\\\n=& \\sigma^2_\\varepsilon(\\tilde{X}'\\tilde{X})^{-1}\\tilde{X}'(I_{n}\\otimes M_\\ell)\\tilde{X}(\\tilde{X}'\\tilde{X})^{-1} \\\\\n=& \\sigma^2_\\varepsilon(\\tilde{X}'\\tilde{X})^{-1}\n\\end{aligned}\n\\] The final line follows from the fact that \\(\\tilde{X}=(I_{n}\\otimes M_\\ell)X\\) and \\(I_{n}\\otimes M_\\ell\\) is, itself, an idempotent projection matrix.\nAn unbiased and consistent estimator for \\(\\sigma^2_\\varepsilon\\) is given by,\n\\[\n\\hat{\\sigma}^2_\\varepsilon = \\frac{RSS}{dof}=\\frac{\\sum_{i=1}^n\\sum_{t=1}^T\\big(Y_{it}-\\bar{Y}_i-(X_{it}-\\bar{X}_i)'\\hat{\\beta}^{WG}\\big)^2}{nT-n-k}\n\\] The residual degrees of freedom is equal to \\(nT-n-k\\), not \\(nT-k\\). While there are \\(nT\\) observations and \\(k\\) parameters, we must deduct the \\(n\\) unit-level means computed. Another way to see this is to use the above projection matrix decomposition. For the purpose of this decomposition, denote \\(I_{n}\\otimes M_\\ell=M_{n\\ell}\\) an idempotent (orthogonal) projection matrix. Then,\n\\[\n\\begin{aligned}\nRSS =& \\hat{\\varepsilon}'\\hat{\\varepsilon} \\\\\n=& \\tilde{Y}'(I_{nT}-\\tilde{X}(\\tilde{X}'\\tilde{X})^{-1}\\tilde{X}')\\tilde{Y} \\\\\n=& Y'M_{n\\ell}(I_{nT}-M_{n\\ell}X(X'M_{n\\ell}X)^{-1}X'M_{n\\ell})M_{n\\ell}Y \\\\\n=& Y'\\underbrace{(M_{n\\ell}-M_{n\\ell}X(X'M_{n\\ell}X)^{-1}X'M_{n\\ell})}_{M_+}Y\n\\end{aligned}\n\\] This new matrix \\(M_+\\) is also an idempotent projection matrix, with rank \\(rank(M_+) = rank(M_{n\\ell})-\\min\\{rank(M_{n\\ell}),rank(X)\\} = nT-n-k\\). As we saw with the CLRM, this term with have a \\(\\chi^2\\) distribution with dof equal to the rank of the projection matrix.\n\n\n8.2 Consistency and asymptotic distribution\nUnder assumptions SLPM 1-6, \\(\\hat{\\beta}^{WG}\\) is consistent,\n\\[\n\\hat{\\beta}^{WG}\\rightarrow_p \\beta \\qquad \\text{as}\\quad n\\rightarrow\\infty\n\\] and asymptotically normal,\n\\[\n\\hat{\\beta}^{WG}\\overset{a}{\\sim} N\\big(\\beta,\\sigma^2_{\\varepsilon}\\big(\\sum_i\\tilde{X}_i'\\tilde{X}_i\\big)^{-1}\\big)\n\\]\nBoth results require that \\(E[\\tilde{X}_i,\\tilde{\\varepsilon}_i]=0\\). This is maintained by strict exogeneity (CLPM 4), which states that in addition to \\(E[X_{is},\\varepsilon_{it}]=0\\;\\forall\\;s,t\\),\n\\[\n  E[\\bar{X}_i,\\bar{\\varepsilon}_i]=0\n\\] This would not be true under the weak exogeneity assumption.\n\n\n8.3 Fixed Effects\nIn Stata’s xtreg package, the WG estimator is referred to as the Fixed Effects (FE) estimator. This not not mean that \\(\\alpha_i\\) is non-random. It simply means that the WG estimator is equivalent to the OLS estimator for a (individual/unit) fixed effects model. Consider the model,\n\\[\nY_{it} = \\sum_{j=1}^n\\phi_j\\mathbf{1}\\{i = j\\}+X_{it}'\\beta + \\upsilon_{it}\n\\]\nThis model includes a dummy variable for each unit. For each unit, only one dummy variable can be =1, the dummy variable with parameter \\(\\phi_i\\). Thus, the expression \\(\\sum_{j=1}^n\\phi_j\\mathbf{1}\\{i = j\\}=\\phi_i\\) for any \\(i\\). The model can therefore be written as,\n\\[\nY_{it} = \\phi_i+X_{it}'\\beta + \\upsilon_{it}\n\\]\nThis looks very similar to our SLPM, but with the distinguishing feature that \\(\\phi_i\\) is taken as a non-random population parameter. This is sometimes referred as a unit-specific constant.\nIncluding a dummy variable each \\(i\\) has the same effect as demeaning the model prior to estimation (as with WG). This approach is referred to as the Least Squares Dummy Variable (LSDV) method.\n\\[\n\\hat{\\beta}^{WG} = \\hat{\\beta}^{LSDV}\n\\] This equivalence can be shown using Frisch Waugh Lovell Theorem (or partitioned regression). The LSDV estimator can be computed in two steps. First, regress each regressor and outcome on a setting of unit level dummies.\n\\[\nX_{k} = \\sum_{j=1}^n\\phi_j\\mathbf{1}\\{i = j\\} + \\xi\n\\] Each \\(\\mathbf{1}\\{i = j\\}\\) corresponds to a dummy variable where \\(T\\) values are =1 (for unit \\(i\\)), and the remainder 0. Next, use the residuals in the main equation. The residual from the regression is given by,\n\\[\n(I_n\\otimes M_\\ell)X_k  = \\tilde{X}_{k}\n\\] Thus, we regression is given by,\n\\[\n(I_n\\otimes M_\\ell)Y = \\tilde{Y} = (I_n\\otimes M_\\ell)X\\beta + (I_n\\otimes M_\\ell)\\varepsilon = \\tilde{X}\\beta + \\tilde{\\varepsilon}\n\\] Employing the LSDV approach, one can estimated the unit-FE as,\n\\[\n\\hat{\\phi}_i = \\bar{Y}_i - \\bar{X}_i'\\hat{\\beta}^{LSDV}\n\\]\nWhile this estimator is unbiased, \\(E[\\hat{\\phi}_i|X_i]=\\phi\\), it is NOT consistent for fixed \\(T\\). It is only consistent if \\(T\\rightarrow \\infty\\) as \\(n\\rightarrow \\infty\\).\nThe equivalene of these approaches also explains why the degrees of freedom in the residual is \\(nT-n-k\\). By including \\(n\\) dummy variables, the number of parameters we need to estimate is \\(n+k\\)."
  },
  {
    "objectID": "handout-5.html#first-difference",
    "href": "handout-5.html#first-difference",
    "title": "Linear Panel Data Models",
    "section": "9 First Difference",
    "text": "9 First Difference\nAs with the WG estimator, he first-difference (FD) estimator removes \\(\\alpha_i\\) from the model through differencing. However, this time the transformation is just a single difference:\n\\[\n\\underbrace{Y_{it}-Y_{it-1}}_{\\Delta Y_{it}} = \\underbrace{(X_{it}-X_{it-1})'\\beta}_{\\Delta X_{it}'\\beta}+\\underbrace{\\alpha_{i}-\\alpha_i}_{=0}+ \\underbrace{\\varepsilon_{it}-\\varepsilon_{it-1}}_{\\Delta \\varepsilon_{it}}\n\\] As a result, the estimation sample will include 1 less period: \\(t=2,...,T\\) for each \\(i\\). In addition, we must make a modified rank assumption,\n\nSLPM 4”: \\(rank(\\Delta X)=k\\)\n\nThis transformation can be describe using the linear transformation (i.e. matrix) \\(D\\):\n\\[\nD = \\begin{bmatrix}-1 & 1 & 0& 0 & \\cdots & 0 \\\\\n0 & -1 & 1& 0 & \\cdots & 0 \\\\\n0 & 0 & -1& 1 &  &  \\\\\n\\vdots & \\vdots & & \\ddots & \\ddots &  \\\\\n0 & 0 & &  & -1 & 1 \\\\\n\\end{bmatrix}\n\\] \\(D\\) is a \\((T-1)\\times T\\) matrix. When applied to \\(Y_i\\) or \\(X_i\\), it reduces the number of observations by 1. Note, we assume here that the data is sorted by \\(t\\). Thus, \\(DX_i\\) is a \\((T-1)\\times k\\) matrix of first-differences. The model can therefore be expressed as,\n\\[\nDY_i = DX_i\\beta+D\\varepsilon_{i}\n\\]\nUnder SLPM 1-6, \\(\\hat{\\beta}^{FD}\\) is consistent and asympotitcally normal. However, we need to account for the error term structure. This is because,\n\\[\nE[\\Delta \\varepsilon_{is}\\Delta \\varepsilon_{it}|X_i] =\\begin{cases}E[(\\varepsilon_{is}-\\varepsilon_{is-1})(\\varepsilon_{it}-\\varepsilon_{it-1})|X_i] = 2\\sigma^2_{\\varepsilon} \\quad \\text{for}\\;s=t \\\\\nE[(\\varepsilon_{is}-\\varepsilon_{is-1})(\\varepsilon_{it}-\\varepsilon_{it-1})|X_i] = -\\sigma^2_{\\varepsilon} \\quad \\text{for}\\;s=t-1 \\\\\n0 \\quad \\text{otherwise}\\end{cases}\n\\]\nThis is a MA(1) error term structure, in which the first-order correlation is non-zero. Using the linear transformation \\(D\\), we can express this as,\n\\[\nE[D \\varepsilon_{i}(D \\varepsilon_{i})'|X_i] = \\sigma^2_{\\varepsilon}DD'\n\\]\n\n9.1 OLS vs GLS\nThe OLS estimator is given by,\n\\[\n\\hat{\\beta}^{FD} = \\bigg(\\sum_{i=1}^n(DX_i)'DX_i\\bigg)^{-1}\\sum_{i=1}^n(DX_i)'DY_i\n\\] For \\(T=2\\), you can show that,\n\\[\n\\hat{\\beta}^{FD}=\\hat{\\beta}^{WG}\n\\]\nAs scene above, the error term is not homoskedastic, which makes this estimator inefficient. The efficient GLS estimator is given by,\n\\[\n\\hat{\\beta}^{GLS} = \\underset{b}{\\arg \\min} \\sum_{i=1}^n (DY_i-DX_ib)'(\\sigma^2_\\varepsilon DD')^{-1}(DY_i-DX_ib)\n\\] Since the scalar \\(\\sigma^2_\\varepsilon\\) can be ignored, the GLS solution is given by,\n\\[\n\\hat{\\beta}^{GLS} = \\bigg(\\sum_{i=1}^nX_i'D'(DD')^{-1}DX_i\\bigg)^{-1}\\sum_{i=1}^nX_i'D'(DD')^{-1}DY_i\n\\] It turns out that,\n\\[\nD'(DD')^{-1}D = M_\\ell\n\\] This result holds for \\(T\\geq 2\\). The GLS estimator for first differences is equivalent to the Within-Group estimator."
  },
  {
    "objectID": "handout-5.html#wu-hausman-test",
    "href": "handout-5.html#wu-hausman-test",
    "title": "Linear Panel Data Models",
    "section": "10 Wu-Hausman Test",
    "text": "10 Wu-Hausman Test\nThe Wu-Hausman test is used to test the exogeneity assumption underlying a particular estimator. You need two estimators: \\(\\{\\hat{\\beta}_1, \\hat{\\beta}_2\\}\\) such that,\n\nUnder \\(H_0: \\beta_1=\\beta_2\\)\n\\(\\hat{\\beta}_1\\) is consistent\n\\(\\hat{\\beta}_2\\) is consistent\n\\(Var(\\hat{\\beta}_1|X)&lt;Var(\\hat{\\beta}_2|X)\\): the former is more efficient\nUnder \\(H_1:  \\beta_1\\neq\\beta_2\\)\n\\(\\hat{\\beta}_1\\) is inconsistent\n\\(\\hat{\\beta}_2\\) is consistent\n\nThe test statistic is given by,\n\\[\n  \\text{Stat} = (\\hat{\\beta}_2-\\hat{\\beta}_1)'\\big(Var(\\hat{\\beta}_2-\\hat{\\beta}_1|X)\\big)^{-1}(\\hat{\\beta}_2-\\hat{\\beta}_1)\n\\] Under \\(H_0\\), this statistic converges in distribution to \\(\\chi_k\\), where \\(k\\) is the number of regressors. The inner matrix is the inverse of the variance-covariance matrix.\n\\[\n\\begin{aligned}\nVar(\\hat{\\beta}_2-\\hat{\\beta}_1|X) =& Var(\\hat{\\beta}_2|X) + Var(\\hat{\\beta}_1|X)-2 Cov(\\hat{\\beta}_2,\\hat{\\beta}_1|X) \\\\\n=&Var(\\hat{\\beta}_2|X)-Var(\\hat{\\beta}_1|X)\n\\end{aligned}\n\\] Line 2 follows from line 1, because of a result demonstrated by Hausman (1978):\n\\[\n0 = Cov(\\hat{\\beta}_1,\\hat{\\beta}_1-\\hat{\\beta}_2) = Var(\\hat{\\beta}_1)-Cov(\\hat{\\beta}_1,\\hat{\\beta}_2)\n\\] This result only holds in cases where the variances of the respectively estimators can be ranked: i.e. one estimator is more efficient than the other. As a result, the \\(Var(\\hat{\\beta}_2-\\hat{\\beta}_1|X)\\) matrix is positive definite.\nThis test can be applied in this setting to test the null: \\(H_0: E[X_{it}\\alpha_i] = 0\\) (i.e. uncorrelatedness). This condition must hold for the (F)GLS estimator to be consistent. If \\(H_0\\) is false, then we know that the WG estimator is consistent, but that it is less efficient. To test this hypothesis we use the coefficients from the FGLS and WG estimators. Of course, this means that you can only test for restrictions on time-varying regressors (a restriction of WG estimator).\n\n10.1 Mundlack correction\nThe Mundlack correction provides a way of including time-invariant variables in a WG estimator, and therefore also in a Hausman test. Consider, the within-group transformation gives us the model,\n\\[\n\\tilde{Y}_{i} = \\tilde{X}_{i}\\beta+\\tilde{\\varepsilon}_{i}\n\\] which can be written as,\n\\[\nM_\\ell Y_{i} = M_\\ell X_{i}\\beta+ M_\\ell \\varepsilon_{i}\n\\] given that \\(M_\\ell\\) is the orthogonal projection matrix that demeans each variable. It turns out that the OLS estimator for \\(\\beta\\) in the above expression is equivalent to the OLS estimator from the equation,\n\\[\nY_{i} = X_{i}\\beta+ \\bar{X}_i\\ell \\gamma + \\varepsilon_{i}\n\\] Where \\(\\bar{X}_i\\ell=P_\\ell X_i\\), the (individual-specific) mean of each variable. Demeaning each variable is equivalent to controlling for the mean.\nWe can demonstrate this result using partitioned regression. In this new equation, there are two sets of regressors: \\(X_i\\) and \\(\\bar{X}_i\\ell\\) (the unit-specific mean of each \\(X\\)). We know that the OLS estimator for \\(\\beta\\) can be arrived at by first regressing \\(X_i\\) on \\(\\bar{X}_i\\ell\\), \\[\nX_{i} = \\bar{X}_i\\ell\\eta + \\nu_{i}\n\\] and then regressing the residual from this equation on \\(Y\\). The residual from this expression is given by the orthogonal projection of \\(\\bar{X}_i\\ell=P_\\ell X_i\\). In this case, tt is sufficient to consider the orthogonal projection for unit \\(i\\),\n\\[\n\\begin{aligned}\n  &(I_T-P_\\ell X_i(X_i'P_\\ell P_\\ell X_i)^{-1}X_i'P_\\ell)X_i \\\\\n  =& X_i - P_\\ell X_i(X_i' P_\\ell X_i)^{-1}X_i'P_\\ell X_i \\\\\n  =&(I_T-P_\\ell)X_i \\\\\n  =& M_\\ell X_i\n\\end{aligned}  \n\\] Thus, partitioned regression says that the Mundlack correction will get the same OLS estimator as the regression of,\n\\[\n  Y_i = M_\\ell X_i \\beta + \\epsilon_{i}\n\\] which is equivalent, in terms of its OLS projection to,\n\\[\nM_\\ell Y_{i} = M_\\ell X_{i}\\beta+ M_\\ell \\varepsilon_{i}\n\\] Why does this matter? It means that we can estimate a model of the form, \\[\nY_{it} = X_{it}'\\beta + W_i'\\psi + \\bar{X}_i\\gamma + \\epsilon_{it}\n\\] that includes time invariant regressors \\(W_i\\), and gives us the same OLS estimates for \\(\\beta\\) as \\(\\hat{\\beta}^{WG}\\). Since this model, with time invariant regressors, can also be estimated using FGLS (under the assumption \\(E[\\alpha_i|X_i]=0\\)), we can conduct a Hausman test that compares the estimates of both \\(\\beta\\) and \\(\\psi\\)."
  },
  {
    "objectID": "handout-3.html",
    "href": "handout-3.html",
    "title": "Estimation Methods",
    "section": "",
    "text": "In this handout we will look at several approaches to generate estimators:\n\nLeast Squares\nMethod of Moments\nMaximum Likelihood\n\nWe will discuss each approach in the context of the Classical Linear Regression Model discussed in Lecture 1. You may also wish to revise the notes on Linear Algrebra.\nFurther reading can be found in:\n\nSection 5.6 of Cameron and Trivedi (2005)\nSection 6.1 of Verbeek (2017)"
  },
  {
    "objectID": "handout-3.html#overview",
    "href": "handout-3.html#overview",
    "title": "Estimation Methods",
    "section": "",
    "text": "In this handout we will look at several approaches to generate estimators:\n\nLeast Squares\nMethod of Moments\nMaximum Likelihood\n\nWe will discuss each approach in the context of the Classical Linear Regression Model discussed in Lecture 1. You may also wish to revise the notes on Linear Algrebra.\nFurther reading can be found in:\n\nSection 5.6 of Cameron and Trivedi (2005)\nSection 6.1 of Verbeek (2017)"
  },
  {
    "objectID": "handout-3.html#review-of-clrm",
    "href": "handout-3.html#review-of-clrm",
    "title": "Estimation Methods",
    "section": "2 Review of CLRM",
    "text": "2 Review of CLRM\nThe classical linear regression model is states that the conditional expectation function \\(E[Y_i|X_i]\\) is linear in parameters.\nFor the random sample \\(i=1,...,n\\),\n\\[\nY_i = X_i'\\beta + u_i\n\\] where \\(X_i\\) is a random k-dimensional vector (k-vector) and \\(\\beta\\) a non-random k-vector of population parameters. Both \\(Y_i\\) and \\(u_i\\) are random scalars.\nAs we saw, we can stack each observation into a column vector:\n\\[\nY =\\underbrace{\\begin{bmatrix}Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix}}_{n\\times 1} = \\underbrace{\\begin{bmatrix}X_1'\\beta \\\\ X_2'\\beta \\\\ \\vdots \\\\ X_n'\\beta\\end{bmatrix}}_{n\\times 1}  + \\underbrace{\\begin{bmatrix}u_1 \\\\ u_2 \\\\ \\vdots \\\\ u_n\\end{bmatrix}}_{n\\times 1}  \n= \\underbrace{\\begin{bmatrix}X_{11} & X_{12} & \\cdots & X_{1k}\\\\ X_{21} & X_{22} && \\\\ \\vdots & & \\ddots & \\\\ X_{n1} & & & X_{nk} \\end{bmatrix}}_{n\\times k} \\underbrace{\\begin{bmatrix}\\beta_1 \\\\ \\beta_2 \\\\ \\vdots \\\\ \\beta_k\\end{bmatrix}}_{k\\times 1} + \\begin{bmatrix}u_1 \\\\ u_2 \\\\ \\vdots \\\\ u_n\\end{bmatrix} = X\\beta + u\n\\]\nwhere \\(X\\) is now a \\(n\\times k\\) random matrix, but \\(\\beta\\) remains a non-random k-vector of population parameters.\n\n\n\n\n\n\nData as a Matrix\n\n\n\nIf you have any experience working datasets in Stata/R, you will know that they tend to have a rectangular structure: each row typically represents an observation and each column a variable. This is the structure depicted above in matrix notation: each row of the \\(X\\) matrix depicts an observation and each column a regressor. The dataset you are using contains is a matrix of observations for both the outcome variable and regressors: \\([Y,X,]\\). Of course, we do not observe the error term.\n\n\nIn this section, we will employ the CLRM assumptions as we discuss the three approaches to estimation."
  },
  {
    "objectID": "handout-3.html#ordinary-least-squares",
    "href": "handout-3.html#ordinary-least-squares",
    "title": "Estimation Methods",
    "section": "3 (Ordinary) Least Squares",
    "text": "3 (Ordinary) Least Squares\nThe Ordinary Least Squares (OLS) estimator is the ‘work-horse’ of applied economics research.1 It is not the only Least Squares estimator, but as the simplest case is the most useful place to start. Other Least Squares estimators include Weighted Least Squares (WLS), (Feasible) Generalized Least Squares (GLS), Non-Linear Least Squares and Two-Stage Least Squares (2SLS).\nOLS is “ordinary” in the sense that there is no additional features to the method. For example, WLS applies a unique weight to each observation while OLS weights each observation equally. While OLS is arguably ‘vanilla’ in this way, it is efficient (as we shall see).\nIn general, LS estimators minimize a measure of ‘distance’ between the observed outcomes and the fitted values of the model. The measure of distance is sum of squared deviations (squared Euclidean or \\(\\ell_2\\) norm).2\n\n3.1 Application to CLRM\nIn the case of OLS estimator to the CLRM, the goal is to find the \\(b\\)-vector that minimizes,\n\\[\n\\sum_{i=1}^n(Y_i-\\underbrace{\\tilde{Y}_i}_{X_i'b})^2 = \\sum_{i=1}^n\\tilde{u}_i^2\n\\] This sum-of-squares can be written as the inner product of two-vectors:3\n\\[\n\\sum_{i=1}^n\\tilde{u}_i^2 = \\tilde{u}'\\tilde{u} = (Y-Xb)'(Y-Xb)\n\\]\nApplying the rules of matrix transposition, the inner product of these two matrices is given by,\n\\[\n(Y'-b'X')(Y-Xb) = Y'Y -b'X'Y-Y'Xb+b'X'Xb\n\\] Since all terms are scalars, \\(b'X'Y=Y'Xb\\); which then gives us, \\[\nY'Y -2b'X'Y+b'X'Xb\n\\] Thus, the (ordinary) least-squares estimator the vector that solves this linear expression. \\[\n\\hat{\\beta}^{OLS} = \\underset{b}{\\text{arg min}}\\quad Y'Y -2b'X'Y+b'X'Xb\n\\]\nUsing the rules of vector differentiation (see Linear Algrebra) we can find the first order conditions:\n\\[\n-2X'Y +2X'X\\hat{\\beta}^{OLS}= 0\n\\] If the \\(X\\) matrix is full rank (=\\(k\\)), then \\(X'X\\) is non-singular and its inverse exists. Recall, this was one of the CLRM assumptions. Then, \\[\n\\hat{\\beta}^{OLS} = (X'X)^{-1}X'Y\n\\]\n\n\n3.2 Bias\nIs the OLS estimator unbiased? The answer will depend on the assumption of the model. Here, we have assumed that the model being estimated is a CLRM. This means that we have assumed conditional mean independence of the error term:\n\\[\nE[u|X] = 0\n\\] The OLS-estimator is given by,\n\\[\n\\hat{\\beta}^{OLS} = (X'X)^{-1}X'Y = (X'X)^{-1}X'(X\\beta+u) = \\underbrace{(X'X)^{-1}X'X}_{I_n}\\beta+(X'X)^{-1}X'u=\\beta+(X'X)^{-1}X'u\n\\] plugging in the definition of \\(Y\\) from the model.\nHence,\n\\[\nE[\\hat{\\beta}^{OLS}|X] = E[\\beta+(X'X)^{-1}X'u|X] = \\beta+E[(X'X)^{-1}X'u|X]\n\\] Here we apply the linearity of the expectation operator and the factor that \\(\\beta\\) is a non-random vector. Next, we exploit the fact that conditional on \\(X\\), any function of \\(X\\) is non-random and can come out of the expectation operator.\n\\[\nE[\\hat{\\beta}^{OLS}|X] = \\beta+(X'X)^{-1}X'\\underbrace{E[u|X]}_{=0} = \\beta\n\\]\nNotice, we require the stronger assumption of conditional mean independence, \\(E[u|X]=0\\). Uncorrelateness, \\(E[X'u]=0\\), is insufficient for unbiasedness.\n\n\n\n\n\n\nImportant\n\n\n\nNotice, unbiasedness depends on the assumptions of the model and not any properties of the estimator. The estimator is simply a calculation using observed data. The properties and interpretation of this computation depend on the assumptions we make regarding the underlying model.\n\n\nIt is also worth noting that the unbiasedness of the OLS estimator does NOT depend on any assumptions regarding the variance or distribution of the error term.\n\n\n3.3 Efficiency\nThe OLS estimator is a \\(k\\)-dimensional random vector. The variance of this vector is a \\(k\\times k\\) variance-covariance matrix.\n\\[\nVar(\\hat{\\beta}) = E\\big[\\underbrace{(\\hat{\\beta}-E[\\hat{\\beta}])}_{k\\times 1}\\underbrace{(\\hat{\\beta}-E[\\hat{\\beta}])'}_{1\\times k}\\big]\n\\] The off-diagonals of the matrix are the covariances: \\(Cov(\\hat{\\beta}_j,\\hat{\\beta}_k)\\) for \\(j\\neq k\\).\nWe have just shown that \\(E[\\hat{\\beta}]=\\beta\\) and\n\\[\n\\hat{\\beta}^{OLS} -\\beta=(X'X)^{-1}X'u\n\\]\nThus, the (conditional) variance of this estimator is then given by,\n\\[\n\\begin{aligned}\nVar(\\hat{\\beta}^{OLS}|X) =& E\\big[(X'X)^{-1}X'uu'X(X'X)^{-1}|X\\big] \\\\\n=& (X'X)^{-1}X'E[uu'|X]X(X'X)^{-1} \\\\\n=& (X'X)^{-1}X'Var(u|X)X(X'X)^{-1}\n\\end{aligned}\n\\]\nThe variance of the estimator depends on the variance of the error term, the unexplained part of the model. In order to any further expressions for this variance calculation, we need to go back to the model. What assumptions did we make concerning the variance in the CLRM?\nUnder the assumption CLRM 3 of homoskedasticity,\n\\[\nVar(u|X) = \\sigma^2 I_n = \\begin{bmatrix}\\sigma^2& 0 & \\cdots & 0 \\\\\n0 & \\sigma^2 & & \\\\\n\\vdots & & \\ddots & \\\\\n0 & & & \\sigma^2\\end{bmatrix}\n\\]\nthe above expression simplies to\n\\[\n\\begin{aligned}\nVar(\\hat{\\beta}^{OLS}|X) =& (X'X)^{-1}X'\\sigma^2I_nX(X'X)^{-1} \\\\\n=&\\sigma^2(X'X)^{-1}X'X(X'X)^{-1} \\\\\n=&\\sigma^2(X'X)^{-1}\n\\end{aligned}\n\\]\nIf we made a different assumption of heteroskedasticty (CLRM 3), then\n\\[\nVar(u|X) = \\begin{bmatrix}\\sigma^2_1& 0 & \\cdots & 0 \\\\\n0 & \\sigma^2_2 & & \\\\\n\\vdots & & \\ddots & \\\\\n0 & & & \\sigma^2_n\\end{bmatrix} = \\Omega\n\\]\nthe variance matrix does not reduce to a scalar multiplied by the identity matrix. And,\n\\[\nVar(\\hat{\\beta}^{OLS}|X) = (X'X)^{-1}X'\\Omega X(X'X)^{-1}\n\\]\nThis is commonly referred to as a ‘sandwich’ formula, given the way \\(Var(u|X)=\\Omega\\) is sandwiched between two linear transformations. The Eicker-Huber-White estimator for heteroskedastic standard errors of \\(\\hat{\\beta}^{OLS}\\) replaces \\(Var(u|X)=E[uu'|X]\\) with \\(\\hat{u}\\hat{u}'\\), the OLS residuals.\n\n\n3.4 Finite-sample distribution\nThe finite sample distribution of the OLS estimator depends on the assumptions of the model. Under CLRM 5,\n\\[\nu|X \\sim N(0,\\sigma^2 I_n)\n\\] And we have already shown that the OLS estimator is simply a linear transformation of the error term, \\[\n\\hat{\\beta}^{OLS}=\\beta+(X'X)^{-1}X'u\n\\]\nThen, using the properties of the Normal distribution4\n\\[\n\\hat{\\beta}^{OLS}|X=N\\big(\\beta,\\sigma^2(X'X)^{-1}\\big)\n\\] assuming homoskedasticity. With heteroskedastic variance, you simply change the variance, as the assumption has no implications for biasedness.\n\n\n3.5 Consistency\nRecall from Lecture 2 that an estimator is consistent if it converges in probability to the parameter. In this case, we want to show that\n\\[\n\\hat{\\beta}^{OLS}\\rightarrow_p \\beta\\qquad\\text{as}\\qquad n\\rightarrow \\infty\n\\] Using the derivation \\(\\hat{\\beta}^{OLS} -\\beta=(X'X)^{-1}X'u\\), we need to show that \\((X'X)^{-1}X'u \\rightarrow_p 0\\). To emphasize the fact that \\(\\hat{\\beta}\\) is a function of the sample size, I am going to switch to the notation \\(\\hat{\\beta}_n\\) for this section.\nFor the consistency of the OLS estimator we require a few assumptions,\n\nCLRM 1: linear in parameters\nCLRM 2b: uncorrelatedness, \\(E[X_iu_i]=0\\)\nCLRM 4: \\(rank(X) = k\\)\nCLRM 6: data is iid\n(NEW) CLRM 7: \\(E[X_iX_i']\\) is a finite, positive-definite matrix.\n\nWe begin by re-writing the expression, \\(\\hat{\\beta}^{OLS}=\\beta+(X'X)^{-1}X'u\\) in summation notation and then scaling by \\(n\\),\n\\[\n\\hat{\\beta}_n=\\beta+\\bigg(\\sum_{i=1}^nX_iX_i'\\bigg)^{-1}\\sum_{i=1}^nX_iu_i = \\beta+\\bigg(n^{-1}\\sum_{i=1}^nX_iX_i'\\bigg)^{-1}n^{-1}\\sum_{i=1}^nX_iu_i\n\\]\nBy the WLLN,5\n\\[\nn^{-1}\\sum_{i=1}^nX_iu_i\\rightarrow_p E[X_1u_1] = 0\n\\]\nSimilarly, by WLLN, underpinned by finiteness of \\(E[X_1X_1']\\) (CLRM 7), \\[\nn^{-1}\\sum_{i=1}^nX_iX_i'\\rightarrow_p E[X_1X_1']\n\\]\nSince \\(E[X_1X_1']\\) is also positive definite (CLRM 7), then by Slutzky’s Theorem\n\\[\n\\bigg(n^{-1}\\sum_{i=1}^nX_iX_i'\\bigg)^{-1}\\rightarrow_p \\big(E[X_1X_1']\\big)^{-1}\n\\]\nHence, by Slutzky’s Theorem, which says that the product of two consistent estimators converges in probability to the product of their targets,\n\\[\n(X'X)^{-1}X'u \\rightarrow_p \\big(E[X_1X_1']\\big)^{-1}E[X_1u_1] = 0\n\\]\nThus,\n\\[\np \\lim(\\hat{\\beta}_n) = \\beta\n\\]\n\n\n\n\n\n\nImportant\n\n\n\nScaling each term by \\(n\\) is very important, as without it, both terms do not have a finite mean. Consider, under iid,\n\\[\nE\\bigg[\\sum_{i=1}^nX_iX_i'\\bigg] = nE[X_1X_1']\n\\] while, \\[\nE\\bigg[n^{-1}\\sum_{i=1}^nX_iX_i'\\bigg] = E[X_1X_1']\n\\]\n\n\n\n\n3.6 Asymptotic Distribution\nTo derive the asymptotic distribution of the OLS estimator we will need to apply the Central Limit Theorem. We will need to scale by \\(\\sqrt{n}\\), to derive the distribution of,\n\\[\n\\sqrt{n}(\\hat{\\beta}_n-\\beta)\n\\]\nRecall from Lecture 2 that by Cramer’s Convergence Theorem, \\(Y_nX_n\\rightarrow_d cX\\) where \\(X_n\\rightarrow_d\\) and \\(Y_n\\rightarrow_p c\\). This result holds for the case where \\(Y_n\\) is a random matrix. \\[\n\\sqrt{n}(\\hat{\\beta}_n-\\beta) = \\big(n^{-1}X'X\\big)^{-1}n^{-1/2}Xu\n\\] We have already established that, \\[\n\\big(n^{-1}X'X\\big)^{-1}\\rightarrow_p \\big(E[X_1X_1']\\big)^{-1}\n\\] under assumptions CLRM 1, 2b, 6, and 7.\nWe therefore need to consider the asymptotic distribution of \\(n^{-1/2}Xu\\). By CLRM 2b \\(E[X_1u_1]=0\\), fulfilling one of the CLT conditions. We then need the second moment to be finite: \\(Var(X_1u_1) = E[u_1^2X_1X_1']\\). This is a \\(k\\times k\\) matrix.\nWe will need to make some additional assumptions:\n\n(NEW) CLRM 8: \\(E[u_1^2X_1X_1']\\) is a finite positive-definite matrix.6\n\nUnder assumptions CLRM 1, 2, 6, and 8, by CLT,\n\\[\nn^{-1/2}Xu\\rightarrow N(0,E[u_1^2 X_1 X_1'])\n\\] There, \\[\n\\sqrt{n}(\\hat{\\beta}_n-\\beta) \\rightarrow_d N\\bigg(0,\\big(E[X_1X_1']\\big)^{-1}E[u_1^2X_1X_1']\\big(E[X_1X_1']\\big)^{-1}\\bigg)\n\\] Under the homoskedasticity, \\(E[u_1^2X_1X_1']=\\sigma^2 E[X_1X_1']\\), giving us, \\[\n\\sqrt{n}(\\hat{\\beta}_n-\\beta) \\rightarrow_d N\\bigg(0,\\sigma^2\\big(E[X_1X_1']\\big)^{-1}\\bigg)\n\\] We can approximate the asymptotic distribution \\(\\hat{\\beta}_n\\) by multiplying by \\(\\sqrt{n}\\) and replacing \\(\\big(E[X_1X_1']\\big)^{-1}\\) with the approximation \\(\\big(n^{-1}X'X\\big)^{-1}\\). \\[\n\\hat{\\beta}_n \\overset{a}{\\sim}N\\big(\\beta,\\sigma^2(X'X)^{-1}\\big)\n\\] The \\(n\\) in the variance formula is cancelled out by the pre-multiply of \\(\\sqrt{n}\\).\n\n\n3.7 Other properties\n\nAmong the class of unbiased linear estimators of the CLRM, the OLS is the Best Linear Unbiased Estimator (BLUE). “Best” here means lowest variance. Can you show this?"
  },
  {
    "objectID": "handout-3.html#method-of-moments",
    "href": "handout-3.html#method-of-moments",
    "title": "Estimation Methods",
    "section": "4 Method of Moments",
    "text": "4 Method of Moments\nThe method of moments (MM) approach is to match assumed ‘moments’ given by the model with their sample analogue. This is a very general approach and is used extensively in applied macroeconomics, where a structural model gives rise to moments between economic variables that can be matched in the data.\nA general principle of MM is that the number of moments, \\(m\\), must be \\(\\geq k\\), the number of parameters being estimated. This akin to saying, the number of equations must be greater or equal to the number of variables being solved for. If the number of moments exceeds the number of parameters, we say that the model is overidentified. In the case of instrumental variables, overidentification allows you two test certain model assumptions.\nIn term 2, you will study instrumental variables which adopts a GMM approach to estimation. MM approaches are also used extensively in time series. For now, we will apply the MM approach to the CLRM.\n\n4.1 General setup\nThe observed data is given by, \\(W_1,...,W_n\\), read \\(W_i\\) is a \\(p\\)-dimension random vector. Let \\(g(W_i,\\theta)\\) be a \\(l\\)-dimension function (i.e. \\(\\in \\mathbf{R}^l\\)) and \\(\\theta\\in\\mathbf{R}^k\\):\n\\[\ng(W_i,\\theta) = \\begin{bmatrix}g_1(W_i,\\theta) \\\\ \\vdots \\\\g_l(W_i,\\theta)\\end{bmatrix}\n\\] We assume that the true value of the parameter \\(\\theta_0\\in\\Theta\\subset \\mathbf{R}^{k}\\) satisifies the condition, \\[\nE\\big[g(W_i,\\theta_0)\\big] = 0\n\\] We say that the model is identified if there is a unique solution to the above equations. That is, \\(E\\big[g(W_i,\\theta)\\big] = E\\big[g(W_i,\\tilde{\\theta})\\big] = 0\\;\\Rightarrow\\;\\theta=\\tilde{\\theta}\\). A necessary condition for identifiction is \\(l\\geq k\\); i.e. the number of equations is at least as large as the number of unknown parameters. A model can be underidentified, which typically means that there is not a unique solution for some of the parameters.\n\n\n4.2 Estimator\nThe basic principle of MM estimation is to replacing the expectation operator with the average function and solve for \\(\\hat{\\theta}\\). \\[\nn^{-1}\\sum_{i=1}^n g(W_i,\\hat{\\theta}^{MM}) = 0\n\\] However, this only works when \\(l=k\\) (exactly identified cases). For \\(l&gt;k\\) (overidentified cases) there is no unique vector that solves all \\(l\\) equations.\nThe Generalized Method of Moments (GMM) approach applies a set of weights to the minimization problem. Let \\(A_n\\) be a \\(l\\times l\\) weight matrix, such that \\(A_n\\rightarrow_p A\\). Then,\n\\[\n\\hat{\\theta}^{GMM} = \\underset{\\theta\\in\\Theta}{\\text{arg min}}\\;\\bigg|\\bigg|A_n n^{-1}\\sum_{i=1}^n g(W_i,\\hat{\\theta}^{MM})\\bigg|\\bigg|^2\n\\]\nHere, \\(||v||\\) denotes the Euclidean norm of vector \\(v\\): \\(||v|| = \\sqrt{v'v}\\).\n\n\n4.3 Application to CLRM\nAssumption CLRM 2b tells us that the regressors are uncorrelated with the error term. \\[\nE[X_iu_i] = 0\n\\] This is the moment that gives rise to identification in the CLRM. Given CLRM 1, we can replace the error term in the above moment with \\(Y_i-X_i'\\beta\\). \\[\nE[X_i(Y_i-X_i'\\beta)] = 0\n\\] Thus, the \\(g(W_i,\\beta)=X_i(Y_i-X_i'\\beta)\\) for \\(W_i = [Y_i,X_i']'\\).\nHow many equations are there? Recall, \\(X_i\\) is a \\(k\\)-dimension random vector. So,\n\\[\nE[X_i(Y_i-X_i'\\beta)] = \\begin{bmatrix}E[X_{i1}(Y_i-X_i'\\beta)]\\\\E[X_{i2}(Y_i-X_i'\\beta)]\\\\ \\vdots \\\\ E[X_{ik}(Y_i-X_i'\\beta)]\\end{bmatrix}=0\n\\] The are \\(k\\)-moments (or equations), meaning that we can estimate up to \\(k\\) parameters. In this instance, we have a failure of identification if \\(rank(E[X_iX_i'])&lt;k\\); which is required for the invertibility of \\(E[X_iX_i']\\). This condition is met by assumption CLRM 4; ensuring exact identification.\nThe MM estimator for the CLRM is then given by the solution to,\n\\[\nn^{-1}\\sum_{i=1}^n X_i(Y_i-X_i'\\hat{\\beta}^{MM}) = 0\n\\]\nThe solution is equivalent to the OLS estimator: \\[\n\\hat{\\beta}^{MM} = \\bigg(n^{-1}\\sum_{i=1}^n X_iX_i'\\bigg)^{-1}n^{-1}\\sum_{i=1}^nX_iY_i = \\big(X'X\\big)^{-1}X'Y = \\hat{\\beta}^{OLS}\n\\]\n\n\n4.4 Consistency\nAs we have already established the consistency of the OLS estimator, we will briefly review the case of the GMM estimator here. A more detailed discussion will be provided in term 2.\nRecall, the assumption \\(A_n\\rightarrow_p A\\). Then,\n\\[\n\\bigg|\\bigg|A_n n^{-1}\\sum_{i=1}^n g(W_i,\\theta)\\bigg|\\bigg|^2\\rightarrow_p \\big|\\big|A E\\big[ g(W_i,\\theta)\\big]\\big|\\big|^2\n\\] In instance where identification is exact/unique, \\(E\\big[ g(W_i,\\theta)\\big]=0\\iff\\theta=\\theta_0\\). Which is to say that the true value of \\(\\theta\\) is the unique minimizer.\nThen,\n\\[\n\\begin{aligned}\n\\hat{\\theta}^{GMM} =&\\underset{\\theta\\in\\Theta}{\\text{arg min}}\\; \\bigg|\\bigg|A_n n^{-1}\\sum_{i=1}^n g(W_i,\\theta)\\bigg|\\bigg|^2 \\\\\n\\rightarrow_p&\\underset{\\theta\\in\\Theta}{\\text{arg min}}\\;\\big|\\big|A E\\big[ g(W_i,\\theta)\\big]\\big|\\big|^2 \\\\\n=&\\theta_0\n\\end{aligned}\n\\] The formal proof requires a number of additional regularity assumptions; including, the compactness of \\(\\Theta\\).\n\n\n4.5 Asymptotic Normality\nThe GMM estimator is asymptotically normal.\n\\[\n\\sqrt{n}\\big(\\hat{\\theta}^{GMM}-\\theta_0\\big)\\rightarrow_d N(0,V)\n\\] where, \\[\n\\begin{aligned}\nV =& (Q'A'AQ)^{-1}QA'A\\Omega A'AQ (Q'A'AQ)^{-1} \\\\\nQ =& E\\bigg[\\frac{\\partial g(W_i,\\theta_0)}{\\partial \\theta'}\\bigg] \\\\\n\\Omega =& E\\big[ g(W_i,\\theta)g(W_i,\\theta)'\\big]\n\\end{aligned}\n\\] Where does this result come from? We won’t go through the proof in details. However, it starts from the FOCs. The GMM estimator solves,\n\\[\n\\bigg[\\underbrace{n^{-1}\\sum_{i=1}^n\\frac{\\partial g(W_i,\\hat{\\theta}^{GMM})}{\\partial \\theta'}}_{Q_n\\big(\\hat{\\theta}^{GMM}\\big)}\\bigg]'A_n'A_n n^{-1}\\sum_{i=1}^{n}g(W_i,\\hat{\\theta}^{GMM}) = 0\n\\] In the above expression, the matrix of derivatives will converge (under some regularity conditions) in probability: \\(Q_n\\big(\\hat{\\theta}^{GMM}\\big)\\rightarrow_p Q\\). Second, since \\(E\\big[ g(W_i,\\theta)\\big]=0\\), by CLT we know,\n\\[\nn^{-1/2}\\sum_{i=1}^{n}g(W_i,\\theta_0)\\rightarrow_d N(0,\\underbrace{E\\big[ g(W_i,\\theta)g(W_i,\\theta)'\\big]}_\\Omega)\n\\] We can therefore see where the components of the variance come from. The proof requires a bit more work. First, we need the distribution of \\(\\sqrt{n}\\big(\\hat{\\theta}^{GMM}-\\theta_0\\big)\\), not \\(n^{-1/2}\\sum_{i=1}^{n}g(W_i,\\theta_0)\\). Second, the FOCs contain \\(n^{-1}\\sum_{i=1}^{n}g(W_i,\\hat{\\theta}^{GMM})\\) and not \\(n^{-1}\\sum_{i=1}^{n}g(W_i,\\theta_0)\\).\nThis is resolve using a mean value expansion:\n\\[\ng(W_i,,\\hat{\\theta}^{GMM}) = g(W_i,\\theta_0) + \\frac{\\partial g(W_i,\\hat{\\theta}^*)}{\\partial \\theta'}\\big(\\hat{\\theta}^{GMM}-\\theta_0\\big)\n\\] Plugging this expansion into the FOCs, you can rearrange to solve,\n\\[\n\\sqrt{n}\\big(\\hat{\\theta}^{GMM}-\\theta_0\\big) = -[Q_n\\big(\\hat{\\theta}^{GMM}\\big)'A_n'A_nQ_n\\big(\\hat{\\theta}^*\\big)]^{-1}Q_n\\big(\\hat{\\theta}^{GMM}\\big)'A_n'A_nn^{-1/2}\\sum_{i=1}^{n}g(W_i,\\theta_0)\n\\] Since \\(\\hat{\\theta}^*\\) is a mean value, it is a also consistent and \\(Q_n\\big(\\hat{\\theta}^*\\big)\\rightarrow_p Q\\).\n\n\n4.6 Additional comments\n\nThe targetted moments may be highly non-linear. For example, the Lucas Model pins down the rate of return on a risky asset \\(R_{j,t}\\) using the relative utility of consumption today and tomorrow. The equilibrium condition for assets \\(j=1,...,m\\) is given by,\n\\[\nE\\bigg[\\underbrace{\\delta\\bigg(\\frac{C_{t+1}}{C_t}\\bigg)^{-\\alpha}(1+R_{j,t})-1}_{g(W_{j,t},\\theta)}\\bigg]=0\n\\]\nwhere \\(W_{j,t}=[C_t,C_{t+1},R_{j,t}]'\\) and \\(\\theta = [\\alpha,\\delta]'\\). As the moment must hold for each asset, \\(\\theta\\) is identified so long as \\(m\\geq 2\\).\nGiven the non-linearity of the \\(g\\)-function, there is no closed for solution. Instead, the GMM estimator must be solved for using numerical optimization.\nSome (macroeconomic) models are not identified (or underidentified). For example, a simple RBC model (with a random government component) yields the following moment condition from Euler equation,7\n\\[\nE\\bigg[\\underbrace{\\beta\\frac{C_{t+1}}{C_t}\\big(f_K + (1-\\delta)\\big)-1}_{g(W_i,\\theta)}\\bigg]=0\n\\] where \\(W_{j,t}=[C_t,C_{t+1},f_K]\\) and \\(\\theta = [\\beta,\\delta]\\). In this application, there is only a single moment but two unknown parameters. For this reason, you will need to find an additional instrument that introduces an additional moment to identify both parameters."
  },
  {
    "objectID": "handout-3.html#maximum-likelihood",
    "href": "handout-3.html#maximum-likelihood",
    "title": "Estimation Methods",
    "section": "5 Maximum Likelihood",
    "text": "5 Maximum Likelihood\nMaximum Likelihood (ML or MLE) are a general class of estimators that exploit a knowledge of the underlying distribution of unobservables in the model. As the name suggests, the goal will be to maximize the likelihood (i.e. probability) of observing the a given sample of data, given the assumed distribution of the data, governed by a fixed set of parameters.\n\n5.1 General setup\nConsider an iid random sample of data: \\(W_1,...,W_n\\). We will assume that the data is drawn from a known distribution, \\(f(w_i;\\theta)\\), where \\(\\theta\\in\\Theta\\subset \\mathbf{R}^k\\) is an unknown vector of population parameters.8\n\n\n\n\n\n\nNotation\n\n\n\nThe notation used to describe ML estimation varies quite a bit across texts. One key difference appears to be how to denote a parameterized distribution. The density function, \\(f(w_i;\\theta)\\), is the density at \\(w_i\\) (the realized value for observation \\(i\\)), where the distribution is parameterized by \\(\\theta\\). Some texts use the conditional notation, \\(f(w_i|\\theta)\\), as the distribution depends on \\(\\theta\\). However, probabilistic conditions tend to be based on random variables and not non-random parameters. I found this StackExchange discussion on the topic quite interesting. Needless to say, there is much disagreement and notation appears to differ across Mathematics and Statistics, and among the Statisticians, between frequentists and Bayesians. Even the Wikipedia page on MLE uses a combination of the two notations. I will use ‘;’; which also happens to be the notation used by Wooldridge (2010).\n\n\nAs the sample is iid, the joint density (or pdf) of the realized observations is given by the product of marginals,\n\\[\nf(w;\\theta)=\\prod_{i=1}^n f(w_i;\\theta)\n\\] This is referred to as the likelihood function.\nSuppose \\(W_i = [Y_i,X_i']'\\), a vector contain a single outcome variable and a set of covariates. We can then define the joint conditional likelihood as,\n\\[\n\\begin{aligned}\n\\ell_i(\\theta) =& f(Y_i|X_i;\\theta) \\\\\nL_n(\\theta) =& \\prod_{i=1}^n f(Y_i|X_i;\\theta)\n\\end{aligned}\n\\] Here my notation differs from Wooldridge (2010), who uses \\(\\ell_i(\\theta)\\) to denote the conditional log-likelihood for observation \\(i\\) (see Wooldridge 2010, 471). Take note of the fact that the likelihood function is a random function of \\(\\theta\\), since it depends on the random variables \\(W_i = [Y_i,X_i']'\\).9\n\n\n5.2 Estimator\nThe goal of ML estimation is to solve the value of \\(\\hat{\\theta}\\) that maximizes the likelihood of observing the data.\n\\[\n\\hat{\\theta}^{ML} = \\underset{\\theta}{\\text{arg max}}\\;L_n(\\theta)\n\\] In practice, we apply a monotonic transformation to the likelihood function. By taking the log of the likelihood, the product of marginal distributions becomes a sum. As the transformation is monotonic, the solution to the above problem is equivalent to the solution to,\n\\[\n\\hat{\\theta}^{ML} = \\underset{\\theta}{\\text{arg max}}\\;n^{-1}\\log L_n(\\theta) = \\underset{\\theta}{\\text{arg max}}\\;n^{-1}\\sum_{i=1}^n\\log\\ell_i(\\theta)\n\\] In addition, the division by \\(n\\) makes this problem the sample analogue of, \\[\n      \\underset{\\theta\\in\\Theta}{\\text{max}}\\;E[\\log\\ell_i(\\theta)]\n\\] It turns out that the true value of the parameter, \\(\\theta_0\\), is the solution to the above problem [see Wooldridge (2010), pp. 473].10 We will prove this for the unconditional case when we discuss consistency of ML.\nAssuming a continuous, concave density function, we can solve for \\(\\hat{\\theta}^{ML}\\) using first-order conditions.\n\\[\n\\frac{1}{n}\\frac{\\partial \\log L_n(\\hat{\\theta})}{\\partial \\theta} =\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial \\log\\ell_i(\\hat{\\theta})}{\\partial \\theta}= n^{-1}S(\\hat{\\theta})=0\n\\]\nThe vector of partial derivatives is referred to as the score function: \\(S(\\theta)\\). When evaluated at the ML estimator, the score function is 0. This is a \\(k\\)-dimensional vector in which row is the partial derivative with respect to \\(\\theta_k\\).\n\n\n5.3 Application to CLRM\nUnder CLRM 5, \\(U|X \\sim N(0,\\sigma^2 I_n)\\). Together with CLRM 1 and 6, we know the conditional distribution of \\(Y\\). \\[\nY_i|X_i\\sim_{iid} N(X_i'\\beta,\\sigma^2)\n\\] where \\(X_i'\\beta\\) is the conditional mean of \\(Y_i\\). Therefore, the conditional likelihood of the data is given by, \\[\nL_n(\\beta,\\sigma^2) = \\prod_{i=1}^n \\bigg[\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\bigg(-\\frac{1}{2}\\bigg(\\frac{Y_i-X_i'\\beta}{\\sigma}\\bigg)^2\\bigg)\\bigg]\n\\] :::{.callout-important} We are working with the conditional likelihood. To define the likelihood of observing the entire sample, \\(W_1,...,W_n\\), we would also need to consider the distribution of \\(X_i\\). We would then define \\(f(W_i;\\theta) = f(Y_i|X_i;\\theta)\\cdot f(X_i)\\), where \\(f(X_i)\\) may be parameterized by its own set of parameters. \\(\\theta\\) is the set of parameters that parameterize the conditional distribution of \\(Y|X\\). ::: Taking the log transformation and divide by \\(n\\), we get, \\[\nn^{-1}\\log L_n(\\beta,\\sigma^2) =  -\\frac{1}{2}\\log(\\sigma^2)-\\frac{1}{2}\\log(2\\pi)-\\frac{1}{2n\\sigma^2} \\sum_{i=1}^n(Y_i-X_i'\\beta)^2\n\\] It should be immediately clear that maximizing this expression will be equivalent to minimizing the sum of squared errors.\nConsider the FOC’s set to 0 at the optimal point. First, w.r.t. \\(\\beta\\),\n\\[\n\\begin{aligned}\n\\frac{\\partial n^{-1}\\log L_n(\\hat{\\beta},\\hat{\\sigma}^2)}{\\partial\\beta} =& -\\frac{1}{n\\sigma^2} \\sum_{i=1}^nX_i(Y_i-X_i'\\hat{\\beta}) = 0 \\\\\n\\Rightarrow \\hat{\\beta}^{ML} =& \\bigg(\\sum_{i=1}^n X_iX_i'\\bigg)\\sum_{i=1}^nX_iY_i \\\\\n=& \\big(X'X\\big)^{-1}X'Y\n\\end{aligned}\n\\] In the case of the CLRM, \\(\\hat{\\beta}^{ML}=\\hat{\\beta}^{MM}=\\hat{\\beta}^{OLS}\\).\nSecond, w.r.t. \\(\\sigma^2\\), \\[\n\\begin{aligned}\n\\frac{\\partial n^{-1}\\log L_n(\\hat{\\beta},\\hat{\\sigma}^2)}{\\partial\\sigma^2} =& -\\frac{1}{2\\hat{\\sigma}^2}+ \\frac{1}{n2\\hat{\\sigma}^4}\\sum_{i=1}^n(Y_i-X_i'\\hat{\\beta})^2 = 0 \\\\\n\\Rightarrow \\hat{\\sigma}^{2}_{ML} =& n^{-1}\\sum_{i=1}^n(Y_i-X_i'\\hat{\\beta})^2\n\\end{aligned}\n\\] This estimator for the variance is consistent, but biased for small samples. This is because it scales by \\(n\\) and not \\(n-k\\), a distinction that is ignorable as \\(n\\rightarrow\\infty\\). For this reason, when conducting inference you should use the asymptotic distribution of the ML estimator.\n\n\n5.4 Consistency\nThe ML estimator is consistent. This can be shown in a couple of steps. To simplify notation we will examine the proof for the unconditional likelihood, but the same will hold for the conditional. The proof will require Jensen’s inequality:\n\nTheorem 1 For \\(h(\\cdot)\\) concave, then \\(E[h(X)]\\leq h(E[X])\\).\n\n\nProof. By the WLLN, for ALL values of \\(\\theta\\in\\Theta\\),\n\\[\n\\begin{aligned}\nn^{-1}\\sum_{i=1}^n\\log f(W_i;\\theta) \\rightarrow_p&\\;E\\big[\\log f(W_i;\\theta)\\big] \\\\\n=&\\int\\big(\\log f(w;\\theta)\\big)f(w;\\theta_0)dw\n\\end{aligned}\n\\] Note, an important distinction in the last line: the expectation is based on the density function parameterized by the true value, \\(\\theta_0\\). This is because the data is generated by the true density.\nWe have convergence for ALL values of \\(\\theta\\), but now need to establish convergence to the \\(\\theta_0\\). Consider the difference, \\[\n\\begin{aligned}\nE\\big[\\log f(W_i;\\theta)\\big]-E\\big[\\log f(W_i;\\theta_0)\\big]\n=&E\\bigg[\\log\\frac{f(W_i;\\theta)}{f(W_i;\\theta_0)}\\bigg] \\\\\n\\leq&\\log\\bigg[\\frac{f(W_i;\\theta)}{f(W_i;\\theta_0)}\\bigg] \\qquad \\text{by Jensen's} \\\\\n=&\\log \\int\\bigg(\\frac{f(w;\\theta)}{f(w,\\theta_0)}\\bigg)f(w;\\theta_0)dw \\\\\n=&\\log \\int f(w;\\theta)dw \\\\\n=&\\log 1 \\\\\n=&0\n\\end{aligned}\n\\] The inequality can be made strict if we assume that \\(Pr\\big(f(W_i;\\theta_0)\\neq f(W_i;\\theta)\\big)&gt;0\\) \\(\\forall \\theta\\neq\\theta_0\\). This ensures that \\(\\theta_0\\) is a unique solution. Since the difference is \\(\\leq 0\\), it follows that, \\[\n\\theta_0 = \\underset{\\theta\\in\\Theta}{\\text{arg max}} E[\\log f(W_i;\\theta)]\n\\] Which implies, \\[\n\\begin{aligned}\n\\hat{\\theta}^{ML}_n =& \\;\\underset{\\theta\\in\\Theta}{\\text{arg max}} \\;n^{-1}\\log L_n(\\theta) \\\\\n\\rightarrow_p& \\;\\underset{\\theta\\in\\Theta}{\\text{arg max}} E\\big[\\log f(W_i,\\theta)\\big]\\\\\n=& \\theta_0\n\\end{aligned}\n\\]\n\n\n\n5.5 Asymptotic Normality\nThe ML estimator is asymtotically normal. We will not prove this result, but rather focus on the form of the asymptotic variance and its estimator. The proof uses the Mean Value Theorem and CLT.\n\\[\n\\sqrt{n}\\big(\\hat{\\theta}^{ML}_n-\\theta_0\\big)\\rightarrow_d N(0,V)\n\\]\nwhere \\(V=[J(\\theta_0)]^{-1}\\). \\(J(\\theta)\\) is referred to as the information matrix, given by the expectation of the (Hessian) matrix of second-order derivatives:\n\\[\nJ(\\theta) = -E\\bigg[\\frac{\\partial^2}{\\partial\\theta\\partial\\theta'}\\log f(W_i,\\theta_0)\\bigg]\n\\] \\([nJ(\\theta_0)]^{-1}\\) is used to approximate the variance, but since \\(J\\) is not observed, it must be estimated. This is done by replacing the expectation in the information matrix with sample average:\n\\[\n\\hat{V}_H = \\bigg[\\frac{1}{n}\\sum_{i=1}^n\\frac{\\partial^2}{\\partial\\theta\\partial\\theta'}\\log f(W_i,\\hat{\\theta})\\bigg]^{-1}\n\\]\n\n\n5.6 Additional comments\n\nML estimators are invariant. If \\(\\hat{\\theta}\\) is the ML-estimator for \\(\\theta\\), then \\(\\ln(\\hat{\\theta})\\) is the ML-estimator for \\(\\ln(\\theta)\\).\nIn general, there are no closed form solutions for ML estimators; the CLRM being one exception. For this reason, ML estimation requires numerical optimization.\nThe ML estimator is efficient. That is, its variance is at least as small as any other consistent (and asymptotically normal) estimator.\nML estimators require as to know the true PDF, up to its parameters. For example, probit (logit) models assumes that the error term is normally (logistically) distributed.\nIn some cases, the estimator may be consistent even if the PDF is misspecified. As is the case for the OLS estimator of the linear model. These estimators are referred to as a quasi-ML estimators."
  },
  {
    "objectID": "handout-3.html#footnotes",
    "href": "handout-3.html#footnotes",
    "title": "Estimation Methods",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMachine Learning techniques, such as neural networks, use non-linear operators such as the Softmax function and Rectified Linear Unit (ReLU). Who knows, in a few years, we may no longer think of OLS as the ‘work-horse’ of applied statistics and research.↩︎\nA measure of distance must be positive. You can use the (sum of) absolute-value deviations, but the least squares has nice properties including ease of differentiation and overall efficiency (of the estimator).↩︎\nThe inner (or dot) product of two equal-length vectors, \\(a\\) and \\(b\\), is defined as: \\[\n\\langle a,b\\rangle=a\\cdot b = \\sum_{i=1}^k a_i\\times b_i = a'b\n\\]↩︎\nIf \\(Y\\sim N(\\mu,\\Sigma)\\), \\(Y\\in\\mathbf{R}^k\\), then \\(AY+b\\sim N(A\\mu+b,A\\Sigma A')\\) for any non-random \\(m\\times k\\) \\(A\\)-matrix and \\(m\\times 1\\) \\(b\\)-vector.↩︎\nThe assumptions are fulfilled by CLRM 2b and CLRM 6.↩︎\nThe finiteness of this matrix requires two assumptions:\n\n\\(E[X_{i,j}^4]&lt;\\infty\\) for all \\(j=1,...k\\) (i.e. each regressor has finite fourth moment)\n\\(E[U_j^4]&lt; \\infty\\)\n\nThese assumptions are sufficient for all elements of matrix \\(E[u_1^2 X_1 X_1']\\) to be finite. The proof is an application of the Cauchy-Schwartz Inequality, which we haven’t covered.↩︎\nThis example is taken from Canova (2007, ch. 5, p. 167).↩︎\n\\(\\Theta\\) is a parameter space and is typically assumed to be compact: a closed and bounded subset of Euclidean space.↩︎\nYou may also see the equivalent notation \\(L(W_i;\\theta)\\equiv L_n(\\theta)\\). The subscript-\\(n\\) implies that the function depends on the sample.↩︎\nThis is actually a non-trivial issue and beyond the scope of this module. As noted by Wooldridge (2010), we can arrive at the ML estimator by picking the value of \\(\\theta\\) to maximize the joint likelihood. However, this approach assumes that the true value of \\(\\theta\\in\\Theta\\), \\(\\theta_0\\), maximizes the joint likelihood. This is not immediately evident. Once established, we have a more robust basis of the ML estimator.↩︎"
  },
  {
    "objectID": "handout-1.html",
    "href": "handout-1.html",
    "title": "Classical Linear Regression Model & Ordinary Least Squares",
    "section": "",
    "text": "In this handout we will revisit the Classical Linear Regression Model (CLRM) (see Wooldridge 2010, chaps. 1–2). The goal of this week’s lecture is to:\n\nunderstand the model specification;\nit’s underlying assumptions;\nand the appropriate interpretation;\nthe OLS estimator, using linear algebra;\nthe geometry of OLS and partitioned regression result."
  },
  {
    "objectID": "handout-1.html#overview",
    "href": "handout-1.html#overview",
    "title": "Classical Linear Regression Model & Ordinary Least Squares",
    "section": "",
    "text": "In this handout we will revisit the Classical Linear Regression Model (CLRM) (see Wooldridge 2010, chaps. 1–2). The goal of this week’s lecture is to:\n\nunderstand the model specification;\nit’s underlying assumptions;\nand the appropriate interpretation;\nthe OLS estimator, using linear algebra;\nthe geometry of OLS and partitioned regression result."
  },
  {
    "objectID": "handout-1.html#model-specification",
    "href": "handout-1.html#model-specification",
    "title": "Classical Linear Regression Model & Ordinary Least Squares",
    "section": "2 Model Specification",
    "text": "2 Model Specification\nThe linear population regression model is given by,\n\\[\n\\begin{aligned}\n  Y_i =& X_i'\\beta+\\varepsilon_i \\\\\n  =& \\beta_1\\mathbf{1}+\\beta_2X_{i2}+\\beta_3X_{i3}+...+\\beta_kX_{ik}+\\varepsilon_i\n\\end{aligned}\n\\]\nfor \\(i = 1,2,...,n\\). Where,\n\n\\(i\\): unit of observation; e.g. individual, firm, union, political party, etc.\n\\(Y_i \\in \\mathbb{R}\\): scalar random variable.\n\\(X_i \\in \\mathbb{R}^k\\): \\(k\\)-dimensional (column1) vector of regressors, with \\(k&lt;n\\).2\n\\(\\beta\\): \\(k\\)-dimensional, non-random vector of unknown population parameters.\n\\(\\varepsilon_i\\): unobserved, random error term.3\n\nThe linear population regression equation is linear in parameters. This is an important assumption that does NOT restrict the model from being non-linear in regressors. For example, the equation\n\\[Y_i = \\beta_1 + \\beta_2X_{i2} + \\beta_3X_{i2}^{\\color{red}{2}} + \\varepsilon_i\\] non-linear in \\(X_{i2}\\), but still linear in parameters. In contrast, the equation\n\\[Y_i = \\beta_1 + \\beta_2X_{i2} + ({\\color{red}{\\beta_2\\beta_3}})X_{i3} + \\varepsilon_i\\] is non-linear in parameters.\n\n2.1 Intercept\nThe constant (intercept) in the equation serves an important purpose. While there is no a priori reason for the model to have a constant term, it does ensure that the error term is mean zero.\n\nProof. Suppose \\(E[\\varepsilon_i] = \\gamma\\).\nWe can then define a new error term, \\(\\upsilon_i = \\varepsilon_i - \\gamma\\), such \\(E[\\upsilon_i] = 0\\). The population regression model can be rewritten as, \\[ \\begin{aligned} Y_i =& X_ i'\\beta + v_i + \\gamma  \\\\\n=& \\underbrace{(\\beta_1+\\gamma)}_{\\tilde{\\beta}_1}\\mathbf{1} + \\beta_2X_{i2} + \\beta_3X_{i3} + ... + \\beta_kX_{ik} + v_i\n\\end{aligned}\n\\] The model has a new intercept \\(\\tilde{\\beta}_1=\\beta_1 + \\gamma\\), but the other parameters remain unchanged.\n\n\n\n2.2 Matrix notation\nFor a sample of \\(n\\) observations, we can stack the unit-level linear regression equation into a vector,\n\\[\nY =\\underbrace{\\begin{bmatrix}Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix}}_{n\\times 1} = \\underbrace{\\begin{bmatrix}X_1'\\beta \\\\ X_2'\\beta \\\\ \\vdots \\\\ X_n'\\beta\\end{bmatrix}}_{n\\times 1}  + \\underbrace{\\begin{bmatrix}\\varepsilon_1 \\\\ \\varepsilon_2 \\\\ \\vdots \\\\ \\varepsilon_n\\end{bmatrix}}_{n\\times 1}  \n= \\underbrace{\\begin{bmatrix}X_{11} & X_{12} & \\cdots & X_{1k}\\\\ X_{21} & X_{22} && \\\\ \\vdots & & \\ddots & \\\\ X_{n1} & & & X_{nk} \\end{bmatrix}}_{n\\times k} \\underbrace{\\begin{bmatrix}\\beta_1 \\\\ \\beta_2 \\\\ \\vdots \\\\ \\beta_k\\end{bmatrix}}_{k\\times 1} + \\begin{bmatrix}\\varepsilon_1 \\\\ \\varepsilon_2 \\\\ \\vdots \\\\ \\varepsilon_n\\end{bmatrix} = X\\beta + \\varepsilon\n\\] Notice, in matrix notation, you lose the transpose from \\(X_i'\\beta\\). Apart from the absence of the \\(i\\) subscript, this is a useful way of knowing the dimension of the equation (in my notes). You MUST always write \\(X\\beta\\) and not \\(\\beta X\\). For the scalar case, \\(X_i'\\beta = \\beta'X_i\\), but for the vector case \\(\\beta X\\) is not defined since \\(\\beta\\) is \\(k\\times 1\\) and \\(X\\) is \\(n\\times k\\)."
  },
  {
    "objectID": "handout-1.html#clrm-assumptions",
    "href": "handout-1.html#clrm-assumptions",
    "title": "Classical Linear Regression Model & Ordinary Least Squares",
    "section": "3 CLRM Assumptions",
    "text": "3 CLRM Assumptions\nAssumption CLRM 1. Population regression equation is linear in parameters: \\[Y = X\\beta+\\varepsilon\\]\nAssumption CLRM 2. Conditional mean independence of the error term: \\[E[\\varepsilon|X]=0\\]\nAssumption CLRM 2. is stronger than \\(E[\\varepsilon_i|X_i]\\) (mean independence for unit \\(i\\)). If all units were independent, then \\(E[\\varepsilon_i|X_i]\\) would imply \\(E[\\varepsilon|X]=0\\). However, since we have not (yet) assumed this, we need this stronger exogeneity assumption. Consider, if \\(i\\) represented units of time (\\(t\\)), as in time-series models, independence across \\(i\\) will not hold.\nTogether, CLRM 1. and CLRM 2. imply that\n\\[ E[Y|X] = X\\beta  \\] This means that the Conditional Expectation Function is known and linear in parameters.\nConditional mean independence implies - by the Law of Iterated Expectations - mean independence of the error term,\n\\[E[\\varepsilon|X]=0 \\Rightarrow E\\big[E[\\varepsilon|X]\\big]=E[\\varepsilon]=0\\]\nand uncorrelatedness,\n\\[E[\\varepsilon|X]=0 \\Rightarrow E[\\varepsilon X]=0\\] Note, neither of the above statements hold the other way around. Mean independence does not imply conditional mean independence and uncorrelatedness (zero correlation/covariance) does not imply conditional mean independence.\nUncorrelatedness rules out linear relationships between the regressors and error term while conditional mean independence rules out non-linear relationships too.\nIn general, distributional independence implies mean independence which then implies uncorrelatedness.\n\nIn the case joint-normally distributed random variables, uncorrelatedness implies independence. That is, if \\[ \\begin{bmatrix}Y_1 \\\\ Y_2\\end{bmatrix}\\sim N\\bigg(\\begin{bmatrix}\\mu_1 \\\\ \\mu_2\\end{bmatrix},\\begin{bmatrix}\\sigma_1^2 & \\sigma_{12}\\\\ \\sigma_{21} & \\sigma_2^2\\end{bmatrix}\\bigg)\\] Then \\(\\sigma_{12}=\\sigma_{21}=0 \\iff f_1*f_2 = f_{12}\\).\n\nWe will later show that uncorrelatedness is sufficient for consistency of the Ordinary Least Squares estimater, while conditional mean independence is required for unbiasedness of OLS.\nAssumption CLRM 3. Homoskedasticity: \\(Var(\\varepsilon|X) = E[\\varepsilon\\varepsilon'|X] =  \\sigma^2I_n\\)\nCLRM 3. states that the variance of the error term is independent of \\(X\\) and constant across units. The diagonal nature of the covariance matrix also implies that the error terms are uncorrelated across units in the data. Note, this does not imply independence of the error terms across units.\nModels with heteroskedasticity relax the assumption of constant variance, allowing for a richer variance-covariance matrix that typically depends on \\(X\\).\nThis assumption is unlikely to hold in time-series models where units represent repeated observations across time. Such violations are referred to as serial correlation or autocorrelation.\nEven in cross-sectional data settings, you can have non-zero correlations across units in the data. A common instance of this is the case of clustering. Clustering can occur when units experience common/correlated ‘shocks’; for example, the data contains groups of students from the same classroom who have a the same teacher. This can also be the result of clustered sampling, a common practice in multi-stage survey design.\nAssumption CLRM 4. Full rank: \\(rank(X)=k\\quad a.s.\\) a.s.4\nSince \\(X\\) is a random variable we should add to the assumption: \\(rank(X) = k\\) almost surely (abbreviated a.s.). This means that the set of events in which \\(X\\) is not full rank occur with probability 0. The reason for this addition is that such a set of events (in the sample space) may not be empty.\nCLRM 4. is some time referred to as the absence of perfect (or exact) collinearity. Do not confuse this with multicollinearity. Multicollinearity occurs when regressors are highly (linearly) correlated with one another, yielding imprecise estimates.\nAssumption CLRM 5. Normality of the error term: \\(\\varepsilon|X \\sim N(0,\\sigma^2 I_n)\\)\nAssumption CLRM 6. Observations \\(\\{(Y_i,X_i): i=1,...,n\\}\\) are independently and identically distributed (iid).\nCLRM 5 & 6 are not part of the Classical assumptions, but do simplify the problem of inference. Note, CLRM 5 implies independence across error terms, not implied by CLRM 3.\n\n3.1 Non-random \\(X\\)\nThere is an alternative version of the CLRM in which \\(X\\) is a non-random, matrix of regressors/predictors. With \\(X\\) fixed, the error term is the only only random variable in the model. CLRM assumptions 1 and 4 remain the same, while CLRM 2, 3, 5, and 6 become:\nAssumption CLRM 2a. Mean independence of the error term: \\[E[\\varepsilon]=0\\]\nAssumption CLRM 3a. Homoskedasticity: \\(Var(\\varepsilon) = \\sigma^2I_n\\)\nAssumption CLRM 5a. Normality of the error term: \\(\\varepsilon \\sim N(0,\\sigma^2 I_n)\\)\nAssumption CLRM 6a. Observations \\(\\{\\varepsilon_i: i=1,...,n\\}\\) are independently and identically distributed (iid).\n\n\n3.2 Identification\nCLRM 1,2 and 4. are the identifying assumptions of the model. These assumptions allow us to write the parameter of interest as a set of ‘observable’ moments in the data. We can demonstrate this as follows.\n\nProof. Start with CLRM 2.\n\\[\n    E[\\varepsilon_i|X_i]=0\n\\]\nPre-multiply by the vector \\(X_i\\), \\[\n        X_iE[\\varepsilon_i|X_i]=0\n\\] Since the expectation is conditional on \\(X_i\\), we can bring \\(X_i\\) inside the expectation function,\n\\[\n        E[X_i\\varepsilon_i|X_i]=0\n    \\] This conditional expectation is a random-function of \\(X_i\\). If we take the expectation of this function w.r.t. \\(X\\), we achieve the aforementioned result that conditional mean independence implies zero covariance, \\[\n        E\\left[E[X_i\\varepsilon_i|X_i]\\right]=E[X_i\\varepsilon_i]=0\n    \\]\nNow substitute in for \\(\\varepsilon_i\\) using the linear regression model from CLRM 1 and separate the resulting two terms,\n\\[\n\\begin{aligned}\n    &E[X_i(Y_i-X_i'\\beta)]=0 \\\\\n    \\Rightarrow &E[X_iX_i']\\beta=E[X_iY_i]\n\\end{aligned}\n\\]\nSince \\(\\beta\\) is a non-random vector, we can remove it from the expectation function.\nNow we have a system of linear equations (of the form \\(Av = b\\)) with a unique solution if and only if the matrix \\(E[X_iX_i']\\) is invertible. For the inverse of \\(E[X_iX_i']\\) to exist, we require CLRM 4, since \\(rank(X)=k\\quad a.s.\\Rightarrow rank(E[X_iX_i'])=k\\).5\n\\[\n    \\beta = E[X_iX_i']^{-1}E[X_iY_i]\n\\]\n\nWe cannot compute \\(\\beta\\) because we do not know the joint distribution of \\((Y_i,X_i)\\) needed to solve for the variance-covariance matrices. However, \\(\\beta\\) is (point) identified because both \\(Y\\) and \\(X\\) are observed in the data and the parameters are “pinned down” by a unique set of ‘observable’ moments in the data.\n\\(\\beta\\) is not identified if the above system of linear equations does not have a unique solution. This will occur if two or more of the regressors are perfectly colinear.6 \\(\\beta\\) is also not be identified if the resulting expression for \\(\\beta\\) includes ‘objects’ (moments, distribution/scale parameters) that are not ‘observed’ in the data. For example, if the error is not mean independent, the above expression will include a bias term that depends on \\(E[X_i'\\varepsilon_i]\\).\nIn this instance, the identification of \\(\\beta\\) is scale dependent. That is, if we multiply \\(Y_i\\) by a scalar, \\(\\beta\\) is multiplied by the same scalar. For example, in cases where a researcher is modelling standardized test-scores."
  },
  {
    "objectID": "handout-1.html#interpretation",
    "href": "handout-1.html#interpretation",
    "title": "Classical Linear Regression Model & Ordinary Least Squares",
    "section": "4 Interpretation",
    "text": "4 Interpretation\nIn this linear regression model each slope coefficient has a partial derivative interpretation,\n\\[\n\\beta_j = \\frac{\\partial E[Y_i|X_i]}{\\partial X_{ij}}\n\\] or, as a vector, \\[\n\\beta = \\frac{\\partial E[Y_i|X_i]}{\\partial X_{i}} = \\begin{bmatrix}\\frac{\\partial E[Y_i|X_i]}{\\partial X_{i1}}\\\\ \\vdots \\\\ \\frac{\\partial E[Y_i|X_i]}{\\partial X_{ik}}\\end{bmatrix} = \\begin{bmatrix}\\beta_1\\\\ \\vdots \\\\ \\beta_k\\end{bmatrix}\n\\]\nNote, the derivative is expressed in terms of changes in the expected value of \\(Y_i\\) (conditional on \\(X_i\\)), not \\(Y_i\\) itself. This is because \\(Y_i\\) is a random variable, but under CLRM 1 & 2\n\\[\nE[Y_i|X_i] = X_i'\\beta\n\\]\nFor a given value of \\(X_i\\), the above expression is non-random.\nAs \\(\\beta_j\\) is a partial derivative, its interpretation is one that “holds fixed” the value of other regressors (i.e. ceteris paribus). Because of this, many researchers apply the experimental language of control variables when interpretting regression coefficients. However, this is dependent on the assumed linearity of the CEF."
  },
  {
    "objectID": "handout-1.html#ordinary-least-squares",
    "href": "handout-1.html#ordinary-least-squares",
    "title": "Classical Linear Regression Model & Ordinary Least Squares",
    "section": "5 Ordinary Least Squares",
    "text": "5 Ordinary Least Squares\nOLS is an estimator for \\(\\beta\\). As will become evident in Lecture 3, it is not the only estimator for \\(\\beta\\).\nThe OLS estimator is the solution to,\n\\[\n\\min_b\\;\\sum_{i=1}^n(Y_i-X_i'b)^2\n\\]\nUsing vector notation, we can rewrite this as\n\\[\n\\begin{aligned}\n&\\min_b\\;(Y-Xb)'(Y-Xb)\\\\\n=&\\min_b\\;Y'Y-Y'Xb-b'X'Y+b'X'Xb \\\\\n=&\\min_b\\;Y'Y-2b'X'Y+b'X'Xb\n\\end{aligned}\n\\] From line 2 to 3 we use the fact that \\(Y'Xb\\) is a scalar and therefore symmetric: \\(Y'Xb=b'X'Y\\).7\nDifferentiating the above expression w.r.t. the vector \\(b\\) and setting the first-order conditions to \\(0\\), we find that the following condition must hold for \\(\\hat{\\beta}\\), the solution.\n\\[\n  \\begin{aligned}\n  &0=-2X'Y+2X'X\\hat{\\beta}\n  \\\\ \\Rightarrow& X'X\\hat{\\beta} = X'Y\n  \\end{aligned}\n\\]\n\nHow did we get this result? Deriving the first order conditions requires knowledge of how to solve for the derivative of a scalar respect to a column vector (in this case \\(b\\in R^k\\)). The extra material on Linear Algebra has some notes on vector differentiation.\nWe can ignore the first term \\(Y'Y\\) as it does not depend on \\(b\\). The second term is \\(-2b'X'Y\\). Here we can use the rule that, \\[\n  \\frac{\\partial z'a}{\\partial z} = \\frac{\\partial a'z}{\\partial z} = a\n\\] In this instance, \\(a = X'Y \\in R^k\\). Thus, \\[\n  \\frac{\\partial -2b'X'Y}{\\partial b} = -2\\frac{\\partial b'X'Y}{\\partial b} = -2X'Y\n\\] The third term is \\(b'X'Xb\\). This is what is commonly referred to as a quadratic form: \\(z'Az\\). We know that the derivative of this form is, \\[\n  \\frac{\\partial z'Az}{\\partial z} = Az + A'z\n\\] and if \\(A\\) is symmetric, the result simplies to \\(2Az\\). In this instance, \\(A = X'X\\) is symmetric and the derivative is given by, \\[\n  \\frac{\\partial b'X'Xb}{\\partial b} = 2X'X\n\\]\n\nIn order to solve for \\(\\hat{\\beta}\\) we need to move the \\(X'X\\) term to the right-hand side. If these were scalars we would simply divide both sides by the same constant. However, as \\(X'X\\) is a matrix, division is not possible. Instead, we need to pre-multiply both sides by the inverse of \\(X'X\\): \\((X'X)^{-1}\\). Here’s the issue: the inverse of a matrix need not exist.\nGiven a square \\(k\\times k\\) matrix \\(A\\), its inverse exists if and only if \\(A\\) is non-singular. For \\(A\\) to be non-singular its rank must have full rank: \\(r(A)=k\\), the number of rows/columns. This means that all \\(k\\) columns/rows must be linearly independent. (See Material on Linear Algebra for a more detailed discussion of all these terms.)\nIn our application, \\(A=X'X\\) and\n\\[ r(X'X) = r(X) = colrank(X)\\leq k \\]\nTo insure that the inverse of \\(X'X\\) exists, \\(X\\) must have full column rank: all column vectors must be linearly independent. In practice, this means that no regressor can be a perfect linear combination of others. However, we have this from\nCLRM 4: \\(rank(X)=k\\)\nYou may know this assumption by another name: the absence of perfect colinearity between regressors.\n\nThe rank condition is the reason we exclude a base category when working with categorical variables.\nRecall, most linear regression models are specified with a constant. Thus, the first column of \\(X\\) is\n\\[ X_1 = \\begin{bmatrix}1 \\\\ 1 \\\\ \\vdots \\\\ 1\\end{bmatrix} \\] a \\(n\\times 1\\) vector vector of \\(1\\)’s, denoted here as \\(\\ell\\). Suppose you have a categorical - for example, gender in an individual level dataset - that splits the same in two. The categories are assumed to be exhaustive and mutually exclusive. If you create two dummy variables, one for each category,\n\\[ X_2 = \\begin{bmatrix}1 \\\\ \\vdots \\\\1\\\\0\\\\ \\vdots \\\\ 0\\end{bmatrix}\\qquad\\text{and}\\qquad X_3 = \\begin{bmatrix}0 \\\\ \\vdots \\\\0\\\\1\\\\ \\vdots \\\\ 1\\end{bmatrix} \\]\nit is evident that \\(X_2+X_3 = \\ell\\). (Here I have depicted the sample as sorted along these two categories.) If \\(X=[X_1\\;X_2\\;X_3]\\), then it is rank-deficient: \\(r(X) = 2&lt;3\\), since \\(X_3=X_1-X_2\\). Thus, we can only include two of these three regressors. We can even exclude the constant and have \\(X=[X_2\\;X_3]\\).\n\nIf \\(X\\) is full rank, then \\((X'X)^{-1}\\) exists and,\n\\[\n\\hat{\\beta} = (X'X)^{-1}X'Y\n\\]\nThis relatively simple expression is the solution to least squares minimization problem. Just think, it would take less than three lines of code to programme this. That is the power of knowing a little linear algebra.\nWe can write the same expression in terms of summations over unit-level observations,\n\\[\n\\hat{\\beta} = \\bigg(\\sum_{i=1}^nX_iX_i'\\bigg)^{-1}\\sum_{i=1}^nX_iY_i\n\\]\nNote, the change in position of the transpose: \\(X_i\\) is a column vector \\(\\Rightarrow\\) \\(X_i'X_i\\) is a scalar while \\(X_iX_i'\\) is a \\(k\\times k\\) matrix. To match the first expression, the term inside the parenthesis must be a \\(k\\times k\\) matrix. Similarly, \\(X'Y\\) is a \\(k\\times 1\\) vector, as is \\(X_iY_i\\).\n\n5.1 Univariate case\nUndergraduate textbooks all teach a very similar expression for the OLS estimator of a uni-variate regression model (with a constant); typically, something like,8\n\\[\nY_i = \\beta_1+\\beta_2X_{i2}+\\varepsilon_i\n\\] We know that the OLS estimators are give by,\n\\[\n\\begin{aligned}\n\\tilde{\\beta}_2 =& \\frac{\\sum(Y_i-\\bar{Y})(X_{i2}-\\bar{X}_2)}{\\sum_{i=1}^n(X_{i2}-\\bar{X}_2)^2} \\\\\n\\text{and}\\qquad \\tilde{\\beta}_1 =& \\bar{Y}-\\tilde{\\beta_2}\\bar{X}_2\n\\end{aligned}\n\\]\nI am deliberately using the notation \\(\\tilde{\\beta}\\) to distinguish these two estimators from the expression below. Let us see if we can replicate this result, using vector notation. The the model is,\n\\[\n\\begin{aligned}\nY =& X\\beta+\\varepsilon \\\\\n=& \\begin{bmatrix}1&X_{12} \\\\ 1 & X_{22} \\\\ \\vdots & \\vdots \\\\ 1 & X_{n2}\\end{bmatrix}\\begin{bmatrix}\\beta_1 \\\\ \\beta_2 \\end{bmatrix} + \\varepsilon \\\\\n=& \\begin{bmatrix}\\ell &X_{2} \\end{bmatrix}\\begin{bmatrix}\\beta_1 \\\\ \\beta_2 \\end{bmatrix} + \\varepsilon\n\\end{aligned}\n\\]\nTherefore,\n\\[\n\\begin{aligned}\n\\hat{\\beta} = \\begin{bmatrix}\\hat{\\beta}_1 \\\\ \\hat{\\beta}_2\\end{bmatrix}=&(X'X)^{-1}X'Y \\\\\n=&\\bigg(\\begin{bmatrix}\\ell' \\\\ X_{2}' \\end{bmatrix}\\begin{bmatrix}\\ell &X_{2}\\end{bmatrix}\\bigg)^{-1}\\begin{bmatrix}\\ell'  \\\\ X_{2}' \\end{bmatrix}Y \\\\\n=&\\begin{bmatrix}\\ell'\\ell & \\ell'X_2 \\\\ X_{2}'\\ell & X_2'X_2 \\end{bmatrix}^{-1}\\begin{bmatrix}\\ell'Y  \\\\ X_{2}'Y \\end{bmatrix}\n\\end{aligned}\n\\]\nI went through this rather quickly, using a number of linear algebra rules that you may not be familiar with. Do not worry, the point of the exercise is not become a linear algebra master, but instead to focus on the element of each of each matrix/vector. Each element is a scalar (size \\(1\\times 1\\)).\nIf we right them each down as sums you they might be a little more familiar. First consider the \\(2\\times 2\\) matrix:\n\nelement [1,1]: \\(\\ell'\\ell = \\sum_{i=1}^n 1 = n\\)\nelement [1,2]: \\(\\ell'X_2 = \\sum_{i=1}^nX_{i2} = n\\bar{X}_2\\)\nelement [2,1]: \\(X_2'\\ell = \\sum_{i=1}^nX_{i2} = n\\bar{X}_2\\) (as above, since scalars are symmetric)\nelement [2,2]: \\(X_2'X_2=\\sum_{i=1}^nX_{i2}^2\\)\n\nNext, consider the final \\(2\\times 1\\) vector,\n\nelement [1,1]: \\(\\ell'Y = \\sum_{i=1}^n Y_i = n\\bar{Y}\\)\nelement [2,1]: \\(X_2'Y = \\sum_{i=1}^nY_iX_{i2}\\)\n\nOur OLS estimator is therefore,\n\\[\n\\hat{\\beta} = \\begin{bmatrix} n & n\\bar{X}_2 \\\\ n\\bar{X}_2 & \\sum_{i=1}^nX_{i2}^2 \\end{bmatrix}^{-1}\\begin{bmatrix}n\\bar{Y}  \\\\ \\sum_{i=1}^nY_iX_{i2} \\end{bmatrix}\n\\]\nWe now need to solve for the inverse of the \\(2\\times 2\\) matrix. You can easily find notes on how to do this online. Here, I will just provide the solution.\n\\[\n\\hat{\\beta} = \\frac{1}{n\\sum_{i=1}^nX_{i2}^2-n^2\\bar{X}_2^2}\\begin{bmatrix} \\sum_{i=1}^nX_{i2}^2 & -n\\bar{X}_2 \\\\ -n\\bar{X}_2 &  n\\end{bmatrix}\\begin{bmatrix}n\\bar{Y}  \\\\ \\sum_{i=1}^nY_iX_{i2} \\end{bmatrix}\n\\]\nRemember, this is still a \\(2\\times 1\\) vector. We can now solve for the final solution:\n\\[\n\\begin{aligned}\n\\hat{\\beta} =& \\frac{1}{n\\sum_{i=1}^nX_{i2}^2-n^2\\bar{X}_2^2}\\begin{bmatrix} n\\bar{Y}\\sum_{i=1}^nX_{i2}^2 -n\\bar{X}_2\\sum_{i=1}^nY_iX_{i2} \\\\ n\\sum_{i=1}^nY_iX_{i2}-n^2\\bar{X}_2\\bar{Y}\\end{bmatrix} \\\\\n=& \\frac{1}{n\\sum_{i=1}^n(X_{i2}-\\bar{X}_2)^2}\\begin{bmatrix} n\\bar{Y}\\sum_{i=1}^nX_{i2}^2 + n^2\\bar{Y}\\bar{X}^2 - n^2\\bar{Y}\\bar{X}^2 -n\\bar{X}_2\\sum_{i=1}^nY_iX_{i2} \\\\ n\\sum_{i=1}^n(Y_i-\\bar{Y})(X_{i2}-\\bar{X}_2) \\end{bmatrix} \\\\\n=& \\frac{1}{n\\sum_{i=1}^n(X_{i2}-\\bar{X}_2)^2}\\begin{bmatrix} n\\bar{Y}\\sum_{i=1}^n(X_{i2}-\\bar{X})^2 -n\\bar{X}_2\\sum_{i=1}^n(Y_i-\\bar{Y})(X_{i2}-\\bar{X}_2) \\\\ n\\sum_{i=1}^n(Y_i-\\bar{Y})(X_{i2}-\\bar{X}_2) \\end{bmatrix} \\\\\n=& \\begin{bmatrix} \\bar{Y}  -\\frac{n\\sum_{i=1}^n(Y_i-\\bar{Y})(X_{i2}-\\bar{X}_2)}{n\\sum_{i=1}^n(X_{i2}-\\bar{X}_2)^2}\\bar{X}_2 \\\\ \\frac{n\\sum_{i=1}^n(Y_i-\\bar{Y})(X_{i2}-\\bar{X}_2)}{n\\sum_{i=1}^n(X_{i2}-\\bar{X}_2)^2} \\end{bmatrix} \\\\\n=& \\begin{bmatrix} \\bar{Y}  -\\tilde{\\beta}_2\\bar{X}_2 \\\\ \\tilde{\\beta}_2 \\end{bmatrix} \\\\\n=& \\begin{bmatrix}\\tilde{\\beta}_1 \\\\ \\tilde{\\beta}_2 \\end{bmatrix}\n\\end{aligned}\n\\]\nThe math is a little involved, but it shows you these solutions are are the same. Unfortunately, the working gets even more arduous in a multivariate context. However, there are useful tools to help us with that we will discuss next.\n\n\n5.2 Geometry of OLS\nIn the last section we saw how the OLS estimator can, more generally, be described as a linear transformation of the \\(Y\\) vector.\n\\[\n\\hat{\\beta} = (X'X)^{-1}X'Y\n\\]\nWe also saw that in order for there to be a (unique) solution to the least squared problem, the \\(X\\) matrix must be full rank. This rules out any perfect colinearity between columns (i.e. regressors) in the \\(X\\) matrix, including the constant.\nGiven the vector of OLS coefficients, we can also estimate the residual,\n\\[\n\\begin{aligned}\n\\hat{\\varepsilon} =& Y - X\\hat{\\beta} \\\\\n=&Y-X(X'X)^{-1}X'Y \\\\\n=&(I_n-X(X'X)^{-1}X)Y\n\\end{aligned}\n\\]\nby plugging the definition of \\(\\hat{\\beta}\\). Thus, the OLS estimator separates the vector \\(Y\\) into two components:\n\\[\n\\begin{aligned}\nY =& X\\hat{\\beta} + \\hat{\\varepsilon} \\\\\n=&\\underbrace{X(X'X)^{-1}X'}_{P_X}Y + (\\underbrace{I_n-X(X'X)^{-1}X}_{I_n-P_X = M_X})Y \\\\\n=&P_XY + M_XY\n\\end{aligned}\n\\]\nThe matrix \\(P_X = X(X'X)^{-1}X'\\) is a \\(n\\times n\\) projection matrix. It is a linear transformation that projects any vector into the span of \\(X\\): \\(S(X)\\subset\\mathbb{R}^n\\). (See for more information on these terms.) \\(S(X)\\) is the vector space spanned by the columns of \\(X\\). The dimensions of this vector space depends on the rank of \\(P_X\\),\n\\[\ndim(S(X)) = r(P_X) = r(X) = k\n\\]\nThe matrix \\(M_X = I_n-X(X'X)^{-1}X'\\) is also a \\(n\\times n\\) projection matrix. It projects any vector into \\(X\\)’s orthogonal span: \\(S^{\\perp}(X)\\). Any vector \\(z\\in S^{\\perp}(X)\\) is orthogonal to \\(X\\). This includes the estimated residual, which is by definition orthogonal to the predicted values and, indeed, any column of \\(X\\) (i.e. any regressor). The dimension of this orthogonal vector space depends on the rank of \\(M_X\\),\n\\[\ndim(S^{\\perp}(X)) = r(M_X) = r(I_n)-r(X) = n-k\n\\]\nThe orthogonality of these two projections can be easily shown, since projection matrices are idempotent (\\(P_XP_X = P_X\\)) and symmetric (\\(P_X' = P_X\\)). Consider the inner product of these two projections,\n\\[\nP_X'M_X = P_X(I_n-P_X) = P_X-P_XP_X = P_X-P_X = 0\n\\]\nThe least squares estimator is a projection of Y into two vector spaces: one the span of the columns of \\(X\\) and the other a space orthogonal to \\(X\\).\nWhy is this useful? Well, it helps us understand the “mechanics” (technically geometry) of OLS. When working with linear regression models, we typically assume either strict exogeneity - \\(E[\\varepsilon|X]=0\\) - or uncorrelatedness - \\(E[X'\\varepsilon]=0\\) - where the former implies the latter (but not the other way around).\nWhen we use OLS, we estimate the vector \\(\\hat{\\beta}\\) such that,\n\\[\nX'(Y-X\\hat{\\beta})=X'\\hat{\\varepsilon}=0 \\quad always\n\\]\nThis is true, not just in expectation, but by definition. The relationship is “mechanical”: the regressors and estimated residual are perfectly uncorrelated. This can be easily shown:\n\\[\n\\begin{aligned}\nX'\\hat{\\varepsilon} =& X'M_XY \\\\\n=& X'(I_n-P_X)Y \\\\\n=&X'I_nY-X'X(X'X)^{-1}X'Y \\\\\n=&X'Y-X'Y \\\\\n=&0\n\\end{aligned}\n\\]\nYou are essentially imposing the assumption of uncorrelatedness between the explained and unexplained components of Y on the data. This means that if the assumption is wrong, so is the projection.\n\n\n5.3 Partitioned regression\nThe tools of linear algebra can help us better understand partitioned regression. Indeed, I would go as far to to say that it is quite difficult to understand partitioned regression without an understanding of projection matrices. Moreover, we need to understand partitioned regression to really understand multivariate regression. The partitioned regression result is referred to as Frisch-Waugh-Lovell Theorem (FWL).\n\nTheorem 1 FWL says that if you have two sets of regressors, \\([X_1,X_2]\\), then \\(\\hat{\\beta}_1\\), the OLS estimator for \\(\\beta_1\\), from the regression,\n\\[\n        Y = X_1\\beta_1 + X_2\\beta_2 + \\varepsilon\n\\]\nis also given by the regression,\n\\[\n        M_2Y = M_2X_1\\beta_1 + \\xi\n\\]\n\nWe can demonstrate the FWL theorem result using projection matrices. Two simplify matters, we will divide the set of regressors into two groups: \\(X_1\\) a single regressor and \\(X_2\\) a \\(n\\times (k-1)\\) matrix. We can rewrite the linear model as,\n\\[\nY = X\\beta+ \\varepsilon =\\beta_1X_1+X_2\\beta_2+\\varepsilon\n\\]\nLet us begin by applying our existing knowledge. From above, we know that the residual from the regression of \\(X_1\\) on \\(X_2\\) is,\n\\[\n\\hat{\\upsilon} = M_2X_1\n\\]\nwhere \\(M_2 = I_n-X_2(X_2'X_2)^{-1}X_2'\\). It turns out, it does not matter if we residualize \\(Y\\) too. Can you see why? Thus, the model we estimate in the second step, is\n\\[\nY = \\gamma_1\\underbrace{M_2X_1}_{\\hat{\\upsilon}}+\\xi\n\\]\nWe know that \\(\\hat{\\gamma}_1 = (\\hat{\\upsilon}'\\hat{\\upsilon})^{-1}\\hat{\\upsilon}'Y\\). Replacing the value of the residual, we get,\n\\[\n\\begin{aligned}\n\\hat{\\gamma}_1 =& (\\hat{\\upsilon}'\\hat{\\upsilon})^{-1}\\hat{\\upsilon}'Y \\\\\n=&(X_1'M_2M_2X_1)^{-1}X_1'M_2Y \\\\\n=&(X_1'M_2X_1)^{-1}X_1'M_2Y\n\\end{aligned}\n\\] Here, we use both the symmetric and idempotent qualities of \\(M_2\\). Next we want to show that \\(\\hat{\\beta}_1\\) is given by the same value. This part is more complicated. Let’s start with by reminding ourselves of the following:\n\\[\n\\begin{aligned}\nX'X\\hat{\\beta} &= X'Y \\\\\n\\begin{bmatrix}X_1 & X_2\\end{bmatrix}'\\begin{bmatrix}X_1 & X_2\\end{bmatrix}\\begin{bmatrix}\\hat{\\beta}_1 \\\\ \\hat{\\beta}_2\\end{bmatrix} &= \\begin{bmatrix}X_1 & X_2\\end{bmatrix}'Y \\\\\n\\begin{bmatrix}X_1'X_1 & X_1'X_2 \\\\ X_2'X_1 & X_2'X_2 \\end{bmatrix}\\begin{bmatrix}\\hat{\\beta}_1 \\\\ \\hat{\\beta}_2\\end{bmatrix} &= \\begin{bmatrix}X_1'Y \\\\ X_2'Y\\end{bmatrix}\n\\end{aligned}\n\\]\nWe could solve for \\(\\hat{\\beta}_1\\) by solving for the inverse of \\(X'X\\); however, this will take a long time. An easier approach is to simply verify that \\(\\hat{\\beta}_1=(X_1'M_2X_1)^{-1}X_1'M_2Y\\). Recall, \\(\\hat{\\beta}\\) splits \\(Y\\) into two components:\n\\[\nY = \\hat{\\beta}_1X_1+X_2\\hat{\\beta}_2 + \\hat{\\varepsilon}\n\\]\nIf we plug this definition of \\(Y\\) into the above expression we get,\n\\[\n\\begin{aligned}\n&(X_1'M_2X_1)^{-1}X_1'M_2(\\hat{\\beta}_1X_1+X_2\\hat{\\beta}_2 + \\hat{\\varepsilon}) \\\\\n=&\\hat{\\beta}_1\\underbrace{(X_1'M_2X_1)^{-1}X_1'M_2X_1}_{=I_n} \\\\\n&+\\underbrace{(X_1'M_2X_1)^{-1}X_1'M_2X_2\\hat{\\beta_2}}_{=0} \\\\\n&+\\underbrace{(X_1'M_2X_1)^{-1}X_1'M_2\\hat{\\varepsilon}}_{=0} \\\\\n=&\\hat{\\beta}_1\n\\end{aligned}\n\\]\nIn line 2, I use the fact that \\(\\hat{\\beta}_1\\) is a scalar and can be moved to the front (since the order of multiplication does not matter with a scalar). In line 3, I use the fact that \\(M_2X_2=0\\) by definition. Line 4 uses the fact that \\(M_2\\hat{\\varepsilon}=\\hat{\\varepsilon}\\) which means that \\(X_1'M_2\\hat{\\varepsilon}=X_1'\\hat{\\varepsilon}=0\\).\nThe OLS estimator solves for \\(\\beta_1\\) using the variance in \\(X_1\\) that is orthogonal to \\(X_2\\). This is the manner in which we “hold \\(X_2\\) constant”: the variation in \\(M_2X_1\\) is orthogonal to \\(X_2\\). Changes in \\(M_2X_1\\) are uncorrelated with changes in \\(X_2\\); as if the variation in \\(M_2X_1\\) arose independently of \\(X_2\\). However, uncorrelatedness does NOT imply independence."
  },
  {
    "objectID": "handout-1.html#footnotes",
    "href": "handout-1.html#footnotes",
    "title": "Classical Linear Regression Model & Ordinary Least Squares",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMy notation assumes that \\(X_i\\) is a column vector, which makes \\(X_i'\\beta\\) a scalar. Wooldridge (2010) uses the notation \\(X_i\\beta\\), implying that \\(X_i\\) is a row vector. This is a matter of preference.↩︎\nYou might also refer to the vector of regressors or explanatory variables. The terms covariates or control variables are more common in Microeconometrics literature, where regressors are typically included for identification of causal effects. Some texts will use the term independent variables, but this name implies a specific relationship between \\(Y\\) and \\(X\\) that need not hold. Note, we will asssume in this term that \\(n&gt;k\\); i.e. this is “small” data.↩︎\nThis is NOT the residual.↩︎\nSee extra material on Linear Algebra to read more on rank.↩︎\nFor \\(n\\) large, \\(rank(E[X_iX_i'])=k\\Rightarrow rank(X)=k\\). This follows from Law of Large Numbers, since \\(plim(n^{-1}X'X) = E[X_iX_i']\\).↩︎\nThere are many such failures of (parametric) identification in models that include dummy variables (or fixed effects). Earlier we saw that the intercept is not separately identified from the mean of the error term. Mean independence of the error term, \\(E[\\varepsilon_i]=0\\), is required for us to separately ‘identify’ \\(\\beta_1\\).↩︎\nWhen working with vectors and matrices it is important to keep track of their size. You can only multiply two matrices/vectors if their column and row dimensions match. For example, if \\(A\\) and \\(B\\) are both \\(n\\times k\\) matrices (\\(n\\neq k\\)), then \\(AB\\) is not defined since \\(A\\) has \\(k\\) columns and \\(B\\) \\(n\\) rows. For the same reason \\(BA\\) is also not defined. However, you can pre-multiply \\(B\\) with \\(A'\\) as \\(A'\\) is a \\(k\\times n\\) matrix: \\(A'B\\) is therefore a \\((k\\times n)\\cdot (n\\times k)=k\\times k\\) matrix. Similarly, \\(B'A\\) is defined, but is a \\(n\\times n\\) matrix.\nOrder matters when working with matrices and vectors. Pre-multiplication and post-multiplication are not the same thing.\nKeep track of the size of each term to ensure they correspond to one another. In this instance, each term should be a scalar. For example, \\(-2b'X'Y\\) is the multiplication of a scalar (\\(-2\\): size \\(1\\times 1\\)), row vector (\\(b'\\): size \\(1\\times k\\)), matrix (\\(X'\\): size \\(k\\times n\\)), and column vector (\\(Y\\): size \\(n\\times 1\\)). Thus we have a \\((1\\times 1)\\cdot (1\\times k)\\cdot (k\\times n)\\cdot (n\\times 1)=1\\times 1\\).↩︎\nOnce you are familiar with vector notation, it is relatively easy to tell whether a model is uni- or multi-variate. This is because the notation \\(\\beta_2 X_{i2}\\) is not consistent with \\(X_{2i}\\) being a vector (row or column).\nIf \\(X_{i2}\\) is a \\(k\\times 1\\) vector then so is \\(\\beta_2\\). Thus, \\(\\beta_2 X_{i2}\\) is \\((k\\times 1)\\cdot (k\\times1)\\), which is not defined.\nIf \\(X_{i2}\\) is a row vector (as in Wooldridge, 2011), \\(\\beta_2 X_{i2}\\) will then be \\((k\\times 1)\\cdot (1\\times k)\\), a \\(k\\times k\\) matrix. This cannot be correct since the model is defined at the unit level.\nThus, if you see a model written with the parameter in front of the regressor, you know that this must be a single regressor. This is subtle, yet imporant, distinction that researchers often use to convey the structure of their model. Whenever \\(X_{i2}\\) is a vector, researchers will almost always use the notation \\(X_{i2}'\\beta\\) or \\(X_{i2}\\beta\\), depending on whether \\(X_{i2}\\) is assumed to be a column or row vector.↩︎"
  },
  {
    "objectID": "handout-2.html",
    "href": "handout-2.html",
    "title": "Estimators and their Properties",
    "section": "",
    "text": "Code\nlibrary(ggplot2)\nlibrary(latex2exp)"
  },
  {
    "objectID": "handout-2.html#overview",
    "href": "handout-2.html#overview",
    "title": "Estimators and their Properties",
    "section": "1 Overview",
    "text": "1 Overview\nIn this handout we will investigate the desirable properties of an estimator. Further reading can be found in:\n\nAppendix A of Cameron and Trivedi (2005)\nSection 2.6 of Verbeek (2017)"
  },
  {
    "objectID": "handout-2.html#an-estimator",
    "href": "handout-2.html#an-estimator",
    "title": "Estimators and their Properties",
    "section": "2 An estimator",
    "text": "2 An estimator\nAn estimator is a rule that tells you how to get your estimate given the observed data. As it is a that it is a function of random variables, an estimator is a random variable itself.\nWithout further information, we do not know anything about the distribution of this random variable and, in practice, will typically have to estimate the parameters of this distribution: mean, variance, skewness.\nThe assumed model (or data generating process) may pin down certain parameters or distributions. For example, under assumpions CLRM 5 & 6 the ordinary least squares estimator for \\(\\beta\\) will be normally distributed."
  },
  {
    "objectID": "handout-2.html#properties-of-estimators",
    "href": "handout-2.html#properties-of-estimators",
    "title": "Estimators and their Properties",
    "section": "3 Properties of estimators",
    "text": "3 Properties of estimators\nWhen evaluating an estimator you want to consider:\n\nbias: does the estimator yield estimates that ‘hit the right target’ on average?\nefficiency: do the estimates generated by the estimator have a limited dispersion/variance?\ndistribution: do you know the exact/approximate distribution of the estimator with which you can construct a valid hypothesis test?\n\nIt is important to consider both the small sample and large sample (asymptotic) properties of an estimator.\nUnbiasedness (property 1) is typically emphasized over properties 2 and 3. Afterall, a precise estimator of the wrong target is not particularly useful. That said, the expected value of an estimator is not always well defined.\n\n\n\n\n\n\nImportant\n\n\n\nThese are properties of the estimator, a random variable, and not the estimate. The estimate is a just a constant: a non-random realization of the estimator.\n\n\n\n3.1 Small vs large sample properties\nSometimes it is easier to study the large-sample, or asymptotic, properties of an estimator. That is, the properties of the estimator as the sample size gets large: \\(n\\rightarrow \\infty\\).\nSmall-sample properties include:\n\nBiasedness\nEfficiency/variance\nFinite-sample distribution (which might not be known)\n\nLarge-sample properties include:\n\nConsistency\nAsymptotic distribution\n\nAn estimator can have an asymptotic variance (efficiency); although, in EC910 will mostly discuss estimators with variances that shrink to zero as \\(n\\rightarrow \\infty\\).\n\n\n\n\n\n\nImportant\n\n\n\nThe phrase “finite-sample” is also used in other contexts. For example, in the Causal Inference (or Treatment Effects) literature “finite-sample” is used to describe estimands. This literature distinguishes between finite-sample and super-population estimands. The former relate to settings where the sample is treated as fixed, while the latter to settings where the sample is take as a draw from an unknown super population.\nThe estimator for a super-population estimand will have both finite and asymptotic properities.\n\n\n\n\n3.2 Bias\nThe estimator \\(\\hat{\\theta}\\) is unbiased for \\(\\theta\\) if,\n\\[\nE[\\hat{\\theta}] = \\theta\n\\] \\(\\hat{\\theta}\\) is a function of sample size, \\(n\\), while \\(\\theta\\) is not. Some texts will use the notation \\(\\hat{\\theta}_n\\) to emphasize this point.\n\n\nCode\nx_values &lt;- seq(-2, 3, length.out = 100)\nggplot(data.frame(x = x_values), aes(x = x)) +\n  stat_function(fun = dnorm, args = list(mean = 1, sd = 1), color = \"red\") + \n  stat_function(fun = dnorm, args = list(mean = 0, sd = .5), color = \"blue\") + \n  geom_vline(xintercept = 1, linetype = \"dashed\", color = \"red\", linewidth = 1) +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"blue\", linewidth = 1) + \n  labs(\n    title = NULL,\n    x = NULL,\n    y = \"Density\"\n  ) +\n  scale_x_continuous(\n    breaks = c(0, 1),            \n    labels = c(TeX(\"$\\\\theta$\"), TeX(\"$E[\\\\hat{\\\\theta}]$\"))  \n  ) +\n  theme_minimal() +\n  annotate(\"text\", x = 1.75, y = 0.35, label = \"Biased\", vjust = -1, hjust = 0.5, size = 5, color = \"red\") +\n  annotate(\"text\", x = -0.75, y = 0.6, label = \"Unbiased\", vjust = -1, hjust = 0.5, size = 5, color = \"blue\")\n\n\n\n\n\n\n\n\n\n\n\n3.3 Efficiency\nEfficiency is a relative concept. The estimator \\(\\hat{\\theta}\\) is more efficient the estimator \\(\\tilde{\\theta}\\), if both are unbiased for \\(\\theta\\) and\n\\[\nVar(\\hat{\\theta})&lt;Var(\\tilde{\\theta})\n\\] What if the estimator is biased? We can use Mean Square Error:\n\nDefinition 1 (MSE) \\(MSE = E\\big[(\\hat{\\theta}-\\theta)^2\\big] = \\text{bias}^2+\\text{variance}\\)\n\nNote, the above definition assumes that a finite variance.\n\nProof. \\[\n\\begin{aligned}\nE\\big[(\\hat{\\theta}-\\theta)^2\\big] &= E\\big[\\big((\\hat{\\theta}-E[\\hat{\\theta}])+(E[\\hat{\\theta}]-\\theta)\\big)^2\\big] \\\\\n&=E\\big[(\\hat{\\theta}-E[\\hat{\\theta}])^2\\big]+ E\\big[(\\theta-E[\\hat{\\theta}])^2\\big] + 2E\\big[(\\hat{\\theta}-E[\\hat{\\theta}])(\\theta-E[\\hat{\\theta}])\\big] \\\\\n&=Var(\\hat{\\theta}) + \\text{bias}^2\n\\end{aligned}\n\\]\n\nThe first line adds and subtracts the mean of the estimator, which need not be \\(\\theta\\), the targetted population parameter. The second expands the outside exponent over the two bracketted terms.\nThe cross-product term in line 2 is 0. Can you show this?\n\n\n3.4 Consistency\nConsistency is an asymptotic property of an estimator.\n\nDefinition 2 The estimator \\(\\hat{\\theta}_n\\) is a consistent estimator of \\(\\theta\\) if it converges in probability to \\(\\theta\\) as \\(n\\rightarrow\\infty\\),\n\\[\nPr(|\\hat{\\theta}_n-\\theta|\\geq\\varepsilon)\\rightarrow 0\\qquad n\\rightarrow \\infty\n\\] for \\(\\varepsilon\\) very small.1\n\nIntuitively, this means that tail area probabilities (i.e. probability of an estimator very far from the true value) goes to zero as the sample size gets large.\nWe use the notations, \\(p\\lim\\),\n\\[\np\\lim\\hat{\\theta}_n = \\theta\n\\]\nor convergence in probability,\n\\[\\hat{\\theta}_n \\rightarrow_p \\theta\\qquad\\text{as}\\qquad n\\rightarrow \\infty\\]\nAn important theorem regarding the consistency of estimators is Slutzky’s theorem concerning continuous functions of estimators. For example, we will use this theorem to prove the consistency of the OLS estimator.\n\nTheorem 1 (Slutzky’s Theorem) If \\(p\\lim\\hat{\\theta}_n = \\theta\\) and \\(h(\\cdot)\\) is a continuous function, then\n\\[\np\\lim\\;h(\\hat{\\theta}_n) = h\\big(p\\lim\\hat{\\theta}_n\\big)=h(\\theta)\n\\]\n\nHere are some useful examples,\n\nExample 1 Given two consistent estimators \\(\\big[\\hat{\\theta}_n,\\hat{\\beta}_n\\big]\\),\n\n\\(p\\lim\\;(a\\hat{\\theta}_n+ b\\hat{\\beta}_n) = a(p\\lim\\;\\hat{\\theta}_n)+b(p\\lim\\;\\hat{\\beta}_n) =a\\theta +b\\beta\\) for constants \\(a\\) and \\(b\\)\n\\(p\\lim\\;(\\hat{\\theta}_n\\times\\hat{\\beta}_n) = (p\\lim\\;\\hat{\\theta}_n)\\times (p\\lim\\;\\hat{\\beta}_n)=\\theta\\beta\\)\n\\(p\\lim\\;(\\hat{\\theta}_n^2) = (p\\lim\\;\\hat{\\theta}_n)^2 = \\theta^2\\)\n\\(p\\lim\\;\\bigg(\\frac{\\hat{\\theta}_n}{\\hat{\\beta}_n}\\bigg) = \\frac{\\theta}{\\beta}\\)\n\\(p\\lim\\;\\exp(\\hat{\\theta}_n) = \\exp(p\\lim\\;\\hat{\\theta}_n) = \\exp(\\theta)\\)\n\n\nMSE convergence (to zero) is a sufficient condition for consistency. However, it is not a necessary.\nFor an unbiased estimator,\n\\[\nVar(\\hat{\\theta}_n)\\rightarrow 0 \\quad \\text{as}\\quad n\\rightarrow \\infty \\implies p\\lim\\hat{\\theta}_n = \\theta\n\\]\nFor a biased estimator,\n\\[\nMSE(\\hat{\\theta}_n)\\rightarrow 0 \\quad \\text{as}\\quad n\\rightarrow \\infty \\implies p\\lim\\hat{\\theta}_n = \\theta\n\\]\n\nExercise 1 Given a sample of independently and identically distributed (iid) random variables, \\[\nX_1,...,X_n \\sim N(\\mu,\\sigma^2)\n\\] Show that the mean estimator - \\(\\overline{X} = \\frac{1}{n}\\sum_{i=1}X_i\\) is a consistent estimator of \\(\\mu\\); i.e. \\(p \\lim\\;\\overline{X} = \\mu\\).\n\nThe above exercise relates to one of the most important examples of convergence in probability:\n\nTheorem 2 (Weak Law of Large Numbers) Let \\(X_1,...,X_n\\) be a sample of iid random variables, such that \\(E[|X_1|]&lt;\\infty\\). Then, \\[\nn^{-1}\\sum_{i=1}^{n}X_i \\rightarrow_p E[X_1]\\qquad \\text{as}\\qquad n\\rightarrow \\infty\n\\]\n\nHere we use the notation \\(E[X_1]\\), since the data is iid and \\(E[X_i]=E[X_1]\\) for \\(i=1,...,n\\). We will prove a modified version of the WLLN theorem, assuming \\(E[X_1^2]&lt;\\infty\\). Since \\(E[X_1^2]&lt;\\infty\\implies\\) both \\(E[|X_1|]&lt;\\infty\\) and \\(Var(X_1)&lt;\\infty\\), we will have proven WLLN theorem.\n\nTheorem 3 Let \\(X_1,...,X_n\\) be a sample of iid random variables, such that \\(Var(X_1)&lt;\\infty\\). Then, \\[\nn^{-1}\\sum_{i=1}^{n}X_i \\rightarrow_p E[X_1]\\qquad \\text{as}\\qquad n\\rightarrow \\infty\n\\]\n\nTo complete the proof of WLLN, we will need to use Markov’s Inequality. We will not prove this lemma, but versions of the proof are readily available online.\n\nLemma 1 (Markov’s Inequality) Let \\(X\\) be a random variable. For \\(\\varepsilon&gt;0\\) and \\(r&gt;0\\), then\n\\[\nPr(|X|\\geq \\varepsilon)\\leq \\frac{E\\big[|X|^r\\big]}{\\varepsilon^r}\n\\]\n\nNow, we can complete the proof of WLLN, assuming a finite second moment.\n\nProof. \\[\n\\begin{aligned}\nPr\\bigg(\\bigg|n^{-1}\\sum_{i=1}^{n}X_i - E[X_1]\\bigg|\\geq \\varepsilon\\bigg) &= Pr\\bigg(\\bigg|n^{-1}\\sum_{i=1}^{n}(X_i - E[X_1])\\bigg|\\geq \\varepsilon\\bigg) \\\\\n&\\leq \\frac{E\\big[|\\sum_{i=1}^{n}(X_i - E[X_1])|^2\\big]}{n^2\\varepsilon^2} \\\\\n&=\\frac{\\sum_{i=1}^{n}\\sum_{j=1}^{n}E\\big[(X_i - E[X_1])(X_j - E[X_1])\\big]}{n^2\\varepsilon^2} \\\\\n&=\\frac{\\sum_{i=1}^{n}E\\big[(X_i - E[X_1])^2\\big]}{n^2\\varepsilon^2} \\\\\n&=\\frac{nVar(X_1)}{n^2\\varepsilon^2} \\\\\n&\\rightarrow 0 \\qquad \\text{as}\\qquad n\\rightarrow \\infty\n\\end{aligned}\n\\]\n\nNote, the WLLN holds under a weaker condition than iid. Between lines 3 and 4 we used the independence of observations to set correlations between units to 0. Thus, we required a weaker assumption of uncorrelateness: \\(Cov(X_i,X_j)=0\\;\\forall\\;i\\neq j\\).\nUnder the WLLN, you can show that the sample variance converges in probability to the population variance.\n\\[\np\\lim \\big(n^{-1}\\sum_{i=1}^{n}(X_i - \\overline{X})^2\\big) \\rightarrow_p Var(X_1)\n\\]\n\n\n3.5 Distribution\nIn this section we will focus on the asymptotic distribution of an estimator. If we know the joint distribution of the data, then we can potentially work out the distribution of the estimator in a finite sample. This is especially true when the random variables in question are drawn from known families of distributions.\nFor example, we know that the sum of Normal distributed random variables is Normally distributed itself. And the sum of the square of standard-Normally distributed random variables is Chi-squared distributed. We used these results do determine the distribution of test-statistics corresponding to the Classical Linear Regression Model.2\nHowever, what do you do when you do not know the underlying distribution of the random variables? Here we will rely on a the Central Limit Theorem, which tells us about the approximate distribution of an estimator when the sample is large.\nBefore discussing CLT, we must define a new convergence concept: convergence in distribution.\n\nDefinition 3 Let \\(X_1,...,X_n\\) be a sequence of random variables and let \\(F_n(x)\\) denote the marginal CDF of \\(X_n\\), \\[\nF_n(x) = Pr(X_n\\leq x)\n\\] Then, \\(X_n\\) converges in distribution to \\(X\\) if \\(F_n(x)\\rightarrow F(x)\\) as \\(n\\rightarrow \\infty\\), where \\(F(x)\\) is continuous.\n\nConvergence in distribution can be denoted,\n\\[\nX_n\\rightarrow_d X\n\\]\nwhere X is a random variable with distribution function \\(F(x)\\). Note, it is not the random variables that are converging, but rather the distributions of said random variables.\nAs with convergence in probability, there are some basic rules for manipulation.\n\nThe first is Cramer Convergence Theorem: Suppose \\(X_n\\rightarrow_dX\\) and \\(Y_n\\rightarrow_p c\\). then,\n\n\\(X_n+Y_n\\rightarrow_d X+c\\)\n\\(Y_nX_n\\rightarrow_d cX\\)\n\\(X_n/Y_n\\rightarrow_d X/c\\), for \\(c\\neq 0\\)\n\nIf \\(X_n\\rightarrow_p X\\), then \\(X_n\\rightarrow_d X\\). The converse is not true, with the exception:\nIf \\(X_n\\rightarrow_d C\\), a constant, then \\(X_n\\rightarrow_p C\\).\nIf \\(X_n-Y_n\\rightarrow_p 0\\), and \\(Y_n\\rightarrow_d Y\\), then \\(X_n\\rightarrow_d Y\\).\n\nUnlike with convergence in probability, \\(X_n\\rightarrow_d X\\) and \\(Y_n\\rightarrow_d Y\\) does NOT imply \\(X_n + Y_n \\rightarrow_d X+Y\\), unless joint convergences holds too. This is because the former are statements concerning the marginal distributions of \\(X_n\\) and \\(Y_n\\), while the distribution of \\(X_n + Y_n\\) depends on the joint distribution.\nIf \\((X_n,Y_n)\\rightarrow_d (X,Y)\\) (joint convergence), then \\(X_n+Y_n\\rightarrow_dX+Y\\). This result follows from the Central Mapping Theorem.\n\nTheorem 4 (Continuous Mapping Theorem) Suppose \\(X_n\\rightarrow_d X\\) and let \\(h(\\cdot)\\) be a continuous function. Then, \\(h(X_n)\\rightarrow_d h(X)\\).\n\nCMT holds for a vector of random variables as well as single random variable. Thus, if\n\\[\n\\begin{bmatrix}X_n \\\\ Y_n\\end{bmatrix}\\rightarrow_d \\begin{bmatrix}X \\\\ Y\\end{bmatrix}\n\\] then by CMT, \\[\nX_n+Y_n=\\begin{bmatrix}1 & 1\\end{bmatrix}\\begin{bmatrix}X_n \\\\ Y_n\\end{bmatrix} \\rightarrow_d\\begin{bmatrix}1 & 1\\end{bmatrix}\\begin{bmatrix}X \\\\ Y\\end{bmatrix} = X+Y\n\\]\nCan you show:\n\n\\((X_n,Y_n)\\rightarrow_d (X,Y)\\) (i.e. joint convergence) implies \\(X_n\\rightarrow_d X\\) (i.e. marginal convergence)?\n\\(\\sum_{j=1}^k X_{jn}^2 \\sim \\chi^2_k\\) for \\(X_n\\sim N(0,I_k)\\), \\(X_n\\in \\mathbf{R}^k\\).\n\nHaving discussed the CMT, we are now ready to discuss the Central Limit Theorem. Both are used extensively in Econometrics. We will not prove CLT as the proof requires a more detailed discussion of Moment Generating Functions.\nRecall, for an iid random sample, the sample average converges in probability to the population mean:\n\\[\np \\lim\\;\\overline{X} = E[X_1]\n\\]\nThe rate of convergence for this estimator is \\(\\sqrt{n}\\). The estimator is said to be root-\\(n\\)-consistent.\n\nTheorem 5 (Central Limit Theorem) Let \\(X_1,...,X_n\\) be a sample of iid random variables such that \\(E[X_1]=0\\) and \\(0&lt;E[X_1^2]&lt;\\infty\\). Then,\n\\[\nn^{-1/2}\\sum_{i=1}^{n}X_i\\rightarrow_d N(0,E[X_1^2])\n\\]\n\nConsider a random sample drawn independently from a distribution with mean \\(\\mu\\) and variance \\(\\sigma\\). Note, this distribution need not be Normal. It holds that, $X_1-,…,X_n-$ are iid and \\(E[X_1-\\mu]\\). In addition, \\(E[(X_1-\\mu)^2]=\\sigma^2&lt;\\infty\\). Therefore, by CLT\n\\[\nn^{1/2}(\\overline{X}-\\mu) = n^{-1/2}\\sum_{i=1}^{n}(X_i-\\mu)\\rightarrow_d N(0,\\sigma^2)\n\\]\nIn practice, we use CLT to determine the approximate distribution of an estimator in large samples. Based on the above result, we can say\n\\[\nn^{1/2}(\\overline{X}-\\mu)\\overset{a}{\\sim} N(0,\\sigma^2)\n\\] or,\n\\[\n\\overline{X}\\overset{a}{\\sim}N(\\mu,\\sigma^2/n)\n\\]\nwhere \\(\\sigma^2/n\\) is referred to as the asymptotic variance of \\(\\overline{X}\\). Here, the symbol \\(\\overset{a}{\\sim}\\) can be interpreted as “approximately in large samples”.\nWith CMT and CLT, we need one more theorem before we continue. We know from the WLLN that \\(\\overline{X}\\rightarrow_p E[X_1]=\\mu\\). Moreover, by Slutzky’s theorem we know that \\(h(\\overline{X})\\rightarrow_p h(\\mu)\\), for \\(h(\\cdot)\\) continuous. However, we do not know the approximate distribution of \\(h(\\overline{X})\\) in a large sample.\nConsider, \\(h(\\mu)\\) is a non-random constant and CLT applies to \\(n^{1/2}(\\overline{X}-\\mu)\\) and not \\(\\overline{X}\\). The latter implying that we cannot use CMT.\n\nTheorem 6 (Delta Method) Let \\(\\hat{\\theta}_n\\) be a random k-dimensional vector. Suppose that \\(n^{1/2}(\\hat{\\theta}_n-\\theta)\\rightarrow_d Y\\), where \\(\\theta\\) is a non-random k-dimensional vector and \\(Y\\) a random k-dimensional vector.\nLet \\(h: \\mathbf{R}^k\\rightarrow\\mathbf{R}^m\\) be a function (mapping) that is continuously differentiable on some open neighbourhood of \\(\\theta\\). Then,\n\\[\nn^{1/2}\\big(h(\\hat{\\theta}_n)-h(\\theta)\\big)\\rightarrow_d \\frac{\\partial h(\\theta)}{\\partial\\theta'}Y\n\\]\n\nThe proof involves Cramer’s Convergence Theorem, Slutzky’s Theorem, as well as the Mean Value Theorem (which we have not discussed). This result is used to derive the limiting distribution of non-linear models and their marginal effects (e.g. probit/logit), as well as non-linear tests of regression coefficients."
  },
  {
    "objectID": "handout-2.html#properties-of-the-expectation-operator",
    "href": "handout-2.html#properties-of-the-expectation-operator",
    "title": "Estimators and their Properties",
    "section": "4 Properties of the E[xpectation] operator",
    "text": "4 Properties of the E[xpectation] operator\n\nThe expectation of a continuously-distributed, random variable \\(X\\) can be defined as:\n\\[\nE[X] = \\int_{-\\infty}^{\\infty}tf_Xdt = \\int_{-\\infty}^{\\infty}tdF_X(t)\n\\]\nIf \\(X\\) takes on discrete values, \\(X \\in \\mathbf{X} = \\{x_1,x_2,...,x_m\\}\\), we can replace the integral with a summation and the probability density function (pdf: \\(f_X\\)) with a probability mass function (pmf: \\(p_X\\)).\n\\[\nE[X] = \\sum_{t\\in \\mathbf{X}}tp_X(t)\n\\]\nAn important property of the Exptation operator is that it is linear. Let \\(\\{a,b\\}\\) be two constants (non-random scalars), then\n\\[\nE[aX+b] = aE[X]+b\n\\]\nCan you show this, using the above definition of the expectation operator?\nSimilarly, consider two random varibales \\(X\\) and \\(Y\\). Then,\n\\[\nE[aX + bY] = E[aX] + E[bY] = aE[X] + bE[Y]\n\\]\nHowever, note\n\n\\(E[XY] = E[X] \\times E[Y]\\) if \\(X\\) and \\(Y\\) are independent\n\nThis follows from the fact that \\(f_{X,Y} = f_X\\cdot f_Y\\) if \\(X\\) and \\(Y\\) are independent. Note, this is not an iff (if-and-only-if) statement.\n\n\\(E\\left[\\frac{X}{Y}\\right]\\neq\\frac{E[X]}{E[Y]}\\) for \\(E[Y]\\neq0\\)\n\\(E[\\ln(X)]\\neq \\ln(E[Y])\\)\n\nIn general, \\(E[h(X)]\\neq h(E[Y])\\) with the exception of \\(h(\\cdot)\\) linear function."
  },
  {
    "objectID": "handout-2.html#footnotes",
    "href": "handout-2.html#footnotes",
    "title": "Estimators and their Properties",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n\\(|x-a|&gt;b\\; \\implies \\;-b&gt;x-a\\quad\\text{and}\\quad b&lt;x-a\\)↩︎\nIn this setting, we make an assumption around the distribution of the error term in the model.↩︎"
  },
  {
    "objectID": "handout-4.html",
    "href": "handout-4.html",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "In this handout we will see how to test linear hypotheses. These could be,\n\nsingle hypothesis concerning single parameter;\nsingle hypothesis concerning multiple parameters;\nmultiple hypothesis concerning more than one parameter.\n\nFurther reading can be found in:\n\nSection 7.1-7.4 of Cameron and Trivedi (2005)\nSection 6.2 & 6.3 of Verbeek (2017)"
  },
  {
    "objectID": "handout-4.html#overview",
    "href": "handout-4.html#overview",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "In this handout we will see how to test linear hypotheses. These could be,\n\nsingle hypothesis concerning single parameter;\nsingle hypothesis concerning multiple parameters;\nmultiple hypothesis concerning more than one parameter.\n\nFurther reading can be found in:\n\nSection 7.1-7.4 of Cameron and Trivedi (2005)\nSection 6.2 & 6.3 of Verbeek (2017)"
  },
  {
    "objectID": "handout-4.html#linear-hypotheses",
    "href": "handout-4.html#linear-hypotheses",
    "title": "Hypothesis Testing",
    "section": "2 Linear Hypotheses",
    "text": "2 Linear Hypotheses\nAssume that the underlying model is linear in parameters, \\[\n  Y = X\\beta + u\n\\] Consider the linear hypothesis, \\[\n  R\\beta = r\n\\] where \\(R\\) is an \\(q\\times k\\), non-random matrix and \\(r\\) a \\(q\\times 1\\) non-random vector of constants.\nFor example, consider the linear model from Problem Set 2, \\[\nY = \\beta_1 + \\beta_2X_2 + \\beta_3X_3 + \\beta_4X_4 + \\beta_5X_5 + u\n\\] we can express the single (\\(q=1\\)) linear hypothesis \\(H_0: \\beta_2 = \\beta_3 \\Rightarrow\\beta_2-\\beta_3 = 0\\) as \\[\nH_0: R\\beta = \\begin{bmatrix}0&1&-1&0&0\\end{bmatrix}\\begin{bmatrix}\\beta_1 \\\\ \\beta_2 \\\\ \\beta_3 \\\\ \\beta_4 \\\\ \\beta_5\\end{bmatrix} = 0\n\\] In the case of a single linear hypothesis, we can also use the notation \\(c'\\beta = r\\) where \\(c\\) is a \\(k\\times 1\\) non-random vector.\nWe can express the multiple (\\(q=2\\)) linear hypotheses from this exercise \\[\nH_0: \\beta_2 = \\beta_3\\quad\\text{and}\\quad\\beta_4 + \\beta_5 = 1\n\\] as, \\[\nR\\beta = \\begin{bmatrix}0&1&-1&0&0 \\\\ 0&0&0&1&1\\end{bmatrix}\\begin{bmatrix}\\beta_1 \\\\ \\beta_2 \\\\ \\beta_3 \\\\ \\beta_4 \\\\ \\beta_5\\end{bmatrix} = \\begin{bmatrix}0 \\\\1 \\end{bmatrix}\n\\]"
  },
  {
    "objectID": "handout-4.html#distribution-under-h_0",
    "href": "handout-4.html#distribution-under-h_0",
    "title": "Hypothesis Testing",
    "section": "3 Distribution under \\(H_0\\)",
    "text": "3 Distribution under \\(H_0\\)\nLet us assume the CLRM assumptions, including normality and homoskedasticity\n\nCLRM 5: \\(u|X \\sim N(0,\\sigma^2 I_n)\\)\n\nWe know that under these assumptions the OLS estimator is normally distributed,\n\\[\n\\hat{\\beta}|X \\sim N(\\beta,\\sigma^2 (X'X)^{-1})\n\\]\nThis implies that,1\n\\[\nR\\hat{\\beta}|X \\sim N(R\\beta,\\sigma^2 R(X'X)^{-1}R')\n\\]\nUnder the null hypothesis, \\(H_0: R\\beta = r\\),\n\\[\nR\\hat{\\beta}|X \\sim N(r,\\sigma^2 R(X'X)^{-1}R')\n\\] From Handout 3, we know the asymptotic distribution of \\(\\hat{\\beta}\\) is normal, even if the (conditional) finite sample distribution of \\(u\\) is not. Applying the Delta Method (see Handout 2) to this result,\n\\[\n  \\sqrt{n} (R\\hat{\\beta}_n-R\\beta)\\rightarrow_d N(0,\\sigma^2R(E[X_1X_1'])^{-1}R')\\qquad\\text{as}\\quad n\\rightarrow\\infty\n\\]\nWhich means that, under \\(H_0\\),\n\\[\n  \\sqrt{n} (R\\hat{\\beta}_n-r)\\rightarrow_d N(0,\\sigma^2R(E[X_1X_1'])^{-1}R')\\qquad\\text{as}\\quad n\\rightarrow\\infty\n\\]"
  },
  {
    "objectID": "handout-4.html#single-linear-hypothesis",
    "href": "handout-4.html#single-linear-hypothesis",
    "title": "Hypothesis Testing",
    "section": "4 Single linear hypothesis",
    "text": "4 Single linear hypothesis\nAssuming CLRM 5, we know that for any \\(k\\times 1\\) non-random vector \\(c\\),\n\\[\nc'\\hat{\\beta}|X \\sim N(c'\\beta,\\sigma^2 c'(X'X)^{-1}c)\n\\] Which then implies, \\[\n\\frac{c'\\hat{\\beta}-c'\\beta}{\\sqrt{\\sigma^2 c'(X'X)^{-1}c}}|X \\sim N(0,1)\n\\] Under \\(H_0: c'\\beta = r\\), \\[\n\\frac{c'\\hat{\\beta}-r}{\\sqrt{\\sigma^2 c'(X'X)^{-1}c}}|X \\overset{H_0}{\\sim} N(0,1)\n\\] Note, this statement is only true under \\(H_0\\), while the former is true always (under CLRM assumptions).\nWhen evaluating against the alternative hypothesis, \\(H_1: c'\\beta\\neq r\\), the valid two-sided rejection rule is given by,\n\nReject \\(H_0\\) if \\(|\\text{Z-stat}|&gt;z_{1-\\alpha/2}\\)\n\nwhere \\(\\alpha\\) is the chosen significance level (e.g., 1%, 5%, or 10%) and \\(z_{1-\\alpha/2}\\) the \\(1-\\alpha/2\\) percentile of the standard normal distribution. We can write the two sided rejection in this way given the symmetry of the normal distribution. In general, the rejection rule is,\n\nReject \\(H_0\\) if \\(\\text{Z-stat}&lt;z_{\\alpha/2}\\) or \\(\\text{Z-stat}&gt;z_{1-\\alpha/2}\\)\n\nHowever, given the symmetric of \\(N(0,1)\\), \\(z_{\\alpha/2}=-z_{1-\\alpha/2}\\):\n\\[\n\\begin{aligned}\n\\alpha =& Pr(Z&lt;z_{\\alpha/2})+Pr(Z&gt;z_{1-\\alpha/2}) \\\\\n=& Pr(Z&lt;-z_{1-\\alpha/2})+Pr(Z&gt;z_{1-\\alpha/2}) \\\\\n=& Pr(|Z|&gt;z_{1-\\alpha/2})\n\\end{aligned}\n\\] For asymmetric distributions, this is not the case, and you must write the two rejection cases separately.\nThis test is valid for the true \\(\\sigma^2\\), the homoskedastic variance of the error term. We typically do not know this variance and need to replace it with the estimator,\n\\[\ns^2 = \\frac{1}{n-k}\\sum_{i=1}^{n}\\hat{u}_i^2 = \\frac{\\hat{u}'\\hat{u}}{n-k}\n\\] where \\(\\hat{u}\\) is the residual from the linear model: \\(\\hat{u} = Y-X\\hat{\\beta}\\). This begs the question: what is the conditional distribution of,\n\\[\n\\text{T-stat} = \\frac{c'\\hat{\\beta}-r}{\\sqrt{s^2 c'(X'X)^{-1}c}}\n\\] under \\(H_0\\). You will know from undergraduate texts that it is \\(t\\)-distributed. We can demonstrate this result using the idempotent properties of projection matrices.\nFirst-off, substitue back in \\(\\sigma^2\\)\n\\[\n\\text{T-stat} = \\frac{c'\\hat{\\beta}-r}{\\sqrt{\\sigma^2 c'(X'X)^{-1}c}}\\times\\sqrt{\\frac{\\sigma^2}{s^2}}\n\\] The T-statistic can be rewritten as the Z-statistic (which has a standard normal distribution under \\(H_0\\)) multiplied by the square-root of the ratio of \\(s^2\\) and \\(\\sigma^2\\). Consider the distribution of this ratio:\n\\[\n  \\frac{s^2}{\\sigma^2} = \\frac{1}{n-k} \\frac{\\hat{u}'\\hat{u}}{\\sigma^2}\n\\] We know that \\(\\hat{u} = M_X Y = M_X u\\), where \\(M_X\\) is idempotent and symmetric. Therefore,\n\\[\n  \\frac{s^2}{\\sigma^2} = \\frac{1}{n-k} \\frac{u'M_Xu}{\\sigma^2}=\\frac{1}{n-k} \\bigg(\\frac{u}{\\sigma}\\bigg)'M_X\\bigg(\\frac{u}{\\sigma}\\bigg)\n\\] By the properties of symmetric matrices (see Linear Algebra notes), \\(M_X\\) has a eigenvector decomposition\n\\[\nM_X = C\\Lambda C' \\qquad\\text{where} \\quad CC' = C'C = I_n\n\\] Moreover, since \\(M_X\\) is idempotent, all eigenvalues are \\(\\lambda_j\\in\\{0,1\\}\\) for \\(j=1,...,n\\). This implies that,\n\\[\nrank(M_X) = tr(M_X) = \\sum_{j=1}^n\\lambda_j\n\\] We know that the \\(rank(M_X) = rank(I_n-P_X) = rank(I_n)-rank(P_X) = n-rank(X) = n-k\\). Therefore, \\(M_X\\) has \\(n-k\\) non-zero eigenvalues. Applying this eigenvector decomposition, we get that, \\[\n  \\frac{s^2}{\\sigma^2} = \\frac{1}{n-k} \\bigg(\\frac{u}{\\sigma}\\bigg)'C\\Lambda C'\\bigg(\\frac{u}{\\sigma}\\bigg) = \\frac{1}{n-k} \\bigg(\\frac{C'u}{\\sigma}\\bigg)'\\Lambda \\bigg(\\frac{C'u}{\\sigma}\\bigg)\n\\]\nSince \\(u|X\\sim N(0,\\sigma^2 I_n)\\),\n\\[\n\\frac{C'u}{\\sigma}\\sim N(0,\\underbrace{C'I_nC}_{=C'C=I_n})\n\\] Thus,\n\\[\n  \\frac{s^2}{\\sigma^2} =  \\frac{1}{n-k} Z'\\Lambda Z = \\frac{1}{n-k}\\sum_{j=1}^n \\lambda_jZ_j^2\n\\] Under the \\(H_0\\), the T-statistic’s distribution can be described by the ratio of two random variables: \\(Z\\sim N(0,1)\\) and \\(W\\sim\\chi^2_{n-k}\\).\n\\[\n\\text{T-stat} = \\frac{Z}{\\sqrt{W/(n-k)}} \\sim T_{n-k}\n\\] This distribution is known as the \\(t\\)-distribution, with \\(n-k\\) degrees of freedom (dof). Note, the degrees of freedom correspond to the rank of the \\(M_X\\) matrix, since this determines the number of squared standard normal distributions summed in the denominator.\nFor a two-sided test, we reject \\(H_0\\) when,\n\\[\n  |\\text{T-stat}| &gt;t_{n-k,1-\\alpha/2}\n\\] where \\(t_{n-k,1-\\alpha}\\) is the \\(1-\\alpha/2\\) percentile of the \\(t\\)-distribution (with \\(n-k\\) dof). As with the standard normal distribution, the \\(t\\)-distribution is symmetric.\n\n4.1 Asymptotic distribution\nWe will see that for \\(n\\) large, it does not make a difference that we do not know \\(\\sigma^2\\). The fact that \\(s^2\\rightarrow_p \\sigma^2\\) (as \\(n\\rightarrow\\infty\\)) means that the T-statistic will be normally distributed in the limit.\nTo demonstrate this result we just need to concern ourself with,\n\\[\n  \\frac{s^2}{\\sigma^2} =  \\frac{1}{n-k}\\sum_{j=1}^n \\lambda_jZ_j^2\\sim \\chi^2_{n-k}/(n-k)\n\\] The \\(E[\\sum_{j=1}^n \\lambda_jZ_j^2] = n-k\\), since \\(E[Z_1^2] = 1\\) for any standard normal random variable. Moreover, \\(E[(\\sum_{j=1}^n \\lambda_jZ_j^2)^2]=2(n-k)\\). Therefore, by the WLLN,\n\\[\n\\frac{1}{n-k}\\sum_{j=1}^n \\lambda_jZ_j^2\\rightarrow_p 1\n\\] As a result, under \\(H_0\\),\n\\[\n  \\text{T-stat}\\rightarrow_d N(0,1)\n\\]"
  },
  {
    "objectID": "handout-4.html#multiple-linear-hypotheses",
    "href": "handout-4.html#multiple-linear-hypotheses",
    "title": "Hypothesis Testing",
    "section": "5 Multiple Linear Hypotheses",
    "text": "5 Multiple Linear Hypotheses\nWe will follow similar steps to above. For any non-random \\(q\\times k\\) matrix \\(R\\),\n\\[\nR\\hat{\\beta}|X \\sim N(R\\beta,\\sigma^2 R(X'X)^{-1}R')\n\\] Under the CLRM assumptions, this statement is true always, while under \\(H_0: R\\beta = r\\),\n\\[\nR\\hat{\\beta}|X \\overset{H_0}{\\sim} N(r,\\sigma^2 R(X'X)^{-1}R')\n\\]\nTo construct a scalar test static for these \\(q\\) hypotheses we will first need to consider the distribution of,\n\\[\n(R\\hat{\\beta}-r)'\\big(\\sigma^2 R(X'X)^{-1}R'\\big)^{-1}(R\\hat{\\beta}-r)\\sim \\chi^2_{q}\n\\] We can prove this result as follows.\n\nProof. Suppose \\(U\\sim N(0,\\Omega)\\). Then, \\(\\Omega\\) is a positive-definite, symmetric variance-covariance matrix. By symmetry, this matrix has an eigenvector decomposition,\n\\[\n  \\Omega = C\\Lambda C' \\qquad\\text{where} \\quad CC' = C'C =I_n\n  \\] Given it’s positive definiteness, all eigenvalues in \\(\\lambda\\) are strictly positive. We can therefore define,\n\\[\n  \\Lambda^{1/2} = \\begin{bmatrix}\\lambda_1^{1/2} & 0 & \\cdots & 0 \\\\ 0 & \\lambda_2^{1/2} &  & \\\\\n  \\vdots & & \\ddots & \\\\ 0 & & & \\lambda_q^{1/2}\\end{bmatrix}\n  \\] Likewise, \\[\n  \\Lambda^{-1/2} = \\begin{bmatrix}\\lambda_1^{-1/2} & 0 & \\cdots & 0 \\\\ 0 & \\lambda_2^{-1/2} &  & \\\\\n  \\vdots & & \\ddots & \\\\ 0 & & & \\lambda_q^{-1/2}\\end{bmatrix}\n  \\] This allows us to define,\n\\[\n  \\Omega^{1/2} = C\\Lambda^{1/2}C' \\qquad \\text{since}\\quad C\\Lambda^{1/2}C'C\\Lambda^{1/2}C' = C\\Lambda C' = \\Omega\n\\] and \\[\n  \\Omega^{-1/2} = C\\Lambda^{-1/2}C' \\qquad \\text{since}\\quad C\\Lambda^{-1/2}C'C\\Lambda^{-1/2}C' = C\\Lambda C' = \\Omega^{-1}\n\\] This allows us to standardize the distribution of \\(U\\).\n\\[\n\\Omega^{-1/2}U \\sim N(0,\\Omega^{-1/2}\\Omega\\Omega^{-1/2}) = N(0,I_q)\n\\] and \\[\n(\\Omega^{-1/2}U)'\\Omega^{-1/2}U =  U'\\Omega^{-1}U  = \\sum_{j=1}^qZ_i^2\\sim \\chi^2_q\n\\]\n\nUsing this result, we can then define the F-statistic as,\n\\[\n  \\text{F-stat} = (R\\hat{\\beta}-r)'\\big(s^2 R(X'X)^{-1}R'\\big)^{-1}(R\\hat{\\beta}-r)/q\n\\] where we replace the unknown \\(\\sigma^2\\) by \\(s^2\\) and scale by \\(q\\), the \\(rank(R)\\) (or number of hypotheses).\nAs above, we can rewrite this test-statistic as,\n\\[\n\\begin{aligned}\n  \\text{F-stat} =& \\frac{(R\\hat{\\beta}-r)'\\big(\\sigma^2 R(X'X)^{-1}R'\\big)^{-1}(R\\hat{\\beta}-r)/q}{s^2/\\sigma^2} \\\\\n  \\sim&\\frac{\\chi_q/q}{\\chi_{n-k}/(n-k)}\\\\\n  =&F_{q,n-k}\n\\end{aligned}  \n\\] We reject \\(H_0\\) when,\n\\[\n  \\text{F-stat} &gt;f_{q,n-k,1-\\alpha}\n\\] where \\(f_{q,n-k,1-\\alpha}\\) is the \\(1-\\alpha\\) percentile of the \\(F\\)-distribution with dof \\(q\\) and \\(n-k\\). This is a one-sided test since the F-statistic is strictly positive.\nNote, for \\(q=1\\), the F-statistic is given by the square of the T-statistic.\n\n5.1 Asymptotic distribution\nAs before, the replacement of \\(\\sigma^2\\) with \\(s^2\\) has no baring on the asymptotic distribution of the test. Since, \\(p \\lim (s_n^2/\\sigma^2) = 1\\),\n\\[\n\\text{F-stat} \\rightarrow_d \\chi^2_{q}/q\\qquad \\text{as}\\quad n\\rightarrow\\infty\n\\]"
  },
  {
    "objectID": "handout-4.html#restricted-ols",
    "href": "handout-4.html#restricted-ols",
    "title": "Hypothesis Testing",
    "section": "6 Restricted OLS",
    "text": "6 Restricted OLS\nYou will likely be familiar with a second description of the F-statistic:\n\\[\n\\text{F-stat} = \\frac{(RSS_R - RSS_U)/q}{RSS_U/(n-k)}\n\\]\nThis expression is equivalent to above and can be derived by solving the restricted (ordinary) least squares problem.\n\\[\n  \\underset{b}{\\min} \\;(Y-Xb)'(Y-Xb) \\qquad \\text{s.t.}\\quad Rb = r\n\\]\nThis is a constrained optimization problem that can be solved using the Lagrangian function,\n\\[\nL(b,\\lambda) = (Y-Xb)'(Y-Xb) + 2\\lambda'(Rb-r)\n\\] The F.O.C.’s (w.r.t. \\(b\\) and \\(\\lambda\\)), state at the optimum it must be that,\n\\[\n\\begin{aligned}\n0 =& 2X'X\\tilde{\\beta}-2X'Y-2R'\\tilde{\\lambda} \\\\\n0 =& R\\tilde{\\beta}-r\n\\end{aligned}\n\\]\nFrom (1), we have,\n\\[\n\\begin{aligned}\n\\tilde{\\beta} =& (X'X)^{-1}(X'Y-R'\\tilde{\\lambda}) \\\\\n=& \\underbrace{(X'X)^{-1}X'Y}_\\text{OLS}- (X'X)^{-1}R'\\tilde{\\lambda} \\\\\n=& \\hat{\\beta} - (X'X)^{-1}R'\\tilde{\\lambda}\n\\end{aligned}\n\\] Pluggin this solution into (2), you get,\n\\[\n\\begin{aligned}\nr =& R\\hat{\\beta}-R(X'X)^{-1}R'\\tilde{\\lambda} \\\\\n\\Rightarrow \\tilde{\\lambda} =& (R(X'X)^{-1}R')^{-1}(R\\hat{\\beta}-r) \\\\\n\\Rightarrow \\tilde{\\beta} =& \\hat{\\beta}- (X'X)^{-1}R'(R(X'X)^{-1}R')^{-1}(R\\hat{\\beta}-r)\n\\end{aligned}\n\\]\nThe residuals from the restricted model are given by,\n\\[\n\\begin{aligned}\n\\tilde{U} =& Y-X\\tilde{\\beta} \\\\\n=&\\underbrace{Y-X\\hat{\\beta}}_{M_XY}+X(X'X)^{-1}R'(R(X'X)^{-1}R')^{-1}(R\\hat{\\beta}-r) \\\\\n=&\\hat{U}+X(X'X)^{-1}R'(R(X'X)^{-1}R')^{-1}(R\\hat{\\beta}-r)\n\\end{aligned}\n\\] The Residual Sum of Squares (RSS) from the restricted model is then given by, \\[\n\\begin{aligned}\n\\tilde{U}'\\tilde{U} =& \\big(\\hat{U}+X(X'X)^{-1}R'(R(X'X)^{-1}R')^{-1}(R\\hat{\\beta}-r)\\big)'\\big(\\hat{U}+X(X'X)^{-1}R'(R(X'X)^{-1}R')^{-1}(R\\hat{\\beta}-r)\\big) \\\\\n=&\\hat{U}'\\hat{U} + (R\\hat{\\beta}-r)'(R(X'X)^{-1}R')^{-1}(R\\hat{\\beta}-r)\n\\end{aligned}\n\\] The cross-product terms cancel, because \\(\\hat{U}'X(X'X)^{-1}R'(R(X'X)^{-1}R')^{-1}(R\\hat{\\beta}-r) = Y'M_XX(X'X)^{-1}R'(R(X'X)^{-1}R')^{-1}(R\\hat{\\beta}-r) = 0\\). Thus,\n\\[\n  (R\\hat{\\beta}-r)'(R(X'X)^{-1}R')^{-1}(R\\hat{\\beta}-r) = \\underbrace{\\tilde{U}'\\tilde{U}}_{RSS_R}-\\underbrace{\\hat{U}'\\hat{U}}_{RSS_U}\n\\] Since, \\(s^2 = RSS_U/(n-k)\\), a scalar,\n\\[\n  \\text{F-stat} = \\frac{(RSS_R - RSS_U)/q}{RSS_U/(n-k)}\n\\]\nWhere \\(q = dof_r-dof_u\\), the difference in residual degrees of freedom for the two models.\nYou can estimate the restricted model by imposing the restrictions of \\(H_0\\) on the model. For example, in Problem Set 2, we tested \\(H_0: \\beta_2 = \\beta_3\\) and \\(\\beta_4 + \\beta_5 = 1\\), by estimating the restricted model,\n\\[\n    (Y-X_4) = \\gamma_1 + \\gamma_2(X_2+X_3) + \\gamma_5(X_5-X_4) + \\varepsilon\n\\]\n\n6.1 Testing full model\nIn the unique case where the hypothesis is given by,\n\\[\nH_0: \\beta_2 = ... = \\beta_k = 0\n\\]\nAnd the model includes a constant (i.e. \\(X_1 = \\mathbf{1}\\)). Then, the restricted model amounts to a regression of \\(Y\\) on a constant: \\(Y = \\beta_1 + u\\).\n\\[\n  RSS_R = (Y-\\bar{Y})'(Y-\\bar{Y}) = TSS\n\\]\nIn this unique case, the F-statistic can be written in terms of \\(R^2\\) (of the unrestricted model),\n\\[\n  \\text{F-stat} = \\frac{R^2/(k-1)}{(1 - R^2)/(n-k)}\n\\]\nThis is the F-statistic reported by Stata when you estimate a linear regression model."
  },
  {
    "objectID": "handout-4.html#heteroskedasticity",
    "href": "handout-4.html#heteroskedasticity",
    "title": "Hypothesis Testing",
    "section": "7 Heteroskedasticity",
    "text": "7 Heteroskedasticity\nBoth the t- and F-tests described above have their corresponding finite sample distributions under the assumptions that the error term in the CLRM is normally distributed (conditional on X) and that its variance is homoskedastic.\nThese distributions are not valid, for the finite sample, if the error terms are heteroskedastic. However, the tests do have the same asymptotic distributions with heteroskedasticity. For this reason, you should not use the t-distribution or F-distribution if your SEs are computed assuming heteroskedasticity. Instead, use the normal and chi-squared distributions."
  },
  {
    "objectID": "handout-4.html#footnotes",
    "href": "handout-4.html#footnotes",
    "title": "Hypothesis Testing",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf \\(X\\sim N(\\mu,\\Omega)\\) then the linear transformation of \\(X\\), \\(Y = AX +b\\), is distributed \\(Y\\sim N(A\\mu+b,A\\Omega A')\\).↩︎"
  },
  {
    "objectID": "handout-6.html",
    "href": "handout-6.html",
    "title": "Binary Choice Models",
    "section": "",
    "text": "In this handout we will review the binary case of discrete choice models (a type of limited dependent variable model). We will review:\n\nlinear probability model (LPM);\nprobit model;\nand logit model.\n\nStudying the simple case binary choice models, will provide you with tools needed to explore richer models like multinomial logit/probit, ordered logit/probit, and conditional logit.\nFurther reading can be found in:\n\nSection 14-14.4 of Cameron and Trivedi (2005)\nSection 7-7.1.6 of Verbeek (2017)"
  },
  {
    "objectID": "handout-6.html#overview",
    "href": "handout-6.html#overview",
    "title": "Binary Choice Models",
    "section": "",
    "text": "In this handout we will review the binary case of discrete choice models (a type of limited dependent variable model). We will review:\n\nlinear probability model (LPM);\nprobit model;\nand logit model.\n\nStudying the simple case binary choice models, will provide you with tools needed to explore richer models like multinomial logit/probit, ordered logit/probit, and conditional logit.\nFurther reading can be found in:\n\nSection 14-14.4 of Cameron and Trivedi (2005)\nSection 7-7.1.6 of Verbeek (2017)"
  },
  {
    "objectID": "handout-6.html#binary-outcomes",
    "href": "handout-6.html#binary-outcomes",
    "title": "Binary Choice Models",
    "section": "2 Binary Outcomes",
    "text": "2 Binary Outcomes\nConsider a Bernoulli random variable \\(Y\\in\\{0,1\\}\\). Given the vector of random variables \\(X\\in \\mathbb{R}^k\\), we can define\n\\[\n\\rho(x) = Pr(Y = 1|X=x)\n\\] and, \\[\n1-\\rho(x) = 1-Pr(Y = 1|X=x) = Pr(Y = 0|X=x)\n\\] Then, the conditional mean is given by,\n\\[\nE[Y|X] = 0\\times Pr(Y = 0|X)+1\\times Pr(Y = 1|X) = \\rho(X)\n\\] Likewise, the conditional variance is, \\[\n\\begin{aligned}\nVar(Y|X) =& E[Y^2|X]-E[Y|X]^2 \\\\\n=&E[Y|X]-E[Y|X]^2 \\\\\n=&\\rho(X)-\\rho(X)^2 \\\\\n=& \\rho(X)\\big(1-\\rho(X)\\big)\n\\end{aligned}\n\\] where the second line follows from the fact that \\(Y = Y^2\\)."
  },
  {
    "objectID": "handout-6.html#linear-probability-model",
    "href": "handout-6.html#linear-probability-model",
    "title": "Binary Choice Models",
    "section": "3 Linear Probability Model",
    "text": "3 Linear Probability Model\nThe LPM is simply a standard linear model with a binary outcome:\n\\[\nY_i = X_i'\\beta + \\varepsilon_i\n\\]\nwhere \\(Y_i\\in\\{0,1\\}\\). We can rationalize this model by assuming that the conditional expectation function of \\(Y\\) is linear (in parameters),\n\\[\nE[Y_i|X_i] = X_i'\\beta = Pr(Y_i = 1|X_i)\n\\] However, since the conditional expectation is equivalent to the conditional probability (that \\(Y=1\\)), it must be that: \\[\n0\\leq X_i'\\beta\\leq 1\n\\] Outside of this interval, the model is not defined.\nRecall, assuming knowledge of the CEF is equivalent to assuming conditional mean independence of the error term, since we can always write \\(Y_i = E[Y_i|X_i]+\\varepsilon_i\\) where \\(E[\\varepsilon_i|X_i] = 0\\). In this setting, the error term can take on 2 values for any given value of \\(X_i\\):\n\\[\n\\varepsilon_i = \\begin{cases}1-X_i'\\beta\\qquad\\text{for}\\quad Y_i = 1\\\\\n-X_i'\\beta \\qquad\\text{for}\\quad Y_i = 0\n\\end{cases}\n\\]\nThis implies that the error cannot be normally distributed (conditional on \\(X_i\\)). This is limiting in terms of the finite sample distribution of estimators for \\(\\beta\\). However, the asymptotic distribution of estimators will remain normal under CLT.\nWhat of the conditional variance? For continuous outcomes, we could assume homoskedasticity: \\(Var(\\varepsilon_i|X_i) = \\sigma^2\\). Now, with a binary outcome,\n\\[\nVar(\\varepsilon_i|X_i) = Var(Y_i|X_i) = \\rho(X_i)\\big(1-\\rho(X_i)\\big)\n\\] Thus, the error term is by definition heteroskedastic.\n\n3.1 Estimation\nThe LPM can be estimated by OLS. So long as, \\[\n0\\leq X_i'\\beta\\leq 1\n\\] OLS is unbiased and consistent. If not, OLS is neither unbiased nor consistent.\nRegardless, OLS is inefficient, given the heteroskedasticity of the error term. You should therefore estimate heteroskedastic robust SEs. An alternative is to estimate a Weighted Least Squares model.\n\n\n3.2 Predicted values\nAn issue with the LPM model is predictions outside of the range \\([0,1]\\). For example,\nThis usually occurs in instances where \\(X\\) has a large support, with the majority of observations close to the mean (of \\(X\\)). For example, if \\(X\\) is normally distributed."
  },
  {
    "objectID": "handout-6.html#latent-variable-model",
    "href": "handout-6.html#latent-variable-model",
    "title": "Binary Choice Models",
    "section": "4 Latent Variable Model",
    "text": "4 Latent Variable Model\nA latent variable model is one way of justifying the assumptions underlying the probit and logit models; however, it is not strictly required. This approach generalizes the model in a way that restricts the outcome (to a binary outcome), either conditionally or unconditionally.\nWe observe \\(Y_i\\in\\{0,1\\}\\). We can express this outcome as a function of \\(\\{X,\\varepsilon\\}\\) in the following way:\n\\[\nY_i = \\mathbf{1}\\{X_i'\\beta + \\varepsilon_i&gt;0\\}\n\\] Here, \\(\\mathbf{1}\\{\\cdot\\}\\) is an indicator function: equal to 1 when the statement is true and 0 otherwise. We can then write this as, \\[\nY_i = \\mathbf{1}\\{Y_i^*&gt;0\\}\\qquad\\text{where}\\quad Y_i^*=X_i'\\beta + \\varepsilon_i\n\\] We refer to \\(Y^*\\) as a latent variable. It is unobserved; hence, the name ‘latent’ which can be interpretted as hidden or concealed.\nThere are a couple of important properties of this set-up:\n\nThe observed outcome is unique up to a scalar multiplication of the latent variable.\n\n\\[\nY_i = \\mathbf{1}\\{X_i'\\beta+\\varepsilon_i&gt;0\\} = \\mathbf{1}\\{aX_i'\\beta+a\\varepsilon_i&gt;0\\}\n\\] This implies that the absolute magnitude of \\(\\beta\\) cannot be identified. We will see this morning clearly in our discussion of the probit model.\n\nThe observed outcome is unique up to the addition and substraction of a constant:\n\n\\[\nY_i = \\mathbf{1}\\{X_i'\\beta+\\varepsilon_i&gt;0\\} = \\mathbf{1}\\{X_i'\\beta+c+\\varepsilon_i-c&gt;0\\}\n\\]\nThis implies that the threshold at which decisions are made cannot be identified and need not be zero.\nPractically, these two probabilities mean that we will need to normalize the location (mean) and variance of the unobserved error (\\(\\varepsilon\\)).\n\n4.1 Utility maximization\nSuch a latent variable framework should be familiar to an Economics student. Afterall, von-Neumann and Morgenstern’s theory of utility maximization states that under certain axioms (concerning the preferences of the individual) individual choices are consistent with the maximization of a continuously differentiable utility function. We observe individual choices (which are often discrete), not their utility function: a latent variable determining their choices.\nConsider the choice between two goods/options \\(j=0,1\\); for example, travel by car (=1) or bus (=0). Suppose the utility derived from each choice is given by,\n\\[\nU_{ij}=S_{ij}+\\varepsilon_{ij}\n\\]\nwhere \\(S_ij\\) is a deterministic component, and \\(\\varepsilon_{ij}\\) stochastic. For example, in our example of travel options the deterministic component could be the known cost of each option, while the stochastic component could be due to unexpected variation in travel times if a private vehicle is more affected by traffic.\nAssuming utility maximization, the individual chooses to travel by car if \\(U_{i1}&gt;U_{i0}\\).\n\\[\n\\begin{aligned}\nPr(Y_i = 1) =& Pr(U_{i1}&gt;U_{i0})\\\\\n=&Pr(S_{i1}+\\varepsilon_{i1}&gt;S_{i0}+\\varepsilon_{i0}) \\\\\n=&Pr(\\varepsilon_{i0}-\\varepsilon_{i1}&lt;S_{i1}-S_{i0})\n\\end{aligned}\n\\] Given known values of the deterministic components \\(S_{i1}-S_{i0}\\), we could component this probability given the CDF of \\(\\varepsilon_{i0}-\\varepsilon_{i1}\\). This, of course requires a an assumption.\nSuppose that the deterministic component depended on option-specific variables (\\(W_{ij}'\\lambda\\); e.g. cost of transport option) and individual-specific variables with option-specific marginal utility (\\(Z_i'\\gamma_j\\); e.g. age).\n\\[\nS_{ij} = W_{ij}'\\lambda + Z_i'\\gamma_j\n\\] Then, the probability of observing choice \\(j=1\\) (i.e. personal vehicle) is given by, \\[\nPr(Y_i = 1|W_i,Z_i) =Pr(\\varepsilon_{i0}-\\varepsilon_{i1}&lt;(W_{i1}-W_{i0})'\\lambda+Z_i'(\\gamma_1-\\gamma_0))\n\\] Suppose, \\(var(\\varepsilon_i|W_i,Z_i) =\\sigma^2\\). Then we can, we can define, \\[\nPr(Y_i = 1|W_i,Z_i)=Pr(\\varepsilon_i &lt; \\Delta W_i'\\eta + Z_i'\\gamma)\n\\]\nwhere, \\[\n\\varepsilon_i = \\frac{\\varepsilon_{i0}-\\varepsilon_{i1}}{\\sigma};\\;\\eta = \\frac{\\delta}{\\sigma};\\;\\text{and}\\;\\gamma = \\frac{\\gamma_1-\\gamma_0}{\\sigma}\n\\]\nAs a result we have a \\(Pr(Y_i=1|X_i) = Pr(\\varepsilon_i&lt;X_i'\\beta)\\) where \\(X_i'\\beta = \\Delta W_i'\\eta + Z_i'\\gamma\\). This normalization of the parameters is standard in these models. Moreover, we cannot identify the option-specific parameters (\\(\\gamma_j\\)) on the invariant variables.\nGiven a choice of the distribution of \\(\\varepsilon_i\\),\n\\[\nPr(Y_i = 1|X_i) = F_\\varepsilon(X_i'\\beta)\n\\]\n\n\n4.2 Probit\nThe probit model assumes that the error term from the expression, \\[\nY_i = \\mathbf{1}\\{X_i'\\beta + \\varepsilon_i&gt;0\\}\n\\] is independently and identicially distributed (conditional on \\(X\\)),\n\\[\n\\varepsilon_i|X_i\\sim N(0,1)\n\\]\nNote, if we assume that the conditional variance is \\(\\sigma^2\\), then we have to normalize the model since only \\(\\beta/\\sigma\\) is identified.\nThe PDF of the standard normal distribution is given by,\n\\[\n\\phi(z) = \\frac{1}{\\sqrt{2\\pi}}exp(-z^2/2)\n\\] and the CDF by, \\[\n\\Phi(z) = \\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^z exp(-u^2/2)du\n\\] This latter expression has no closed form solution.\nThe conditional probability of \\(Y_i=1\\) (expectation of \\(Y_i\\)), is given by,\n\\[\n\\begin{aligned}\nPr(Y_i=1|X_i) =& Pr(X_i'\\beta + \\varepsilon_i&gt;0|X_i)\\\\\n=&Pr(\\varepsilon_i&gt;-X_i'\\beta|X_i) \\\\\n=&Pr(\\varepsilon_i&lt;X_i'\\beta|X_i) \\\\\n=&\\Phi(X_i'\\beta)\n\\end{aligned}\n\\] where line 3 follows from the symmetry of \\(N(0,1)\\).\n\n\n4.3 Logit\nIn this case, we assume that the conditional distribution of the error term is logistic. The CDF of the (standard) logistic distribution is given by,\n\\[\n\\Lambda(z) = \\frac{exp(z)}{1+exp(z)} = \\frac{1}{1+exp(-z)}\n\\] and the PDF by, \\[\n\\lambda(z) = \\Lambda(z)(1-\\Lambda(z))\n\\] Like the standard normal distribution, the standard logistic distribution is symmetric. It’s variance is \\(\\pi^2/3\\) and has slightly thicker tails compared to the standard normal.\n\n\n4.4 LPM\nIf we assume that the conditional distribution of the error is uniform with mean 0 and variance=1,1 then\n\\[\nPr(Y_i = 1|X_i) = X_i'\\beta\n\\]\nThus, one way of justifying the use of a linear probability model is through assumption of a uniformly distributed error."
  },
  {
    "objectID": "handout-6.html#index-model-approach",
    "href": "handout-6.html#index-model-approach",
    "title": "Binary Choice Models",
    "section": "5 Index Model Approach",
    "text": "5 Index Model Approach\nAn alternative approach is to consider the problem of the modeling the conditional probability (expectation) function. In the case of the LPM we had, \\[\nPr(Y_i = 1|X_i) = X_i'\\beta\n\\] More generally, we can place this linear index - \\(X_i'\\beta\\) - inside a function \\(G(\\cdot)\\).\n\\[\nPr(Y_i = 1|X_i) = G(X_i'\\beta)\n\\]\nThe covariates affect the conditional probability only through the linear index, which is then mapped into a response probability by the function \\(G(\\cdot)\\).\nFor \\(G(\\cdot)\\) linear, we have the LPM. However, we know that this model can give predicted probabilities outside of the bounds of \\([0,1]\\). An ideal candidate for \\(G(\\cdot)\\) would be any function that bounds the values of the predicted probabilities within \\([0,1]\\). Given that a CDF is a function with values within this range, the functions \\(\\Phi(\\cdot),\\Lambda(\\cdot)\\) are prime candidates. However, non-symmetric functions also exist; for example, the Gumbel (extreme-value or complementary log-log),\n\\[\n1-exp\\big(-exp(z)\\big)\n\\]"
  },
  {
    "objectID": "handout-6.html#estimation-1",
    "href": "handout-6.html#estimation-1",
    "title": "Binary Choice Models",
    "section": "6 Estimation",
    "text": "6 Estimation\nWe will examine the estimation of these models by Maximum Likelihood (ML). Recall from Handout 3, we defined the ML estimator as the maximizer of the conditional log-likelihood function. Assuming an iid sample, this is given by, \\[\n\\hat{\\theta}^{ML} = \\underset{\\theta}{\\text{arg max}}\\;n^{-1}\\log L_n(\\theta) = \\underset{\\theta}{\\text{arg max}}\\;n^{-1}\\sum_{i=1}^n\\log\\ell_i(\\theta)\n\\] where \\(ell_i(\\theta) = f(Y_i|X_i;\\theta)\\). In this application, the data \\(W_i=[Y_i,X_i']\\) where \\(Y_i\\in\\{0,1\\}\\) and \\(X_i\\in\\mathbb{R}^k\\). The population parameters are \\(\\theta = \\beta\\); since, we normalize \\(\\sigma = 1\\). Since, the outcome is discrete, the conditional likelihood function is given by two probabilities:\n\n\\(Pr(Y_i=1|X_i) = F(X_i'\\beta)\\)\n\\(Pr(Y_i=0|X_i) = 1-F(X_i'\\beta)\\)\n\nwhere \\(F(\\cdot)\\) is the CDF of the unobserved error. Hence, the joint (conditional) likelihood is given by,\n\\[\nL_n(\\beta) = \\prod_{i:Y_i = 1}F(X_i'\\beta)\\prod_{i:Y_i = 0}\\big(1-F(X_i'\\beta)\\big)\n\\] which can be written as, \\[\nL_n(\\beta) = \\prod_{i= 1}^nF(X_i'\\beta)^{Y_i}\\big(1-F(X_i'\\beta)\\big)^{1-Y_i}\n\\] The ML estimator is then by given by,\n\\[\n\\hat{\\beta}^{ML} = \\underset{\\beta}{\\arg \\max}\\;\\frac{1}{n}\\sum_{i=1}^n Y_i\\times\\ln\\big(F(X_i'\\beta)\\big)+(1-Y_i)\\times\\ln\\big(1-F(X_i'\\beta)\\big)\n\\] Solving for the first-order conditions (FOCs), we get:\n\\[\n\\frac{1}{n}\\frac{\\partial L_n(\\beta)}{\\partial\\beta}= \\frac{1}{n}\\sum_{i=1}^n \\bigg[Y_i \\frac{f(X_i'\\beta)}{F(X_i'\\beta)}-(1-Y_i)\\frac{ f(X_i'\\beta)}{1-F(X_i'\\beta)}\\bigg]X_i\n\\] The term in the square parenthesis is referred to as the generalized error. In this was, the above score function resembles the FOCs from a linear model. The generalized error can be solved for by taking the derivative of \\(L_n(\\theta)\\) with-respect-to the constant term in the model (assuming there is one).\n\\[\n\\text{generalized error}=\\begin{cases}\\frac{f(X_i'\\beta)}{F(X_i'\\beta)} \\qquad \\text{for}\\quad Y_i=1\\\\\n\\frac{-f(X_i'\\beta)}{1-F(X_i'\\beta)} \\qquad \\text{for}\\quad Y_i=0\\\\\n\\end{cases}\n\\] The scaling by \\(1/n\\) is no impact on the solution to the FOCs, but is needed to ensure consistency of the variance-covariance matrix. This was the case for the linear model as well.\nAssuming a concave likelihood function, there is a unique maximum and we can solve for \\(\\hat{\\beta}\\) by setting the score function \\(= 0\\).\n\n6.1 Probit\nIn the case of the probit model, \\(F(z)=\\Phi(z)\\). Solving the FOCs, we get\n\\[\n\\begin{aligned}\n0=& \\sum_{i=1}^n \\bigg[Y_i \\frac{\\phi(X_i'\\beta)}{\\Phi(X_i'\\beta)}-(1-Y_i)\\frac{\\phi(X_i'\\beta)}{1-\\Phi(X_i'\\beta)}\\bigg]X_i \\\\\n=&\\sum_{i=1}^n \\bigg[\\frac{Y_i -\\Phi(X_i'\\beta)}{\\Phi(X_i'\\beta)\\big(1-\\Phi(X_i'\\beta)\\big)}\\bigg]X_i\\cdot\\phi(X_i'\\beta)\n\\end{aligned}\n\\]\n\n\n6.2 Logit\nIn the case of the logit model, \\(F(z)=\\Lambda(z)\\) and \\(f(z)=\\Lambda(z)(1-\\Lambda(z))\\). This yields a simpler score function:\n\\[\n0= \\sum_{i=1}^n \\bigg[Y_i -\\Lambda(X_i'\\beta)\\bigg]X_i\n\\] However, neither the probit nor logit has an analytical solution and both must be solved numerically.\n\n\n6.3 Asymptotic distribution\nFor both probit and logit, the estimators are asymptotically normal,\n\\[\n\\sqrt{n}(\\hat{\\beta}_n^{ML}-\\beta_0)\\rightarrow_d N(0,V)\n\\]\nwhere \\(\\beta_0\\) is the true value of \\(\\beta\\),\n\\[\nV =  (J(\\beta_0))^{-1}=\\bigg(\\underbrace{-E\\bigg[\\frac{\\partial^2}{\\partial\\beta\\partial\\beta'}\\ln f(W_i,\\beta_0)\\bigg]}_{J(\\beta_0)}\\bigg)^{-1}\n\\] The variance is given by the inverse of the Jacobian matrix, evaluating at the true value of \\(\\beta\\). The Jacobian matrix is the (negative) expected value of the Hessian matrix of second derivatives. This result derives from the Delta Method which is used to explain the asymptotic distribution of the ML estimator.\nLet us prove this result. In Handout 3 we established the consistency of the ML-estimator. To prove its asymptotic normality, we will need to use the Mean Value Theorem (MVT).2\n\nTheorem 1 (Mean Value Theorem) For \\(f(\\cdot)\\) continuous on \\([a,b]\\), and continuously differentiable on \\((a,b)\\), \\(\\exists \\;c\\in(a,b)\\) s.t., \\[\nf'(c) = \\frac{f(a)-f(b)}{a-b}\n\\]\n\nUsing MVT, we can prove asymptotic normality and demonstrate the Jacobian-variance result.\n\nProof. Consider the F.O.C.s from the ML estimator, evaluated at \\(\\hat{\\beta}^{ML}_n\\). \\[\n0 = n^{-1}\\sum_{i=1}^n \\frac{\\partial}{\\partial\\beta}\\log \\underbrace{f(Y_i|X_i;\\hat{\\beta}_n)}_{\\ell_i(\\hat{\\beta}_n)}\n\\] Apply MVT to the F.O.C.,\n\\[\n\\begin{aligned}\n0=&n^{-1}\\sum_{i=1}^n \\frac{\\partial}{\\partial\\beta}\\log \\ell_i(\\beta_0) + n^{-1}\\sum_{i=1}^n \\frac{\\partial^2}{\\partial\\beta\\partial\\beta'}\\log \\ell_i(\\beta^*_n)(\\hat{\\beta}_n-\\beta_0) \\\\\n\\Rightarrow \\sqrt{n}(\\hat{\\beta}_n-\\beta_0) =& \\bigg(n^{-1}\\sum_{i=1}^n \\frac{\\partial^2}{\\partial\\beta\\partial\\beta'}\\log \\ell_i(\\beta^*_n)\\bigg)^{-1}n^{-1/2}\\sum_{i=1}^n \\frac{\\partial}{\\partial\\beta}\\log \\ell_i(\\beta_0)\n\\end{aligned}\n\\] where \\(\\beta^*_n\\) is a mean-value between the estimator, \\(\\hat{\\beta}_n\\), and the true value of the parameter, \\(\\beta_0\\). Since, \\(\\hat{\\beta}_n\\rightarrow_p\\beta_0\\) and \\[\n|\\hat{\\beta}_n-\\beta_0|&gt;|\\beta^*_n-\\beta_0|\n\\] by definition of a mean value, then \\(\\beta^*_n\\rightarrow_p\\beta_0\\). The mean-value will converge in probability to the true value as \\(n\\rightarrow \\infty\\).\nThen by WLLN and Slutzky’s Theorem, \\[\n\\bigg(n^{-1}\\sum_{i=1}^n \\frac{\\partial^2}{\\partial\\beta\\partial\\beta'}\\log \\ell_i(\\beta^*_n)\\bigg)^{-1}\\rightarrow_p \\bigg(E\\bigg[\\frac{\\partial^2}{\\partial\\beta\\partial\\beta'}\\log \\ell_i(\\beta_0)\\bigg]\\bigg)^{-1}\n\\] Consider the second term, \\[\n\\begin{aligned}\n&E\\bigg[n^{-1}\\sum_{i=1}^n \\frac{\\partial}{\\partial\\beta}\\log \\ell_i(\\beta_0)\\bigg] \\\\\n=& E\\bigg[ \\frac{\\partial}{\\partial\\beta}\\log f(Y_i|X_i;\\beta_0)\\bigg] \\\\\n=& E\\bigg[\\frac{\\partial f(Y_i|X_i;\\beta_0)/\\partial\\beta}{f(Y_i|X_i;\\beta_0)}\\bigg] \\\\\n=& \\int \\frac{\\partial f(y|X_i;\\beta_0)/\\partial\\beta}{f(y|X_i;\\beta_0)}f(y|X_i;\\beta_0)dy \\\\\n=& \\frac{\\partial}{\\partial\\beta} \\underbrace{\\int f(y|X_i;\\beta_0)dy}_{=1} \\\\\n=& 0\n\\end{aligned}\n\\] Thus, an important condition of CLT is met: zero mean. By CLT, \\[\nn^{-1/2}\\sum_{i=1}^n \\frac{\\partial}{\\partial\\beta}\\log \\ell_i(\\beta_0)\\rightarrow_dN(0,\\Omega)\n\\] where the variance is a \\(k\\times k\\) matrix, \\[\n\\Omega = E\\bigg[\\frac{\\partial}{\\partial\\beta}\\log \\ell_i(\\beta_0)\\frac{\\partial}{\\partial\\beta'}\\log \\ell_i(\\beta_0)\\bigg]\n\\] Combining these two results, \\[\n\\sqrt{n}(\\hat{\\beta}-\\beta_0)\\rightarrow_d N(0,V)\n\\] where, \\[\n\\begin{aligned}\nV =& \\bigg(E\\bigg[\\frac{\\partial^2}{\\partial\\beta\\partial\\beta'}\\log \\ell_i(\\beta_0)\\bigg]\\bigg)^{-1}E\\bigg[\\frac{\\partial}{\\partial\\beta}\\log \\ell_i(\\beta_0)\\frac{\\partial}{\\partial\\beta'}\\log \\ell_i(\\beta_0)\\bigg]\\bigg(E\\bigg[\\frac{\\partial^2}{\\partial\\beta\\partial\\beta'}\\log \\ell_i(\\beta_0)\\bigg]\\bigg)^{-1} \\\\\n=&-\\bigg(E\\bigg[\\frac{\\partial^2}{\\partial\\beta\\partial\\beta'}\\log \\ell_i(\\beta_0)\\bigg]\\bigg)^{-1} \\\\\n=& \\big(J(\\beta_0)\\big)^{-1}\n\\end{aligned}\n\\] We can show the second line as follows, \\[\n\\begin{aligned}\nE\\bigg[\\frac{\\partial^2}{\\partial\\beta\\partial\\beta'}\\log \\ell_i(\\beta_0)\\bigg] =& E\\bigg[\\frac{\\partial}{\\partial\\beta'}\\frac{\\partial f(Y_i|X_i;\\beta_0)/\\partial\\beta}{f(Y_i|X_i;\\beta_0)}\\bigg]\\\\\n=& E\\bigg[\\frac{\\partial}{\\partial\\beta'}\\frac{\\partial f(Y_i|X_i;\\beta_0)/\\partial\\beta}{f(Y_i|X_i;\\beta_0)}\\bigg]\\\\\n=& E\\bigg[\\frac{\\partial^2 f(Y_i|X_i;\\beta_0)/\\partial\\beta\\partial\\beta'}{f(Y_i|X_i;\\beta_0)}-\\frac{(\\partial f(Y_i|X_i;\\beta_0)/\\partial\\beta)(\\partial f(Y_i|X_i;\\beta_0)/\\partial\\beta')}{f(Y_i|X_i;\\beta_0)^2}\\bigg] \\\\\n=& \\underbrace{E\\bigg[\\frac{\\partial^2 f(Y_i|X_i;\\beta_0)/\\partial\\beta\\partial\\beta'}{f(Y_i|X_i;\\beta_0)}\\bigg]}_{=0}-E\\bigg[\\frac{\\partial}{\\partial\\beta}\\log f(Y_i|X_i;\\beta_0)\\frac{\\partial}{\\partial\\beta'}\\log f(Y_i|X_i;\\beta_0)\\bigg] \\\\\n=&-E\\bigg[\\frac{\\partial}{\\partial\\beta}\\log \\ell_i(\\beta_0)\\frac{\\partial}{\\partial\\beta'}\\log \\ell_i(\\beta_0)\\bigg]\n\\end{aligned}\n\\] where the first term is 0 by the same arguments used to demonstrate the zero-mean CLT condition.\n\nIn this application, with a binary Y, the Hessian matrix is given by,\n\\[\nH_n(\\beta) = \\frac{1}{n}\\sum_{i=1}^n\\bigg[Y_i\\frac{f'(X_i'\\beta)F(X_i'\\beta)-f(X_i'\\beta)^2}{F(X_i'\\beta)^2}-(1-Y_i)\\frac{f'(X_i'\\beta)(1-F(X_i'\\beta))+f(X_i'\\beta)^2}{(1-F(X_i'\\beta))^2}\\bigg]X_iX_i'\n\\] The solution is given by taking the derivative of the score function, applying the quotient rule.\nFor \\(F(\\cdot)=\\Phi(\\cdot)\\), you can show \\(f'(z) = -zf(z)\\). Therefore, the Hessian matrix for a probit model is,\n\\[\n\\begin{aligned}\nH_n(\\beta) =& \\frac{1}{n}\\sum_{i=1}^n\\bigg[Y_i\\frac{-X_i'\\beta\\phi(X_i\\beta)\\Phi(X_i'\\beta)-\\phi(X_i'\\beta)^2}{\\Phi(X_i'\\beta)^2}-(1-Y_i)\\frac{-X_i'\\beta\\phi(X_i\\beta)\\big(1-\\Phi(X_i'\\beta)\\big)+\\phi(X_i'\\beta)^2}{\\big(1-\\Phi(X_i'\\beta)\\big)^2}\\bigg]X_iX_i' \\\\\n=&-\\frac{1}{n}\\sum_{i=1}^n\\phi(X_i\\beta)\\bigg[Y_i\\frac{X_i'\\beta\\Phi(X_i'\\beta)+\\phi(X_i'\\beta)}{\\Phi(X_i'\\beta)^2}+(1-Y_i)\\frac{-X_i'\\beta\\big(1-\\Phi(X_i'\\beta)\\big)+\\phi(X_i'\\beta)}{\\big(1-\\Phi(X_i'\\beta)\\big)^2}\\bigg]X_iX_i'\n\\end{aligned}\n\\] For a logit model, we can use the simplified version of the score function to solve the Hessian matrix.\n\\[\nH_n(\\beta) = -\\frac{1}{n}\\sum_{i=1}^n\\big[\\lambda(X_i'\\beta)\\big]X_iX_i' = -\\sum_{i=1}^n\\big[\\Lambda(X_i'\\beta)\\big(1-\\Lambda(X_i'\\beta)\\big)\\big]X_iX_i'\n\\] In both cases, the Hessian matrix is a negative-definite matrix given the strict concavity of distributions. However, a variance-covariance matrix must be positive-definite, which is why the Jacobian matrix is the negative of the Hessian.\nWe say that the approximate distribution of \\(\\hat{\\beta}^{ML}\\) is,\n\\[\n\\hat{\\beta}^{ML}\\overset{a}{\\sim} N(\\beta_0,\\hat{V}_n/n)\n\\] where \\(\\hat{V}_n=-H_n(\\hat{\\beta})^{-1}\\) is the hessian matrix evaluated at \\(\\hat{\\beta}_n^{ML}\\). Under certain regularity conditions, we know that \\(-H_n(\\hat{\\beta})\\rightarrow_p J(\\beta_0)\\) as \\(n\\rightarrow\\infty\\) by WLLN, since \\(\\hat{\\beta}_n^{ML}\\rightarrow_p \\beta_0\\) and the likelihood functions in question are continuously differentiable. This ensures that \\(\\hat{V}_n\\rightarrow_p V\\). This is where the scaling of \\(1/n\\) is important."
  },
  {
    "objectID": "handout-6.html#interpretation-fit",
    "href": "handout-6.html#interpretation-fit",
    "title": "Binary Choice Models",
    "section": "7 Interpretation & Fit",
    "text": "7 Interpretation & Fit\nA general problem with the probit and logit models is the interpretability of the coefficients. From a latent variable model perspective, \\(\\beta\\) has a clear marginal-effect interpretation (just as in a linear model). However, the latent variable is not a well-defined variable.\nMoreover, the discrete outcome need not consistently vary with \\(X_i\\). Suppose you change \\(X_i\\) by a value \\(\\delta\\). Then, \\[\nY_i'-Y_i = \\mathbf{1}\\{X_i'\\beta + \\delta'\\beta + \\varepsilon_i\\geq 0\\} -\\mathbf{1}\\{X_i'\\beta + \\varepsilon_i\\geq 0\\}\n\\] could take on values \\(\\{-1,0,1\\}\\), depending on the error term. Thus, the marginal effect of \\(X_i\\) on \\(Y_i\\) is by definition heterogeneous.\n\n7.1 Marginal Effects\nFor this reason, we focus on the effect of \\(X_i\\) on the \\(E[Y_i|X_i]=Pr(Y_i = 1|X_i)\\). For a continuous regressor this is given by,\n\\[\nME=\\frac{\\partial E[Y_i|X_i]}{\\partial X_i} = \\frac{\\partial F(X_i'\\beta)}{\\partial X_i} = f(X_i'\\beta)\\beta\n\\]\nFor the probit model, this is \\(\\phi(X_i'\\beta)X_i\\), while for logit it is (X_i’)(1-(X_i’))X_i.\nFor a discrete covariate, then you should evaluate the difference between predicted probabilities:\n\\[\nME = E[Y_i|X_i,D_i=1]-E[Y_i|X_i,D_i=0] = F(X_i'\\beta+\\gamma)-F(X_i'\\beta)\n\\] where \\(\\gamma\\) is the coefficient on the discrete dummy variable \\(D\\).\nThe marginal effect depends on \\(X_i\\), which means that we can evaluate it at different values of \\(X_i\\). A common choice is ME at the Mean (MEM):\n\\[\nMEM_n = f(\\bar{X}'\\hat{\\beta})\\hat{\\beta}\n\\] One issue with this is the non-representativity of a the mean covariate value. For example, if \\(X\\) includes a categorical variable (represented by dummy variables), the mean is an observation that does not exist in the data.\nAlternatively, you can then compute the Average ME (AME). \\[\nAME_n=\\frac{1}{n}\\sum_{i=1}^n f(X_i'\\hat{\\beta})\\hat{\\beta}\n\\] You can show that \\(AME_n\\rightarrow_p E[f(X_i'\\beta_0)\\beta_0]\\).\n\n\n7.2 Odds ratios\nAn alternative approach to interpretation that is more commonly applied to the logit model, is the odds ratio: the probability that \\(Y=1\\) divided by the probability that \\(Y=0\\).\nFor the logit model, this given by, \\[\n\\frac{\\Lambda(X_i'\\beta)}{1-\\Lambda(X_i'\\beta)} = exp(X_i'\\beta)\n\\] And the log odds-ratio is just \\(X_i'\\beta\\).\nThe ratio of two odds ratios can provide a useful interpretation interpretation. Consider, if you increase the \\(j\\)-th regressor by a single unit then the change in the index is,\n\\[\n\\frac{exp(X_i'\\beta+\\beta_j)}{exp(X_i'\\beta)} = exp(\\beta_j)\n\\] A 1-unit change in \\(X_j\\) increases the odds-ratio by a factor of \\(exp(\\beta_j)\\). This interpretation is common in medical journals, which favour logit models and the presentation of odds ratios. In general, Economics journals tend to prefer probit models, with the estimation of marginal effects.\n\n\n7.3 Goodness of fit\nGiven the non-linearity of probit and logit models, we cannot use \\(R^2\\) to describe goodness of fit. There are two alternatives:\n\nNumber of correct predictions.\n\nThe predicted value of the model is the predicted probability \\(Y_i=1\\): a number \\(\\in[0,1]\\). However, the outcome is discrete: \\(Y_i\\in\\{0,1\\}\\). Pick a threshold - for example, \\(\\hat{\\rho} = 0.5\\) - and assign the predicted outcome as follows:\n\\[\n\\hat{Y}_i = \\begin{cases}Y_i=1 \\qquad \\text{if} \\quad \\hat{\\rho} \\geq 0.5 \\\\\nY_i=0 \\qquad \\text{otherwise}\n\\end{cases}\n\\] Test what share of the data is correctly predicted.\n\nMcFadden’s \\(R^2\\)\n\nMcFadden’s \\(R^2\\) is is given by the ratio of the log-likelihood of the model.\n\\[\n\\text{pseudo } R^2 = 1-\\frac{\\log(L_n(\\hat{\\beta}_U))}{\\log(L_n(\\hat{\\beta}_R))}\n\\] where \\(\\ln(L_n(\\hat{\\beta}_U))\\) is the log-likelihood computed for the unrestricted model (the model with all regressors) and \\(\\ln(L_n(\\hat{\\beta}_R))\\) the log-lihood from the restricted model (with just a constant).\n\n\n7.4 Likelihood Ratio Test\nThis the ratio of log-likelihoods of two models can also be used to test hypotheses of the form,\n\\[\nH_0: h(\\beta_0) = 0\n\\] Here, \\(h(\\cdot)\\) is a function from \\(R^k\\rightarrow R^q\\). These need not be linear hypotheses. There exists two tests for hypotheses of this form. The first is the Wald statistic, which assumes that \\(h(\\cdot)\\) is a continuous function and we know the distribution of a continuous of \\(h(\\hat{\\beta}^{ML})\\).\n\\[\n\\text{Wald-stat} = nh\\big(\\hat{\\beta}^{ML}\\big)'\\bigg(\\frac{\\partial h\\big(\\hat{\\beta}^{ML}\\big)}{\\partial \\beta'}\\hat{V}^{-1}\\frac{\\partial h'\\big(\\hat{\\beta}^{ML}\\big)}{\\partial \\beta}\\bigg)h\\big(\\hat{\\beta}^{ML}\\big)\\rightarrow_d \\chi^2_q\n\\] However, the second option is more common. This is the simpler Likelihood Ratio test derived using a restricted and unrestricted estimator. The restricted estimator is given by,\n\\[\n\\hat{\\beta}_R = \\underset{\\beta\\in B_R}{\\arg\\max} \\;n^{-1}\\log L_n(\\beta)\n\\] where \\(B_R=\\{\\beta\\in B:\\;h(\\beta)=0\\}\\). The LR test statistic is then easily computed as,\n\\[\n\\text{LR-stat} = 2\\times\\big(\\log\\big(L_n(\\hat{\\beta}_U)\\big)-\\log\\big(L_n(\\hat{\\beta}_R)\\big)\\big)\\rightarrow_d \\chi^2_q\n\\] These two tests are asymptotically equivalent, but the LR-test is far easier to compute."
  },
  {
    "objectID": "handout-6.html#footnotes",
    "href": "handout-6.html#footnotes",
    "title": "Binary Choice Models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote, this is not standard uniform distribution: \\(U(0,1)\\). A standard uniform distribution has mean 1/2 and variance 1/12.↩︎\nThe Mean Value Theorem looks very similar to the Taylor Series approximation: \\(f(a) \\approx f(b) + f'(b)(a-b)\\). When \\(f'(\\cdot)\\) is evaluated at the mean-value, \\(c\\), the Taylor Series approximation is an equality.↩︎"
  },
  {
    "objectID": "handout-8.html",
    "href": "handout-8.html",
    "title": "Causal Inference",
    "section": "",
    "text": "In this note we will review some more contemporary topics from the field of Microeconometrics. These are the literatures related to Causal Inference, Treatment Effects, and Policy Evaluation. We will not be able to cover everything. In this handout we will discuss:\n\nThe Potential Outcomes Framework\nRandomized Experiments\nInstrumental Variables\nObservational Studies\nDifference-in-Differences\n\nTopics that we will not be able to cover include:\n\nStaggered-DiD or Event-studies\nSynthetic Control\nRegression Discontinuity Designs\n\nFurther reading can be found in:\n\nChapters 2.7, 25.1-25.3, 25.5, 25.8 of Cameron and Trivedi (2005)\nSection 7.7 of Verbeek (2017)\n\nSome other texts on the topic include:\n\n(MHE) Angrist and Pischke (2009) Mostly Harmless Econometrics: An Empiricist’s Companion. Princeton, NJ: Princeton University Press.\nAngrist (2014) Mastering Metrics: The Path from Cause to Effect. Princeton, NJ: Princeton University Press.\n(IR) Imbens and Rubin (2015) Causal Inference for Statistics, Social, and Biomedical Sciences: An Introduction. Cambridge, UK: Cambridge University Press\nCunningham (2021) Causal inference: The Mixtape. Yale University Press.\n\nMHE is more technical than Matering ’Metrics, but with the foundation provided in this module, you should get through much of the content. Note, it was published in 2009 and does not include some more recent topics.\nHere are a few papers worth reading:\n\nAngrist and Krueger (2001)\nHeckman (2008)\nImbens and Wooldridge (2009)\nAngrist and Pischke (2010)\nAthey and Imbens (2017)"
  },
  {
    "objectID": "handout-8.html#overview",
    "href": "handout-8.html#overview",
    "title": "Causal Inference",
    "section": "",
    "text": "In this note we will review some more contemporary topics from the field of Microeconometrics. These are the literatures related to Causal Inference, Treatment Effects, and Policy Evaluation. We will not be able to cover everything. In this handout we will discuss:\n\nThe Potential Outcomes Framework\nRandomized Experiments\nInstrumental Variables\nObservational Studies\nDifference-in-Differences\n\nTopics that we will not be able to cover include:\n\nStaggered-DiD or Event-studies\nSynthetic Control\nRegression Discontinuity Designs\n\nFurther reading can be found in:\n\nChapters 2.7, 25.1-25.3, 25.5, 25.8 of Cameron and Trivedi (2005)\nSection 7.7 of Verbeek (2017)\n\nSome other texts on the topic include:\n\n(MHE) Angrist and Pischke (2009) Mostly Harmless Econometrics: An Empiricist’s Companion. Princeton, NJ: Princeton University Press.\nAngrist (2014) Mastering Metrics: The Path from Cause to Effect. Princeton, NJ: Princeton University Press.\n(IR) Imbens and Rubin (2015) Causal Inference for Statistics, Social, and Biomedical Sciences: An Introduction. Cambridge, UK: Cambridge University Press\nCunningham (2021) Causal inference: The Mixtape. Yale University Press.\n\nMHE is more technical than Matering ’Metrics, but with the foundation provided in this module, you should get through much of the content. Note, it was published in 2009 and does not include some more recent topics.\nHere are a few papers worth reading:\n\nAngrist and Krueger (2001)\nHeckman (2008)\nImbens and Wooldridge (2009)\nAngrist and Pischke (2010)\nAthey and Imbens (2017)"
  },
  {
    "objectID": "handout-8.html#potential-outcomes-framework",
    "href": "handout-8.html#potential-outcomes-framework",
    "title": "Causal Inference",
    "section": "2 Potential Outcomes Framework",
    "text": "2 Potential Outcomes Framework\nThe notion of a ‘causal effect’ is bound by framework within which you define causal effect. In this handout we will build upon the Potential Outcomes (PO) Framework attributed to Jerzy Neyman and Donald Rubin. There are other frameworks, including traditional Econometrics, that seek to model causal relationships (see Heckman and Pinto 2024). Recently, a lot of consideration has been given to the Dynamic Acyclic Graphs approach popularized by Judea Pearl (see Pearl and Mackenzie 2018; also Imbens 2020; Heckman and Pinto 2024). For those interested in the broader question of causality within Economics, take a look at Hoover (2008).\nThe following is an adaption of the paracetomal example in Imbens and Rubin (2015, 5):\n\nExample 1 Consider a new hypothetical policy in the UK, wherein time spent studying in the UK counted towards permanent residency (‘indefinite leave to remain’). How would this affect your decision to apply for a graduate visa upon graduation?\nThere are four potential scenarios:\n\nApply under new rule only:\n\n\\[Y (\\text{reform}) = \\text{apply} \\qquad{and}\\qquad Y (\\text{status quo}) = \\text{don’t apply}\\]\n\nDon’t apply in either case:\n\n\\[Y (\\text{reform}) = \\text{don't apply} \\qquad{and}\\qquad Y (\\text{status quo}) = \\text{don’t apply}\\]\n\nApply regardless:\n\n\\[Y (\\text{reform}) = \\text{apply} \\qquad{and}\\qquad Y (\\text{status quo}) = \\text{apply}\\]\n\nDon’t apply with new rule, but under status quo:\n\n\\[Y (\\text{reform}) = \\text{don't apply} \\qquad{and}\\qquad Y (\\text{status quo}) = \\text{apply}\\] We say that in scenarios (2) and (3) there is no causal effect, while in scnearios (1) and (4) there is a causal effect.\n\nNotice two things about this definition of causal effect:\n\n“First, the definition of the causal effect depends on the potential outcomes, but it does not depend on which outcome is actually observed.” (Imbens and Rubin 2015, 6)\n“Second, the causal effect is the comparison of potential outcomes, for the same unit, at the same moment in time post-treatment. In particular, the causal effect is not defined in terms of comparisons of outcomes at different times.” (Imbens and Rubin 2015, 6)\n\nThere are three important components to the Potential Outcomes framework:\n\nPotential outcomes: the outcomes corresponding to different levels of treatment or manipulation: “no causation without manipulation” (Rubin 1975, 238).\nMultiple units: given the limitation of observation, one must observe multiple units to infer a causal effect.\nAssignment mechanism: what determines treatment?\n\n\n2.1 Potential outcomes\nFor a discrete treatment, we denote the PO of outcome Y as,1\n\\[\n\\begin{aligned}\n    Y_i(1)\\quad&\\text{if}\\quad D_i=1 \\\\\n    Y_i(0)\\quad&\\text{if}\\quad D_i=0\n\\end{aligned}\n\\] where \\(D_i=\\mathbf{1}\\{\\text{treated}\\}\\).2 The observed outcome, \\(Y^{obs}_i\\), can written as\n\\[\n\\begin{aligned}\n    Y^{obs}_i=Y_i(D_i)&=Y_i(0)+D_i\\cdot\\left(Y_i(1)-Y_i(0)\\right) \\\\\n    &=(1-D_i)Y_i(0)+D_iY_i(1)\n\\end{aligned}\n\\]\n\n\n2.2 Multiple units\nWe cannot observe both POs for any unit \\(i\\) (in the same period of time). At a fundamental level, this means that we cannot observe the unit-level treatment effect:\n\\[\n\\tau_i = Y_i(1)-Y_i(0)\n\\] We always need to learn about \\(f_{(1)}\\) and \\(f_{(0)}\\) - the marginal distributions of each potential outcome - from two (or more) samples; be this,\n\ndifferent units at the same time;\nthe same units at different times;\nor a combination of both.\n\nThis requires us to make an important assumption: the Stable Unit Treatment Value Assumption (SUTVA).\n\nDefinition 1 - SUTVA “The potential outcomes for any unit do not vary with the treatments assigned to other units, and, for each unit, there are no different forms or versions of each treatment level, which lead to different potential outcomes.” (Imbens and Rubin 2015, 10)\n\nThis assumption does two things. First, it rules out interference between treatment units. This rules out any spillover effects between ‘treatment’ and ‘control’. Second, it ensures that there are no hidden variations in the treatment level. It is okay for there to be different levels of treatment, so long as they are explicit and well-defined a priori.\nGeneral equilibrium effects violate SUTVA. In almost all case, we need to assume a partial equilibrium setting. As Economists, this means that the PO framework cannot be used to answer all important empirical questions. There remains a value to more structural models that can identify general equilibrium effects.\n\n\n2.3 Assignment mechanism\nIn experimental data, the assignment mechanism is randomization, while in observational data the assignment mechanism is not known.\nRandomization is an assignment mechanism that ensures independence/unconfoundedness between the potental outcomes and treatment assignment. This can either be unconditional,3\n\\[\nY_i(1),Y_i(0)\\perp D_i\n\\] or conditional on a set of known covariates, \\[\nY_i(1),Y_i(0)\\perp D_i|X_i\n\\] The latter is also referred to as the Conditional Independence Assumption (as in MHE & MM) and (strong) ignorability. Both assumptions are referred to as unconfoundedness in the literature.\nNote, “[w]ithout unconfoundedness, there is no general approach to estimating treatment effects.” (Imbens and Wooldridge 2009, 7). Randomization gives you unconfoundedness, but unconfoundedness can be assumed without randomization; for example, in observational studies. In such cases, you might say it is as if assignment is randomized (conditional on X).\nNote, in our study of linear models, we made conditional mean independence assumptions of the form \\(E[\\varepsilon_i|X_i]=0\\). Mean independence is sufficient for identification in linear models; however, randomization gives you independence between treatment assignment and potential outcomes. You can therefore identify the the marginal distributions \\(f_{(0)}\\) and \\(f_{(1)}\\) and not just their mean \\(E[Y(0)]\\) and \\(E[Y(1)]\\).\n\n\n2.4 Causal estimands\nWe cannot identify the unit-level treatment effect, even with randomization. For this reason, the literature focuses on identifying particular causal estimands.\nUsing the linearity of expectation function, we can identify (from different samples) the Average Treatment Effect from the difference in uncondtional means:\n\\[\n  E[Y_i(1)]-E[Y_i(0)] = E[Y_i(1)-Y_i(0)] = ATE\n\\] Similarly, we can identify the Average Treatment Effect of the Treated (ATT) and the Untreated (ATU):\n\\[\n\\begin{aligned}\n  E[Y_i(1)|D_i=1]-E[Y_i(0)|D_i=1] =& E[Y_i(1)-Y_i(0)|D_i=1] = ATT \\\\\n  E[Y_i(1)|D_i=0]-E[Y_i(0)|D_i=0] =& E[Y_i(1)-Y_i(0)|D_i=0] = ATU \\\\\n\\end{aligned}\n\\]\nThe above statements are non-trivial. We can learn about the mean of the unit-level treatment effect from different samples. This follows from the fact that the expectation (or average) operator is linear. Unfortunately, this DOES NOT extend beyond the mean.\nThe q-th percentile of the distribution of unit-level treatment effects, cannot necessarily be written as the difference between q-th percentile of \\(f_{(0)}\\) and \\(f_{(1)}\\).\n\\[\nF^{-1}_{(1)-(0)}(q) \\neq F^{-1}_{(1)}(q)-F^{-1}_{(0)}(q)\n\\]\nFor the above to be an equality, there must be perfect rank correlation between the distributions of \\(Y(1)\\) and \\(Y(0)\\).\nThere are additional causal estimands important within this literature:\n\nConditional ATE, ATT, and ATU; e.g., \\[\nATE(X_i) = E[Y_i(1)-Y_i(0)|X_i]\n\\]\nAverage Causal Effect, typically used to describe continuous (or mutliple dosage) treatments; e.g., \\[\nACE(s) = E[Y_i(s+1)-Y_i(s)|S_i=s]\n\\]\nLocal Average Treatment Effects (LATE), as identified by instrumenal variables and regression discontinuity designs.\nCohort-specific ATT, the ATT for a specific treatment cohort in a staggered difference-in-differences (or event-study); e.g., as denoted by Sun and Abraham (2021)\n\n\\[\nCATT(c) = E[Y_i(c)-Y_i(\\infty)]\n\\] where \\(Y_i(\\infty)\\) denotes the PO when “never-treated”.\nI have ignored an important distinction between finite-sample and super-population estimands. The distinction comes down to whether you think of the sample as the population - in which case, the vectors \\(Y(1)\\) and \\(Y(0)\\) are non-random vectors - or whether you think of the sample as a random draw from a (infinite) super-population. The distinction has important implications for how you compute the variance of an estimator (see Imbens and Rubin 2015, chap. 6). Finite-sample estimands are typically written as averages; hence, the name ‘average’ TEs. For exaemple, the finite-sample ATE is given by, \\[\nATE^{fs} = \\frac{1}{N} \\sum_{i=1}^{N} Y_i(1)-Y_i(0)\n\\] The above is an estimand and cannot be computed as it requires you to observe the unit-level treatment effect for all \\(i\\).\n\n\n2.5 Heterogeneity\nIt is important to recognise that treatment effects need not be homogeneous.\n\nDefinition 2 - Homogenous TEs \\[\nY_i(1)-Y_i(0) = \\tau \\qquad \\forall\\;i=1,\\dots,n\n\\]\n\nIndeed, you could argue that this literature is largely focused on the estimation of heterogeneous treatment effects. Homogenous treatment effects negate the need to speak about the ATE, ATT, and ATU as all estimands are equivalent and equal to the unit-level TE.\nHeterogeneity also has important implications for the use of models to estimate TEs. Consider, if TEs are homogenous, we can write the observed outcome as,\n\\[\n\\begin{aligned}\nY_i =& Y_i(0) + \\tau D_i \\\\\n=& E[Y_i(0)] + \\tau D_i + Y_i(0)-E[Y_i(0)] \\\\\n=& \\alpha + \\tau D_i + \\varepsilon_i\n\\end{aligned}\n\\] We can express the observed outcome as a model that is linear in parameters with an error term that is homoskedastic (assuming \\(Y_i(0)\\) is drawn from the same distribution).\nWith heterogeneous TEs, we get\n\\[\n\\begin{aligned}\nY_i =& Y_i(0) + \\tau_i D_i \\\\\n=& E[Y_i(0)] + \\tau_{ATE} D_i + Y_i(0)-E[Y_i(0)]+ (\\tau_i-\\tau_{ATE})D_i \\\\\n=& \\alpha + \\tau_{ATE} D_i + \\upsilon_i\n\\end{aligned}\n\\] The model only explains the average difference between the treated and control. The heterogeneity remains in the error term. With random assignment, the error term remains conditionally mean independent; however, it is no longer homoskedastic (see Deaton 2010).\nIn models that include a time dimension - e.g., difference-in-differences - you also need to cosider whether TEs are static. ::: {#def-staticTE} - Static TEs \\[\nY_{it}(1)-Y_{it}(0) = \\tau_i \\qquad \\forall\\;t=1,\\dots,T\n\\] ::: Models with dynamics, will often normalize time relative to the period of treatment; sometimes referred to as event-time. For example, event-time \\(s = -1\\) is the period just before treatment and \\(s=0\\) the period of treatment."
  },
  {
    "objectID": "handout-8.html#randomized-experiments",
    "href": "handout-8.html#randomized-experiments",
    "title": "Causal Inference",
    "section": "3 Randomized Experiments",
    "text": "3 Randomized Experiments\nIn Economics, randomized experiments are typically referred to as Randomized Control Trials (RCTs); a name borrowed from the Medical field. However, randomization is also used in lab experiments and does sometimes appear in real-world policies (see Angrist 1990).\n\n3.1 Selection\nWhen comparing two groups, the difference between the mean of their outcomes can be decomposed into two terms: the ATT and a selection term.\n\\[\n\\begin{aligned}\n&E[Y_i|D_i =1]-E[Y_i|D_i=0] \\\\\n=&E[Y_i(1)|D_i =1]-[Y_i(0)|D_i =1]+E[Y_i(0)|D_i =1]-E[Y_i(0)|D_i=0] \\\\\n=&E[Y_i(1)-Y_i(0)|D_i=1] + E[Y_i(0)|D_i =1]-E[Y_i(0)|D_i=0]  \\\\\n=&ATT + \\text{selection}\n\\end{aligned}\n\\] where \\(E[Y_i(0)|D_i =1]\\) is an unobserved counterfactual: the mean of the treated group had it not been treated.\nNote, this is a particular definition of selection. When discussing the “problem of selection” in this literature, the question is whether:\n\\[\nE[Y_i(0)|D_i =1]-E[Y_i(0)|D_i=0]\\overset{?}{=}0\n\\]\n\n\n3.2 Identification\nRecall, under randomization assignment is unconfounded; therefore, the observed difference between the mean of the treatment and control group identifies,\n\\[\n\\begin{aligned}\n&E[Y_i|D_i =1]-E[Y_i|D_i=0] \\\\\n=&E[Y_i(1)|D_i =1]-E[Y_i(0)|D_i=0] \\\\\n=&E[Y_i(1)]-E[Y_i(0)] \\\\\n=&ATE\n\\end{aligned}\n\\] where line 3 follows by unconfoundedness. This implies:\n\nthe selection term \\(=0\\);\nthe \\(ATT = ATE = ATU\\).\n\nAn RCT can tell you about the ATE of the treatment, for the sampled population. It cannot tell you about the ATE for any unsampled population; an issue referred to as external validity.\n\n\n3.3 Efficiency\nOne limitation of RCTs is their limited (sample) size. They can be expensive to run, thereby reducing the size of the treated and control sample. For this reason, researchers will take action to design the most efficient (powerful) experiment.\nIf \\(N\\) (the sample size) is fixed, it is optimal (from an efficiency perspective) to assign \\(N_t = 0.5 N\\) units to treatment.\nA second action taken by researchers is to control for characteristics in a model; for example,\n\\[\n  Y_i = \\alpha + \\beta D_i + X_i'\\gamma + \\upsilon_i\n\\] In this instance, these regressors are not included for identification as treatment is unconfounded (this is not the case for stratified experiments; see below). Instead, they are there to reduce the residual variation of the error term.\nIt is important that these are good controls. The included covariates must not be potential outcomes of an experiment. In RCTs, you will typically see that researchers include covariates from a baseline survey.\nThe OLS estimator for \\(\\beta\\) from the above model is consistent, but may be biased in small-samples depending on whether the heterogeneity of the TE relates to the included \\(X\\)’s (Deaton 2010). \\[\np \\lim \\hat{\\beta} = ATE\n\\]\n\n\n3.4 Stratification\nMany real-world RCTs stratify assignment into treatment. They divide the sample into blocks - typically based on observable characteristics - and assign each block into treatment independently. By implication, each group can be assigned into treatment with a different probability.\nStratification has an important advantage: since assignmet is unconfounded within each block (or conditional on the stratification covariates \\(X_i\\)), you can conduct more efficient sub-sample analyses.\nHowever, stratification also complicates the use of models to estimate TEs. Since, treatment is independent conditional on \\(X_i\\), you need to account for \\(X_i\\) in the model.\nSuppose, the statification variable(s) takes on \\(m\\) distinct values: \\(X_i\\in\\{x_1,x_2,\\dots,x_m\\}\\). Then, the unconditional ATE can be written as a probablity weighted sum of conditional ATEs. \\[\nATE = \\sum_{j=1}^m ATE(x_j)\\cdot Pr(X_i=x_j)\n\\] If you estimate a model, that conditions on each value of \\(X_i\\) (referred to as ‘saturated controls’), the coefficient on the treatment indicator gives you a variance weighted average.\n\\[\nY_i = \\beta D_i + \\sum_{j=1}^{m}\\gamma_j \\mathbf{1}\\{X_i = x_j\\} + \\zeta_i\n\\] then,\n\\[\n\\beta = \\frac{\\sum_{j=1}^{m}ATE(x_j)Var(W_i|X_i=x_j)Pr(X_i=x_j)}{\\sum_{l=1}^{m}Var(W_i|X_i=x_l)Pr(X_i=x_l)}\n\\] where, \\[\nVar(D_i|X_i) =Pr(D_i=1|X_i)\\cdot \\big(1-Pr(D_i=1|X_i)\\big)\n\\] Assigning treatment with different probabilities (based on \\(X_i\\)), implies that the OLS estimator of a linear model with covariates will estimate a variance weighted estimand. If the ATE is independent of \\(X_i\\), this not a concern, since the weights still sum to 1.\n\n\n3.5 Propensity score\nIn this literature, the probability of treatment (conditional on \\(X_i\\)) is referred to as the propensity score:\n\\[\n\\rho(X_i) = Pr(D_i = 1|X_i)\n\\] You can show that the unconditional ATE can be written as an (inverse) propensity weighted estimand:\n\\[\n            E[Y_i(1)] = E\\left[\\frac{Y_i\\cdot W_i}{\\rho(X_i)}\\right]\n\\] and \\[\n            E[Y_i(0)] = E\\left[\\frac{Y_i\\cdot (1-D_i)}{1-\\rho(X_i)}\\right]\n\\]\nWhich means that,\n\\[\nE[Y_i(1)-Y_i(0)] =E\\left[\\frac{Y_i\\cdot D_i}{\\rho(X_i)}-\\frac{Y_i\\cdot (1-D_i)}{1-\\rho(X_i)}\\right]= E\\left[\\frac{Y_i\\cdot (D_i-\\rho(X_i))}{\\rho(X_i)\\cdot(1-\\rho(X_i))}\\right]\n\\]\nThis Weighted Least Squares estimator for \\(\\beta\\) from the univariate model,\n\\[\nY_i = \\alpha + \\beta D_i + \\varepsilon_i\n\\] using weights,\n\\[\n\\lambda_i = \\frac{1}{\\rho(X_i)^{D_i}\\cdot(1-\\rho(X_i))^{1-D_i}}\n\\] is an unbiased estimator for the unconditional ATE under stratified assignment. This referred to as the Horovitz-Thompson estimator.\nThis points to a wider issue within this literature: this approach to causal inference is built upon an experiment framework, which does not necessarily map to a (linear) regression-model econometrics framework."
  },
  {
    "objectID": "handout-8.html#instrumental-variables",
    "href": "handout-8.html#instrumental-variables",
    "title": "Causal Inference",
    "section": "4 Instrumental Variables",
    "text": "4 Instrumental Variables\n\n\n\n\n\n\nNote\n\n\n\nThe following might be a slightly different presentation of instrumental variables to what you are used to from your undergraduate degree.\n\n\nIn some experiments, compliance - i.e. take-up of the treatment - is not a given. This could be for a number of reasons, including ethical reasons. In this case, assignment into the treatment-group does not gaurantee that the individual is treated.\nConsider the RCT in Banerjee et al. (2015). The authors randomly assigned a new micro-loan agency to a sub-sample of villages in India. It would be unethical to force a household to take on credit. Instead, the treatment randomly increased access to credit. In this application, the mean difference in \\(Y_i\\) between the treatment and control villages cannot identify the causal impact of having a micro-loan. It identifies the ATE of living in a village with increased access to micro-loans.\nIn imperfect-compliance setting, we adopt the language of Intention to Treat (ITT) effects. In the words of the authors: “given the sampling frame, ours will be an intent-to-treat (ITT) analysis on a sample of “likely borrowers.” This is thus neither the effect on those who borrow nor the average effect on the neighborhood. Rather, it is the average effect of easier access to microfinance on those who are its primary targets.” (Banerjee et al. 2015, 35).\nYou might be wondering, why can’t we just condition on receipt of the treatment? If compliance is imperfect, then take-up of the treatment is endogenous: determined by factors other than random assignment. As a result the treatment-receiving sample is no longer randomly assigned.\nYou can have both one-sided and two-sided non-compliance. Here, we will focus on the latter, as it more closely relates to the use of instruments in Applied Economics (for example Angrist 1990). One-sided non-compliance means that some individuals in the treated group may not receive the treatment, but does not allow for the case where individuals in the control group may receive it. Two-sided non-compliance allows for both.\n\n4.1 Compliance\nLet \\(D_i\\in\\{0,1\\}\\) denote receipt of treatment and let \\(Z_i\\in\\{0,1\\}\\) denote treat-group assignment. With two-sided non-compliance, we can consider the potential outcomes of \\(D_i\\):\n\\[\nD_i(Z_i)\\in\\{D_i(1),D_i(0)\\}\n\\]\nUsing these potential outcomes we can assign names to each possible compliance status:\n\nCompliance status:\n\n\n\n\\(D_i(1) = 0\\)\n\\(D_i(1)=1\\)\n\n\n\n\n\\(D_i(0) = 0\\)\nnever-treated (nt)\ncomplier (c)\n\n\n\\(D_i(0) = 1\\)\ndefier (d)\nalways-treated (at)\n\n\n\nDefine \\(G_i\\in\\{nt,c,d,at\\}\\), with \\(Pr(G_i = g)\\in\\{\\rho_{nt},\\rho_c,\\rho_d, \\rho_{at}\\}\\).\nWhen you look at the data, you don’t observe each potential outcome. Instead, you observe the pair \\(\\{Z_i,D_i\\}\\); each combination of which will contain a mix of the above groups.\n\nCompliance status:\n\n\n\n\\(D_i = 0\\)\n\\(D_i=1\\)\n\n\n\n\n\\(Z_i = 0\\)\nnever-treated + compliers\nalways-treated + defiers\n\n\n\\(Z_i = 1\\)\nnever-treated + defiers\nalways-treated + compliers\n\n\n\nThe ITT of assignment on treatment receipt is given by:\n\\[\n\\begin{aligned}\nITT_D =& E[D_i(1)-D_i(0)] \\\\\n=& E[D_i(1)]-E[D_i(0)] \\\\\n=& \\rho_c + \\rho_{at} - (\\rho_d + \\rho_{at}) \\\\\n=&\\rho_c-\\rho_d\n\\end{aligned}\n\\]\nThis is the difference in the proportion of compliers (those who take-up treatment only when assigned) and defiers (those who take up treatment only when not assigned) in the (super) population.\nThe potential outcomes for the outcome variable are:\n\\[\nY_i(Z_i,D_i(Z_i)) \\in\\{Y_i(0,0),Y_i(0,1),Y_i(1,0),Y_i(1,1)\\}\n\\]\nFor example, \\(Y_i(1,0)\\) is the potential outcome of unit \\(i\\) were they to receive assignment into treatment, but not take it up. Likewise, \\(Y_i(1,1)\\) represents the potential outcome of unit \\(i\\) were they to take-up the treatment after being assigned it.\nThe ITT of assignment on the main outcome is:\n\\[\nITT_Y = E[Y_i(1)-Y_i(0)] = E[Y_i(1,D_i(1))-Y_i(0,D_i(0))]\n\\]\nThis term is what you identify when comparing the means of the outcome in the treated and control groups, under randomization.\nUnder randomization, the instrument is unconfounded with respect to both sets of potential outcomes:\n\\[\nZ_i \\perp D_i(1),D_i(0),Y_i(0,0),Y_i(0,1),Y_i(1,0),Y_i(1,1)\n\\]\n\n\n4.2 Identification\nIn addition to unconfoundedness, we require the following exclusion restrictions assumptions:\n\nfor all never-takers: \\(Y_i(1,0)=Y_i(0,0)\\)\nfor all always-takers: \\(Y_i(0,1)=Y_i(1,1)\\)\n\nThen we need a final monotonicity (i.e., no defiers) assumption:\n\nno defiers: \\(D_i(1)\\geq D_i(0)\\Rightarrow \\rho_d = 0\\)\n\nUnder these four assumptions:\n\\[\n  \\frac{ITT_Y}{ITT_D} = E[Y_i(1)-Y_i(0)|G_i = c ] = LATE\n\\] This is referred to as the Local Average Treatment Effect: the ATE of compliers. Note, you cannot directly observe the population of compliers in the data, as this would require observing both states of the outcome \\(D_i(Z_i)\\). In contrast, when we identify the ATT (as with DiD), this the ATE of the observed Treated population.\nWe can prove this result as follows. In the denominator, we have:\n\\[\n\\begin{aligned}\nE[D_i(1)-D_i(0)] =& \\rho_{nt} E[D_i(1)-D_i(0)|G_i=nt] \\\\\n&+\\rho_{at} E[D_i(1)-D_i(0)|G_i=at] \\\\\n&+\\rho_{c} E[D_i(1)-D_i(0)|G_i=c] \\\\\n=&\\rho_c\n\\end{aligned}\n\\] since by the monotonicity (no defiers) assumption \\(\\rho_d=0\\); \\(D_i(1)=D_i(0)\\) for never- and always-takers; and \\(D_i(1) - D_i(0)=1\\) for compliers.\nIn the numerator, we have:\n\\[\n\\begin{aligned}\n&E[Y_i(1,D_i(1))-Y_i(0,D_i(0))] \\\\\n=& E[Y_i(1,1)|D_i(1)=1]\\cdot Pr(D_i(1)=1) + E[Y_i(1,0)|D_i(1)=0]\\cdot Pr(D_i(1)=0) \\\\\n&- E[Y_i(0,1)|D_i(0)=1]\\cdot Pr(D_i(0)=1) - E[Y_i(0,0)|D_i(0)=0]\\cdot Pr(D_i(0)=0) \\\\\n\\end{aligned}\n\\] Assuming no defiers, \\[\n\\begin{aligned}\nE[Y_i(0,1)|D_i(0)=1]\\cdot Pr(D_i(0)=1) =&\\rho_{at}E[Y_i(0,1)|G_i=at] \\\\\n=&\\rho_{at}E[Y_i(1,1)|G_i=at]\n\\end{aligned}\n\\] Where the last line follows from the exclusion restriction. Similarly,\n\\[\n\\begin{aligned}\nE[Y_i(1,0)|D_i(1)=0]\\cdot Pr(D_i(1)=0) =&\\rho_{nt}E[Y_i(1,0)|G_i=nt] \\\\\n=&\\rho_{nt}E[Y_i(0,0)|G_i=nt]\n\\end{aligned}\n\\] Applying the exclusion restrictions we get:\n\\[\n\\begin{aligned}\n=& \\rho_c E[Y_i(1,1)|G_i=c]- \\rho_c E[Y_i(0,0)|G_i = c] \\\\\n&+ \\rho_{at} E[Y_i(1,1)|G_i=at]- \\rho_{at}E[Y_i(1,1)|G_i=at]\\\\\n&+\\rho_{nt}E[Y_i(0,0)|G_i=nt]-\\rho_{nt}E[Y_i(0,0)|G_i=nt] \\\\\n=&\\rho_c E[Y_i(1,1)-Y_i(0,0)|G_i=c]  \\\\\n\\end{aligned}\n\\] Dividing by the \\(ITT_D\\), we get the LATE result.\n\n\n4.3 Estimation\nThe LATE can be estimated by two-stage-least-squares (2SLS).\n\nFirst, estimate the first-stage (\\(ITT_D\\)):\n\n\\[\nD = \\phi_1 + \\phi_2 Z + \\nu\n\\]\n\nSecond, estimate the second-stage, substituting \\(D\\) with the predicted values of the first-stage:4\n\n\\[\nY = \\beta_1 + \\beta_2 P_ZD + \\epsilon\n\\] In this just-identified case,\n\\[\n    \\hat{\\beta}^{2SLS}_2 = \\frac{\\hat{\\gamma}_2}{\\hat{\\phi}_2}\n\\] where \\(\\hat{\\gamma}_2\\) is the OLS estimator of the reduced form equation,\n\\[\nY = \\gamma_1 + \\gamma_2 Z + \\zeta\n\\] Recall, the reduced form equation identifies the \\(ITT_Y\\).\n\n\n4.4 Reduced form\nRecall, \\(Z\\) denotes random treatment-assignment. Much of empirical in applied microeconomics takes on this form: regressing the outcome directly on treatment-assignment. It is for this reason that it is often referred to as “reduced form” research.\nHowever, the phrase “reduced form” comes from an older literature, where the regression of \\(Y\\) on \\(D\\), where \\(D\\) was potentially endogenous, was a “structural equation”, including parameters from a structural model. In this setting, we do not typically adopt this phrase to describe the relationship between \\(Y\\) and \\(D\\).\nThis framework provides a useful perspective on experiments. Instruments are central to the way economists think about causation (see Angrist and Krueger 2001), and randomized experiments provide the ideal instrumental variable. You can either examine the reduced-form relationship, given by treatment-assignment, or use the experiment as an instrument to study an endogenous relationship.\nConsider the following setting. Suppose, you are interested in the causal relationship between household income (RHS) and investment into child-education (LHS) in poorer countries. Evidently household income (a continuous variable) is endogenous, and comparing households with high and low incomes will give rise to differences largely explained by selection.\nWe need a source of exogenous variation in household income. We could design a RCT that enrolls a random sample of households into a universal basic income (UBI) scheme. Comparing the difference in education of households in treatment and control, would give us the reduced form effect of the experiment. Alternatively, we could use treatment assignment as an instrument for household income. Of course, we hope to find that treatment increases household income by an amount close to the value of the UBI scheme."
  },
  {
    "objectID": "handout-8.html#observational-studies",
    "href": "handout-8.html#observational-studies",
    "title": "Causal Inference",
    "section": "5 Observational Studies",
    "text": "5 Observational Studies\nIn observational studies, the functional form of the assignment mechanism is not known. The standard approach in these settings is to assume Conditional Independence (CIA/unconfoundedness). Afterall, “[w]ithout unconfoundedness, there is no general approach to estimating treatment effects” (Imbens and Wooldridge, 2009, p.7).\n\\[\nY_i(1),Y_i(0)\\perp D_i|X_i\n\\] This assumption suggests that conditional on \\(X\\) it is as if assignment is randomized. Thus,\n\\[\nE[Y_i(0)|D_i=1,X_i]-E[Y_i(0)|D_i=0,X_i] = 0\n\\] There is no selection, conditional on \\(X_i\\). The above statement, is also referred to as “selection on observables”. The treatment and control group can differ in terms of the distribution of \\(X_i\\); however, conditional on \\(X_i\\) they are balanced. This rules out “selection on unobservables”.\n\n5.1 Common support\nIn controlled experiments the researcher picks the number of treated and control units. There is just one requirement: \\(1\\leq N_t \\leq N-1\\), at least one unit must be (un)treated. In observational studies, you do not have direct control over the assignment mechanism. We therefore need an additional assumption: common support (or overlap).\n\nDefinition 3 - Common Support \\[\n0&lt; \\rho(X_i) &lt; 1\n\\]\n\nThis assumption ensures that for every \\(X_i\\), you observe both treated and control units with a non-zero probability.\n\n\n5.2 Matching\nThe most direct approach to matching is covariate-matching: given common support, you can estimate a Conditional ATE for each \\(X_i\\), after which you aggregate up to the unconditional ATE using the distribution of \\(X\\). \\[\nATE = \\sum_{j=1}^m ATE(x_j)\\cdot Pr(X_i=x_j)\n\\] Unfortunately, covariate matching quickly encounters the curse of dimensionality. You need to match on all possible values of the vector \\(X_i\\in\\mathbb{R}^k\\). Consider, if each of the \\(X_{ik}\\) covariates is a dummy variable, then the number of possible values for the vector \\(X_i\\) is \\(2^k\\).\nIn addition, there is the challenge of matching on continuous variables. This can be solved by discretizing the support of the continuous variable, but necessarily increases the number of values to matching on.\nAn alternative is Propensity Score Matching (PSM). Rosenbaum and Rubin (1983) show that you can match on the propensity score instead of covariates. This is because the propensity score is a balancing score.\n\nDefinition 4 - Balancing Score: A function \\(b:\\mathbb{R}^k\\rightarrow\\mathbb{R}\\), such that, \\[\n  D_i\\perp X_i | b(X_i)\n\\]\n\nYou can show that \\(\\rho(X_i)=Pr(D_i=1|X_i)\\) is a balancing score, by demonstrating the equality between: \\[\nPr(D_i=1|X_i,\\rho(X_i)) = Pr(D_i=1|\\rho(X_i))\n\\] Given this result, unconfoundedness implies unconfoundedness on the propensity score, \\[\nY_i(1),Y_i(0)\\perp D_i|\\rho(X_i)\n\\] Take a look at Hirano, Imbens, and Ridder (2003) and Caliendo and Kopeinig (2008), if you are interested in some of the practicialities of PSM estimation. The major challenge for PSM, in observational study settings, is that we do not know the true functional form of \\(\\rho(X_i)\\).\n\n\n5.3 Regression\nYou can think of linear regression as a matching estimator. Recall, we can always write:\n\\[\nY_i = E[Y_i|D_i,X_i]+\\varepsilon_i\n\\] where \\(E[\\varepsilon_i|D_i,X_i]=0\\). Given that \\(Y_i\\) is a combination of potential outcomes,\n\\[\nY_i = E[Y_i(0)|D_i,X_i]+D_iE[Y_i(1)-Y_i(0)|D_i,X_i]+\\varepsilon_i\n\\] Assuming CIA/unconfoundedness, \\[\nY_i = E[Y_i(0)|X_i]+D_iE[Y_i(1)-Y_i(0)|X_i]+\\varepsilon_i\n\\] If we are willing to assuming that \\(E[Y_i(0)|X_i]\\) is linear (in parameters), then we get \\[\nY_i = X_i'\\gamma+D_iE[Y_i(1)-Y_i(0)|X_i]+\\varepsilon_i\n\\] We then need to make an assumption concerning the heterogeneity w.r.t. \\(X\\). Alternatively, we can opt for a more flexible saturated-controls model, \\[\nY_i = \\sum_{j=1}^{m}\\gamma_j \\mathbf{1}\\{X_i = x_j\\}+D_iE[Y_i(1)-Y_i(0)|X_i]+\\zeta_i\n\\] Clearly, we need to make an assumption concerning how heteroegeneity relates to \\(X_i\\). If it is independent of \\(X_i\\), then the saturated model will approximate any functional form of \\(E[Y_i(0)|X_i]\\), but is limited by the curse of dimensionality.\nSuppose we are unwilling to assume homogeneity w.r.t. \\(X\\). If we allow the coefficient on \\(D\\) to vary with every value of \\(X\\), we are back to covariate-matching (i.e. a separate model for each covariate value). If we estimate a single parameter coefficient for \\(D\\), we end up with variance weighting. There is no obvious path forward.\nWe also need to take note of common-support. Saturated-controls models require common-support, but linear models do not. You should be aware that linear models can allow you to extrapolate a counterfactual to parts of the data without common support."
  },
  {
    "objectID": "handout-8.html#difference-in-differences",
    "href": "handout-8.html#difference-in-differences",
    "title": "Causal Inference",
    "section": "6 Difference-in-Differences",
    "text": "6 Difference-in-Differences\nDifference-in-differences (DiD) is one of the most commonly used empirical strategies in applied microeconomics research. This is partly because the method is well suited to the study of “natural”/quasi-experiments. These are empirical settings where the conditions of an experiment are met - that is, assignment into a treatment and control group - but the researcher has no control over the assignment. For example, a natural disaster that shocks a particular region, but does not affect a neighbouring region (see Card 1990). Or, the introduction of a new policy that affects one group of people, but not another.\nCrucially, DiD does not require unconfoundedness. Instead, identification is based on a parallel trends assumption, as well as a few exclusion restrictions. The DiD approach utilizes a time-dimesion that has so far been ignored. It also has the advantage of being feasible with repeated cross-sections of data, as panel data is not always avaiable.\n\n6.1 2-group-2-period\nThe simple 2-group-2-period DiD set-up has the following characteristics,\n\ntreatment takes place in period \\(t_0\\);\nyou observe both treated and control samples in a period before and after treatment;\nand there exists a never-treated control group.\n\nAs before, let the time-invariant dummy variable, \\(D_i\\), denote the treatment-group status of unit \\(i\\). Next, let the time-varying dummy variable, \\(T_t=\\mathbf{1}\\{t\\geq t_0\\}\\), be \\(=1\\) in the period of treatment (\\(t_0\\)) and \\(0\\) before.\nWe will make the following exclusion restrictions,\n\nDefinition 5 - Exclusion Restriction: No Anticipation (no pre-emptive behaviour) \\[\nY_{it} = Y_{it}(1) = Y_it(0)   \\qquad\\forall\\; (i,t)\\;\\text{s.t. }\\;t&lt; t_0\n\\]\n\nSome texts will assume random/unexpected timing of the treatment, as a mechanism that rules out anticipation. Note, this assumption is stronger than what is required for identification of the ATT. For example, Wooldridge (2023) assumes the a weaker assumption:\n\\[\nE[Y_{it}(1)|D_i=1,T_t=0] = E[Y_{it}(0)|D_i=1,T_t=0]\n\\]\n\nDefinition 6 - Exclusion Restriction: No Spillovers \\[\nY_{it}=Y_{it}(0)   \\qquad \\forall \\;(i,t) \\;\\text{s.t.}\\;D_i=0\n\\]\n\nEvidently, this assumption is met by SUTVA.\nThen, we can write the observed \\(Y_{it}\\) as, \\[\n    \\begin{aligned}\n        Y_{it} =& \\begin{cases}\n             Y_{it}(0) \\qquad\\forall\\quad t&lt;t_0 \\\\\n            Y_{it}(0) + D_i\\cdot(Y_{it}(1)-Y_{it}(0))  \\qquad\\forall\\quad t\\geq t_0\n        \\end{cases} \\\\\n        =&Y_{it}(0) + T_t\\cdot D_i\\cdot(Y_{it}(1)-Y_{it}(0))\n    \\end{aligned}\n\\]\n\n\n6.2 Parallel trends\nThe main identifying assumption of a DiD model is parallel trends. This can be stated as,\n\nDefinition 7 - Parallel Trends \\[\n\\begin{aligned}\n                &E[Y_{it}(0)|D_i=0,T_t=1]-E[Y_{it}(0)|D_i=0,T_t=0] \\\\\n                =&E[Y_{it}(0)|D_i=1,T_t=1]-E[Y_{it}(0)|D_i=1,T_t=0]\n            \\end{aligned}\n\\]\n\nUnder these assumptions, the difference-in-differences (DiD) of conditional means gives you the ATT.\n\\[\n\\begin{aligned}\n            &\\big[E[Y_{it}|D_i=1,T_t=1]-E[Y_{it}|D_i=1,T_t=0]\\big]\\\\\n            &-\\big[E[Y_{it}|D_i=0,T_t=1]-E[Y_{it}|D_i=0,T_t=0]\\big] \\\\\n            =&\\big[E[Y_{it}(1)|D_i=1,T_t=1]-E[Y_{it}(0)|D_i=1,T_t=0]\\big] \\\\\n            &-\\big[E[Y_{it}(0)|D_i=0,T_t=1]-E[Y_{it}(0)|D_i=0,T_t=0]\\big] \\\\\n            =&\\big[E[Y_{it}(1)|D_i=1,T_t=1]-E[Y_{it}(0)|D_i=1,T_t=1]\\big] \\\\\n            &+\\big[E[Y_{it}(0)|D_i=1,T_t=1]-E[Y_{it}(0)|D_i=1,T_t=0]\\big] \\\\\n            &-\\big[E[Y_{it}(0)|D_i=0,T_t=1]-E[Y_{it}(0)|D_i=0,T_t=0]\\big] \\\\\n            =&E[Y_{it}(1)-Y_{it}(0)|D_i=1,T_t=1]\\\\\n            =&ATT(t_0)\n        \\end{aligned}\n\\] Where, parallel trends implies that the difference between the last two parentheses in line 3 is 0. Note, I have denoted this as the ATT in period \\(t_0\\) as the treatment effect may not be static.\n\n\n6.3 Regression\nThis DiD estimand can be expressed within a linear regression model,\n\\[\nY_{it} = \\alpha + \\psi D_i + \\delta T_t + \\beta D_i\\times T_t + \\varepsilon_i\n\\] where \\(\\beta\\), the parameter on the interaction term is given by the above DiD.\nIn the above model, the \\(E[\\varepsilon_i|D_i,T_t]=0\\), since the model is fully saturated. However, this does not mean that the \\(\\beta=ATT\\), it just means that \\(\\beta\\) gives you the DiD of conditional means and the OLS estimator, \\(\\hat{\\beta}^{DD}\\), is an unbiased estimator. The fact that \\(\\beta=ATT\\) depends on the parallel trends assumption, stated in terms of the potential outcome \\(Y_{it}(0)\\). This is a perfect example of how identification of (population) parameters within a regression model is different to identification of TEs.\nGiven this linear regression model, some texts will state the parallel trends in parametric form as:\n\nDefinition 8 - Parallel Trends (parametric version) \\[\n                E[Y_{it}(0)|D_i=0,T_t=1]=\\alpha + \\psi D_i + \\delta T_t\n\\]\n\nThe two definitions are equivalent, if you consider the definition of the parameters in the above expression.\nThe above specification does not require longitudinal data; however, if it is available you can then estimate the Fixed Effects model:\n\\[\nY_{it} = \\alpha_i + \\delta T_t + \\beta D_i\\times T_t + \\varepsilon_i\n\\] Controlling for unit-FEs tends to yield a more efficient estimator, as the unit-level dummies explain more of the variation in \\(Y\\). Moreover, with a balanced panel, you can show:\n\\[\n\\hat{\\beta}^{LSDV} = \\hat{\\beta}^{DD}\n\\] The result arises from the fact that the regressors in the simple DiD model provide the same projection of the interaction term as the FE model which replaces the constant and \\(D\\)-dummy with unit-specific dummy variables.\n\n\n6.4 2-group-multi-period\nWe can extend the above set-up to include multiple periods, before and after the treatment. We do then need to add an assumption that the treatment is absorbing (or ‘always-on’). We can also now allow for dynamic TEs.\nWith multiple periods, the above specification (including regressors \\([1,D,T,D\\times T]\\)) is referred to as a ‘static’ specification. This is because it estimates a single ATT for all post-treatment periods; as would be appropriate if the TE is indeed static.\nIt is preferable to estimate one of the following dynamic specifications:\n\nsemi-dynamic specification \\[\nY_{it} = \\psi D_i + \\delta_t + \\sum_{j\\geq t_0} \\beta_j D_i\\times \\mathbf{1}\\{t=j\\} + \\varepsilon_i\n\\] or with panel data, \\[\nY_{it} = \\alpha_i + \\delta_t + \\sum_{j\\geq t_0} \\beta_j D_i\\times \\mathbf{1}\\{t=j\\} + \\upsilon_i\n\\]\nfully-dynamic specification \\[\nY_{it} = \\psi D_i + \\delta_t + \\sum_{j\\neq t_0-k} \\beta_j D_i\\times \\mathbf{1}\\{t=j\\} + \\varepsilon_i\n\\] or with panel data, \\[\nY_{it} = \\alpha_i + \\delta_t + \\sum_{j\\neq t_0-k} \\beta_j D_i\\times \\mathbf{1}\\{t=j\\} + \\upsilon_i\n\\]\n\nfor a chosen base period (in the pre-period): \\(k\\geq 1\\).\nThe fully-dynamic specification is preferred for the following reason. The pre-treatment \\(\\beta_j\\) coefficients (\\(j&lt;t_0\\)), provide a valid test for parallel trends in the pre-period. This can be used to support the assumption of parallel trends in the post-period. Note, this is NOT a test of parallel trends in the post-period when it needs to hold.\nSuppose \\(k=1\\), then the base period is \\(t_0-1\\). Assuming no anticipation, we can then write \\(\\beta_{t_0-2}\\) as,\n\\[\n\\begin{aligned}\n            \\beta_{t_0-2} =& \\big[E[Y_{it}|D_i=1,t=t_0-2]-E[Y_{it}|D_i=1,t=t_0-1]\\big]\\\\\n            &-\\big[E[Y_{it}|D_i=0,t=t_0-2]-E[Y_{it}|D_i=0,t=t_0-1]\\big] \\\\\n            =&\\big[E[Y_{it}(0)|D_i=1,t=t_0-2]-E[Y_{it}(0)|D_i=1,t=t_0-1]\\big] \\\\\n            &-\\big[E[Y_{it}(0)|D_i=0,t=t_0-2]-E[Y_{it}(0)|D_i=0,t=t_0-1]\\big]\n        \\end{aligned}\n\\] The test, \\(H_0:\\beta_{t_0-2}=0\\) is a valid test for parallel trends between period \\(t_0-2\\) and \\(t_0-1\\) (assuming no anticipation).\nSuppose \\(k=2\\), then the base period is \\(t_0-2\\). Assuming parallel trends, we can then write \\(\\beta_{t_0-1}\\) as,\n\\[\n\\begin{aligned}\n            \\beta_{t_0-1} =& \\big[E[Y_{it}|D_i=1,t=t_0-1]-E[Y_{it}|D_i=1,t=t_0-2]\\big]\\\\\n            &-\\big[E[Y_{it}|D_i=0,t=t_0-1]-E[Y_{it}|D_i=0,t=t_0-2]\\big] \\\\\n            =&\\big[E[Y_{it}(1)-Y_{it}(0)|D_i=1,t=t_0-1] \\\\\n            =&ATT(t_0-1)\n        \\end{aligned}\n\\] The test, \\(H_0:\\beta_{t_0-1}=0\\) is a valid test for no anticipation in period \\(t_0-1\\) (assuming parallel trends). Crucially, in both instances, we must assume one assumption to test the other. That is, you cannot disentangle a pre-emptive behaviour from a failure of parallel trends in the data.\n\n\n6.5 Further reading\nThere is a large body of literature that discusses a range of topics related to DiD models. Two topics you should take note of when applying these methods are: the appropriate estimation of SEs, and heterogeneity in staggered DiDs (also referred to as event-studies). I have provided a few citations below.\nTake a look at the following texts concerning clustered SEs within DiD models:\n\nWooldridge (2003)\nBertrand, Duflo, and Mullainathan (2004)\nAbadie et al. (2023)\n\nTake a look at the following texts to learn more about staggered DiD (event-study) models:\n\nImai and Kim (2019)\nClément De Chaisemartin and D’Haultfœuille (2020); Clement De Chaisemartin and D’Haultfœuille (2023); and Clément De Chaisemartin and D’Haultfœuille (2023)\nSun and Abraham (2021) (see also Stata package eventstudyweights and eventstudyinteract)\nCallaway and Sant’Anna (2021) (see also Stata package csdid and drdid)\nGoodman-Bacon (2021)\nAthey and Imbens (2022)\nBorusyak, Jaravel, and Spiess (2024)\n\nTake a look at the following texts to learn more about parallel trends for non-linear models:\n\nWooldridge (2023)\n\nTake a look at the following texts to learn more about conditional parallel trends:\n\nCaetano and Callaway (2024)"
  },
  {
    "objectID": "handout-8.html#footnotes",
    "href": "handout-8.html#footnotes",
    "title": "Causal Inference",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHere, I follow the notation of Guido W. Imbens. You should be comfortable with alternative notations, such as superscripts (\\(Y^1_{i},Y^0_{i}\\)) and subscripts (\\(Y_{i1},Y_{i0}\\); as in MM and MHE).↩︎\nThis statement can be generalized to continuous treatment, but it will require some assumptions regarding the causal relationship.↩︎\nHere, I am using super-population notation. This means that each \\(i\\) is thought of as an independent draw from a potentially infinite super-population. For finite sample analysis, the vectors of potential outcomes are treated as non-random. It is only the assignment vector, \\(D\\), that is random. You would therefore define unconfoundedness as independence across the full vectors of potential outcomes: \\(Y(1),Y(0)\\perp D |X\\).↩︎\nThis is equivalent to including the control function: \\(Y = \\beta_1 + \\beta_2 Z + \\beta_3M_ZD + \\varepsilon\\).↩︎"
  },
  {
    "objectID": "material-cef.html",
    "href": "material-cef.html",
    "title": "Conditional Expectation Function",
    "section": "",
    "text": "Consider the random variable \\(Y_i\\in\\mathbb{R}\\) and the random vector \\(X_i\\in\\mathbb{R}^k\\), \\(k\\geq1\\).1\n\n\nThe Conditional Expecation Function (CEF) - denoted \\(E[Y_i|X_i]\\) - is a random function. It is a function that returns the expected value of \\(Y_i\\) for each realized value of \\(X_i\\). Since \\(X_i\\) is a random vector the resulting function is random itself.\nIf we fix \\(X_i=x\\), then the value at which we are evaluating the function is no longer random. The result is a constant: the expected value of \\(Y_i\\) at the given \\(x\\).\n\\[\nE[Y_i|X_i=x] = \\int y \\cdot f_{Y}(y|X_i=x)dy = \\int y dF_{Y}(y|X_i=x)\n\\]\nThis follows the same logic that the expectation of a random variable is, \\(E[Y_i]\\), is not random.\nDiscrete case. The book devotes a lot time to the discussion of cases were \\(X_i\\) is a discrete random variable; using the notation \\(W_i\\in\\{0,1\\}\\) or \\(D_i\\in\\{0,1\\}\\). In this unique case, we can write the CEF as,\n\\[\nE[Y_i|D_i] = E[Y_i|D_i=0] + D_i\\cdot\\big(E[Y_i|D_i=1]-E[Y_i|D_i=0]\\big)\n\\]\nThe above function returns \\(E[Y_i|D_i=0]\\) when \\(D_i=0\\) and \\(E[Y_i|D_i=1]\\) when \\(D_i=1\\). This expression for the CEF will be useful in latter chapters of the book.\n\n\n\nThe Law of Iterated Expectations says that given two random variables2 \\([Y_i,X_i]\\), we can express the unconditional expected value of \\(Y_i\\) as the expected value of the conditional expectation of \\(Y_i\\) on \\(X_i\\).\n\\[\n        E[Y_i] = E\\big[E[Y_i|X_i]\\big]\n\\]\nWhere the outside expectation is with respect to \\(X_i\\),3 since the CEF is a random function of \\(X_i\\). We can expand this as follows,\n\\[\nE[Y_i] = \\int t \\cdot f_{Y_i}(t)dt = \\int\\int y \\cdot f_{Y_i|X}(y|x)dyf_X(x)dx = E\\big[E[Y_i|X_i]\\big]\n\\]\n\nExample 1 Suppose \\(Y_i\\) and \\(X_i\\) are both discrete, \\(Y_i\\in\\{1,2\\}\\) and \\(X_i\\in\\{3,4\\}\\), with the joint distribution:\n\n\\(f_{Y,X}\\)\n\n\n\n\\(X_i=3\\)\n\\(X_i=2\\)\n\n\n\\(Y_i=1\\)\n1/10\n3/10\n\n\n\\(Y_i=2\\)\n2/10\n4/10\n\n\n\nWe can then define the two marginal distributions,\n\n\\(f_Y\\)\n\n\n\\(Y_i=1\\)\n\\(Y_i=2\\)\n\n\n4/10\n6/10\n\n\n\nand,\n\n\\(f_X\\)\n\n\n\\(X_i=3\\)\n\\(X_i=4\\)\n\n\n3/10\n7/10\n\n\n\nLikewise, we know the conditional distribution \\(f_{Y|X}\\); which we get by dividing the joint distribution by the marginal distribution of \\(X_i\\). Each column of the conditional distribution should add up to 1.\n\n\\(f_{Y|X}\\)\n\n\n\n\\(X_i=3\\)\n\\(X_i=4\\)\n\n\n\\(Y_i=1\\)\n1/3\n3/7\n\n\n\\(Y_i=2\\)\n2/3\n4/7\n\n\n\nNow we can calculate the following objects:\n\n\\(E[Y_i]\\)\n\n\\[\n\\begin{aligned}\n        E[Y_i] =& 1\\cdot Pr(Y_i=1)+2\\cdot Pr(Y_i=2) \\\\\n        =&1\\cdot 4/10+2\\cdot 6/10 \\\\\n        =&16/10\n    \\end{aligned}\n\\]\n\n\\(E[Y_i|X_i=3]\\)\n\n\\[\n\\begin{aligned}\n        E[Y_i|X_i=3] =& 1\\cdot Pr(Y_i=1|X_i=3)+2\\cdot Pr(Y_i=2|X_i=3) \\\\\n        =&1\\cdot 1/3+2\\cdot 2/3 \\\\\n        =&5/3\n\\end{aligned}\n\\]\n\n\\(E[Y_i|X_i=4]\\)\n\n\\[\n\\begin{aligned}\n        E[Y_i|X_i=4] =& 1\\cdot Pr(Y_i=1|X_i=4)+2\\cdot Pr(Y_i=2|X_i=4) \\\\\n        =&1\\cdot 3/7+2\\cdot 4/7 \\\\\n        =&11/7\n    \\end{aligned}\n\\]\n\n\\(E\\big[E[Y_i|X_i]\\big]\\)\n\n\\[\n\\begin{aligned}\n        E\\big[E[Y_i|X_i]\\big] =& E[Y_i|X_i=3]\\cdot Pr(X_i=3)+ E[Y_i|X_i=4]\\cdot Pr(X_i=4) \\\\\n        =&5/3\\cdot3/10+11/7\\cdot 7/10 \\\\\n        =&16/10\n    \\end{aligned}\n\\]\nWe have therefore demonstrated the law of iterated expectations.\n\nWe can extend this principle to conditional expectations. Suppose you have three random variables/vectors \\(\\{Y_i,X_i,Z_i\\}\\), we can express the conditional expected value of \\(Y_i\\) on \\(X_i\\) as the (conditional) expected value of the conditional expectation of \\(Y_i\\) on \\(X_i\\) and \\(Z_i\\).\n\\[\n        E[Y_i|X_i] = E\\big[E[Y_i|X_i,Z_i]|X_i\\big]\n\\]\nHere the outside expectation is with respect \\(Z_i\\) conditional on \\(X_i\\). It utilizes the conditional distribution \\(f_{Z|X}\\) to form the outside expectation,\n\\[\nE[Y_i|X_i] = \\int y \\cdot f_{Y|X}(y|X_i)dt = \\int\\int y \\cdot f_{Y|X,Z}(y|X_i,z)dyf_{Z|X}(z|X_i)dz = E\\big[E[Y_i|X_i,Z_i]|X_i\\big]\n\\]\n\n\n\nThe following three theorems can be found in a range of Econometrics textbooks and Microeconometrics texts, including MM & MHE\n\nTheorem 1 We can express the observed outcome \\(Y_i\\) as a sum of \\(E[Y_i|X_i]+\\varepsilon_i\\) where \\(E[\\varepsilon_i|X_i]=0\\) (i.e., mean independent).\n\n\nProof. \n\n\\(E[\\varepsilon_i | X_i] = E[Y_i - E[Y_i | X_i] | X_i] = E[Y_i | X_i] - E[Y_i | X_i] = 0\\)\n\\(E[h(X_i)\\varepsilon_i] = E[h(X_i)E[\\varepsilon_i | X_i]] = E[h(X_i) \\times 0] = 0\\)\n\n\n\nTheorem 2 \\(E[Y_i|X_i]\\) is the best predictor of \\(Y_i\\).\n\n\nProof. \\[\n\\begin{aligned}\n(Y_i - m(X_i))^2 =& \\left((Y_i - E[Y_i | X_i]) + (E[Y_i | X_i] - m(X_i))\\right)^2 \\\\\n=& (Y_i - E[Y_i \\| X_i])^2 + (E[Y_i | X_i] - m(X_i))^2 \\\\&+ 2(Y_i - E[Y_i | X_i]) \\times (E[Y_i | X_i] - m(X_i))\n\\end{aligned}\n\\]\nThe last term (cross product) is mean zero. Thus, the function is minimized by setting \\(m(X_i) = E[Y_i | X_i]\\).\n\n\nTheorem 3 [ANOVA Theorem] The variance of \\(Y_i\\) can be decomposed as \\(V(E[Y_i|X_i])+E(V(Y_i|X_i))\\)\n\n\nProof. \\[\n\\begin{aligned}\n        V(Y_i)=&V(E[Y_i|X_i] + \\varepsilon_i) \\\\\n        =&V(E[Y_i|X_i])+V(\\varepsilon_i) \\\\\n        =&V(E[Y_i|X_i])+E[\\varepsilon_i^2]\n    \\end{aligned}\n\\] The second line follows from Theorem 1.1 (independence) and\n\\[\n        E[\\varepsilon_i^2]=E\\left[E[\\varepsilon_i^2|X_i]\\right]=E\\left[V(Y_i|X_i)\\right]\n\\]"
  },
  {
    "objectID": "material-cef.html#definition",
    "href": "material-cef.html#definition",
    "title": "Conditional Expectation Function",
    "section": "",
    "text": "The Conditional Expecation Function (CEF) - denoted \\(E[Y_i|X_i]\\) - is a random function. It is a function that returns the expected value of \\(Y_i\\) for each realized value of \\(X_i\\). Since \\(X_i\\) is a random vector the resulting function is random itself.\nIf we fix \\(X_i=x\\), then the value at which we are evaluating the function is no longer random. The result is a constant: the expected value of \\(Y_i\\) at the given \\(x\\).\n\\[\nE[Y_i|X_i=x] = \\int y \\cdot f_{Y}(y|X_i=x)dy = \\int y dF_{Y}(y|X_i=x)\n\\]\nThis follows the same logic that the expectation of a random variable is, \\(E[Y_i]\\), is not random.\nDiscrete case. The book devotes a lot time to the discussion of cases were \\(X_i\\) is a discrete random variable; using the notation \\(W_i\\in\\{0,1\\}\\) or \\(D_i\\in\\{0,1\\}\\). In this unique case, we can write the CEF as,\n\\[\nE[Y_i|D_i] = E[Y_i|D_i=0] + D_i\\cdot\\big(E[Y_i|D_i=1]-E[Y_i|D_i=0]\\big)\n\\]\nThe above function returns \\(E[Y_i|D_i=0]\\) when \\(D_i=0\\) and \\(E[Y_i|D_i=1]\\) when \\(D_i=1\\). This expression for the CEF will be useful in latter chapters of the book."
  },
  {
    "objectID": "material-cef.html#law-of-iterated-expectations",
    "href": "material-cef.html#law-of-iterated-expectations",
    "title": "Conditional Expectation Function",
    "section": "",
    "text": "The Law of Iterated Expectations says that given two random variables2 \\([Y_i,X_i]\\), we can express the unconditional expected value of \\(Y_i\\) as the expected value of the conditional expectation of \\(Y_i\\) on \\(X_i\\).\n\\[\n        E[Y_i] = E\\big[E[Y_i|X_i]\\big]\n\\]\nWhere the outside expectation is with respect to \\(X_i\\),3 since the CEF is a random function of \\(X_i\\). We can expand this as follows,\n\\[\nE[Y_i] = \\int t \\cdot f_{Y_i}(t)dt = \\int\\int y \\cdot f_{Y_i|X}(y|x)dyf_X(x)dx = E\\big[E[Y_i|X_i]\\big]\n\\]\n\nExample 1 Suppose \\(Y_i\\) and \\(X_i\\) are both discrete, \\(Y_i\\in\\{1,2\\}\\) and \\(X_i\\in\\{3,4\\}\\), with the joint distribution:\n\n\\(f_{Y,X}\\)\n\n\n\n\\(X_i=3\\)\n\\(X_i=2\\)\n\n\n\\(Y_i=1\\)\n1/10\n3/10\n\n\n\\(Y_i=2\\)\n2/10\n4/10\n\n\n\nWe can then define the two marginal distributions,\n\n\\(f_Y\\)\n\n\n\\(Y_i=1\\)\n\\(Y_i=2\\)\n\n\n4/10\n6/10\n\n\n\nand,\n\n\\(f_X\\)\n\n\n\\(X_i=3\\)\n\\(X_i=4\\)\n\n\n3/10\n7/10\n\n\n\nLikewise, we know the conditional distribution \\(f_{Y|X}\\); which we get by dividing the joint distribution by the marginal distribution of \\(X_i\\). Each column of the conditional distribution should add up to 1.\n\n\\(f_{Y|X}\\)\n\n\n\n\\(X_i=3\\)\n\\(X_i=4\\)\n\n\n\\(Y_i=1\\)\n1/3\n3/7\n\n\n\\(Y_i=2\\)\n2/3\n4/7\n\n\n\nNow we can calculate the following objects:\n\n\\(E[Y_i]\\)\n\n\\[\n\\begin{aligned}\n        E[Y_i] =& 1\\cdot Pr(Y_i=1)+2\\cdot Pr(Y_i=2) \\\\\n        =&1\\cdot 4/10+2\\cdot 6/10 \\\\\n        =&16/10\n    \\end{aligned}\n\\]\n\n\\(E[Y_i|X_i=3]\\)\n\n\\[\n\\begin{aligned}\n        E[Y_i|X_i=3] =& 1\\cdot Pr(Y_i=1|X_i=3)+2\\cdot Pr(Y_i=2|X_i=3) \\\\\n        =&1\\cdot 1/3+2\\cdot 2/3 \\\\\n        =&5/3\n\\end{aligned}\n\\]\n\n\\(E[Y_i|X_i=4]\\)\n\n\\[\n\\begin{aligned}\n        E[Y_i|X_i=4] =& 1\\cdot Pr(Y_i=1|X_i=4)+2\\cdot Pr(Y_i=2|X_i=4) \\\\\n        =&1\\cdot 3/7+2\\cdot 4/7 \\\\\n        =&11/7\n    \\end{aligned}\n\\]\n\n\\(E\\big[E[Y_i|X_i]\\big]\\)\n\n\\[\n\\begin{aligned}\n        E\\big[E[Y_i|X_i]\\big] =& E[Y_i|X_i=3]\\cdot Pr(X_i=3)+ E[Y_i|X_i=4]\\cdot Pr(X_i=4) \\\\\n        =&5/3\\cdot3/10+11/7\\cdot 7/10 \\\\\n        =&16/10\n    \\end{aligned}\n\\]\nWe have therefore demonstrated the law of iterated expectations.\n\nWe can extend this principle to conditional expectations. Suppose you have three random variables/vectors \\(\\{Y_i,X_i,Z_i\\}\\), we can express the conditional expected value of \\(Y_i\\) on \\(X_i\\) as the (conditional) expected value of the conditional expectation of \\(Y_i\\) on \\(X_i\\) and \\(Z_i\\).\n\\[\n        E[Y_i|X_i] = E\\big[E[Y_i|X_i,Z_i]|X_i\\big]\n\\]\nHere the outside expectation is with respect \\(Z_i\\) conditional on \\(X_i\\). It utilizes the conditional distribution \\(f_{Z|X}\\) to form the outside expectation,\n\\[\nE[Y_i|X_i] = \\int y \\cdot f_{Y|X}(y|X_i)dt = \\int\\int y \\cdot f_{Y|X,Z}(y|X_i,z)dyf_{Z|X}(z|X_i)dz = E\\big[E[Y_i|X_i,Z_i]|X_i\\big]\n\\]"
  },
  {
    "objectID": "material-cef.html#properties-of-the-cef",
    "href": "material-cef.html#properties-of-the-cef",
    "title": "Conditional Expectation Function",
    "section": "",
    "text": "The following three theorems can be found in a range of Econometrics textbooks and Microeconometrics texts, including MM & MHE\n\nTheorem 1 We can express the observed outcome \\(Y_i\\) as a sum of \\(E[Y_i|X_i]+\\varepsilon_i\\) where \\(E[\\varepsilon_i|X_i]=0\\) (i.e., mean independent).\n\n\nProof. \n\n\\(E[\\varepsilon_i | X_i] = E[Y_i - E[Y_i | X_i] | X_i] = E[Y_i | X_i] - E[Y_i | X_i] = 0\\)\n\\(E[h(X_i)\\varepsilon_i] = E[h(X_i)E[\\varepsilon_i | X_i]] = E[h(X_i) \\times 0] = 0\\)\n\n\n\nTheorem 2 \\(E[Y_i|X_i]\\) is the best predictor of \\(Y_i\\).\n\n\nProof. \\[\n\\begin{aligned}\n(Y_i - m(X_i))^2 =& \\left((Y_i - E[Y_i | X_i]) + (E[Y_i | X_i] - m(X_i))\\right)^2 \\\\\n=& (Y_i - E[Y_i \\| X_i])^2 + (E[Y_i | X_i] - m(X_i))^2 \\\\&+ 2(Y_i - E[Y_i | X_i]) \\times (E[Y_i | X_i] - m(X_i))\n\\end{aligned}\n\\]\nThe last term (cross product) is mean zero. Thus, the function is minimized by setting \\(m(X_i) = E[Y_i | X_i]\\).\n\n\nTheorem 3 [ANOVA Theorem] The variance of \\(Y_i\\) can be decomposed as \\(V(E[Y_i|X_i])+E(V(Y_i|X_i))\\)\n\n\nProof. \\[\n\\begin{aligned}\n        V(Y_i)=&V(E[Y_i|X_i] + \\varepsilon_i) \\\\\n        =&V(E[Y_i|X_i])+V(\\varepsilon_i) \\\\\n        =&V(E[Y_i|X_i])+E[\\varepsilon_i^2]\n    \\end{aligned}\n\\] The second line follows from Theorem 1.1 (independence) and\n\\[\n        E[\\varepsilon_i^2]=E\\left[E[\\varepsilon_i^2|X_i]\\right]=E\\left[V(Y_i|X_i)\\right]\n\\]"
  },
  {
    "objectID": "material-cef.html#footnotes",
    "href": "material-cef.html#footnotes",
    "title": "Conditional Expectation Function",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe subscript \\(i\\) is not necessary here. However, this notation is consistent with the rest of the book. In this book, \\(Y_i\\) denotes a random variable, \\(\\in \\mathbb{R}\\), and \\(Y\\) a random vector, \\(\\in \\mathbb{R}^n\\). Likewise, \\(X_i\\) is a random vector, \\(\\in \\mathbb{R}^k\\), while \\(X\\) will represent a random matrix, \\(\\in \\mathbb{R}^n \\times \\mathbb{R}^k\\).↩︎\nThis can be extended to random vectors.↩︎\nSome texts use the notation \\(E_X\\big[E[Y_i|X_i]\\big]\\) to demonstrate that the outside expectation is with respect to \\(X_i\\).↩︎"
  },
  {
    "objectID": "material-inference.html",
    "href": "material-inference.html",
    "title": "Statistical Inference",
    "section": "",
    "text": "This note reviews material that should be familiar from undergraduate texts. However, we will borrow the notation from Handouts 1-4 to make it relevant to this module.\nThe note reviews the following topics:\n\nconfidence intervals;\nsimple hypothesis tests;\np-values;\nstatistical power.\n\nIt does not cover (multiple) linear hypotheses, which is the subject of Handout 4.\n\n\n\nConsider the model, \\[\n  Y = \\beta_1X_1 + X_2\\beta_2 +u\n\\] where \\(\\beta_1\\) is a scalar. We know that under CLRM 1-6,1\n\\[\n  \\hat{\\beta}_1 |X\\sim N(\\beta_1,\\sigma^2/(X_1'M_2X_1))\n\\] We can write \\(\\sigma^2(X_1'M_2X_1)^{-1}\\) as a fraction in this way, since \\(\\hat{\\beta}_1\\) is a scalar.\nLet us define,\n\\[\n\\begin{aligned}\nV =& \\sigma^2/(X_1'M_2X_1) \\\\\n\\text{and}\\quad\\hat{V} =& s^2/(X_1'M_2X_1)\n\\end{aligned}\n\\] Where the difference is that in \\(\\hat{V}\\) includes, \\(s^2\\), the estimator for \\(\\sigma^2\\). Thus,\n\\[\n\\begin{aligned}\n\\frac{\\hat{\\beta}_1-\\beta_1}{\\sqrt{V}} & \\sim N(0,1) \\\\\n\\text{and}\\quad\\frac{\\hat{\\beta}_1-\\beta_1}{\\sqrt{\\hat{V}}} & \\sim T_{n-k}\n\\end{aligned}\n\\]\n\n\n\nHere we are going to descibe the the confidence interval (CI) for a single estimator. You can expand this to the vector case, but must then consider the joint distribution of the estimators.\nGiven the unknown population parameter, \\(\\beta_1\\), we want to define the CI such that,\n\\[\nPr(\\beta_1 \\in CI_{1-\\alpha}|X) = 1-\\alpha\n\\] We know that \\((\\hat{\\beta}_1-\\beta_1)/\\sqrt{V} \\sim N(0,1)\\). Therefore, using the percentiles of the standard normal distribution, we can say that,\n\\[\n\\begin{aligned}\n1-\\alpha =& Pr\\bigg(z_{\\alpha/2}\\leq \\frac{\\hat{\\beta}_1-\\beta_1}{\\sqrt{V}} \\leq z_{1-\\alpha/2}\\bigg) \\\\\n=& Pr\\big(z_{\\alpha/2}\\sqrt{V}\\leq \\hat{\\beta}_1-\\beta_1 \\leq z_{1-\\alpha/2}\\sqrt{V}\\big) \\\\\n=& Pr\\big(z_{\\alpha/2}\\sqrt{V}-\\hat{\\beta}_1\\leq -\\beta_1 \\leq z_{1-\\alpha/2}\\sqrt{V}-\\hat{\\beta}_1\\big) \\\\\n=& Pr\\big(\\hat{\\beta}_1-z_{1-\\alpha/2}\\sqrt{V}\\leq \\beta_1 \\leq \\hat{\\beta}_1-z_{\\alpha/2}\\sqrt{V}\\big)\n\\end{aligned}\n\\] Pay careful attention to the switch from lines 3 to 4. The multiplying of the inequalities by -1 switches the upper and lower thresholds.\nIn this case, we can use the symmetry of the normal distribution. Since the normal distribution is symmetric, \\(-z_{\\alpha/2} =  z_{1-\\alpha/2}\\). This gives us the symmetric CI:\n\\[\nCI_{1-\\alpha}= \\big[\\hat{\\beta}_1-z_{1-\\alpha/2}\\sqrt{V},\\hat{\\beta}_1+z_{1-\\alpha/2}\\sqrt{V}\\big]\n\\] However, not all distributions are symmetric. For example, can you solve the confidence interval of \\(\\sigma^2\\), using the estimator \\(s^2\\)?\nSince, the t-distribution is also symmetric, we can define the a very similar CI using \\(\\hat{V}\\):\n\\[\nCI_{1-\\alpha}= \\big[\\hat{\\beta}_1-t_{n-k,1-\\alpha/2}\\sqrt{V},\\hat{\\beta}_1+t_{n-k,1-\\alpha/2}\\sqrt{V}\\big]\n\\] where \\(t_{n-k,1-\\alpha/2}\\) is the \\(1-\\alpha/2\\) percentile of the t-distribution (with \\(n-k\\) dof).\n\n\n\nWe will consider both two-sided and one-sided hypothesis tests. The former are actually simpler than the latter, since they (typically) involve sharp null hypotheses.\n\n\nConsider the two-sided hypothesis:\n\\[\n\\begin{aligned}\n  &H_0: \\beta_1 = r \\\\\n  \\text{against}\\quad &H_1: \\beta_1 \\neq r\n\\end{aligned}\n\\] Here, \\(r\\) is just a constant (i.e. non-random scalar). For example, \\(r=0\\) is a simple test for whether \\(\\beta_1\\) is statistically significant.\nA test requires a rejection rule that determines when you reject \\(H_0\\) based on the value of the test-statistic. Consider, the T-statistic:\n\\[\n  \\text{T-stat} = \\frac{\\hat{\\beta}_1-r}{\\sqrt{\\hat{V}}}\n\\] Under \\(H_0: \\beta_1=r\\), implying that the T-statistic has a t-distribution. Note, it has this distribution only under \\(H_0\\). The statistic is a R.V. with infinite support. It has the continuous distribution over the real line, meaning that with a non-zero probability it can take on any value. If this is the case, any realization of the statistic (in by implication the estimator \\(\\hat{\\beta}_1\\)) is consistent with \\(H_0\\).\nThe goal is to control the probability of type 1 errors: the probability of rejecting \\(H_0\\) when it is true. We do so, be choosing a significance level \\(\\alpha\\) which will determine the size of the test.2\nConsider the rejection rule:\n\nReject \\(H_0\\) if \\(\\text{T-stat}\\leq t_{n-k,\\alpha/2}\\) or \\(\\text{T-stat}\\geq t_{n-k,1-\\alpha/2}\\).\n\nThe probability of a type 1 error is:\n\\[\n\\begin{aligned}\nPr(\\text{Reject}\\;H_0|\\beta_1 = r,X) =& Pr(\\text{T-stat}\\leq t_{n-k,\\alpha/2}|\\beta_1 = r,X) \\\\\n&+Pr(\\text{T-stat}\\geq t_{n-k,1-\\alpha/2}|\\beta_1 = r,X) \\\\\n=&\\alpha/2+\\alpha/2 \\\\\n=&\\alpha\n\\end{aligned}\n\\] This test has size \\(\\alpha\\). Moreover, given the symmetry of the t-distribution, we can write the rejection rule as:\n\nReject \\(H_0\\) if \\(|\\text{T-stat}|\\geq t_{n-k,1-\\alpha/2}\\).\n\n\n\n\nConsider the one-sided hypothesis:\n\\[\n\\begin{aligned}\n  &H_0: \\beta_1 \\leq r \\\\\n  \\text{against}\\quad &H_1: \\beta_1 &gt; r\n\\end{aligned}\n\\] The null hypothesis is no longer sharp. Any value of \\(\\leq\\) satisifies the null. It therefore becomes more difficult to think about the size of the test.\nConsider the rejection rule:\n\nReject \\(H_0\\) if \\(\\text{T-stat}\\geq t_{n-k,1-\\alpha}\\).\n\nWe reject \\(H_0\\) is the value of the test statistic is much greater than the \\(1-\\alpha\\) percentile of the t-distribution. Does this test have the right size?\n\\[\n\\begin{aligned}\nPr(\\text{Reject}\\;H_0|\\beta_1 \\leq r,X) =& Pr(\\text{T-stat}\\geq t_{n-k,1-\\alpha}|\\beta_1 \\leq r,X) \\\\\n\\leq&Pr(\\text{T-stat}\\geq t_{n-k,1-\\alpha}|\\beta_1 = r,X) \\\\\n=&\\alpha\n\\end{aligned}\n\\] The inequality appears, because for values of \\(\\beta_1&lt;r\\), the probability of rejection is strictly \\(&lt;\\alpha\\). This is not a problem, so long as for the threshold case \\(\\beta_1 = r\\), the probability is still \\(\\leq\\alpha\\).\n\n\n\n\nThe p-value of a test corresponds to the smallest significance level (i.e. \\(\\alpha\\)) at which you reject \\(H_0\\). It is an incredibly useful value to compute, because it can be used a rejection rule:\n\nReject \\(H_0\\) if p-value \\(\\leq \\alpha\\).\n\nThis provides you with a rejection rule that is independent of the distribution of the test-statistic. The critical values of a test-statistic will depend on the distribution, but the p-value not.\nIn the above two-sided test, the p-value is given by\n\\[\n\\text{p-value} = 2\\times(1-T_{n-k}(|\\text{T-stat}|))\n\\] where \\(T_{n-k}(\\cdot)\\) is the CDF of the t-distribution (with \\(n-k\\) dof).\nFor the one-sided test we considered, it would be:\n\\[\n\\text{p-value} = 1-T_{n-k}(\\text{T-stat})\n\\]\nWhile the p-value is computed using the CDF of the test-statistic, and is a value \\(\\in[0,1]\\), it is not a probability. However, since it is a function of the test-statistic, it is a random variable.\nOne interesting feature of the p-value is that it has a uniform distribution. Consider, the probability that the p-value is less than some value \\(\\rho\\). We will use the one-sided test to simplify things.\n\\[\n\\begin{aligned}\nPr(\\text{p-value}\\leq\\rho|X,\\beta_1=r) =& Pr(1-T_{n-k}(\\text{T-stat})\\leq\\rho|X,\\beta_1=r) \\\\\n=&Pr(T_{n-k}(\\text{T-stat})\\geq 1-\\rho|X,\\beta_1 = r) \\\\\n=&Pr(\\text{T-stat}\\geq T_{n-k}^{-1}(1-\\rho)|X,\\beta_1 = r) \\\\\n=&1-T_{n-k}\\big(T_{n-k}^{-1}(1-\\rho)\\big) \\\\\n=& 1-(1-\\rho) \\\\\n=&\\rho\n\\end{aligned}\n\\] Thus, the p-value has the characteristic of a uniformly distributed random variable: for \\(X\\sim U(0,1)\\Rightarrow Pr(X\\leq x) = x\\).\nThis fact is used to evaluate p-hacking and publication bias in published research. By collecting and plotting the distribution of p-values from published research, you can test how much the distribution varies from uniform.\n\n\n\nStastical power refers to the probability of rejecting \\(H_0\\) for a given value of the unknown parameter, which need not correspond to the value under the null hypothesis. Consider the sharp null from the two-sided test: \\(H_0:\\beta_1 = r\\). Suppose, the true value of \\(\\beta_1\\) is some other value \\(\\kappa\\).\n\\[\nPr(\\text{Reject}\\;H_0|\\beta_1 = \\kappa,X) =Pr(|\\text{T-stat}|\\geq t_{n-k,1-\\alpha/2}|\\beta_1 = \\kappa,X)\n\\] The T-statistic has a t-distribution only under the null hypothesis (\\(H_0\\) is true). If \\(\\beta_1=\\kappa\\neq r\\), then this probability is not \\(\\alpha\\). \\[\n\\begin{aligned}\nPr(\\text{Reject}\\;H_0|\\beta_1 = \\kappa,X) =& Pr\\bigg(\\frac{\\hat{\\beta}_1-r}{\\sqrt{\\hat{V}}}\\leq t_{n-k,\\alpha/2}\\bigg|\\beta_1 = \\kappa,X\\bigg) \\\\\n&+Pr\\bigg(\\frac{\\hat{\\beta}_1-r}{\\sqrt{\\hat{V}}}\\geq t_{n-k,1-\\alpha/2}\\bigg|\\beta_1 = \\kappa,X\\bigg) \\\\\n=& Pr\\bigg(\\frac{\\hat{\\beta}_1-\\kappa}{\\sqrt{\\hat{V}}}+\\frac{\\kappa-r}{\\sqrt{\\hat{V}}}\\leq t_{n-k,\\alpha/2}\\bigg|\\beta_1 = \\kappa,X\\bigg) \\\\\n&+Pr\\bigg(\\frac{\\hat{\\beta}_1-\\kappa}{\\sqrt{\\hat{V}}}+\\frac{\\kappa-r}{\\sqrt{\\hat{V}}}\\geq t_{n-k,1-\\alpha/2}\\bigg|\\beta_1 = \\kappa,X\\bigg) \\\\\n=& Pr\\bigg(\\frac{\\hat{\\beta}_1-\\kappa}{\\sqrt{\\hat{V}}}\\leq t_{n-k,\\alpha/2}-\\frac{\\kappa-r}{\\sqrt{\\hat{V}}}\\bigg|\\beta_1 = \\kappa,X\\bigg) \\\\\n&+Pr\\bigg(\\frac{\\hat{\\beta}_1-\\kappa}{\\sqrt{\\hat{V}}}\\geq t_{n-k,1-\\alpha/2}-\\frac{\\kappa-r}{\\sqrt{\\hat{V}}}\\bigg|\\beta_1 = \\kappa,X\\bigg)\n\\end{aligned}\n\\] Under the condition that \\(\\beta_1 = \\kappa\\), \\[\n\\frac{\\hat{\\beta}_1-\\kappa}{\\sqrt{\\hat{V}}}\\sim T_{n-k}\n\\]\n\\[\n\\begin{aligned}\nPr(\\text{Reject}\\;H_0|\\beta_1 = \\kappa,X) =& Pr\\bigg(T\\leq t_{n-k,\\alpha/2}-\\frac{\\kappa-r}{\\sqrt{\\hat{V}}}\\bigg|\\beta_1 = \\kappa,X\\bigg) \\\\\n&+Pr\\bigg(T\\geq t_{n-k,1-\\alpha/2}-\\frac{\\kappa-r}{\\sqrt{\\hat{V}}}\\bigg|\\beta_1 = \\kappa,X\\bigg) \\\\\n&\\geq \\alpha\n\\end{aligned}\n\\]\nWe know that this probability is at least as large as \\(\\alpha\\), the probability of a type 1 error. As a function of \\(\\kappa\\), this probability will tend towards 1, as \\(\\kappa\\) moves further from the null hypothesis. For a two-sided test, it does so symmetrically, creating a bell-shaped function in the case of the normal and t-distributions. Note, as \\(|\\kappa-r|\\) increases, the one probability increases, while the other decreases. Regardless, the power increases because the one probability increases faster than the other decreases. This is a result of the bell-shaped t-distribution.\nFor a one-sided test, the power function will asymptote to 1 only on the side of rejection. We say that a test is uniformly more powerful, if has greater statistical power for all possible values of the parameter.\nWhen comparing tests that have the same distribution, the difference in power will arise from the variance of the estimator \\(\\hat{V}\\). A more efficient estimator, will yield a more power test."
  },
  {
    "objectID": "material-inference.html#overview",
    "href": "material-inference.html#overview",
    "title": "Statistical Inference",
    "section": "",
    "text": "This note reviews material that should be familiar from undergraduate texts. However, we will borrow the notation from Handouts 1-4 to make it relevant to this module.\nThe note reviews the following topics:\n\nconfidence intervals;\nsimple hypothesis tests;\np-values;\nstatistical power.\n\nIt does not cover (multiple) linear hypotheses, which is the subject of Handout 4."
  },
  {
    "objectID": "material-inference.html#set-up",
    "href": "material-inference.html#set-up",
    "title": "Statistical Inference",
    "section": "",
    "text": "Consider the model, \\[\n  Y = \\beta_1X_1 + X_2\\beta_2 +u\n\\] where \\(\\beta_1\\) is a scalar. We know that under CLRM 1-6,1\n\\[\n  \\hat{\\beta}_1 |X\\sim N(\\beta_1,\\sigma^2/(X_1'M_2X_1))\n\\] We can write \\(\\sigma^2(X_1'M_2X_1)^{-1}\\) as a fraction in this way, since \\(\\hat{\\beta}_1\\) is a scalar.\nLet us define,\n\\[\n\\begin{aligned}\nV =& \\sigma^2/(X_1'M_2X_1) \\\\\n\\text{and}\\quad\\hat{V} =& s^2/(X_1'M_2X_1)\n\\end{aligned}\n\\] Where the difference is that in \\(\\hat{V}\\) includes, \\(s^2\\), the estimator for \\(\\sigma^2\\). Thus,\n\\[\n\\begin{aligned}\n\\frac{\\hat{\\beta}_1-\\beta_1}{\\sqrt{V}} & \\sim N(0,1) \\\\\n\\text{and}\\quad\\frac{\\hat{\\beta}_1-\\beta_1}{\\sqrt{\\hat{V}}} & \\sim T_{n-k}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "material-inference.html#confidence-interval",
    "href": "material-inference.html#confidence-interval",
    "title": "Statistical Inference",
    "section": "",
    "text": "Here we are going to descibe the the confidence interval (CI) for a single estimator. You can expand this to the vector case, but must then consider the joint distribution of the estimators.\nGiven the unknown population parameter, \\(\\beta_1\\), we want to define the CI such that,\n\\[\nPr(\\beta_1 \\in CI_{1-\\alpha}|X) = 1-\\alpha\n\\] We know that \\((\\hat{\\beta}_1-\\beta_1)/\\sqrt{V} \\sim N(0,1)\\). Therefore, using the percentiles of the standard normal distribution, we can say that,\n\\[\n\\begin{aligned}\n1-\\alpha =& Pr\\bigg(z_{\\alpha/2}\\leq \\frac{\\hat{\\beta}_1-\\beta_1}{\\sqrt{V}} \\leq z_{1-\\alpha/2}\\bigg) \\\\\n=& Pr\\big(z_{\\alpha/2}\\sqrt{V}\\leq \\hat{\\beta}_1-\\beta_1 \\leq z_{1-\\alpha/2}\\sqrt{V}\\big) \\\\\n=& Pr\\big(z_{\\alpha/2}\\sqrt{V}-\\hat{\\beta}_1\\leq -\\beta_1 \\leq z_{1-\\alpha/2}\\sqrt{V}-\\hat{\\beta}_1\\big) \\\\\n=& Pr\\big(\\hat{\\beta}_1-z_{1-\\alpha/2}\\sqrt{V}\\leq \\beta_1 \\leq \\hat{\\beta}_1-z_{\\alpha/2}\\sqrt{V}\\big)\n\\end{aligned}\n\\] Pay careful attention to the switch from lines 3 to 4. The multiplying of the inequalities by -1 switches the upper and lower thresholds.\nIn this case, we can use the symmetry of the normal distribution. Since the normal distribution is symmetric, \\(-z_{\\alpha/2} =  z_{1-\\alpha/2}\\). This gives us the symmetric CI:\n\\[\nCI_{1-\\alpha}= \\big[\\hat{\\beta}_1-z_{1-\\alpha/2}\\sqrt{V},\\hat{\\beta}_1+z_{1-\\alpha/2}\\sqrt{V}\\big]\n\\] However, not all distributions are symmetric. For example, can you solve the confidence interval of \\(\\sigma^2\\), using the estimator \\(s^2\\)?\nSince, the t-distribution is also symmetric, we can define the a very similar CI using \\(\\hat{V}\\):\n\\[\nCI_{1-\\alpha}= \\big[\\hat{\\beta}_1-t_{n-k,1-\\alpha/2}\\sqrt{V},\\hat{\\beta}_1+t_{n-k,1-\\alpha/2}\\sqrt{V}\\big]\n\\] where \\(t_{n-k,1-\\alpha/2}\\) is the \\(1-\\alpha/2\\) percentile of the t-distribution (with \\(n-k\\) dof)."
  },
  {
    "objectID": "material-inference.html#hypothesis-tests",
    "href": "material-inference.html#hypothesis-tests",
    "title": "Statistical Inference",
    "section": "",
    "text": "We will consider both two-sided and one-sided hypothesis tests. The former are actually simpler than the latter, since they (typically) involve sharp null hypotheses.\n\n\nConsider the two-sided hypothesis:\n\\[\n\\begin{aligned}\n  &H_0: \\beta_1 = r \\\\\n  \\text{against}\\quad &H_1: \\beta_1 \\neq r\n\\end{aligned}\n\\] Here, \\(r\\) is just a constant (i.e. non-random scalar). For example, \\(r=0\\) is a simple test for whether \\(\\beta_1\\) is statistically significant.\nA test requires a rejection rule that determines when you reject \\(H_0\\) based on the value of the test-statistic. Consider, the T-statistic:\n\\[\n  \\text{T-stat} = \\frac{\\hat{\\beta}_1-r}{\\sqrt{\\hat{V}}}\n\\] Under \\(H_0: \\beta_1=r\\), implying that the T-statistic has a t-distribution. Note, it has this distribution only under \\(H_0\\). The statistic is a R.V. with infinite support. It has the continuous distribution over the real line, meaning that with a non-zero probability it can take on any value. If this is the case, any realization of the statistic (in by implication the estimator \\(\\hat{\\beta}_1\\)) is consistent with \\(H_0\\).\nThe goal is to control the probability of type 1 errors: the probability of rejecting \\(H_0\\) when it is true. We do so, be choosing a significance level \\(\\alpha\\) which will determine the size of the test.2\nConsider the rejection rule:\n\nReject \\(H_0\\) if \\(\\text{T-stat}\\leq t_{n-k,\\alpha/2}\\) or \\(\\text{T-stat}\\geq t_{n-k,1-\\alpha/2}\\).\n\nThe probability of a type 1 error is:\n\\[\n\\begin{aligned}\nPr(\\text{Reject}\\;H_0|\\beta_1 = r,X) =& Pr(\\text{T-stat}\\leq t_{n-k,\\alpha/2}|\\beta_1 = r,X) \\\\\n&+Pr(\\text{T-stat}\\geq t_{n-k,1-\\alpha/2}|\\beta_1 = r,X) \\\\\n=&\\alpha/2+\\alpha/2 \\\\\n=&\\alpha\n\\end{aligned}\n\\] This test has size \\(\\alpha\\). Moreover, given the symmetry of the t-distribution, we can write the rejection rule as:\n\nReject \\(H_0\\) if \\(|\\text{T-stat}|\\geq t_{n-k,1-\\alpha/2}\\).\n\n\n\n\nConsider the one-sided hypothesis:\n\\[\n\\begin{aligned}\n  &H_0: \\beta_1 \\leq r \\\\\n  \\text{against}\\quad &H_1: \\beta_1 &gt; r\n\\end{aligned}\n\\] The null hypothesis is no longer sharp. Any value of \\(\\leq\\) satisifies the null. It therefore becomes more difficult to think about the size of the test.\nConsider the rejection rule:\n\nReject \\(H_0\\) if \\(\\text{T-stat}\\geq t_{n-k,1-\\alpha}\\).\n\nWe reject \\(H_0\\) is the value of the test statistic is much greater than the \\(1-\\alpha\\) percentile of the t-distribution. Does this test have the right size?\n\\[\n\\begin{aligned}\nPr(\\text{Reject}\\;H_0|\\beta_1 \\leq r,X) =& Pr(\\text{T-stat}\\geq t_{n-k,1-\\alpha}|\\beta_1 \\leq r,X) \\\\\n\\leq&Pr(\\text{T-stat}\\geq t_{n-k,1-\\alpha}|\\beta_1 = r,X) \\\\\n=&\\alpha\n\\end{aligned}\n\\] The inequality appears, because for values of \\(\\beta_1&lt;r\\), the probability of rejection is strictly \\(&lt;\\alpha\\). This is not a problem, so long as for the threshold case \\(\\beta_1 = r\\), the probability is still \\(\\leq\\alpha\\)."
  },
  {
    "objectID": "material-inference.html#p-values",
    "href": "material-inference.html#p-values",
    "title": "Statistical Inference",
    "section": "",
    "text": "The p-value of a test corresponds to the smallest significance level (i.e. \\(\\alpha\\)) at which you reject \\(H_0\\). It is an incredibly useful value to compute, because it can be used a rejection rule:\n\nReject \\(H_0\\) if p-value \\(\\leq \\alpha\\).\n\nThis provides you with a rejection rule that is independent of the distribution of the test-statistic. The critical values of a test-statistic will depend on the distribution, but the p-value not.\nIn the above two-sided test, the p-value is given by\n\\[\n\\text{p-value} = 2\\times(1-T_{n-k}(|\\text{T-stat}|))\n\\] where \\(T_{n-k}(\\cdot)\\) is the CDF of the t-distribution (with \\(n-k\\) dof).\nFor the one-sided test we considered, it would be:\n\\[\n\\text{p-value} = 1-T_{n-k}(\\text{T-stat})\n\\]\nWhile the p-value is computed using the CDF of the test-statistic, and is a value \\(\\in[0,1]\\), it is not a probability. However, since it is a function of the test-statistic, it is a random variable.\nOne interesting feature of the p-value is that it has a uniform distribution. Consider, the probability that the p-value is less than some value \\(\\rho\\). We will use the one-sided test to simplify things.\n\\[\n\\begin{aligned}\nPr(\\text{p-value}\\leq\\rho|X,\\beta_1=r) =& Pr(1-T_{n-k}(\\text{T-stat})\\leq\\rho|X,\\beta_1=r) \\\\\n=&Pr(T_{n-k}(\\text{T-stat})\\geq 1-\\rho|X,\\beta_1 = r) \\\\\n=&Pr(\\text{T-stat}\\geq T_{n-k}^{-1}(1-\\rho)|X,\\beta_1 = r) \\\\\n=&1-T_{n-k}\\big(T_{n-k}^{-1}(1-\\rho)\\big) \\\\\n=& 1-(1-\\rho) \\\\\n=&\\rho\n\\end{aligned}\n\\] Thus, the p-value has the characteristic of a uniformly distributed random variable: for \\(X\\sim U(0,1)\\Rightarrow Pr(X\\leq x) = x\\).\nThis fact is used to evaluate p-hacking and publication bias in published research. By collecting and plotting the distribution of p-values from published research, you can test how much the distribution varies from uniform."
  },
  {
    "objectID": "material-inference.html#statistical-power",
    "href": "material-inference.html#statistical-power",
    "title": "Statistical Inference",
    "section": "",
    "text": "Stastical power refers to the probability of rejecting \\(H_0\\) for a given value of the unknown parameter, which need not correspond to the value under the null hypothesis. Consider the sharp null from the two-sided test: \\(H_0:\\beta_1 = r\\). Suppose, the true value of \\(\\beta_1\\) is some other value \\(\\kappa\\).\n\\[\nPr(\\text{Reject}\\;H_0|\\beta_1 = \\kappa,X) =Pr(|\\text{T-stat}|\\geq t_{n-k,1-\\alpha/2}|\\beta_1 = \\kappa,X)\n\\] The T-statistic has a t-distribution only under the null hypothesis (\\(H_0\\) is true). If \\(\\beta_1=\\kappa\\neq r\\), then this probability is not \\(\\alpha\\). \\[\n\\begin{aligned}\nPr(\\text{Reject}\\;H_0|\\beta_1 = \\kappa,X) =& Pr\\bigg(\\frac{\\hat{\\beta}_1-r}{\\sqrt{\\hat{V}}}\\leq t_{n-k,\\alpha/2}\\bigg|\\beta_1 = \\kappa,X\\bigg) \\\\\n&+Pr\\bigg(\\frac{\\hat{\\beta}_1-r}{\\sqrt{\\hat{V}}}\\geq t_{n-k,1-\\alpha/2}\\bigg|\\beta_1 = \\kappa,X\\bigg) \\\\\n=& Pr\\bigg(\\frac{\\hat{\\beta}_1-\\kappa}{\\sqrt{\\hat{V}}}+\\frac{\\kappa-r}{\\sqrt{\\hat{V}}}\\leq t_{n-k,\\alpha/2}\\bigg|\\beta_1 = \\kappa,X\\bigg) \\\\\n&+Pr\\bigg(\\frac{\\hat{\\beta}_1-\\kappa}{\\sqrt{\\hat{V}}}+\\frac{\\kappa-r}{\\sqrt{\\hat{V}}}\\geq t_{n-k,1-\\alpha/2}\\bigg|\\beta_1 = \\kappa,X\\bigg) \\\\\n=& Pr\\bigg(\\frac{\\hat{\\beta}_1-\\kappa}{\\sqrt{\\hat{V}}}\\leq t_{n-k,\\alpha/2}-\\frac{\\kappa-r}{\\sqrt{\\hat{V}}}\\bigg|\\beta_1 = \\kappa,X\\bigg) \\\\\n&+Pr\\bigg(\\frac{\\hat{\\beta}_1-\\kappa}{\\sqrt{\\hat{V}}}\\geq t_{n-k,1-\\alpha/2}-\\frac{\\kappa-r}{\\sqrt{\\hat{V}}}\\bigg|\\beta_1 = \\kappa,X\\bigg)\n\\end{aligned}\n\\] Under the condition that \\(\\beta_1 = \\kappa\\), \\[\n\\frac{\\hat{\\beta}_1-\\kappa}{\\sqrt{\\hat{V}}}\\sim T_{n-k}\n\\]\n\\[\n\\begin{aligned}\nPr(\\text{Reject}\\;H_0|\\beta_1 = \\kappa,X) =& Pr\\bigg(T\\leq t_{n-k,\\alpha/2}-\\frac{\\kappa-r}{\\sqrt{\\hat{V}}}\\bigg|\\beta_1 = \\kappa,X\\bigg) \\\\\n&+Pr\\bigg(T\\geq t_{n-k,1-\\alpha/2}-\\frac{\\kappa-r}{\\sqrt{\\hat{V}}}\\bigg|\\beta_1 = \\kappa,X\\bigg) \\\\\n&\\geq \\alpha\n\\end{aligned}\n\\]\nWe know that this probability is at least as large as \\(\\alpha\\), the probability of a type 1 error. As a function of \\(\\kappa\\), this probability will tend towards 1, as \\(\\kappa\\) moves further from the null hypothesis. For a two-sided test, it does so symmetrically, creating a bell-shaped function in the case of the normal and t-distributions. Note, as \\(|\\kappa-r|\\) increases, the one probability increases, while the other decreases. Regardless, the power increases because the one probability increases faster than the other decreases. This is a result of the bell-shaped t-distribution.\nFor a one-sided test, the power function will asymptote to 1 only on the side of rejection. We say that a test is uniformly more powerful, if has greater statistical power for all possible values of the parameter.\nWhen comparing tests that have the same distribution, the difference in power will arise from the variance of the estimator \\(\\hat{V}\\). A more efficient estimator, will yield a more power test."
  },
  {
    "objectID": "material-inference.html#footnotes",
    "href": "material-inference.html#footnotes",
    "title": "Statistical Inference",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn Handout 4 we showed that \\(\\beta_1\\) can be written as \\(c'\\beta\\) where \\(c = \\begin{bmatrix}1 & 0 & \\cdots &0\\end{bmatrix}'\\). Therefore, it must be that \\((X_1'M_2X_1)^{-1} = c'(X'X)^{-1}c\\)↩︎\nWe say that a test has size \\(\\alpha\\) if the probability of a type 1 error is \\(\\leq \\alpha\\).↩︎"
  },
  {
    "objectID": "material-linearalgebra.html",
    "href": "material-linearalgebra.html",
    "title": "Linear Algebra",
    "section": "",
    "text": "Consider a set of \\(k\\) \\(n\\)-dimensional vectors \\(\\{X_{1},X_{2},...,X_{k}\\}\\). These vectors are,\n\nDefinition 1 linearly dependent if there exists a set of scalars \\(\\{a_{1},a_{2},\\cdots,a_{k}\\}\\) such that\n\\[\na_1X_1 + a_2X_2+\\ldots+a_kX_k=0\n\\]\nwhere at least one \\(a_i\\neq0\\).\n\nAlternatively, they are,\n\nDefinition 2 linearly independent if the only set of scalars \\(\\{a_{1},a_{2},\\cdots,a_{k}\\}\\) that satisfies the above condition is \\(a_1,a_2,\\cdots,a_k=0\\).\n\nIf we collect these \\(k\\) column-vectors in a matrix, \\(X=[X_1\\;X_2 \\cdots X_k]\\), then the linear dependence condition can be written as,\n\\[\na_1X_1 + a_2X_2+\\ldots+a_kX_k=\\begin{bmatrix} X_1\\;X_2 \\cdots X_k\\end{bmatrix}\\begin{bmatrix}a_1\\\\a_2\\\\\\vdots\\\\a_k\\end{bmatrix}=Xa = 0\n\\]\nGiven any \\(n\\times k\\) matrix \\(X\\), its columns are,\n\nDefinition 3 linearly dependent if there exists a vector \\(a\\in\\mathbb{R}^k\\) such that \\(a\\neq0\\) and \\(Xa=0\\);\n\nor,\n\nDefinition 4 linearly independent if the only vector \\(a\\in\\mathbb{R}^k\\) such that \\(Xa=0\\) is \\(a=0\\).\n\nFor any matrix there may be more than one vector \\(a\\in\\mathbb{R}^{k}\\) such that \\(Xa=0\\). Indeed, if both \\(a_{1},a_{2}\\in\\mathbb{R}^{k}\\)\nsatisfy this condition and \\(a_{1}\\neq a_{2}\\) then you can show that any linear combination of \\(\\{a_{1},a_{2}\\}\\) satisfies the\ncondition \\(X(a_{1}b_{1}+a_{2}b_{2})=0\\) for \\(b_{1},b_{2}\\in\\mathbb{R}\\). Thus, there exists an entire set of vectors which satisfy this condition. This set is referred to as the,\n\nDefinition 5 null space of \\(X\\), \\[\n\\mathcal{N}(X) = \\{a\\in\\mathbb{R}^k:\\;Xa=0\\}\n\\]\n\nIt should be evident from the definition that if the columns of \\(X\\) are linearly independent then \\(\\mathcal{N}(X)=\\{0\\}\\), a singleton. That is, it just includes the 0-vector.\n\n\n\nHere, we concern ourselves only with real vectors from \\(\\mathbb{R}^n\\).\n\nDefinition 6 A vector space, denoted \\(\\mathcal{V}\\), refers to a set of vectors which is closed under finite addition and scalar multiplication.\n\n\nDefinition 7 A set of \\(k\\) linearly independent vectors, \\(\\{X_1,X_2,\\cdots,X_k\\}\\), forms a basis for vector space \\(\\mathcal{V}\\) if \\(\\forall\\;y\\in\\mathcal{V}\\) there exists a set of \\(k\\) scalars such that, \\[\ny=X_1b_1+X_2b_2+\\ldots+X_kb_k\n\\]\n\nBased on these definitions, it is evident that for the Euclidean space, \\(\\mathbb{E}^n\\), any \\(n\\) linearly independent vectors from \\(\\mathbb{R}^n\\) is a basis. For example, any point in \\(\\mathbb{E}^2\\) can be defined as a multiple of,\n\\[\n\\begin{bmatrix}1\\\\0\\end{bmatrix}\\quad \\text{and} \\quad\\begin{bmatrix}0\\\\1\\end{bmatrix}\n\\]\nConsider again the \\(n\\times k\\) matrix \\(X\\), where \\(k&lt;n\\). Then we define the,\n\nDefinition 8 column space (or span) of \\(X\\), denoted \\(\\mathcal{S}(X)\\), as the vector space generate by the \\(k\\) columns of \\(X\\). Formally, \\[\n\\mathcal{S}(X) = \\{y\\in\\mathbb{R}^n:\\;y=Xb\\quad\\text{for some }b\\in \\mathbb{R}^k\\}\n\\]\n\nA property to note about the span or column space \\(X\\) is,\nResult: \\(\\mathcal{S}(X)=\\mathcal{S}(XX')\\) :::\nwhere \\(XX'\\) is a \\(n\\times n\\) matrix.\nFinally, we can define the,\n\nDefinition 9 orthogonal column space (or orthogonal span) of \\(X\\) as, \\[\n\\mathcal{S}^{\\perp}(X) = \\{y\\in \\mathbb{R}^k:\\;y'x=0\\quad \\forall x\\in\\mathcal{S}(X)\\}\n\\]\n\n\n\n\nConsider a \\(n\\times k\\) matrix \\(X\\), the\n\nDefinition 10 row rank of \\(X\\) is the maximum number of linearly independent rows: \\[\nrowrank(X) \\leq n\n\\]\n\nWe say that matrix \\(X\\) has full row rank if \\(rowrank(X)=n\\).\nThe,\n\nDefinition 11 column rank of \\(X\\) is the maximum number of linearly independent columns:\n\\[\ncolrank(X) \\leq k\n\\]\n\nWe say that matrix \\(X\\) has full column rank if \\(colrank(X)=k\\).\nAn important result is,\n\nResult: the rank of \\(X\\): \\[\nr(X) = rowrank(X)=colrank(X) \\\\\n\\Rightarrow r(X)\\leq min\\{n,k\\}\n\\]\n\nIn addition, since the \\(r(X)\\) depends on the number of linearly independent columns, we can say that,\n\nResult: the dimension of \\(\\mathcal{S}(X)\\), \\(dim(\\mathcal{S}(X))\\), is given by the \\(r(X)\\).\n\nHere are a few additional results,\n\nResult: \\(r(X)=r(X')\\)\nResult: \\(r(XY)\\leq min\\{r(X),r(Y)\\}\\)\nResult: \\(r(XY)=r(X)\\) if \\(Y\\) is square and full rank\nResult: \\(r(X+Y)\\leq r(X) + r(Y)\\)\n\n\n\n\nConsider the case of a square, \\(n\\times n\\), matrix \\(A\\). We say that,\n\nDefinition 12 \\(A\\) is singular if the \\(r(A)&lt;n\\),\n\nor that,\n\nDefinition 13 \\(A\\) is non-singular if the \\(r(A)=n\\).\n\nThe singularity of a square matrix is important as it determines the invertibility of a matrix, which typically relates the existence of a unique solution in systems of linear equations. Here are a few key results,\n\nResult: There exists a matrix \\(B=A^{-1}\\), such that \\(AB=I_n\\) (where \\(I_n\\) is the identity matrix), if and only if \\(A\\) is non-singular.\nResult: \\(A\\) is non-singular if and only if the determinant of \\(A\\) is non-zero: \\(det(A)\\neq0\\).1\nResult: Likewise, \\(A\\) is singular if and only if \\(det(A)=0\\).\nResult: \\(AA^{-1}=A^{-1}A=I\\)\nResult: \\((A')^{-1}=(A^{-1})'\\)\nResult: If their respective inverses exist, then \\((AB)^{-1}=B^{-1}A^{-1}\\).\nResult: \\(det(AB)=det(A)det(B)\\)\nResult: \\(det(A^{-1})=det(A)^{-1}\\)\n\nFor any square matrix \\(A\\),\n\nDefinition 14 the trace of \\(A\\) is the sum of all diagonal elements: \\[\ntr(A) = \\sum_{i=1}^na_{ii}\n\\]\n\nRegarding the trace of a square matrix, here are a few important results:\n\nResult: \\(tr(A+B) = tr(A) + tr(B)\\)\nResult: \\(tr(\\lambda A) = \\lambda tr(A)\\) where \\(\\lambda\\) is a scalar\nResult: \\(tr(A) = tr(A')\\)\nResult: \\(tr(AB) = tr(BA)\\) where \\(AB\\) and \\(BA\\) are both square, but need not be of the same order.\nResult: \\(||A|| = (tr(A'A))^{1/2}\\)\n\n\n\n\nA symmetric matrix has the property that \\(A=A'\\). Therefore, \\(A\\) must be square.\nHere are a few important results concerning symmetric matrices.\n\nResult: \\(A^{-1}\\) exists if \\(det(A)\\neq 0\\) and \\(r(A)=n\\)\nResult: A is diagonalizable.2\nResult: The eigenvector decomposition of a square matrix gives you \\(A=C\\Lambda C^{-1}\\) where \\(\\Lambda\\) is a diagonal matrix of eigenvalues and $C$ a matrix of the corresponding eigenvectors. The symmetry of \\(A\\) gives you that \\(C^{-1}=C'\\Rightarrow A=C\\Lambda C'\\) with \\(C'C=CC'=I_{n}\\).3\n\nA key definition concerning symmetric matrices is their positive definiteness:\n\nDefinition 15 \\(A\\) is positive semi-definite if for any \\(x\\in\\mathbb{R}^n,\\;x'Ax\\geq0\\).\n\nGiven the eigenvector decomposition of a symmetric matrix, positive semi-definiteness implies \\(\\Lambda\\) is positive semi-definite: \\(\\lambda_i\\geq0\\quad\\forall i\\). Likewise,\n\nDefinition 16 \\(A\\) is positive definite if for any \\(x\\in\\mathbb{R}^n,\\;x'Ax&gt;0\\).\n\nAgain, based on the egeinvector decomposition, positive semi-definiteness implies \\(\\Lambda\\) is positive definite: \\(\\lambda_i&gt;0\\quad\\forall i\\).\nA few more results are:\n\nResult: \\(tr(A) = \\sum_{i=1}^n\\lambda_i\\)\nResult: \\(r(A) = r(\\Lambda)\\)\nResult: \\(det(A) = \\prod_{i=1}^n \\lambda_i\\)\n\nThis last result can be used to prove that any positive definite matrix is non-singular and therefore has an inverse.\nAny full-rank, positive semi-definite, symmetric matrix \\(B\\) has the additional properties:\n\nResult: \\(B=C\\Lambda C'\\) and \\(B^{-1} = C\\Lambda^{-1}C'\\)\nResult: We can define the square-root of \\(B\\) as \\(B^{1/2} = C\\Lambda^{1/2}C'\\). Similarly, \\(B^{-1/2} = C\\Lambda^{-1/2}C'\\).\n\n\n\n\nAn idempotent matrix has the property that \\(D=DD\\). Therefore, \\(D\\) must be square.\nHere are a few important results concerning idempotent matrices.\n\nResult: \\(D\\) is positive definite\nResult: \\(D\\) is diagonalizable\nResult: \\((I_n-D)\\) is also an idempotent matrix\nResult: With the exception of \\(I_n\\), all idempotent matrices are singular.\nResult: \\(r(D) = tr(D) = \\sum_{i=1}^n\\lambda_i\\)\nResult: \\(\\lambda_i\\in\\{0,1\\}\\quad \\forall\\;i\\)\n\nProjection matrices are idempotent, but need not be symmetric. However, for the purposes of this module we will deal exclusively with symmetric idempotent projection matrices.\n\n\n\nHere we will look at the derivatives of scalar with respect to (W.r.t.) a vector. You can also define other derivatives, such as the derivative of a vector w.r.t. a vector and the derivative of a scalar with respect to a matrix. However, these are not needed for these notes.\n\n\nSuppose \\(f(x)\\in R\\) (i.e. a scalar) and \\(x\\in R^n\\) (i.e. a \\(n\\times 1\\) vector). Then we can define the partial derivative of \\(f(x)\\) w.r.t. \\(x\\) as,\n\\[\n  \\frac{\\partial f(x)}{\\partial x}  = \\begin{bmatrix}\\frac{\\partial f(x)}{\\partial x_1} \\\\ \\frac{\\partial f(x)}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial f(x)}{\\partial x_n} \\end{bmatrix}\n\\]\n\n\n\nA special case is when \\(f(x)\\) is linear in \\(x\\),\n\\[\nf(x) = a'x = \\sum_{i=1}^n a_ix_i\n\\] for \\(a\\in R^n\\). The derivative of \\(a'x\\) with respect to the vector \\(x\\) can be defined as,\n\\[\n  \\begin{aligned}\n  \\frac{\\partial a'x}{\\partial x} & = \\begin{bmatrix}\\frac{\\partial a'x}{\\partial x_1} \\\\ \\frac{\\partial a'x}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial a'x}{\\partial x_n} \\end{bmatrix} \\\\\n  & = \\begin{bmatrix}a_1 \\\\ a_2 \\\\ \\vdots \\\\ a_n \\end{bmatrix} \\\\\n  & = a\n  \\end{aligned}\n\\] since the the partial derivate of \\(a'x = \\sum_{i=1}^n a_ix_i\\) w.r.t. \\(x_i\\) is just the scalar \\(a_i\\).\nThe derivative of a scalar w.r.t. to a vector yields a vector of partial derivatives.\nSince \\(a'x\\) is a scalar, it is by definition symmetric: \\(a'x = x'a\\). Thus,\n\\[\n\\frac{\\partial x'a}{\\partial x} = \\frac{\\partial a'x}{\\partial x} = a\n\\]\n\n\n\nSuppose \\(f(x)\\) is a linear transformation of \\(x\\),\n\\[\nf(x) = A'x\n\\] for any \\(m\\times n\\) matrix A,\n\\[\n  A = \\begin{bmatrix}a_1' \\\\ a_2' \\\\ \\vdots \\\\ a_m'\\end{bmatrix}\n\\] where \\(a_i\\in R^n\\;\\forall i=1,\\cdots,m\\) and,\n\\[\n  Ax = \\begin{bmatrix}a_1'x \\\\ a_2'x \\\\ \\vdots \\\\ a_m'x\\end{bmatrix}\n\\] Note, \\(f(x)=Ax\\in R^m\\), a \\(m\\times 1\\) vector. We can then define,\n\\[\n  \\begin{aligned}\n  \\frac{\\partial Ax}{\\partial x'} & = \\begin{bmatrix}\\frac{\\partial a_1'x}{\\partial x_1} & \\frac{\\partial a_1'x}{\\partial x_2} & \\cdots & \\frac{\\partial a_1'x}{\\partial x_n}\\\\ \\frac{\\partial a_2'x}{\\partial x_1} & \\frac{\\partial a_2'x}{\\partial x_2} & \\cdots & \\frac{\\partial a_2'x}{\\partial x_n}\\\\ \\vdots & \\vdots & \\ddots & \\vdots\\\\ \\frac{\\partial a_m'x}{\\partial x_1} & \\frac{\\partial a_m'x}{\\partial x_2} & \\cdots & \\frac{\\partial a_m'x}{\\partial x_n}\\\\ \\end{bmatrix} \\\\\n  & = \\begin{bmatrix} a_{11} & a_{12} & \\cdots & a_{1n} \\\\ a_{21} & a_{22} & \\cdots & a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots\\\\ a_{m1} & a_{m2} & \\cdots & a_{mn} \\\\ \\end{bmatrix} \\\\\n  & = A\n  \\end{aligned}\n\\]\nSince Ax is \\(m\\times 1\\) column vector, we take the derivative w.r.t. \\(x'\\) a row vector and not the column vector \\(x\\). This results in a matrix of partial derivatives.\n\n\n\nA second special case is where the function takes on the quadaratic form,\n\\[\nf(x) = x'Ax = \\sum_{i=1}^N\\sum_{j=1}^n a_{ij}x_ix_j\n\\]\nfor \\(n\\times n\\) (square) matrix A. As in the first linear case, \\(f(x)\\) is scalar.\nDefine \\(c = Ax\\), the \\(x'Ax = x'c\\). From the linear case, we know that,\n\\[\n\\frac{\\partial x'c}{\\partial x} = c\n\\]\nSimilarly, if we define \\(d = A'x\\) then \\(x'Ax = d'x\\). From the linear case, we know that,\n\\[\n\\frac{\\partial d'x}{\\partial x} = d\n\\]\nWe can define the total derivative as the sum of the partial derivatives w.r.t. to the first and second \\(x\\). Combining these two results, we have that,\n\\[\n\\frac{\\partial x'Ax}{\\partial x} = Ax + A'x\n\\]\nAnd if \\(A\\) is symmetric, this result simplifies to \\(2Ax\\)."
  },
  {
    "objectID": "material-linearalgebra.html#linear-dependence",
    "href": "material-linearalgebra.html#linear-dependence",
    "title": "Linear Algebra",
    "section": "",
    "text": "Consider a set of \\(k\\) \\(n\\)-dimensional vectors \\(\\{X_{1},X_{2},...,X_{k}\\}\\). These vectors are,\n\nDefinition 1 linearly dependent if there exists a set of scalars \\(\\{a_{1},a_{2},\\cdots,a_{k}\\}\\) such that\n\\[\na_1X_1 + a_2X_2+\\ldots+a_kX_k=0\n\\]\nwhere at least one \\(a_i\\neq0\\).\n\nAlternatively, they are,\n\nDefinition 2 linearly independent if the only set of scalars \\(\\{a_{1},a_{2},\\cdots,a_{k}\\}\\) that satisfies the above condition is \\(a_1,a_2,\\cdots,a_k=0\\).\n\nIf we collect these \\(k\\) column-vectors in a matrix, \\(X=[X_1\\;X_2 \\cdots X_k]\\), then the linear dependence condition can be written as,\n\\[\na_1X_1 + a_2X_2+\\ldots+a_kX_k=\\begin{bmatrix} X_1\\;X_2 \\cdots X_k\\end{bmatrix}\\begin{bmatrix}a_1\\\\a_2\\\\\\vdots\\\\a_k\\end{bmatrix}=Xa = 0\n\\]\nGiven any \\(n\\times k\\) matrix \\(X\\), its columns are,\n\nDefinition 3 linearly dependent if there exists a vector \\(a\\in\\mathbb{R}^k\\) such that \\(a\\neq0\\) and \\(Xa=0\\);\n\nor,\n\nDefinition 4 linearly independent if the only vector \\(a\\in\\mathbb{R}^k\\) such that \\(Xa=0\\) is \\(a=0\\).\n\nFor any matrix there may be more than one vector \\(a\\in\\mathbb{R}^{k}\\) such that \\(Xa=0\\). Indeed, if both \\(a_{1},a_{2}\\in\\mathbb{R}^{k}\\)\nsatisfy this condition and \\(a_{1}\\neq a_{2}\\) then you can show that any linear combination of \\(\\{a_{1},a_{2}\\}\\) satisfies the\ncondition \\(X(a_{1}b_{1}+a_{2}b_{2})=0\\) for \\(b_{1},b_{2}\\in\\mathbb{R}\\). Thus, there exists an entire set of vectors which satisfy this condition. This set is referred to as the,\n\nDefinition 5 null space of \\(X\\), \\[\n\\mathcal{N}(X) = \\{a\\in\\mathbb{R}^k:\\;Xa=0\\}\n\\]\n\nIt should be evident from the definition that if the columns of \\(X\\) are linearly independent then \\(\\mathcal{N}(X)=\\{0\\}\\), a singleton. That is, it just includes the 0-vector."
  },
  {
    "objectID": "material-linearalgebra.html#vector-spaces-bases-and-spans",
    "href": "material-linearalgebra.html#vector-spaces-bases-and-spans",
    "title": "Linear Algebra",
    "section": "",
    "text": "Here, we concern ourselves only with real vectors from \\(\\mathbb{R}^n\\).\n\nDefinition 6 A vector space, denoted \\(\\mathcal{V}\\), refers to a set of vectors which is closed under finite addition and scalar multiplication.\n\n\nDefinition 7 A set of \\(k\\) linearly independent vectors, \\(\\{X_1,X_2,\\cdots,X_k\\}\\), forms a basis for vector space \\(\\mathcal{V}\\) if \\(\\forall\\;y\\in\\mathcal{V}\\) there exists a set of \\(k\\) scalars such that, \\[\ny=X_1b_1+X_2b_2+\\ldots+X_kb_k\n\\]\n\nBased on these definitions, it is evident that for the Euclidean space, \\(\\mathbb{E}^n\\), any \\(n\\) linearly independent vectors from \\(\\mathbb{R}^n\\) is a basis. For example, any point in \\(\\mathbb{E}^2\\) can be defined as a multiple of,\n\\[\n\\begin{bmatrix}1\\\\0\\end{bmatrix}\\quad \\text{and} \\quad\\begin{bmatrix}0\\\\1\\end{bmatrix}\n\\]\nConsider again the \\(n\\times k\\) matrix \\(X\\), where \\(k&lt;n\\). Then we define the,\n\nDefinition 8 column space (or span) of \\(X\\), denoted \\(\\mathcal{S}(X)\\), as the vector space generate by the \\(k\\) columns of \\(X\\). Formally, \\[\n\\mathcal{S}(X) = \\{y\\in\\mathbb{R}^n:\\;y=Xb\\quad\\text{for some }b\\in \\mathbb{R}^k\\}\n\\]\n\nA property to note about the span or column space \\(X\\) is,\nResult: \\(\\mathcal{S}(X)=\\mathcal{S}(XX')\\) :::\nwhere \\(XX'\\) is a \\(n\\times n\\) matrix.\nFinally, we can define the,\n\nDefinition 9 orthogonal column space (or orthogonal span) of \\(X\\) as, \\[\n\\mathcal{S}^{\\perp}(X) = \\{y\\in \\mathbb{R}^k:\\;y'x=0\\quad \\forall x\\in\\mathcal{S}(X)\\}\n\\]"
  },
  {
    "objectID": "material-linearalgebra.html#rank",
    "href": "material-linearalgebra.html#rank",
    "title": "Linear Algebra",
    "section": "",
    "text": "Consider a \\(n\\times k\\) matrix \\(X\\), the\n\nDefinition 10 row rank of \\(X\\) is the maximum number of linearly independent rows: \\[\nrowrank(X) \\leq n\n\\]\n\nWe say that matrix \\(X\\) has full row rank if \\(rowrank(X)=n\\).\nThe,\n\nDefinition 11 column rank of \\(X\\) is the maximum number of linearly independent columns:\n\\[\ncolrank(X) \\leq k\n\\]\n\nWe say that matrix \\(X\\) has full column rank if \\(colrank(X)=k\\).\nAn important result is,\n\nResult: the rank of \\(X\\): \\[\nr(X) = rowrank(X)=colrank(X) \\\\\n\\Rightarrow r(X)\\leq min\\{n,k\\}\n\\]\n\nIn addition, since the \\(r(X)\\) depends on the number of linearly independent columns, we can say that,\n\nResult: the dimension of \\(\\mathcal{S}(X)\\), \\(dim(\\mathcal{S}(X))\\), is given by the \\(r(X)\\).\n\nHere are a few additional results,\n\nResult: \\(r(X)=r(X')\\)\nResult: \\(r(XY)\\leq min\\{r(X),r(Y)\\}\\)\nResult: \\(r(XY)=r(X)\\) if \\(Y\\) is square and full rank\nResult: \\(r(X+Y)\\leq r(X) + r(Y)\\)"
  },
  {
    "objectID": "material-linearalgebra.html#properties-of-square-matrices",
    "href": "material-linearalgebra.html#properties-of-square-matrices",
    "title": "Linear Algebra",
    "section": "",
    "text": "Consider the case of a square, \\(n\\times n\\), matrix \\(A\\). We say that,\n\nDefinition 12 \\(A\\) is singular if the \\(r(A)&lt;n\\),\n\nor that,\n\nDefinition 13 \\(A\\) is non-singular if the \\(r(A)=n\\).\n\nThe singularity of a square matrix is important as it determines the invertibility of a matrix, which typically relates the existence of a unique solution in systems of linear equations. Here are a few key results,\n\nResult: There exists a matrix \\(B=A^{-1}\\), such that \\(AB=I_n\\) (where \\(I_n\\) is the identity matrix), if and only if \\(A\\) is non-singular.\nResult: \\(A\\) is non-singular if and only if the determinant of \\(A\\) is non-zero: \\(det(A)\\neq0\\).1\nResult: Likewise, \\(A\\) is singular if and only if \\(det(A)=0\\).\nResult: \\(AA^{-1}=A^{-1}A=I\\)\nResult: \\((A')^{-1}=(A^{-1})'\\)\nResult: If their respective inverses exist, then \\((AB)^{-1}=B^{-1}A^{-1}\\).\nResult: \\(det(AB)=det(A)det(B)\\)\nResult: \\(det(A^{-1})=det(A)^{-1}\\)\n\nFor any square matrix \\(A\\),\n\nDefinition 14 the trace of \\(A\\) is the sum of all diagonal elements: \\[\ntr(A) = \\sum_{i=1}^na_{ii}\n\\]\n\nRegarding the trace of a square matrix, here are a few important results:\n\nResult: \\(tr(A+B) = tr(A) + tr(B)\\)\nResult: \\(tr(\\lambda A) = \\lambda tr(A)\\) where \\(\\lambda\\) is a scalar\nResult: \\(tr(A) = tr(A')\\)\nResult: \\(tr(AB) = tr(BA)\\) where \\(AB\\) and \\(BA\\) are both square, but need not be of the same order.\nResult: \\(||A|| = (tr(A'A))^{1/2}\\)"
  },
  {
    "objectID": "material-linearalgebra.html#properties-of-symmetric-matrices",
    "href": "material-linearalgebra.html#properties-of-symmetric-matrices",
    "title": "Linear Algebra",
    "section": "",
    "text": "A symmetric matrix has the property that \\(A=A'\\). Therefore, \\(A\\) must be square.\nHere are a few important results concerning symmetric matrices.\n\nResult: \\(A^{-1}\\) exists if \\(det(A)\\neq 0\\) and \\(r(A)=n\\)\nResult: A is diagonalizable.2\nResult: The eigenvector decomposition of a square matrix gives you \\(A=C\\Lambda C^{-1}\\) where \\(\\Lambda\\) is a diagonal matrix of eigenvalues and $C$ a matrix of the corresponding eigenvectors. The symmetry of \\(A\\) gives you that \\(C^{-1}=C'\\Rightarrow A=C\\Lambda C'\\) with \\(C'C=CC'=I_{n}\\).3\n\nA key definition concerning symmetric matrices is their positive definiteness:\n\nDefinition 15 \\(A\\) is positive semi-definite if for any \\(x\\in\\mathbb{R}^n,\\;x'Ax\\geq0\\).\n\nGiven the eigenvector decomposition of a symmetric matrix, positive semi-definiteness implies \\(\\Lambda\\) is positive semi-definite: \\(\\lambda_i\\geq0\\quad\\forall i\\). Likewise,\n\nDefinition 16 \\(A\\) is positive definite if for any \\(x\\in\\mathbb{R}^n,\\;x'Ax&gt;0\\).\n\nAgain, based on the egeinvector decomposition, positive semi-definiteness implies \\(\\Lambda\\) is positive definite: \\(\\lambda_i&gt;0\\quad\\forall i\\).\nA few more results are:\n\nResult: \\(tr(A) = \\sum_{i=1}^n\\lambda_i\\)\nResult: \\(r(A) = r(\\Lambda)\\)\nResult: \\(det(A) = \\prod_{i=1}^n \\lambda_i\\)\n\nThis last result can be used to prove that any positive definite matrix is non-singular and therefore has an inverse.\nAny full-rank, positive semi-definite, symmetric matrix \\(B\\) has the additional properties:\n\nResult: \\(B=C\\Lambda C'\\) and \\(B^{-1} = C\\Lambda^{-1}C'\\)\nResult: We can define the square-root of \\(B\\) as \\(B^{1/2} = C\\Lambda^{1/2}C'\\). Similarly, \\(B^{-1/2} = C\\Lambda^{-1/2}C'\\)."
  },
  {
    "objectID": "material-linearalgebra.html#properties-of-idempotent-matrices",
    "href": "material-linearalgebra.html#properties-of-idempotent-matrices",
    "title": "Linear Algebra",
    "section": "",
    "text": "An idempotent matrix has the property that \\(D=DD\\). Therefore, \\(D\\) must be square.\nHere are a few important results concerning idempotent matrices.\n\nResult: \\(D\\) is positive definite\nResult: \\(D\\) is diagonalizable\nResult: \\((I_n-D)\\) is also an idempotent matrix\nResult: With the exception of \\(I_n\\), all idempotent matrices are singular.\nResult: \\(r(D) = tr(D) = \\sum_{i=1}^n\\lambda_i\\)\nResult: \\(\\lambda_i\\in\\{0,1\\}\\quad \\forall\\;i\\)\n\nProjection matrices are idempotent, but need not be symmetric. However, for the purposes of this module we will deal exclusively with symmetric idempotent projection matrices."
  },
  {
    "objectID": "material-linearalgebra.html#vector-differentiation",
    "href": "material-linearalgebra.html#vector-differentiation",
    "title": "Linear Algebra",
    "section": "",
    "text": "Here we will look at the derivatives of scalar with respect to (W.r.t.) a vector. You can also define other derivatives, such as the derivative of a vector w.r.t. a vector and the derivative of a scalar with respect to a matrix. However, these are not needed for these notes.\n\n\nSuppose \\(f(x)\\in R\\) (i.e. a scalar) and \\(x\\in R^n\\) (i.e. a \\(n\\times 1\\) vector). Then we can define the partial derivative of \\(f(x)\\) w.r.t. \\(x\\) as,\n\\[\n  \\frac{\\partial f(x)}{\\partial x}  = \\begin{bmatrix}\\frac{\\partial f(x)}{\\partial x_1} \\\\ \\frac{\\partial f(x)}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial f(x)}{\\partial x_n} \\end{bmatrix}\n\\]\n\n\n\nA special case is when \\(f(x)\\) is linear in \\(x\\),\n\\[\nf(x) = a'x = \\sum_{i=1}^n a_ix_i\n\\] for \\(a\\in R^n\\). The derivative of \\(a'x\\) with respect to the vector \\(x\\) can be defined as,\n\\[\n  \\begin{aligned}\n  \\frac{\\partial a'x}{\\partial x} & = \\begin{bmatrix}\\frac{\\partial a'x}{\\partial x_1} \\\\ \\frac{\\partial a'x}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial a'x}{\\partial x_n} \\end{bmatrix} \\\\\n  & = \\begin{bmatrix}a_1 \\\\ a_2 \\\\ \\vdots \\\\ a_n \\end{bmatrix} \\\\\n  & = a\n  \\end{aligned}\n\\] since the the partial derivate of \\(a'x = \\sum_{i=1}^n a_ix_i\\) w.r.t. \\(x_i\\) is just the scalar \\(a_i\\).\nThe derivative of a scalar w.r.t. to a vector yields a vector of partial derivatives.\nSince \\(a'x\\) is a scalar, it is by definition symmetric: \\(a'x = x'a\\). Thus,\n\\[\n\\frac{\\partial x'a}{\\partial x} = \\frac{\\partial a'x}{\\partial x} = a\n\\]\n\n\n\nSuppose \\(f(x)\\) is a linear transformation of \\(x\\),\n\\[\nf(x) = A'x\n\\] for any \\(m\\times n\\) matrix A,\n\\[\n  A = \\begin{bmatrix}a_1' \\\\ a_2' \\\\ \\vdots \\\\ a_m'\\end{bmatrix}\n\\] where \\(a_i\\in R^n\\;\\forall i=1,\\cdots,m\\) and,\n\\[\n  Ax = \\begin{bmatrix}a_1'x \\\\ a_2'x \\\\ \\vdots \\\\ a_m'x\\end{bmatrix}\n\\] Note, \\(f(x)=Ax\\in R^m\\), a \\(m\\times 1\\) vector. We can then define,\n\\[\n  \\begin{aligned}\n  \\frac{\\partial Ax}{\\partial x'} & = \\begin{bmatrix}\\frac{\\partial a_1'x}{\\partial x_1} & \\frac{\\partial a_1'x}{\\partial x_2} & \\cdots & \\frac{\\partial a_1'x}{\\partial x_n}\\\\ \\frac{\\partial a_2'x}{\\partial x_1} & \\frac{\\partial a_2'x}{\\partial x_2} & \\cdots & \\frac{\\partial a_2'x}{\\partial x_n}\\\\ \\vdots & \\vdots & \\ddots & \\vdots\\\\ \\frac{\\partial a_m'x}{\\partial x_1} & \\frac{\\partial a_m'x}{\\partial x_2} & \\cdots & \\frac{\\partial a_m'x}{\\partial x_n}\\\\ \\end{bmatrix} \\\\\n  & = \\begin{bmatrix} a_{11} & a_{12} & \\cdots & a_{1n} \\\\ a_{21} & a_{22} & \\cdots & a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots\\\\ a_{m1} & a_{m2} & \\cdots & a_{mn} \\\\ \\end{bmatrix} \\\\\n  & = A\n  \\end{aligned}\n\\]\nSince Ax is \\(m\\times 1\\) column vector, we take the derivative w.r.t. \\(x'\\) a row vector and not the column vector \\(x\\). This results in a matrix of partial derivatives.\n\n\n\nA second special case is where the function takes on the quadaratic form,\n\\[\nf(x) = x'Ax = \\sum_{i=1}^N\\sum_{j=1}^n a_{ij}x_ix_j\n\\]\nfor \\(n\\times n\\) (square) matrix A. As in the first linear case, \\(f(x)\\) is scalar.\nDefine \\(c = Ax\\), the \\(x'Ax = x'c\\). From the linear case, we know that,\n\\[\n\\frac{\\partial x'c}{\\partial x} = c\n\\]\nSimilarly, if we define \\(d = A'x\\) then \\(x'Ax = d'x\\). From the linear case, we know that,\n\\[\n\\frac{\\partial d'x}{\\partial x} = d\n\\]\nWe can define the total derivative as the sum of the partial derivatives w.r.t. to the first and second \\(x\\). Combining these two results, we have that,\n\\[\n\\frac{\\partial x'Ax}{\\partial x} = Ax + A'x\n\\]\nAnd if \\(A\\) is symmetric, this result simplifies to \\(2Ax\\)."
  },
  {
    "objectID": "material-linearalgebra.html#footnotes",
    "href": "material-linearalgebra.html#footnotes",
    "title": "Linear Algebra",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThese notes do not cover how to calculate the determinant of a square matrix. You should be able to find a definition easily online.↩︎\nA matrix is diagonalizable if it is similar to some other diagonal matrix. Matrices \\(B\\) and \\(C\\) are similar if \\(C=PBP^{-1}\\). A square matrix which is not diagonalizable is defective. This property relates closely to eigenvector decomposition.↩︎\nRecall, an eigenvalue and eigenvector pair, \\((\\lambda,c)\\), of matrix \\(A\\) satisfy:\n\\[\nAc = \\lambda c\\Rightarrow (A-\\lambda I_n)c=0\n\\]↩︎"
  },
  {
    "objectID": "problem-set-2.html",
    "href": "problem-set-2.html",
    "title": "Problem Set 2",
    "section": "",
    "text": "This problem set will take you through some Stata commands to estimate simple regression equations with dummy variables. You will learn how to interpret the estimated coefficients and test some linear hypotheses. Interpretation of these coefficients will be useful when we do treatment evaluation models later in term 1.\nThe hypothesis tests discussed in this problem set include standard T-tests and F-tests, which is assumed undergraduate knowledge for this module.\nYou will need to download the dataset problem-set-2-data.dta, which is available on Moodle."
  },
  {
    "objectID": "problem-set-2.html#conditional-expectation-function",
    "href": "problem-set-2.html#conditional-expectation-function",
    "title": "Problem Set 2",
    "section": "Conditional Expectation Function",
    "text": "Conditional Expectation Function\nConsider the Conditional Expectation Function (CEF), \\(E[Y_i|X_i]\\). If \\(X\\) takes on discrete values: \\(X_i\\in\\{x_1,x_2,...,x_m\\}\\), then\n\\[\n    E[Y_i|X_i] =  E[Y_i|X_i=x_1]\\cdot\\mathbf{1}\\{X_i = x_1\\}+...+E[Y_i|X_i=x_m]\\cdot\\mathbf{1}\\{X_i = x_m\\}\n\\] where \\(\\mathbf{1}\\{X_i = x_m\\}\\) is a dummy variable, \\(=1\\) when \\(X_i=x_m\\). Since the values of \\(X_i\\) are mutually exclusive there is no overlap of these dummy variables.\nNote, we do not need to assume that \\(X\\) is a single random variable. It can be a vector of random variables that takes on discrete values.\nWe can re-arrange this expression using anyone of the values of \\(X\\). The natural option is to choose the first, but this is arbitrary.\n\\[\n\\begin{aligned}\n    E[Y_i|X_i] =& E[Y_i|X_i=x_1]+ (E[Y_i|X_i=x_2]-E[Y_i|X_i=x_1])\\cdot\\mathbf{1}\\{X_i = x_2\\}+... \\\\\n&+(E[Y_i|X_i=x_m]-E[Y_i|X_i=x_1])\\cdot\\mathbf{1}\\{X_i = x_m\\}\n\\end{aligned}\n\\]\nSince, \\(E[Y_i|X_i = x_m]\\) is a constant (\\(X_i\\) is set to a specific value), we can express the CEF as a function that is linear in parameters.\n\\[\n    E[Y_i|X_i] = \\beta_1 + \\beta_2D_{i2} + ... + \\beta_m D_{im}\n\\]\nwhere \\(D_{im}=\\mathbf{1}\\{X_i = x_m\\}\\)."
  },
  {
    "objectID": "problem-set-2.html#preamble",
    "href": "problem-set-2.html#preamble",
    "title": "Problem Set 2",
    "section": "Preamble",
    "text": "Preamble\n\n\n\n\n\nCreate a do-file for this problem set and include a preamble that sets the directory and opens the data. For example,\n\nclear \n//or, to remove all stored values (including macros, matrices, scalars, etc.) \n*clear all\n\n* Replace $rootdir with the relevant path to on your local harddrive.\ncd \"$rootdir/problem-sets/ps-2\"\n\ncap log close\nlog using problem-set-2-log.txt, replace\n\nuse problem-set-2-data.dta"
  },
  {
    "objectID": "problem-set-2.html#questions",
    "href": "problem-set-2.html#questions",
    "title": "Problem Set 2",
    "section": "Questions",
    "text": "Questions\n1. Consider the \\(E[ln(Wage_i)|Gender_i]\\), where \\(Gender_i\\in\\{1 ``Male'', 2 ``Female''\\}\\). Show that this CEF implies a linear model,\n\\[\n    ln(Wage_i) = \\beta_1 + \\beta_2 D_{i2} + \\varepsilon_i\n\\]\nWhat do the parameters \\(\\beta_1\\) and \\(\\beta_2\\) imply?\n2. Regress lwage (log wage) on just a set of binary indicators that will enable you to test the hypothesis that males and females are on average, paid the same wage, ceteris paribus. Test this hypothesis.\n3. Extend the specification in (2) that will enable you to test the hypothesis that there is no difference in the wages between the following gender-ethnicity groups. Begin by defining the following dummy variables:\n\nfemale_black = female\\(\\times\\)black\nmale_black = (1-female)\\(\\times\\)black\nfemale_nonblack = female\\(\\times\\)(1-black)\nmale_nonblack = (1-female)\\(\\times\\)(1-black)\n\nThen estimate the following regressions:\n\nlwage on female_black, female_nonblack, male_black, male_nonblack (without a constant: option nocons)\nlwage on female, black, female_black\nlwage on female_black, female_nonblack, male_black\n\nFor some of these exercises you may be able to use Stata’s factor notation. However, in some instances you will need to manually create the above dummy-variable interactions.\nIn each case, identify the base category and write down the parameters of the (implied) model in terms of conditional expectations.\n4. Compare the estimated coefficients with the sample average values for the lwage for the four subgroups. What do you see?\n5. In each of the above models, describe the null hypothesis you would test to evaluate whether there is a significant earnings difference between the earnings of black and non-black females.\n6. Very your solution to 4. by performing a test using the three set of regression output. You can use the post-estimation test command.\n7. In each case, test equality across all four gender-ethnicity groups. Again, you should get the same result.\n8. Try to replicate the F-statistic for one of the above models. Hint, the F-stat for these models is the same as that of the whole model.\n9. Estimate the following model:\n\\[\n    lwage = \\beta_1 + \\beta_2F + \\beta_3B + \\beta_4F\\times B + \\beta_5exp + \\beta_6exp^2 + \\beta_7educ + \\varepsilon\n\\]\n\nInterpret the estimated coeffiecients \\(\\hat{\\beta}_7\\).\nInterpret the effect of experience variable exp. Use the median level of experience to make your calculation.\nA one unit incease in years of education is associated with an increase of 1.78% in expected wages, holding other regressors fixed.\n\n10. Theoretically, how would you test the following restrictions for the model below?\n\n\\(\\beta_2 = \\beta_3\\)\n\\(\\beta_4 + \\beta_5 = 1\\)\n\\(\\beta_2 = \\beta_3\\) and \\(\\beta_4 + \\beta_5 = 1\\)\n\n\\[\n    Y = \\beta_1 + \\beta_2X_2 + \\beta_3X_3 + \\beta_4X_4 + \\beta_5X_5 + \\varepsilon\n\\]"
  },
  {
    "objectID": "problem-set-2.html#postamble",
    "href": "problem-set-2.html#postamble",
    "title": "Problem Set 2",
    "section": "Postamble",
    "text": "Postamble\n\nlog close"
  },
  {
    "objectID": "problem-set-3.html",
    "href": "problem-set-3.html",
    "title": "Problem Set 3",
    "section": "",
    "text": "The purpose of this problem set is for you to see how the ordinary least squares (OLS) estimator behaves under various assumptions in a linear regression model where you know what the model is – since you are going to be generating the data from a known data generating process (DGP).\nThe models estimated are simple bivariate regressions but the properties of the OLS estimator with vary with each case. This is demonstrated by changing the (a) distributional properties of the error term (variance-covariance structure), and (b) inducing correlation between the regressor and the error term. Any resulting bias and/or inconsistency will depend on the DGP.\nTo achieve certain results we will have to use a serially-correlated error structure, which is only appropriate in a time-series setting. For this reason, the models will be written with subscript \\(t\\) and not \\(i\\).\nThe code has been provided for model 1. You can then modify the code for models 2-4."
  },
  {
    "objectID": "problem-set-3.html#preamble",
    "href": "problem-set-3.html#preamble",
    "title": "Problem Set 3",
    "section": "Preamble",
    "text": "Preamble\n\n\n\n\n\nYou do not need to load data for this problem set.\n\nclear \n//or, to remove all stored values (including macros, matrices, scalars, etc.) \n*clear all\n\n* Replace $rootdir with the relevant path to on your local harddrive.\ncd \"$rootdir/problem-sets/ps-3\"\n\ncap log close\nlog using problem-set-3-log.txt, replace\n\nHowever, since we are going to generate random variables, we should set a seed. This ensures replicability of the exercise. The number you choose is arbitrary, it simply ensures that any algorithms used to generate (pseudo) random variables start at the same place.\n\nset seed 981836"
  },
  {
    "objectID": "problem-set-3.html#model-1-clrm",
    "href": "problem-set-3.html#model-1-clrm",
    "title": "Problem Set 3",
    "section": "Model 1: CLRM",
    "text": "Model 1: CLRM\nThis is your classical linear regression model. OLS estimator is unbiased and consistent.\n\\[\nY_t = \\beta_1 + \\beta_2X_t + \\upsilon_t \\qquad \\text{with}\\quad \\upsilon_t\\sim N(0,\\sigma^2)\n\\]\nWe know that the OLS estimator for \\(\\beta_2\\) is given by,\n\\[\n\\begin{aligned}\n  \\hat{\\beta}_2 =& \\frac{\\sum_t \\big[(X_t-\\bar{X})(Y_t-\\bar{Y})\\big]}{\\sum_t (X_t-\\bar{X})^2} \\\\\n  =& \\beta_2 + \\frac{\\sum_t \\big[(X_t-\\bar{X})(\\upsilon_t-\\bar{\\upsilon})\\big]}{\\sum_t (X_t-\\bar{X})^2} \\\\\n  =& \\beta_2 + \\frac{\\sum_t \\tilde{X}_t\\tilde{\\upsilon}_t}{\\sum_t \\tilde{X}_t^2}\n\\end{aligned}  \n\\] where \\(\\tilde{X}_t\\) and \\(\\tilde{\\upsilon}_t\\) represent the demeaned counterparts of these variables. Alternatively, using linear algebra notation:\n\\[\n\\begin{aligned}\n  \\hat{\\beta}_2 =& \\frac{X'M_{\\ell}Y}{X'M_{\\ell}X} \\\\\n  =& \\beta_2 + \\frac{X'M_{\\ell}\\upsilon}{X'M_{\\ell}X} \\\\\n  =& \\beta_2 + \\frac{\\tilde{X}'\\tilde{\\upsilon}}{\\tilde{X}'\\tilde{X}}\n\\end{aligned}  \n\\] where \\(\\tilde{X} = M_{\\ell}X\\), \\(\\tilde{\\upsilon}= M_{\\ell}\\upsilon\\), and \\(M_{\\ell} = I_n-\\ell(\\ell'\\ell)^{-1}\\ell'\\) (the orthogonal projection of the constant regressor).\nWe know from Handouts 2 & 3,\n\n\\(E[\\hat{\\beta}_2] = \\beta_2\\) (i.e., unbiased)\n\\(p \\lim \\hat{\\beta}_2 = \\beta_2\\) (i.e., consistent)\n\nCan you demonstrate these results?\n\nSimulation\nBegin by designing a programme that takes the parameters of the model as arguments, generates the data, estimates the model, and then returns the stored values.\n\ncapture program drop mc1\nprogram define mc1, rclass\n    syntax [, obs(integer 1) s(real 1) b1(real 0) b2(real 0)  sigma(real 1)]\n    drop _all\n    set obs `obs'\n    gen u = rnormal(0,`sigma')            // sigma is the std deviation of the error distribution\n    gen x=uniform()*`s'                   // s is the std devation of the x distribution\n    gen y=`b1'+`b2'*x + u                   // this generates the dep variable y\n    reg y x\n    return scalar b1=_b[_cons]            // intercept estimate\n    return scalar b2=_b[x]                  // coeff on the x variable\n    return scalar se2 = _se[x]            // std error\n    return scalar t2 = _b[x]/_se[x]     // t ratio\n\nend\n\n\n\n\nUse the the simulate command in Stata to estimate the model 100 times:\n\nsimulate b1=r(b1) b2=r(b2) se2=r(se2) t2=r(t2), reps(100): mc1, obs(50) s(6) b1(4) b2(2) sigma(3)\n\n\n      Command: mc1, obs(50) s(6) b1(4) b2(2) sigma(3)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n&gt; ....60.........70.........80.........90.........100 done\n\n\nCalculate the bias and plot the distribution of the bias.\n\ngen bias2=b2-2\nsu b1 b2 se2 t2\nsu bias2\nhistogram bias2, normal xline(`r(mean)')\n\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n          b1 |        100    3.880226    .9977415   1.080851   6.090028\n          b2 |        100    2.041985     .271704   1.365155   2.673303\n         se2 |        100    .2520885    .0291596   .1814694   .3255497\n          t2 |        100    8.216185      1.4968   5.484826   12.60699\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n       bias2 |        100    .0419852     .271704  -.6348448   .6733029\n(bin=10, start=-.63484478, width=.13081477)"
  },
  {
    "objectID": "problem-set-3.html#model-2-serial-correlation",
    "href": "problem-set-3.html#model-2-serial-correlation",
    "title": "Problem Set 3",
    "section": "Model 2: Serial Correlation",
    "text": "Model 2: Serial Correlation\nRelax the assumption of an iid error term and allow for serial correlation. The OLS estimator is unbiased and consistent. However, the std errors are wrong since the software does not know that you have serially correlated errors and you are not taking this into account in the estimation.\n\\[\nY_t = \\beta_1 + \\beta_2X_t + \\upsilon_t \\qquad \\text{where}\\quad \\upsilon_t = \\rho \\upsilon_{t-1}+\\varepsilon_t \\quad\\text{and}\\quad \\varepsilon_t\\sim N(0,\\sigma^2)\n\\] We say that \\(U_t\\) follows an AR(1) process. You can show that \\(\\hat{\\beta}_2\\) remains unbiased and consistant. However, the standard homoskedastic-variance estimator is incorrect:\n\\[\nVar(\\hat{\\beta}_2) \\neq \\frac{\\sigma^2}{Var(X_i)}\n\\]\n\nSimulation\nYou will need to redesign the above programme. The challenging part is the simulation of the error term. This needs to follow an AR(1) process and must therefore be generated in sequence. You can do this as follows:\n\n  gen u=0 \n    gen time=_n\n    gen e = rnormal(0,`sigma')  \n    forvalues i=2/`obs'  {\n    replace u=`rho'*u[_n-1] + e if _n==`i'\n    }"
  },
  {
    "objectID": "problem-set-3.html#model-3-dynamic-model-without-serial-correlation",
    "href": "problem-set-3.html#model-3-dynamic-model-without-serial-correlation",
    "title": "Problem Set 3",
    "section": "Model 3: Dynamic model without serial correlation",
    "text": "Model 3: Dynamic model without serial correlation\nConsider a version of Model 1, where the regressor is the lag of the dependent variable. \\[\nY_t = \\beta_1 + \\beta_2Y_{t-1} + \\upsilon_t \\qquad \\text{with}\\quad \\upsilon_t\\sim N(0,\\sigma^2)\n\\] The OLS estimator is now, \\[\n  \\hat{\\beta}_2 = \\beta_2 + \\frac{\\sum_t \\tilde{Y}_{t-1}\\tilde{\\upsilon}_t}{\\sum_t \\tilde{Y}_{t-1}^2}\n\\] This model is biased, since\n\\[\nE\\bigg[\\frac{\\sum_t \\tilde{Y}_{t-1}\\tilde{\\upsilon}_t}{\\sum_t \\tilde{Y}_{t-1}^2}\\bigg] \\neq \\frac{E\\big[\\sum_t \\tilde{Y}_{t-1}\\tilde{\\upsilon}_t\\big]}{E\\big[\\sum_t \\tilde{Y}_{t-1}^2\\big]}\n\\] When the regressor was \\(X_t\\), the above statement was true given the Law of Iterated Expectations. However, you can use Slutsky’s theorem and the WLLN to show that \\(\\hat{\\beta}_2\\rightarrow_p \\beta_2\\). This result relies on the fact that \\(Y_{t-1}\\) is realized before \\(\\upsilon_t\\) which is iid. Thus, the bias goes to 0 as \\(n\\rightarrow \\infty\\).\n\nSimulation\nYou can use lag-operators in Stata to regress the outcome against its lagged value. For example:\n\n    gen time=_n\n    tsset time\n    reg y  L.y\n\nWhile the error term is serially uncorrelated (as in Model 1), you will need to generate the outcome sequentially (row value by row value). This is because the DGP has a lagged dependent variable structure."
  },
  {
    "objectID": "problem-set-3.html#model-4-dynamic-model-with-serial-correlation",
    "href": "problem-set-3.html#model-4-dynamic-model-with-serial-correlation",
    "title": "Problem Set 3",
    "section": "Model 4: Dynamic model with serial correlation",
    "text": "Model 4: Dynamic model with serial correlation\nConsider a version of Model 2, where the regressor is the lag of the dependent variable. \\[\nY_t = \\beta_1 + \\beta_2Y_{t-1} + \\upsilon_t \\text{where}\\quad \\upsilon_t = \\rho \\upsilon_{t-1}+\\varepsilon_t \\quad\\text{and}\\quad \\varepsilon_t\\sim N(0,\\sigma^2)\n\\] As with model 3, the OLS estimator will be biased. In addition, since \\(Cov(\\upsilon_t,\\upsilon_{t-1})\\neq0\\) and \\(Cov(Y_t,\\upsilon_{t})\\neq 0\\) (for any \\(t\\)), \\[\n\\Rightarrow Cov(Y_{t-1},\\upsilon_{t})\\neq 0\n\\] As a result \\(\\hat{\\beta}_2\\) is inconsistent.\n\nSimulation\nYou will need to use tricks from both models 2 and 3 to simulate this model."
  },
  {
    "objectID": "problem-set-3.html#postamble",
    "href": "problem-set-3.html#postamble",
    "title": "Problem Set 3",
    "section": "Postamble",
    "text": "Postamble\n\nlog close"
  },
  {
    "objectID": "problem-set-4.html",
    "href": "problem-set-4.html",
    "title": "Problem Set 4",
    "section": "",
    "text": "This problem set will revisit some of the material covered in Handouts 3 and 4. You will be required to work with a ‘raw’ dataset, downloaded from an online repository. For this reason, you should take care to check how the data is coded.\nYou will be using a version of the US Current Population Survey (CPS) called the Merged Outgoing Rotation Group (MORG). This data is compiled by the National Bureau of Economic Research (NBER) and has been used in many famous studies of the US economy. The CPS has a rather unique rotating panel design: “The monthly CPS is a rotating panel design; households are interviewed for four consecutive months, are not in the sample for the next eight months, and then are interviewed for four more consecutive months.” (source: IPUMS). The NBER’s MORG keeps only the outgoing rotation group’s observations.\nThe MORG .dta files can be found at: https://data.nber.org/morg/annual/."
  },
  {
    "objectID": "problem-set-4.html#preamble",
    "href": "problem-set-4.html#preamble",
    "title": "Problem Set 4",
    "section": "Preamble",
    "text": "Preamble\nCreate a do-file for this problem set and include a preamble that sets the directory and opens the data directly from the NBER website. Of course, this requires a good internet connection. For example,\n\nclear \n//or, to remove all stored values (including macros, matrices, scalars, etc.) \n*clear all\n\n* Replace $rootdir with the relevant path to on your local harddrive.\ncd \"$rootdir/problem-sets/ps-4\"\n\ncap log close\nlog using problem-set-4-log.txt, replace\n\nuse \"https://data.nber.org/morg/annual/morg19.dta\", clear\n\nYou can, of course, download the data and open it locally on your computer."
  },
  {
    "objectID": "problem-set-4.html#questions",
    "href": "problem-set-4.html#questions",
    "title": "Problem Set 4",
    "section": "Questions",
    "text": "Questions\n1. Create a new variable exper equal to age minus (years of education + 6). This is referred to as potential years of experience. Check how each variable defines missing values before proceeding. You will need to create a years of education variable for this. Here is he the suggested code:\n\ntab grade92, m\ngen eduyrs = .\n    replace eduyrs = .3 if grade92==31\n    replace eduyrs = 3.2 if grade92==32\n    replace eduyrs = 7.2 if grade92==33\n    replace eduyrs = 7.2 if grade92==34\n    replace eduyrs = 9  if grade92==35\n    replace eduyrs = 10 if grade92==36\n    replace eduyrs = 11 if grade92==37\n    replace eduyrs = 12 if grade92==38\n    replace eduyrs = 12 if grade92==39\n    replace eduyrs = 13 if grade92==40\n    replace eduyrs = 14 if grade92==41\n    replace eduyrs = 14 if grade92==42\n    replace eduyrs = 16 if grade92==43\n    replace eduyrs = 18 if grade92==44\n    replace eduyrs = 18 if grade92==45\n    replace eduyrs = 18 if grade92==46\n    lab var eduyrs \"completed education\"\ntab grade92, sum(eduyrs)\n\n2. Keep only those between the ages of 18 and 54. Check the distribution of `exper’ and replace any negative values to 0.\n3. Create a categorical variable that takes on 4 values: 1 “less than High School”; 2 “High School Diploma”; 3 “some Higher Education”; 4 “Bachelors”; 5 “Postgraduate”. This variable should be based on the the grade92 variable. You can find the value labels for this variable in this document: https://data.nber.org/morg/docs/cpsx.pdf. I suggest using the recode command, which allows you to create value labels while assigning values. Check the distributio of exper by education category.\n4. Create the variable lnwage equal to the (natural) log of weekly earnings. Create a figure that shows the predicted linear fit of lwage against exper, by educat. Try to place all 5 fitted lines in the same graph.\n5. Estimate a linear regression model that allows the slope coefficient on exper and constant term to vary by education category (educat). Let the base (excluded) education category be 2 “High School diploma”.\n\\[\n  \\ln(Wage_i) = \\alpha + \\sum_{j\\neq2}\\psi_j \\mathbf{1}\\{Educat_i=j\\} + \\beta Exper_i + \\sum_{j\\neq2}\\gamma_j Exper_i\\times\\mathbf{1}\\{Educat_i=j\\}+\\upsilon_i\n\\]\n6. Show that after 13 years of experience, those with some Higer Education (but no Bachelors), out earn those with just a high school diploma. You can assume that there are is a 2 year difference between the experience (education).\n7. Use the post-estimation test command to test the null hypothesis: \\(H_0: 15\\beta = 13(\\beta+\\gamma_3)+\\psi_3\\).\n8. Estimate a transformed version of the above model allowing you to test the above hypothesis using the coefficient from a single regressor. That is, the resulting test should be a simple t-test of \\(H_0: \\phi=0\\), where \\(\\phi\\) is the coefficient on the interaction of exper and a dummy variable for educat=3. This will be easier to do if you estimate the model using only the relevant sample: those with High School diplomas and some Higher Education. I suggest avoiding the use of factor notation to create the dummy variables and interaction terms for this exercise. For example, the following should replicate the relevant coefficients from Q5.\n\ngen hasHE = educat==3 if inlist(educat,2,3)\ngen hasHEexp = hasHE*exper\n\nreg lnwage exper hasHE hasHEexp\n\n9. Verify that the F-statistic from Q7 is the square of the above T-statistic.\n10. Use the restricted OLS approach to replicate the F-statistic and p-value from Q7.\n11. Use the restricted OLS approach to test the following hypothesis corresponding to the model in Q5:\n\\[\nH_0: \\gamma_j = 0\\qquad \\text{for}\\quad j=1,3,4,5\n\\] Compute the F-statistic and p-value. Verify your result using the post-estimation test command.\n12. Compute the relevant Chi-squared distributed test statistic and corresponding p-value for the above test, assuming \\(n\\) is large (enough).\n13. Using the data from Problem Set 2, estimate the simple linear regression model using OLS,\n\\[\n  \\ln(Wage_i) = \\beta_0 + \\beta_1 Educ_i + \\beta_2 Female_i + \\varepsilon_i\n\\]\n14. Estimate the model using Maximum Likelihood. Take a look at https://www.stata.com/manuals13/rmlexp.pdf, the documentation for the mlexp command. It has a discussion on estimating the CLRM using ML.1\n15. Estimate the model using Method of Moments. You can use the gmm command in Stata. Hint: the regressors will be their own instruments and use the onestep option.2"
  },
  {
    "objectID": "problem-set-4.html#postamble",
    "href": "problem-set-4.html#postamble",
    "title": "Problem Set 4",
    "section": "Postamble",
    "text": "Postamble\n\nlog close"
  },
  {
    "objectID": "problem-set-4.html#footnotes",
    "href": "problem-set-4.html#footnotes",
    "title": "Problem Set 4",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYou can also look at the following resource for a more flexible approach to ML estimation in Stata: https://www.stata.com/features/overview/maximum-likelihood-estimation/↩︎\nHere is a resource on GMM in Stata: https://www.stata.com/features/overview/generalized-method-of-moments/↩︎"
  },
  {
    "objectID": "problem-set-5.html",
    "href": "problem-set-5.html",
    "title": "Problem Set 5",
    "section": "",
    "text": "This problem set will revise some of the material covered in Handout 5 on panel data models. This will require you to familiarize yourself with Stata’s panel-data commands.\n\nhelp xtset\nhelp xttab\nhelp xtreg\n\nYou will be using a dataset that comes with Stata: psidextract.dta. The data is a correct version of the PSID sample in Cornwell and Rupert (1988), found in Baltagi and Khanti-Akom (1990). It includes a sample of 595 individuals observed for the years 1976-82.\n\n\n\n\n\n\n\nCreate a do-file for this problem set and include a preamble that sets the directory and opens the data. For example,\n\nclear \n//or, to remove all stored values (including macros, matrices, scalars, etc.) \n*clear all\n\n* Replace $rootdir with the relevant path to on your local harddrive.\ncd \"$rootdir/problem-sets/ps-5\"\n\ncap log close\nlog using problem-set-5-log.txt, replace\n\nuse problem-set-5-data.dta, clear\n\n\n\n\n1. Set the unit identifier and time variable using xtset. Note, you can also use tsset for this task. This will allow you to use xt package commands.\n2. Describe and summarise the variables in the dataset using the normal describe and summarize commands.\n3. Describe and summarise the variables in the dataset using the panel commands: xtdescribe and xtsummarize. Comment on the information provided.\n4. Use the command xttab and xtrans, freq to describe transitions over time in the variable south.\n5. Create the variable: expsq=exper^2/1000. Why would you scale the variable in this way?\n6. Estimate the following model using pooled OLS, between-group, feasible GLS, within-group, LSDV, and first-difference. For the first-difference estimator, you can define a first-difference in Stata using the time-series operator: D.variable.\n\\[\n\\ln(Wage_{it}) = \\beta_1 + \\beta_2 Exper_{it} + \\beta_3 Exper^2_{it} + \\beta_4 Weeks_{it} + \\beta_5 Eduyrs_{it} + \\varepsilon_{it}\n\\] With each model, store the results using estimates store. For example,\n\n* clear existing stored estimates\nest clear\n\n* Pooled OLS\nregress lwage exper expsq weeks ed\nest store OlS\n\n* alternatively, \neststo OLS: regress lwage exper expsq weeks ed\n\n7. Using the formula from Handout 5, replicate the value of \\(\\theta\\) reported above by the FGLS estimator. Note, you will need to use the stored values of \\(\\sigma^2_{\\varepsilon}\\) and \\(\\sigma^2_{\\alpha}\\).\n8. Make a table of the computed estimates. You can either use estimates table or esttab. The latter is part of the estout package, which you may need to install: ssc install estout.\n9. Perform a Hausman test comparing the results of the FLGS and WG estimators. You should use the hausman command, with the option sigmamore. Be sure to get the order of the estimates correct. What do you learn from the test?\n10. Estimate FGLS for the model below:\n\\[\n\\begin{aligned}\n\\ln(Wage_{it}) =& \\beta_1 + \\beta_2 Exper_{it} + \\beta_3 Exper^2_{it} + \\beta_4 Weeks_{it} + \\beta_5 Eduyrs_{it} \\\\\n&+ \\gamma_2 \\overline{Exper}_{i} + \\gamma_3 \\overline{Exper^2}_{i} + \\gamma_4 \\overline{Weeks}_{i}+\\varepsilon_{it}\n\\end{aligned}\n\\] You will need to manually create the variables: \\(\\{\\overline{Exper}_{i}, \\overline{Exper^2}_{i},\\overline{Weeks}_{i}\\}\\) - the individual-level averages of each variable. This is referred to as the Mundlack correction. Once you have estimated the model, repeat the Hausman test comparing these results with those of the WG estimator. What is the significance of the Mundlack correction?\n11. Export the results as a single CSV/Excel file. You can use esttab for .csv or outreg2 for .xlsx.\n\n\n\n\nlog close"
  },
  {
    "objectID": "problem-set-5.html#preamble",
    "href": "problem-set-5.html#preamble",
    "title": "Problem Set 5",
    "section": "",
    "text": "Create a do-file for this problem set and include a preamble that sets the directory and opens the data. For example,\n\nclear \n//or, to remove all stored values (including macros, matrices, scalars, etc.) \n*clear all\n\n* Replace $rootdir with the relevant path to on your local harddrive.\ncd \"$rootdir/problem-sets/ps-5\"\n\ncap log close\nlog using problem-set-5-log.txt, replace\n\nuse problem-set-5-data.dta, clear"
  },
  {
    "objectID": "problem-set-5.html#questions",
    "href": "problem-set-5.html#questions",
    "title": "Problem Set 5",
    "section": "",
    "text": "1. Set the unit identifier and time variable using xtset. Note, you can also use tsset for this task. This will allow you to use xt package commands.\n2. Describe and summarise the variables in the dataset using the normal describe and summarize commands.\n3. Describe and summarise the variables in the dataset using the panel commands: xtdescribe and xtsummarize. Comment on the information provided.\n4. Use the command xttab and xtrans, freq to describe transitions over time in the variable south.\n5. Create the variable: expsq=exper^2/1000. Why would you scale the variable in this way?\n6. Estimate the following model using pooled OLS, between-group, feasible GLS, within-group, LSDV, and first-difference. For the first-difference estimator, you can define a first-difference in Stata using the time-series operator: D.variable.\n\\[\n\\ln(Wage_{it}) = \\beta_1 + \\beta_2 Exper_{it} + \\beta_3 Exper^2_{it} + \\beta_4 Weeks_{it} + \\beta_5 Eduyrs_{it} + \\varepsilon_{it}\n\\] With each model, store the results using estimates store. For example,\n\n* clear existing stored estimates\nest clear\n\n* Pooled OLS\nregress lwage exper expsq weeks ed\nest store OlS\n\n* alternatively, \neststo OLS: regress lwage exper expsq weeks ed\n\n7. Using the formula from Handout 5, replicate the value of \\(\\theta\\) reported above by the FGLS estimator. Note, you will need to use the stored values of \\(\\sigma^2_{\\varepsilon}\\) and \\(\\sigma^2_{\\alpha}\\).\n8. Make a table of the computed estimates. You can either use estimates table or esttab. The latter is part of the estout package, which you may need to install: ssc install estout.\n9. Perform a Hausman test comparing the results of the FLGS and WG estimators. You should use the hausman command, with the option sigmamore. Be sure to get the order of the estimates correct. What do you learn from the test?\n10. Estimate FGLS for the model below:\n\\[\n\\begin{aligned}\n\\ln(Wage_{it}) =& \\beta_1 + \\beta_2 Exper_{it} + \\beta_3 Exper^2_{it} + \\beta_4 Weeks_{it} + \\beta_5 Eduyrs_{it} \\\\\n&+ \\gamma_2 \\overline{Exper}_{i} + \\gamma_3 \\overline{Exper^2}_{i} + \\gamma_4 \\overline{Weeks}_{i}+\\varepsilon_{it}\n\\end{aligned}\n\\] You will need to manually create the variables: \\(\\{\\overline{Exper}_{i}, \\overline{Exper^2}_{i},\\overline{Weeks}_{i}\\}\\) - the individual-level averages of each variable. This is referred to as the Mundlack correction. Once you have estimated the model, repeat the Hausman test comparing these results with those of the WG estimator. What is the significance of the Mundlack correction?\n11. Export the results as a single CSV/Excel file. You can use esttab for .csv or outreg2 for .xlsx."
  },
  {
    "objectID": "problem-set-5.html#postamble",
    "href": "problem-set-5.html#postamble",
    "title": "Problem Set 5",
    "section": "",
    "text": "log close"
  },
  {
    "objectID": "problem-set-6.html",
    "href": "problem-set-6.html",
    "title": "Problem Set 6",
    "section": "",
    "text": "The purpose of the first part of this problem set is to estimate, interpret the results, and compare the results across different binary dependent variable models. In the second part you will estimate and compare different specifications of an endogenous selection model.\nFirst part will be discussed in week 9 and the second part in week 10 of this term.\nThe data file for this exercise is on Moodle: mus16data.dta. It is a subset of the data used by P. Deb, M. Munkin and P.K. Trivedi (2006): “Bayesian Analysis of Two-Part Model with Endogeneity”, Journal of Applied Econometrics, 21, 1081-1100. Data is for 2001 and comes from the Medical Expenditure Survey. Sample has 3,328 observations.\nThe main outcome variable of interest is ambulatory expenditure (ambexp) and the regressors are given below.\nSince the expenditure data is skewed, we will be using the logged expenditure variable as our dependent variable. You should read Cameron A.C. and Trivedi, P.K. Micro-econometrics using Stata to see the pros and cons regarding whether to log the dependent variable or not.\nNote, there is one individual who has an expenditure=1 and this will get coded as 0 when variable is logged. Since it is only one individual, we will ignore the problem by not doing anything. If there are many individuals like this, you will need to see whether you can say why this might be the case.\nDependent variable\n\nambexp: Ambulatory medical expenditures (excluding dental and outpatient mental). There are 526 individuals with zero expenditure. There is one individual who has expenditure=$1. I am going to assume that this individual did not spend any money.\nlambexp: ln(ambexp) given ambexp &gt; 0 ; missing otherwise\ndambexp: 1 if ambexp &gt; 0 and 0 otherwise (binary indicator)\n\nRegressors\n\nins: health insurance measures, either PPO or HMO type insurance\ntotchr: health status measures: number of chronic diseases\nage: age age in years/10\nfemale: 1 for females, zero otherwise\neduc: years of schooling of decision maker\nblhisp: either black or Hispanic\nincome: income in USD/1000\n\n\n\n\n\n\n\n\nCreate a do-file for this problem set and include a preamble that sets the directory and opens the data. For example,\n\nclear \n//or, to remove all stored values (including macros, matrices, scalars, etc.) \n*clear all\n\n* Replace $rootdir with the relevant path to on your local harddrive.\ncd \"$rootdir/problem-sets/ps-6\"\n\ncap log close\nlog using problem-set-6-log.txt, replace\n\nuse mus16data.dta, clear\n\n\n\n\n\n\n1.1. Obtain and comment on the descriptive statistics for ambexp, lambexp, age, female, educ, blhisp, totchr, ins, income.\n1.2. Estimate a LP, Probit and a Logit model to explain dambexp. Store the \\(\\beta\\) coefficients and report them in a table.\n1.3. Estimate the Marginal Effect at the Mean for each model, and report them in a table. You will want to use the estpost margins post-estimation command, with the relevant option for MEM. Pay special attention to the treatment of discrete regressors. Hint: check to see any differences in the estimated MEs based on whether you use factor notation; for example, i.female vs female.\n1.4. Estimate the Average Marginal Effect at the Mean for each model, and report them in a table.You will want to use the estpost margins post-estimation command, with the relevant option for AME.\n1.5. Check to see how well the prodbit model predicts the outcome using the estat classification post-estimation command.\n1.6. Construct and interpret the LR test for the omission of income in the probit model. Do this in two ways: (1) using the post estimation lrtest; (2) manually recreate (1)’s results (both test-statistic and p-value).\n\n\n\nEstimate the following models for lambexp treating the selection into non-zero lambexp value as endogenous using, both Heckman 2-step method and also MLE.\nIn the main data lambexp is missing for values of ambexp=0. Before proceeding,\n\nreplace lambexp = 0 if ambexp==0\n\nThis will correction will also treat observations with ambexp=1 as equivalent to =0; however, this is only a single observation.\n2.1. Estimate the Heckman 2-step estimator and store the results. In addition, store the Mills ratio as a separate variable. Use income as the excluded variable. This means that income appears in the selection equation, but NOT the main equation.\n2.2. Replicate these results by applying the following steps: (1) estimate the selection equation using a probit model; (2) create the mills ratio; (3) compare your mills ratio with the one stored above; (4) estimate the main equation, including the mills ratio.\n2.3 Estimate the marginal effects of the selection equation. You can do this using the margins command, with predict() option psel. This should correspond to a probit model estimation above.\n2.4. Estimate the Maximum Likelihood version of the Heckmann correction (with an excluded variable) and store the results.\n2.5 Compute the marginal effects of each regressor for: (1) probability of selection; (2) the expected value of the outcome; and (3) the expected value of the outcome, conditional on selection. You will need to use the post-estimation command margins, dydx(*) predict() with predict options: psel, yexpected, and ycond.\n2.6. Now re-estimate the two-step and MLE approach without an excluded variable, storing the results each time. This means that the same set of regressors enter both equations. i.e. include income in the outcome equation.\n2.7. Create a table that reports the four models alongside one another and compare the results.\n\n\n\n\n\nlog close"
  },
  {
    "objectID": "problem-set-6.html#preamble",
    "href": "problem-set-6.html#preamble",
    "title": "Problem Set 6",
    "section": "",
    "text": "Create a do-file for this problem set and include a preamble that sets the directory and opens the data. For example,\n\nclear \n//or, to remove all stored values (including macros, matrices, scalars, etc.) \n*clear all\n\n* Replace $rootdir with the relevant path to on your local harddrive.\ncd \"$rootdir/problem-sets/ps-6\"\n\ncap log close\nlog using problem-set-6-log.txt, replace\n\nuse mus16data.dta, clear"
  },
  {
    "objectID": "problem-set-6.html#questions",
    "href": "problem-set-6.html#questions",
    "title": "Problem Set 6",
    "section": "",
    "text": "1.1. Obtain and comment on the descriptive statistics for ambexp, lambexp, age, female, educ, blhisp, totchr, ins, income.\n1.2. Estimate a LP, Probit and a Logit model to explain dambexp. Store the \\(\\beta\\) coefficients and report them in a table.\n1.3. Estimate the Marginal Effect at the Mean for each model, and report them in a table. You will want to use the estpost margins post-estimation command, with the relevant option for MEM. Pay special attention to the treatment of discrete regressors. Hint: check to see any differences in the estimated MEs based on whether you use factor notation; for example, i.female vs female.\n1.4. Estimate the Average Marginal Effect at the Mean for each model, and report them in a table.You will want to use the estpost margins post-estimation command, with the relevant option for AME.\n1.5. Check to see how well the prodbit model predicts the outcome using the estat classification post-estimation command.\n1.6. Construct and interpret the LR test for the omission of income in the probit model. Do this in two ways: (1) using the post estimation lrtest; (2) manually recreate (1)’s results (both test-statistic and p-value).\n\n\n\nEstimate the following models for lambexp treating the selection into non-zero lambexp value as endogenous using, both Heckman 2-step method and also MLE.\nIn the main data lambexp is missing for values of ambexp=0. Before proceeding,\n\nreplace lambexp = 0 if ambexp==0\n\nThis will correction will also treat observations with ambexp=1 as equivalent to =0; however, this is only a single observation.\n2.1. Estimate the Heckman 2-step estimator and store the results. In addition, store the Mills ratio as a separate variable. Use income as the excluded variable. This means that income appears in the selection equation, but NOT the main equation.\n2.2. Replicate these results by applying the following steps: (1) estimate the selection equation using a probit model; (2) create the mills ratio; (3) compare your mills ratio with the one stored above; (4) estimate the main equation, including the mills ratio.\n2.3 Estimate the marginal effects of the selection equation. You can do this using the margins command, with predict() option psel. This should correspond to a probit model estimation above.\n2.4. Estimate the Maximum Likelihood version of the Heckmann correction (with an excluded variable) and store the results.\n2.5 Compute the marginal effects of each regressor for: (1) probability of selection; (2) the expected value of the outcome; and (3) the expected value of the outcome, conditional on selection. You will need to use the post-estimation command margins, dydx(*) predict() with predict options: psel, yexpected, and ycond.\n2.6. Now re-estimate the two-step and MLE approach without an excluded variable, storing the results each time. This means that the same set of regressors enter both equations. i.e. include income in the outcome equation.\n2.7. Create a table that reports the four models alongside one another and compare the results."
  },
  {
    "objectID": "problem-set-6.html#postamble",
    "href": "problem-set-6.html#postamble",
    "title": "Problem Set 6",
    "section": "",
    "text": "log close"
  }
]