<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Estimation Methods – EC910</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">EC910</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="./index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-lecture-handouts" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Lecture Handouts</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-lecture-handouts">    
        <li>
    <a class="dropdown-item" href="./handout-1.html">
 <span class="dropdown-text">Handout 1</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./handout-2.html">
 <span class="dropdown-text">Handout 2</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./handout-3.html">
 <span class="dropdown-text">Handout 3</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./handout-4.html">
 <span class="dropdown-text">Handout 4</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./handout-5.html">
 <span class="dropdown-text">Handout 5</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./handout-6.html">
 <span class="dropdown-text">Handout 6</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./handout-7.html">
 <span class="dropdown-text">Handout 7</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-problem-sets" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Problem Sets</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-problem-sets">    
        <li>
    <a class="dropdown-item" href="./problem-sets/ps-1/problem-set-1.html">
 <span class="dropdown-text">Problem Set 1</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./problem-sets/ps-1/problem-set-1-solutions.html">
 <span class="dropdown-text">Problem Set 1 (Solutions)</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./problem-set-2.html">
 <span class="dropdown-text">Problem Set 2</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./problem-set-2-solutions.html">
 <span class="dropdown-text">Problem Set 2 (Solutions)</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./problem-set-3.html">
 <span class="dropdown-text">Problem Set 3</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./problem-set-3-solutions.html">
 <span class="dropdown-text">Problem Set 3 (Solutions)</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./problem-set-4.html">
 <span class="dropdown-text">Problem Set 4</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./problem-set-4-solutions.html">
 <span class="dropdown-text">Problem Set 4 (Solutions)</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./problem-set-5.html">
 <span class="dropdown-text">Problem Set 5</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./problem-set-5-solutions.html">
 <span class="dropdown-text">Problem Set 5 (Solutions)</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./problem-set-6.html">
 <span class="dropdown-text">Problem Set 6</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./problem-set-6-solutions.html">
 <span class="dropdown-text">Problem Set 6 (Solutions)</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-additional-material" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Additional Material</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-additional-material">    
        <li>
    <a class="dropdown-item" href="./material-cef.html">
 <span class="dropdown-text">Conditional Expectation Function</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./material-interpretation.html">
 <span class="dropdown-text">Interpreting Linear Models</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./material-dummy.html">
 <span class="dropdown-text">Dummy Variables</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./material-linearalgebra.html">
 <span class="dropdown-text">Linear Algebra</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./material-inference.html">
 <span class="dropdown-text">Statistical Inference</span></a>
  </li>  
    </ul>
  </li>
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#overview" id="toc-overview" class="nav-link active" data-scroll-target="#overview"><span class="header-section-number">1</span> Overview</a></li>
  <li><a href="#review-of-clrm" id="toc-review-of-clrm" class="nav-link" data-scroll-target="#review-of-clrm"><span class="header-section-number">2</span> Review of CLRM</a></li>
  <li><a href="#ordinary-least-squares" id="toc-ordinary-least-squares" class="nav-link" data-scroll-target="#ordinary-least-squares"><span class="header-section-number">3</span> (Ordinary) Least Squares</a>
  <ul class="collapse">
  <li><a href="#application-to-clrm" id="toc-application-to-clrm" class="nav-link" data-scroll-target="#application-to-clrm"><span class="header-section-number">3.1</span> Application to CLRM</a></li>
  <li><a href="#bias" id="toc-bias" class="nav-link" data-scroll-target="#bias"><span class="header-section-number">3.2</span> Bias</a></li>
  <li><a href="#efficiency" id="toc-efficiency" class="nav-link" data-scroll-target="#efficiency"><span class="header-section-number">3.3</span> Efficiency</a></li>
  <li><a href="#finite-sample-distribution" id="toc-finite-sample-distribution" class="nav-link" data-scroll-target="#finite-sample-distribution"><span class="header-section-number">3.4</span> Finite-sample distribution</a></li>
  <li><a href="#consistency" id="toc-consistency" class="nav-link" data-scroll-target="#consistency"><span class="header-section-number">3.5</span> Consistency</a></li>
  <li><a href="#asymptotic-distribution" id="toc-asymptotic-distribution" class="nav-link" data-scroll-target="#asymptotic-distribution"><span class="header-section-number">3.6</span> Asymptotic Distribution</a></li>
  <li><a href="#other-properties" id="toc-other-properties" class="nav-link" data-scroll-target="#other-properties"><span class="header-section-number">3.7</span> Other properties</a></li>
  </ul></li>
  <li><a href="#method-of-moments" id="toc-method-of-moments" class="nav-link" data-scroll-target="#method-of-moments"><span class="header-section-number">4</span> Method of Moments</a>
  <ul class="collapse">
  <li><a href="#general-setup" id="toc-general-setup" class="nav-link" data-scroll-target="#general-setup"><span class="header-section-number">4.1</span> General setup</a></li>
  <li><a href="#estimator" id="toc-estimator" class="nav-link" data-scroll-target="#estimator"><span class="header-section-number">4.2</span> Estimator</a></li>
  <li><a href="#application-to-clrm-1" id="toc-application-to-clrm-1" class="nav-link" data-scroll-target="#application-to-clrm-1"><span class="header-section-number">4.3</span> Application to CLRM</a></li>
  <li><a href="#consistency-1" id="toc-consistency-1" class="nav-link" data-scroll-target="#consistency-1"><span class="header-section-number">4.4</span> Consistency</a></li>
  <li><a href="#asymptotic-normality" id="toc-asymptotic-normality" class="nav-link" data-scroll-target="#asymptotic-normality"><span class="header-section-number">4.5</span> Asymptotic Normality</a></li>
  <li><a href="#additional-comments" id="toc-additional-comments" class="nav-link" data-scroll-target="#additional-comments"><span class="header-section-number">4.6</span> Additional comments</a></li>
  </ul></li>
  <li><a href="#maximum-likelihood" id="toc-maximum-likelihood" class="nav-link" data-scroll-target="#maximum-likelihood"><span class="header-section-number">5</span> Maximum Likelihood</a>
  <ul class="collapse">
  <li><a href="#general-setup-1" id="toc-general-setup-1" class="nav-link" data-scroll-target="#general-setup-1"><span class="header-section-number">5.1</span> General setup</a></li>
  <li><a href="#estimator-1" id="toc-estimator-1" class="nav-link" data-scroll-target="#estimator-1"><span class="header-section-number">5.2</span> Estimator</a></li>
  <li><a href="#application-to-clrm-2" id="toc-application-to-clrm-2" class="nav-link" data-scroll-target="#application-to-clrm-2"><span class="header-section-number">5.3</span> Application to CLRM</a></li>
  <li><a href="#consistency-2" id="toc-consistency-2" class="nav-link" data-scroll-target="#consistency-2"><span class="header-section-number">5.4</span> Consistency</a></li>
  <li><a href="#asymptotic-normality-1" id="toc-asymptotic-normality-1" class="nav-link" data-scroll-target="#asymptotic-normality-1"><span class="header-section-number">5.5</span> Asymptotic Normality</a></li>
  <li><a href="#additional-comments-1" id="toc-additional-comments-1" class="nav-link" data-scroll-target="#additional-comments-1"><span class="header-section-number">5.6</span> Additional comments</a></li>
  </ul></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references"><span class="header-section-number">6</span> References</a></li>
  </ul>
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="handout-3.pdf"><i class="bi bi-file-pdf"></i>PDF</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Estimation Methods</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="overview" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="overview"><span class="header-section-number">1</span> Overview</h2>
<p>In this handout we will look at several approaches to generate estimators:</p>
<ul>
<li>Least Squares</li>
<li>Method of Moments</li>
<li>Maximum Likelihood</li>
</ul>
<p>We will discuss each approach in the context of the Classical Linear Regression Model discussed in <a href="https://neillo88.github.io/warwick-ec910/lecture-1.html">Lecture 1</a>. You may also wish to revise the notes on <a href="https://neillo88.github.io/warwick-ec910/material-linearalgebra.html">Linear Algrebra</a>.</p>
<p>Further reading can be found in:</p>
<ul>
<li>Section 5.6 of <span class="citation" data-cites="cameron2005">Cameron and Trivedi (<a href="#ref-cameron2005" role="doc-biblioref">2005</a>)</span></li>
<li>Section 6.1 of <span class="citation" data-cites="verbeek2017">Verbeek (<a href="#ref-verbeek2017" role="doc-biblioref">2017</a>)</span></li>
</ul>
</section>
<section id="review-of-clrm" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="review-of-clrm"><span class="header-section-number">2</span> Review of CLRM</h2>
<p>The classical linear regression model is states that the conditional expectation function <span class="math inline">\(E[Y_i|X_i]\)</span> is linear in parameters.</p>
<p>For the random sample <span class="math inline">\(i=1,...,n\)</span>,</p>
<p><span class="math display">\[
Y_i = X_i'\beta + u_i
\]</span> where <span class="math inline">\(X_i\)</span> is a random k-dimensional vector (k-vector) and <span class="math inline">\(\beta\)</span> a non-random k-vector of population parameters. Both <span class="math inline">\(Y_i\)</span> and <span class="math inline">\(u_i\)</span> are random scalars.</p>
<p>As we saw, we can stack each observation into a column vector:</p>
<p><span class="math display">\[
Y =\underbrace{\begin{bmatrix}Y_1 \\ Y_2 \\ \vdots \\ Y_n \end{bmatrix}}_{n\times 1} = \underbrace{\begin{bmatrix}X_1'\beta \\ X_2'\beta \\ \vdots \\ X_n'\beta\end{bmatrix}}_{n\times 1}  + \underbrace{\begin{bmatrix}u_1 \\ u_2 \\ \vdots \\ u_n\end{bmatrix}}_{n\times 1}  
= \underbrace{\begin{bmatrix}X_{11} &amp; X_{12} &amp; \cdots &amp; X_{1k}\\ X_{21} &amp; X_{22} &amp;&amp; \\ \vdots &amp; &amp; \ddots &amp; \\ X_{n1} &amp; &amp; &amp; X_{nk} \end{bmatrix}}_{n\times k} \underbrace{\begin{bmatrix}\beta_1 \\ \beta_2 \\ \vdots \\ \beta_k\end{bmatrix}}_{k\times 1} + \begin{bmatrix}u_1 \\ u_2 \\ \vdots \\ u_n\end{bmatrix} = X\beta + u
\]</span></p>
<p>where <span class="math inline">\(X\)</span> is now a <span class="math inline">\(n\times k\)</span> random matrix, but <span class="math inline">\(\beta\)</span> remains a non-random k-vector of population parameters.</p>
<div class="callout callout-style-default callout-tip callout-titled" title="Data as a Matrix">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Data as a Matrix
</div>
</div>
<div class="callout-body-container callout-body">
<p>If you have any experience working datasets in Stata/R, you will know that they tend to have a rectangular structure: each row <em>typically</em> represents an observation and each column a variable. This is the structure depicted above in matrix notation: each row of the <span class="math inline">\(X\)</span> matrix depicts an observation and each column a regressor. The dataset you are using contains is a matrix of observations for both the outcome variable and regressors: <span class="math inline">\([Y,X,]\)</span>. Of course, we do not observe the error term.</p>
</div>
</div>
<p>In this section, we will employ the CLRM assumptions as we discuss the three approaches to estimation.</p>
</section>
<section id="ordinary-least-squares" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="ordinary-least-squares"><span class="header-section-number">3</span> (Ordinary) Least Squares</h2>
<p>The Ordinary Least Squares (OLS) estimator is the ‘work-horse’ of applied economics research.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> It is not the only Least Squares estimator, but as the simplest case is the most useful place to start. Other Least Squares estimators include Weighted Least Squares (WLS), (Feasible) Generalized Least Squares (GLS), Non-Linear Least Squares and Two-Stage Least Squares (2SLS).</p>
<p>OLS is “ordinary” in the sense that there is no additional features to the method. For example, WLS applies a unique weight to each observation while OLS weights each observation equally. While OLS is arguably ‘vanilla’ in this way, it is efficient (as we shall see).</p>
<p>In general, LS estimators minimize a measure of ‘distance’ between the observed outcomes and the fitted values of the model. The measure of distance is sum of squared deviations (squared Euclidean or <span class="math inline">\(\ell_2\)</span> norm).<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<section id="application-to-clrm" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="application-to-clrm"><span class="header-section-number">3.1</span> Application to CLRM</h3>
<p>In the case of OLS estimator to the CLRM, the goal is to find the <span class="math inline">\(b\)</span>-vector that minimizes,</p>
<p><span class="math display">\[
\sum_{i=1}^n(Y_i-\underbrace{\tilde{Y}_i}_{X_i'b})^2 = \sum_{i=1}^n\tilde{u}_i^2
\]</span> This sum-of-squares can be written as the inner product of two-vectors:<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></p>
<p><span class="math display">\[
\sum_{i=1}^n\tilde{u}_i^2 = \tilde{u}'\tilde{u} = (Y-Xb)'(Y-Xb)
\]</span></p>
<p>Applying the rules of matrix transposition, the inner product of these two matrices is given by,</p>
<p><span class="math display">\[
(Y'-b'X')(Y-Xb) = Y'Y -b'X'Y-Y'Xb+b'X'Xb
\]</span> Since all terms are scalars, <span class="math inline">\(b'X'Y=Y'Xb\)</span>; which then gives us, <span class="math display">\[
Y'Y -2b'X'Y+b'X'Xb
\]</span> Thus, the (ordinary) least-squares estimator the vector that solves this linear expression. <span class="math display">\[
\hat{\beta}^{OLS} = \underset{b}{\text{arg min}}\quad Y'Y -2b'X'Y+b'X'Xb
\]</span></p>
<p>Using the rules of vector differentiation (see <a href="https://neillo88.github.io/warwick-ec910/material-linearalgebra.html">Linear Algrebra</a>) we can find the first order conditions:</p>
<p><span class="math display">\[
-2X'Y +2X'X\hat{\beta}^{OLS}= 0
\]</span> If the <span class="math inline">\(X\)</span> matrix is full rank (=<span class="math inline">\(k\)</span>), then <span class="math inline">\(X'X\)</span> is non-singular and its inverse exists. Recall, this was one of the CLRM assumptions. Then, <span class="math display">\[
\hat{\beta}^{OLS} = (X'X)^{-1}X'Y
\]</span></p>
</section>
<section id="bias" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="bias"><span class="header-section-number">3.2</span> Bias</h3>
<p>Is the OLS estimator unbiased? The answer will depend on the assumption of the model. Here, we have assumed that the model being estimated is a CLRM. This means that we have assumed conditional mean independence of the error term:</p>
<p><span class="math display">\[
E[u|X] = 0
\]</span> The OLS-estimator is given by,</p>
<p><span class="math display">\[
\hat{\beta}^{OLS} = (X'X)^{-1}X'Y = (X'X)^{-1}X'(X\beta+u) = \underbrace{(X'X)^{-1}X'X}_{I_n}\beta+(X'X)^{-1}X'u=\beta+(X'X)^{-1}X'u
\]</span> plugging in the definition of <span class="math inline">\(Y\)</span> from the model.</p>
<p>Hence,</p>
<p><span class="math display">\[
E[\hat{\beta}^{OLS}|X] = E[\beta+(X'X)^{-1}X'u|X] = \beta+E[(X'X)^{-1}X'u|X]
\]</span> Here we apply the linearity of the expectation operator and the factor that <span class="math inline">\(\beta\)</span> is a non-random vector. Next, we exploit the fact that conditional on <span class="math inline">\(X\)</span>, any function of <span class="math inline">\(X\)</span> is non-random and can come out of the expectation operator.</p>
<p><span class="math display">\[
E[\hat{\beta}^{OLS}|X] = \beta+(X'X)^{-1}X'\underbrace{E[u|X]}_{=0} = \beta
\]</span></p>
<p>Notice, we require the stronger assumption of conditional mean independence, <span class="math inline">\(E[u|X]=0\)</span>. Uncorrelateness, <span class="math inline">\(E[X'u]=0\)</span>, is insufficient for unbiasedness.</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p>Notice, unbiasedness depends on the assumptions of the model and not any properties of the estimator. The estimator is simply a calculation using observed data. The properties and interpretation of this computation depend on the <em>assumptions</em> we make regarding the underlying model.</p>
</div>
</div>
<p>It is also worth noting that the unbiasedness of the OLS estimator does NOT depend on any assumptions regarding the variance or distribution of the error term.</p>
</section>
<section id="efficiency" class="level3" data-number="3.3">
<h3 data-number="3.3" class="anchored" data-anchor-id="efficiency"><span class="header-section-number">3.3</span> Efficiency</h3>
<p>The OLS estimator is a <span class="math inline">\(k\)</span>-dimensional random vector. The variance of this vector is a <span class="math inline">\(k\times k\)</span> variance-covariance matrix.</p>
<p><span class="math display">\[
Var(\hat{\beta}) = E\big[\underbrace{(\hat{\beta}-E[\hat{\beta}])}_{k\times 1}\underbrace{(\hat{\beta}-E[\hat{\beta}])'}_{1\times k}\big]
\]</span> The off-diagonals of the matrix are the covariances: <span class="math inline">\(Cov(\hat{\beta}_j,\hat{\beta}_k)\)</span> for <span class="math inline">\(j\neq k\)</span>.</p>
<p>We have just shown that <span class="math inline">\(E[\hat{\beta}]=\beta\)</span> and</p>
<p><span class="math display">\[
\hat{\beta}^{OLS} -\beta=(X'X)^{-1}X'u
\]</span></p>
<p>Thus, the (conditional) variance of this estimator is then given by,</p>
<p><span class="math display">\[
\begin{aligned}
Var(\hat{\beta}^{OLS}|X) =&amp; E\big[(X'X)^{-1}X'uu'X(X'X)^{-1}|X\big] \\
=&amp; (X'X)^{-1}X'E[uu'|X]X(X'X)^{-1} \\
=&amp; (X'X)^{-1}X'Var(u|X)X(X'X)^{-1}
\end{aligned}
\]</span></p>
<p>The variance of the estimator depends on the variance of the error term, the unexplained part of the model. In order to any further expressions for this variance calculation, we need to go back to the model. What assumptions did we make concerning the variance in the CLRM?</p>
<p>Under the assumption CLRM 3 of homoskedasticity,</p>
<p><span class="math display">\[
Var(u|X) = \sigma^2 I_n = \begin{bmatrix}\sigma^2&amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; \sigma^2 &amp; &amp; \\
\vdots &amp; &amp; \ddots &amp; \\
0 &amp; &amp; &amp; \sigma^2\end{bmatrix}
\]</span></p>
<p>the above expression simplies to</p>
<p><span class="math display">\[
\begin{aligned}
Var(\hat{\beta}^{OLS}|X) =&amp; (X'X)^{-1}X'\sigma^2I_nX(X'X)^{-1} \\
=&amp;\sigma^2(X'X)^{-1}X'X(X'X)^{-1} \\
=&amp;\sigma^2(X'X)^{-1}
\end{aligned}
\]</span></p>
<p>If we made a different assumption of heteroskedasticty (CLRM 3), then</p>
<p><span class="math display">\[
Var(u|X) = \begin{bmatrix}\sigma^2_1&amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; \sigma^2_2 &amp; &amp; \\
\vdots &amp; &amp; \ddots &amp; \\
0 &amp; &amp; &amp; \sigma^2_n\end{bmatrix} = \Omega
\]</span></p>
<p>the variance matrix does not reduce to a scalar multiplied by the identity matrix. And,</p>
<p><span class="math display">\[
Var(\hat{\beta}^{OLS}|X) = (X'X)^{-1}X'\Omega X(X'X)^{-1}
\]</span></p>
<p>This is commonly referred to as a ‘sandwich’ formula, given the way <span class="math inline">\(Var(u|X)=\Omega\)</span> is sandwiched between two linear transformations. The Eicker-Huber-White estimator for heteroskedastic standard errors of <span class="math inline">\(\hat{\beta}^{OLS}\)</span> replaces <span class="math inline">\(Var(u|X)=E[uu'|X]\)</span> with <span class="math inline">\(\hat{u}\hat{u}'\)</span>, the OLS residuals.</p>
</section>
<section id="finite-sample-distribution" class="level3" data-number="3.4">
<h3 data-number="3.4" class="anchored" data-anchor-id="finite-sample-distribution"><span class="header-section-number">3.4</span> Finite-sample distribution</h3>
<p>The finite sample distribution of the OLS estimator depends on the assumptions of the model. Under CLRM 5,</p>
<p><span class="math display">\[
u|X \sim N(0,\sigma^2 I_n)
\]</span> And we have already shown that the OLS estimator is simply a linear transformation of the error term, <span class="math display">\[
\hat{\beta}^{OLS}=\beta+(X'X)^{-1}X'u
\]</span></p>
<p>Then, using the properties of the Normal distribution<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></p>
<p><span class="math display">\[
\hat{\beta}^{OLS}|X=N\big(\beta,\sigma^2(X'X)^{-1}\big)
\]</span> assuming homoskedasticity. With heteroskedastic variance, you simply change the variance, as the assumption has no implications for biasedness.</p>
</section>
<section id="consistency" class="level3" data-number="3.5">
<h3 data-number="3.5" class="anchored" data-anchor-id="consistency"><span class="header-section-number">3.5</span> Consistency</h3>
<p>Recall from <a href="https://neillo88.github.io/warwick-ec910/lecture-2.html">Lecture 2</a> that an estimator is consistent if it converges in probability to the parameter. In this case, we want to show that</p>
<p><span class="math display">\[
\hat{\beta}^{OLS}\rightarrow_p \beta\qquad\text{as}\qquad n\rightarrow \infty
\]</span> Using the derivation <span class="math inline">\(\hat{\beta}^{OLS} -\beta=(X'X)^{-1}X'u\)</span>, we need to show that <span class="math inline">\((X'X)^{-1}X'u \rightarrow_p 0\)</span>. To emphasize the fact that <span class="math inline">\(\hat{\beta}\)</span> is a function of the sample size, I am going to switch to the notation <span class="math inline">\(\hat{\beta}_n\)</span> for this section.</p>
<p>For the consistency of the OLS estimator we require a few assumptions,</p>
<ul>
<li><p>CLRM 1: linear in parameters</p></li>
<li><p>CLRM 2<sup>b</sup>: uncorrelatedness, <span class="math inline">\(E[X_iu_i]=0\)</span></p></li>
<li><p>CLRM 4: <span class="math inline">\(rank(X) = k\)</span></p></li>
<li><p>CLRM 6: data is iid</p></li>
<li><p>(NEW) CLRM 7: <span class="math inline">\(E[X_iX_i']\)</span> is a finite, positive-definite matrix.</p></li>
</ul>
<p>We begin by re-writing the expression, <span class="math inline">\(\hat{\beta}^{OLS}=\beta+(X'X)^{-1}X'u\)</span> in summation notation and then scaling by <span class="math inline">\(n\)</span>,</p>
<p><span class="math display">\[
\hat{\beta}_n=\beta+\bigg(\sum_{i=1}^nX_iX_i'\bigg)^{-1}\sum_{i=1}^nX_iu_i = \beta+\bigg(n^{-1}\sum_{i=1}^nX_iX_i'\bigg)^{-1}n^{-1}\sum_{i=1}^nX_iu_i
\]</span></p>
<p>By the WLLN,<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a></p>
<p><span class="math display">\[
n^{-1}\sum_{i=1}^nX_iu_i\rightarrow_p E[X_1u_1] = 0
\]</span></p>
<p>Similarly, by WLLN, underpinned by finiteness of <span class="math inline">\(E[X_1X_1']\)</span> (CLRM 7), <span class="math display">\[
n^{-1}\sum_{i=1}^nX_iX_i'\rightarrow_p E[X_1X_1']
\]</span></p>
<p>Since <span class="math inline">\(E[X_1X_1']\)</span> is also positive definite (CLRM 7), then by Slutzky’s Theorem</p>
<p><span class="math display">\[
\bigg(n^{-1}\sum_{i=1}^nX_iX_i'\bigg)^{-1}\rightarrow_p \big(E[X_1X_1']\big)^{-1}
\]</span></p>
<p>Hence, by Slutzky’s Theorem, which says that the product of two consistent estimators converges in probability to the product of their targets,</p>
<p><span class="math display">\[
(X'X)^{-1}X'u \rightarrow_p \big(E[X_1X_1']\big)^{-1}E[X_1u_1] = 0
\]</span></p>
<p>Thus,</p>
<p><span class="math display">\[
p \lim(\hat{\beta}_n) = \beta
\]</span></p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p>Scaling each term by <span class="math inline">\(n\)</span> is very important, as without it, both terms do not have a finite mean. Consider, under iid,</p>
<p><span class="math display">\[
E\bigg[\sum_{i=1}^nX_iX_i'\bigg] = nE[X_1X_1']
\]</span> while, <span class="math display">\[
E\bigg[n^{-1}\sum_{i=1}^nX_iX_i'\bigg] = E[X_1X_1']
\]</span></p>
</div>
</div>
</section>
<section id="asymptotic-distribution" class="level3" data-number="3.6">
<h3 data-number="3.6" class="anchored" data-anchor-id="asymptotic-distribution"><span class="header-section-number">3.6</span> Asymptotic Distribution</h3>
<p>To derive the asymptotic distribution of the OLS estimator we will need to apply the Central Limit Theorem. We will need to scale by <span class="math inline">\(\sqrt{n}\)</span>, to derive the distribution of,</p>
<p><span class="math display">\[
\sqrt{n}(\hat{\beta}_n-\beta)
\]</span></p>
<p>Recall from <a href="https://neillo88.github.io/warwick-ec910/lecture-2.html">Lecture 2</a> that by Cramer’s Convergence Theorem, <span class="math inline">\(Y_nX_n\rightarrow_d cX\)</span> where <span class="math inline">\(X_n\rightarrow_d\)</span> and <span class="math inline">\(Y_n\rightarrow_p c\)</span>. This result holds for the case where <span class="math inline">\(Y_n\)</span> is a random matrix. <span class="math display">\[
\sqrt{n}(\hat{\beta}_n-\beta) = \big(n^{-1}X'X\big)^{-1}n^{-1/2}Xu
\]</span> We have already established that, <span class="math display">\[
\big(n^{-1}X'X\big)^{-1}\rightarrow_p \big(E[X_1X_1']\big)^{-1}
\]</span> under assumptions CLRM 1, 2<sup>b</sup>, 6, and 7.</p>
<p>We therefore need to consider the asymptotic distribution of <span class="math inline">\(n^{-1/2}Xu\)</span>. By CLRM 2<sup>b</sup> <span class="math inline">\(E[X_1u_1]=0\)</span>, fulfilling one of the CLT conditions. We then need the second moment to be finite: <span class="math inline">\(Var(X_1u_1) = E[u_1^2X_1X_1']\)</span>. This is a <span class="math inline">\(k\times k\)</span> matrix.</p>
<p>We will need to make some additional assumptions:</p>
<ul>
<li>(NEW) CLRM 8: <span class="math inline">\(E[u_1^2X_1X_1']\)</span> is a finite positive-definite matrix.<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a></li>
</ul>
<p>Under assumptions CLRM 1, 2, 6, and 8, by CLT,</p>
<p><span class="math display">\[
n^{-1/2}Xu\rightarrow N(0,E[u_1^2 X_1 X_1'])
\]</span> There, <span class="math display">\[
\sqrt{n}(\hat{\beta}_n-\beta) \rightarrow_d N\bigg(0,\big(E[X_1X_1']\big)^{-1}E[u_1^2X_1X_1']\big(E[X_1X_1']\big)^{-1}\bigg)
\]</span> Under the homoskedasticity, <span class="math inline">\(E[u_1^2X_1X_1']=\sigma^2 E[X_1X_1']\)</span>, giving us, <span class="math display">\[
\sqrt{n}(\hat{\beta}_n-\beta) \rightarrow_d N\bigg(0,\sigma^2\big(E[X_1X_1']\big)^{-1}\bigg)
\]</span> We can approximate the asymptotic distribution <span class="math inline">\(\hat{\beta}_n\)</span> by multiplying by <span class="math inline">\(\sqrt{n}\)</span> and replacing <span class="math inline">\(\big(E[X_1X_1']\big)^{-1}\)</span> with the approximation <span class="math inline">\(\big(n^{-1}X'X\big)^{-1}\)</span>. <span class="math display">\[
\hat{\beta}_n \overset{a}{\sim}N\big(\beta,\sigma^2(X'X)^{-1}\big)
\]</span> The <span class="math inline">\(n\)</span> in the variance formula is cancelled out by the pre-multiply of <span class="math inline">\(\sqrt{n}\)</span>.</p>
</section>
<section id="other-properties" class="level3" data-number="3.7">
<h3 data-number="3.7" class="anchored" data-anchor-id="other-properties"><span class="header-section-number">3.7</span> Other properties</h3>
<ul>
<li>Among the class of unbiased linear estimators of the CLRM, the OLS is the Best Linear Unbiased Estimator (BLUE). “Best” here means lowest variance. Can you show this?</li>
</ul>
</section>
</section>
<section id="method-of-moments" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="method-of-moments"><span class="header-section-number">4</span> Method of Moments</h2>
<p>The method of moments (MM) approach is to match assumed ‘moments’ given by the model with their sample analogue. This is a very general approach and is used extensively in applied macroeconomics, where a structural model gives rise to moments between economic variables that can be matched in the data.</p>
<p>A general principle of MM is that the number of moments, <span class="math inline">\(m\)</span>, must be <span class="math inline">\(\geq k\)</span>, the number of parameters being estimated. This akin to saying, the number of equations must be greater or equal to the number of variables being solved for. If the number of moments exceeds the number of parameters, we say that the model is <em>overidentified</em>. In the case of instrumental variables, overidentification allows you two test certain model assumptions.</p>
<p>In term 2, you will study instrumental variables which adopts a GMM approach to estimation. MM approaches are also used extensively in time series. For now, we will apply the MM approach to the CLRM.</p>
<section id="general-setup" class="level3" data-number="4.1">
<h3 data-number="4.1" class="anchored" data-anchor-id="general-setup"><span class="header-section-number">4.1</span> General setup</h3>
<p>The observed data is given by, <span class="math inline">\(W_1,...,W_n\)</span>, read <span class="math inline">\(W_i\)</span> is a <span class="math inline">\(p\)</span>-dimension random vector. Let <span class="math inline">\(g(W_i,\theta)\)</span> be a <span class="math inline">\(l\)</span>-dimension function (i.e.&nbsp;<span class="math inline">\(\in \mathbf{R}^l\)</span>) and <span class="math inline">\(\theta\in\mathbf{R}^k\)</span>:</p>
<p><span class="math display">\[
g(W_i,\theta) = \begin{bmatrix}g_1(W_i,\theta) \\ \vdots \\g_l(W_i,\theta)\end{bmatrix}
\]</span> We assume that the true value of the parameter <span class="math inline">\(\theta_0\in\Theta\subset \mathbf{R}^{k}\)</span> satisifies the condition, <span class="math display">\[
E\big[g(W_i,\theta_0)\big] = 0
\]</span> We say that the model is <em>identified</em> if there is a unique solution to the above equations. That is, <span class="math inline">\(E\big[g(W_i,\theta)\big] = E\big[g(W_i,\tilde{\theta})\big] = 0\;\Rightarrow\;\theta=\tilde{\theta}\)</span>. A necessary condition for identifiction is <span class="math inline">\(l\geq k\)</span>; i.e.&nbsp;the number of equations is at least as large as the number of unknown parameters. A model can be underidentified, which typically means that there is not a unique solution for some of the parameters.</p>
</section>
<section id="estimator" class="level3" data-number="4.2">
<h3 data-number="4.2" class="anchored" data-anchor-id="estimator"><span class="header-section-number">4.2</span> Estimator</h3>
<p>The basic principle of MM estimation is to replacing the expectation operator with the average function and solve for <span class="math inline">\(\hat{\theta}\)</span>. <span class="math display">\[
n^{-1}\sum_{i=1}^n g(W_i,\hat{\theta}^{MM}) = 0
\]</span> However, this only works when <span class="math inline">\(l=k\)</span> (exactly identified cases). For <span class="math inline">\(l&gt;k\)</span> (overidentified cases) there is no unique vector that solves all <span class="math inline">\(l\)</span> equations.</p>
<p>The <em>Generalized</em> Method of Moments (GMM) approach applies a set of weights to the minimization problem. Let <span class="math inline">\(A_n\)</span> be a <span class="math inline">\(l\times l\)</span> weight matrix, such that <span class="math inline">\(A_n\rightarrow_p A\)</span>. Then,</p>
<p><span class="math display">\[
\hat{\theta}^{GMM} = \underset{\theta\in\Theta}{\text{arg min}}\;\bigg|\bigg|A_n n^{-1}\sum_{i=1}^n g(W_i,\hat{\theta}^{MM})\bigg|\bigg|^2
\]</span></p>
<p>Here, <span class="math inline">\(||v||\)</span> denotes the Euclidean norm of vector <span class="math inline">\(v\)</span>: <span class="math inline">\(||v|| = \sqrt{v'v}\)</span>.</p>
</section>
<section id="application-to-clrm-1" class="level3" data-number="4.3">
<h3 data-number="4.3" class="anchored" data-anchor-id="application-to-clrm-1"><span class="header-section-number">4.3</span> Application to CLRM</h3>
<p>Assumption CLRM 2<sup>b</sup> tells us that the regressors are uncorrelated with the error term. <span class="math display">\[
E[X_iu_i] = 0
\]</span> This is the moment that gives rise to identification in the CLRM. Given CLRM 1, we can replace the error term in the above moment with <span class="math inline">\(Y_i-X_i'\beta\)</span>. <span class="math display">\[
E[X_i(Y_i-X_i'\beta)] = 0
\]</span> Thus, the <span class="math inline">\(g(W_i,\beta)=X_i(Y_i-X_i'\beta)\)</span> for <span class="math inline">\(W_i = [Y_i,X_i']'\)</span>.</p>
<p>How many equations are there? Recall, <span class="math inline">\(X_i\)</span> is a <span class="math inline">\(k\)</span>-dimension random vector. So,</p>
<p><span class="math display">\[
E[X_i(Y_i-X_i'\beta)] = \begin{bmatrix}E[X_{i1}(Y_i-X_i'\beta)]\\E[X_{i2}(Y_i-X_i'\beta)]\\ \vdots \\ E[X_{ik}(Y_i-X_i'\beta)]\end{bmatrix}=0
\]</span> The are <span class="math inline">\(k\)</span>-moments (or equations), meaning that we can estimate <em>up to</em> <span class="math inline">\(k\)</span> parameters. In this instance, we have a failure of identification if <span class="math inline">\(rank(E[X_iX_i'])&lt;k\)</span>; which is required for the invertibility of <span class="math inline">\(E[X_iX_i']\)</span>. This condition is met by assumption CLRM 4; ensuring exact identification.</p>
<p>The MM estimator for the CLRM is then given by the solution to,</p>
<p><span class="math display">\[
n^{-1}\sum_{i=1}^n X_i(Y_i-X_i'\hat{\beta}^{MM}) = 0
\]</span></p>
<p>The solution is equivalent to the OLS estimator: <span class="math display">\[
\hat{\beta}^{MM} = \bigg(n^{-1}\sum_{i=1}^n X_iX_i'\bigg)^{-1}n^{-1}\sum_{i=1}^nX_iY_i = \big(X'X\big)^{-1}X'Y = \hat{\beta}^{OLS}
\]</span></p>
</section>
<section id="consistency-1" class="level3" data-number="4.4">
<h3 data-number="4.4" class="anchored" data-anchor-id="consistency-1"><span class="header-section-number">4.4</span> Consistency</h3>
<p>As we have already established the consistency of the OLS estimator, we will briefly review the case of the GMM estimator here. A more detailed discussion will be provided in term 2.</p>
<p>Recall, the assumption <span class="math inline">\(A_n\rightarrow_p A\)</span>. Then,</p>
<p><span class="math display">\[
\bigg|\bigg|A_n n^{-1}\sum_{i=1}^n g(W_i,\theta)\bigg|\bigg|^2\rightarrow_p \big|\big|A E\big[ g(W_i,\theta)\big]\big|\big|^2
\]</span> In instance where identification is exact/unique, <span class="math inline">\(E\big[ g(W_i,\theta)\big]=0\iff\theta=\theta_0\)</span>. Which is to say that the true value of <span class="math inline">\(\theta\)</span> is the unique minimizer.</p>
<p>Then,</p>
<p><span class="math display">\[
\begin{aligned}
\hat{\theta}^{GMM} =&amp;\underset{\theta\in\Theta}{\text{arg min}}\; \bigg|\bigg|A_n n^{-1}\sum_{i=1}^n g(W_i,\theta)\bigg|\bigg|^2 \\
\rightarrow_p&amp;\underset{\theta\in\Theta}{\text{arg min}}\;\big|\big|A E\big[ g(W_i,\theta)\big]\big|\big|^2 \\
=&amp;\theta_0
\end{aligned}
\]</span> The formal proof requires a number of additional regularity assumptions; including, the compactness of <span class="math inline">\(\Theta\)</span>.</p>
</section>
<section id="asymptotic-normality" class="level3" data-number="4.5">
<h3 data-number="4.5" class="anchored" data-anchor-id="asymptotic-normality"><span class="header-section-number">4.5</span> Asymptotic Normality</h3>
<p>The GMM estimator is asymptotically normal.</p>
<p><span class="math display">\[
\sqrt{n}\big(\hat{\theta}^{GMM}-\theta_0\big)\rightarrow_d N(0,V)
\]</span> where, <span class="math display">\[
\begin{aligned}
V =&amp; (Q'A'AQ)^{-1}QA'A\Omega A'AQ (Q'A'AQ)^{-1} \\
Q =&amp; E\bigg[\frac{\partial g(W_i,\theta_0)}{\partial \theta'}\bigg] \\
\Omega =&amp; E\big[ g(W_i,\theta)g(W_i,\theta)'\big]
\end{aligned}
\]</span> Where does this result come from? We won’t go through the proof in details. However, it starts from the FOCs. The GMM estimator solves,</p>
<p><span class="math display">\[
\bigg[\underbrace{n^{-1}\sum_{i=1}^n\frac{\partial g(W_i,\hat{\theta}^{GMM})}{\partial \theta'}}_{Q_n\big(\hat{\theta}^{GMM}\big)}\bigg]'A_n'A_n n^{-1}\sum_{i=1}^{n}g(W_i,\hat{\theta}^{GMM}) = 0
\]</span> In the above expression, the matrix of derivatives will converge (under some regularity conditions) in probability: <span class="math inline">\(Q_n\big(\hat{\theta}^{GMM}\big)\rightarrow_p Q\)</span>. Second, since <span class="math inline">\(E\big[ g(W_i,\theta)\big]=0\)</span>, by CLT we know,</p>
<p><span class="math display">\[
n^{-1/2}\sum_{i=1}^{n}g(W_i,\theta_0)\rightarrow_d N(0,\underbrace{E\big[ g(W_i,\theta)g(W_i,\theta)'\big]}_\Omega)
\]</span> We can therefore see where the components of the variance come from. The proof requires a bit more work. First, we need the distribution of <span class="math inline">\(\sqrt{n}\big(\hat{\theta}^{GMM}-\theta_0\big)\)</span>, not <span class="math inline">\(n^{-1/2}\sum_{i=1}^{n}g(W_i,\theta_0)\)</span>. Second, the FOCs contain <span class="math inline">\(n^{-1}\sum_{i=1}^{n}g(W_i,\hat{\theta}^{GMM})\)</span> and not <span class="math inline">\(n^{-1}\sum_{i=1}^{n}g(W_i,\theta_0)\)</span>.</p>
<p>This is resolve using a mean value expansion:</p>
<p><span class="math display">\[
g(W_i,,\hat{\theta}^{GMM}) = g(W_i,\theta_0) + \frac{\partial g(W_i,\hat{\theta}^*)}{\partial \theta'}\big(\hat{\theta}^{GMM}-\theta_0\big)
\]</span> Plugging this expansion into the FOCs, you can rearrange to solve,</p>
<p><span class="math display">\[
\sqrt{n}\big(\hat{\theta}^{GMM}-\theta_0\big) = -[Q_n\big(\hat{\theta}^{GMM}\big)'A_n'A_nQ_n\big(\hat{\theta}^*\big)]^{-1}Q_n\big(\hat{\theta}^{GMM}\big)'A_n'A_nn^{-1/2}\sum_{i=1}^{n}g(W_i,\theta_0)
\]</span> Since <span class="math inline">\(\hat{\theta}^*\)</span> is a mean value, it is a also consistent and <span class="math inline">\(Q_n\big(\hat{\theta}^*\big)\rightarrow_p Q\)</span>.</p>
</section>
<section id="additional-comments" class="level3" data-number="4.6">
<h3 data-number="4.6" class="anchored" data-anchor-id="additional-comments"><span class="header-section-number">4.6</span> Additional comments</h3>
<ul>
<li><p>The targetted moments may be highly non-linear. For example, the Lucas Model pins down the rate of return on a risky asset <span class="math inline">\(R_{j,t}\)</span> using the relative utility of consumption today and tomorrow. The equilibrium condition for assets <span class="math inline">\(j=1,...,m\)</span> is given by,</p>
<p><span class="math display">\[
E\bigg[\underbrace{\delta\bigg(\frac{C_{t+1}}{C_t}\bigg)^{-\alpha}(1+R_{j,t})-1}_{g(W_{j,t},\theta)}\bigg]=0
\]</span></p>
<p>where <span class="math inline">\(W_{j,t}=[C_t,C_{t+1},R_{j,t}]'\)</span> and <span class="math inline">\(\theta = [\alpha,\delta]'\)</span>. As the moment must hold for each asset, <span class="math inline">\(\theta\)</span> is identified so long as <span class="math inline">\(m\geq 2\)</span>.</p>
<p>Given the non-linearity of the <span class="math inline">\(g\)</span>-function, there is no closed for solution. Instead, the GMM estimator must be solved for using numerical optimization.</p></li>
<li><p>Some (macroeconomic) models are not identified (or underidentified). For example, a simple RBC model (with a random government component) yields the following moment condition from Euler equation,<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a></p>
<p><span class="math display">\[
E\bigg[\underbrace{\beta\frac{C_{t+1}}{C_t}\big(f_K + (1-\delta)\big)-1}_{g(W_i,\theta)}\bigg]=0
\]</span> where <span class="math inline">\(W_{j,t}=[C_t,C_{t+1},f_K]\)</span> and <span class="math inline">\(\theta = [\beta,\delta]\)</span>. In this application, there is only a single moment but two unknown parameters. For this reason, you will need to find an additional instrument that introduces an additional moment to identify both parameters.</p></li>
</ul>
</section>
</section>
<section id="maximum-likelihood" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="maximum-likelihood"><span class="header-section-number">5</span> Maximum Likelihood</h2>
<p>Maximum Likelihood (ML or MLE) are a general class of estimators that exploit a knowledge of the underlying distribution of unobservables in the model. As the name suggests, the goal will be to maximize the likelihood (i.e.&nbsp;probability) of observing the a given sample of data, given the assumed distribution of the data, governed by a fixed set of parameters.</p>
<section id="general-setup-1" class="level3" data-number="5.1">
<h3 data-number="5.1" class="anchored" data-anchor-id="general-setup-1"><span class="header-section-number">5.1</span> General setup</h3>
<p>Consider an iid random sample of data: <span class="math inline">\(W_1,...,W_n\)</span>. We will assume that the data is drawn from a <strong>known</strong> distribution, <span class="math inline">\(f(w_i;\theta)\)</span>, where <span class="math inline">\(\theta\in\Theta\subset \mathbf{R}^k\)</span> is an unknown vector of population parameters.<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a></p>
<div class="callout callout-style-default callout-note callout-titled" title="Notation">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Notation
</div>
</div>
<div class="callout-body-container callout-body">
<p>The notation used to describe ML estimation varies quite a bit across texts. One key difference appears to be how to denote a parameterized distribution. The density function, <span class="math inline">\(f(w_i;\theta)\)</span>, is the density at <span class="math inline">\(w_i\)</span> (the realized value for observation <span class="math inline">\(i\)</span>), where the distribution is <em>parameterized by</em> <span class="math inline">\(\theta\)</span>. Some texts use the conditional notation, <span class="math inline">\(f(w_i|\theta)\)</span>, as the distribution depends on <span class="math inline">\(\theta\)</span>. However, probabilistic conditions tend to be based on random variables and not non-random parameters. I found this <a href="https://stats.stackexchange.com/questions/10234/what-is-the-difference-between-the-vertical-bar-and-semi-colon-notations">StackExchange</a> discussion on the topic quite interesting. Needless to say, there is much disagreement and notation appears to differ across Mathematics and Statistics, and among the Statisticians, between frequentists and Bayesians. Even the <a href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation">Wikipedia page</a> on MLE uses a combination of the two notations. I will use ‘;’; which also happens to be the notation used by <span class="citation" data-cites="wooldridge2010">Wooldridge (<a href="#ref-wooldridge2010" role="doc-biblioref">2010</a>)</span>.</p>
</div>
</div>
<p>As the sample is iid, the joint density (or pdf) of the realized observations is given by the product of marginals,</p>
<p><span class="math display">\[
f(w;\theta)=\prod_{i=1}^n f(w_i;\theta)
\]</span> This is referred to as the likelihood function.</p>
<p>Suppose <span class="math inline">\(W_i = [Y_i,X_i']'\)</span>, a vector contain a single outcome variable and a set of covariates. We can then define the joint conditional likelihood as,</p>
<p><span class="math display">\[
\begin{aligned}
\ell_i(\theta) =&amp; f(Y_i|X_i;\theta) \\
L_n(\theta) =&amp; \prod_{i=1}^n f(Y_i|X_i;\theta)
\end{aligned}
\]</span> Here my notation differs from <span class="citation" data-cites="wooldridge2010">Wooldridge (<a href="#ref-wooldridge2010" role="doc-biblioref">2010</a>)</span>, who uses <span class="math inline">\(\ell_i(\theta)\)</span> to denote the conditional log-likelihood for observation <span class="math inline">\(i\)</span> <span class="citation" data-cites="wooldridge2010">(see <a href="#ref-wooldridge2010" role="doc-biblioref">Wooldridge 2010, 471</a>)</span>. Take note of the fact that the likelihood function is a random function of <span class="math inline">\(\theta\)</span>, since it depends on the random variables <span class="math inline">\(W_i = [Y_i,X_i']'\)</span>.<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a></p>
</section>
<section id="estimator-1" class="level3" data-number="5.2">
<h3 data-number="5.2" class="anchored" data-anchor-id="estimator-1"><span class="header-section-number">5.2</span> Estimator</h3>
<p>The goal of ML estimation is to solve the value of <span class="math inline">\(\hat{\theta}\)</span> that maximizes the likelihood of observing the data.</p>
<p><span class="math display">\[
\hat{\theta}^{ML} = \underset{\theta}{\text{arg max}}\;L_n(\theta)
\]</span> In practice, we apply a monotonic transformation to the likelihood function. By taking the log of the likelihood, the product of marginal distributions becomes a sum. As the transformation is monotonic, the solution to the above problem is equivalent to the solution to,</p>
<p><span class="math display">\[
\hat{\theta}^{ML} = \underset{\theta}{\text{arg max}}\;n^{-1}\log L_n(\theta) = \underset{\theta}{\text{arg max}}\;n^{-1}\sum_{i=1}^n\log\ell_i(\theta)
\]</span> In addition, the division by <span class="math inline">\(n\)</span> makes this problem the sample analogue of, <span class="math display">\[
      \underset{\theta\in\Theta}{\text{max}}\;E[\log\ell_i(\theta)]
\]</span> It turns out that the true value of the parameter, <span class="math inline">\(\theta_0\)</span>, is the solution to the above problem [see <span class="citation" data-cites="wooldridge2010">Wooldridge (<a href="#ref-wooldridge2010" role="doc-biblioref">2010</a>)</span>, pp.&nbsp;473].<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a> We will prove this for the unconditional case when we discuss consistency of ML.</p>
<p>Assuming a continuous, concave density function, we can solve for <span class="math inline">\(\hat{\theta}^{ML}\)</span> using first-order conditions.</p>
<p><span class="math display">\[
\frac{1}{n}\frac{\partial \log L_n(\hat{\theta})}{\partial \theta} =\frac{1}{n}\sum_{i=1}^n\frac{\partial \log\ell_i(\hat{\theta})}{\partial \theta}= n^{-1}S(\hat{\theta})=0
\]</span></p>
<p>The <em>vector</em> of partial derivatives is referred to as the score function: <span class="math inline">\(S(\theta)\)</span>. When evaluated at the ML estimator, the score function is 0. This is a <span class="math inline">\(k\)</span>-dimensional vector in which row is the partial derivative with respect to <span class="math inline">\(\theta_k\)</span>.</p>
</section>
<section id="application-to-clrm-2" class="level3" data-number="5.3">
<h3 data-number="5.3" class="anchored" data-anchor-id="application-to-clrm-2"><span class="header-section-number">5.3</span> Application to CLRM</h3>
<p>Under CLRM 5, <span class="math inline">\(U|X \sim N(0,\sigma^2 I_n)\)</span>. Together with CLRM 1 and 6, we know the conditional distribution of <span class="math inline">\(Y\)</span>. <span class="math display">\[
Y_i|X_i\sim_{iid} N(X_i'\beta,\sigma^2)
\]</span> where <span class="math inline">\(X_i'\beta\)</span> is the conditional mean of <span class="math inline">\(Y_i\)</span>. Therefore, the conditional likelihood of the data is given by, <span class="math display">\[
L_n(\beta,\sigma^2) = \prod_{i=1}^n \bigg[\frac{1}{\sqrt{2\pi\sigma^2}}\exp\bigg(-\frac{1}{2}\bigg(\frac{Y_i-X_i'\beta}{\sigma}\bigg)^2\bigg)\bigg]
\]</span> :::{.callout-important} We are working with the <em>conditional</em> likelihood. To define the likelihood of observing the entire sample, <span class="math inline">\(W_1,...,W_n\)</span>, we would also need to consider the distribution of <span class="math inline">\(X_i\)</span>. We would then define <span class="math inline">\(f(W_i;\theta) = f(Y_i|X_i;\theta)\cdot f(X_i)\)</span>, where <span class="math inline">\(f(X_i)\)</span> may be parameterized by its own set of parameters. <span class="math inline">\(\theta\)</span> is the set of parameters that parameterize the conditional distribution of <span class="math inline">\(Y|X\)</span>. ::: Taking the log transformation and divide by <span class="math inline">\(n\)</span>, we get, <span class="math display">\[
n^{-1}\log L_n(\beta,\sigma^2) =  -\frac{1}{2}\log(\sigma^2)-\frac{1}{2}\log(2\pi)-\frac{1}{2n\sigma^2} \sum_{i=1}^n(Y_i-X_i'\beta)^2
\]</span> It should be immediately clear that maximizing this expression will be equivalent to minimizing the sum of squared errors.</p>
<p>Consider the FOC’s set to 0 at the optimal point. First, w.r.t. <span class="math inline">\(\beta\)</span>,</p>
<p><span class="math display">\[
\begin{aligned}
\frac{\partial n^{-1}\log L_n(\hat{\beta},\hat{\sigma}^2)}{\partial\beta} =&amp; -\frac{1}{n\sigma^2} \sum_{i=1}^nX_i(Y_i-X_i'\hat{\beta}) = 0 \\
\Rightarrow \hat{\beta}^{ML} =&amp; \bigg(\sum_{i=1}^n X_iX_i'\bigg)\sum_{i=1}^nX_iY_i \\
=&amp; \big(X'X\big)^{-1}X'Y
\end{aligned}
\]</span> In the case of the CLRM, <span class="math inline">\(\hat{\beta}^{ML}=\hat{\beta}^{MM}=\hat{\beta}^{OLS}\)</span>.</p>
<p>Second, w.r.t. <span class="math inline">\(\sigma^2\)</span>, <span class="math display">\[
\begin{aligned}
\frac{\partial n^{-1}\log L_n(\hat{\beta},\hat{\sigma}^2)}{\partial\sigma^2} =&amp; -\frac{1}{2\hat{\sigma}^2}+ \frac{1}{n2\hat{\sigma}^4}\sum_{i=1}^n(Y_i-X_i'\hat{\beta})^2 = 0 \\
\Rightarrow \hat{\sigma}^{2}_{ML} =&amp; n^{-1}\sum_{i=1}^n(Y_i-X_i'\hat{\beta})^2
\end{aligned}
\]</span> This estimator for the variance is consistent, but biased for small samples. This is because it scales by <span class="math inline">\(n\)</span> and not <span class="math inline">\(n-k\)</span>, a distinction that is ignorable as <span class="math inline">\(n\rightarrow\infty\)</span>. For this reason, when conducting inference you should use the asymptotic distribution of the ML estimator.</p>
</section>
<section id="consistency-2" class="level3" data-number="5.4">
<h3 data-number="5.4" class="anchored" data-anchor-id="consistency-2"><span class="header-section-number">5.4</span> Consistency</h3>
<p>The ML estimator is consistent. This can be shown in a couple of steps. To simplify notation we will examine the proof for the unconditional likelihood, but the same will hold for the conditional. The proof will require Jensen’s inequality:</p>
<div id="thm-jensen" class="theorem" title="Jensen's Inequality">
<p><span class="theorem-title"><strong>Theorem 1</strong></span> For <span class="math inline">\(h(\cdot)\)</span> concave, then <span class="math inline">\(E[h(X)]\leq h(E[X])\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>By the WLLN, for ALL values of <span class="math inline">\(\theta\in\Theta\)</span>,</p>
<p><span class="math display">\[
\begin{aligned}
n^{-1}\sum_{i=1}^n\log f(W_i;\theta) \rightarrow_p&amp;\;E\big[\log f(W_i;\theta)\big] \\
=&amp;\int\big(\log f(w;\theta)\big)f(w;\theta_0)dw
\end{aligned}
\]</span> Note, an important distinction in the last line: the expectation is based on the density function parameterized by the true value, <span class="math inline">\(\theta_0\)</span>. This is because the data is generated by the true density.</p>
<p>We have convergence for ALL values of <span class="math inline">\(\theta\)</span>, but now need to establish convergence to the <span class="math inline">\(\theta_0\)</span>. Consider the difference, <span class="math display">\[
\begin{aligned}
E\big[\log f(W_i;\theta)\big]-E\big[\log f(W_i;\theta_0)\big]
=&amp;E\bigg[\log\frac{f(W_i;\theta)}{f(W_i;\theta_0)}\bigg] \\
\leq&amp;\log\bigg[\frac{f(W_i;\theta)}{f(W_i;\theta_0)}\bigg] \qquad \text{by Jensen's} \\
=&amp;\log \int\bigg(\frac{f(w;\theta)}{f(w,\theta_0)}\bigg)f(w;\theta_0)dw \\
=&amp;\log \int f(w;\theta)dw \\
=&amp;\log 1 \\
=&amp;0
\end{aligned}
\]</span> The inequality can be made strict if we assume that <span class="math inline">\(Pr\big(f(W_i;\theta_0)\neq f(W_i;\theta)\big)&gt;0\)</span> <span class="math inline">\(\forall \theta\neq\theta_0\)</span>. This ensures that <span class="math inline">\(\theta_0\)</span> is a <em>unique</em> solution. Since the difference is <span class="math inline">\(\leq 0\)</span>, it follows that, <span class="math display">\[
\theta_0 = \underset{\theta\in\Theta}{\text{arg max}} E[\log f(W_i;\theta)]
\]</span> Which implies, <span class="math display">\[
\begin{aligned}
\hat{\theta}^{ML}_n =&amp; \;\underset{\theta\in\Theta}{\text{arg max}} \;n^{-1}\log L_n(\theta) \\
\rightarrow_p&amp; \;\underset{\theta\in\Theta}{\text{arg max}} E\big[\log f(W_i,\theta)\big]\\
=&amp; \theta_0
\end{aligned}
\]</span></p>
</div>
</section>
<section id="asymptotic-normality-1" class="level3" data-number="5.5">
<h3 data-number="5.5" class="anchored" data-anchor-id="asymptotic-normality-1"><span class="header-section-number">5.5</span> Asymptotic Normality</h3>
<p>The ML estimator is asymtotically normal. We will not prove this result, but rather focus on the form of the asymptotic variance and its estimator. The proof uses the Mean Value Theorem and CLT.</p>
<p><span class="math display">\[
\sqrt{n}\big(\hat{\theta}^{ML}_n-\theta_0\big)\rightarrow_d N(0,V)
\]</span></p>
<p>where <span class="math inline">\(V=[J(\theta_0)]^{-1}\)</span>. <span class="math inline">\(J(\theta)\)</span> is referred to as the <em>information matrix</em>, given by the expectation of the (Hessian) matrix of second-order derivatives:</p>
<p><span class="math display">\[
J(\theta) = -E\bigg[\frac{\partial^2}{\partial\theta\partial\theta'}\log f(W_i,\theta_0)\bigg]
\]</span> <span class="math inline">\([nJ(\theta_0)]^{-1}\)</span> is used to approximate the variance, but since <span class="math inline">\(J\)</span> is not observed, it must be estimated. This is done by replacing the expectation in the information matrix with sample average:</p>
<p><span class="math display">\[
\hat{V}_H = \bigg[\frac{1}{n}\sum_{i=1}^n\frac{\partial^2}{\partial\theta\partial\theta'}\log f(W_i,\hat{\theta})\bigg]^{-1}
\]</span></p>
</section>
<section id="additional-comments-1" class="level3" data-number="5.6">
<h3 data-number="5.6" class="anchored" data-anchor-id="additional-comments-1"><span class="header-section-number">5.6</span> Additional comments</h3>
<ul>
<li><p>ML estimators are invariant. If <span class="math inline">\(\hat{\theta}\)</span> is the ML-estimator for <span class="math inline">\(\theta\)</span>, then <span class="math inline">\(\ln(\hat{\theta})\)</span> is the ML-estimator for <span class="math inline">\(\ln(\theta)\)</span>.</p></li>
<li><p>In general, there are no closed form solutions for ML estimators; the CLRM being one exception. For this reason, ML estimation requires numerical optimization.</p></li>
<li><p>The ML estimator is efficient. That is, its variance is at least as small as any other consistent (and asymptotically normal) estimator.</p></li>
<li><p>ML estimators require as to know the true PDF, up to its parameters. For example, probit (logit) models assumes that the error term is normally (logistically) distributed.</p></li>
<li><p>In some cases, the estimator may be consistent even if the PDF is misspecified. As is the case for the OLS estimator of the linear model. These estimators are referred to as a quasi-ML estimators.</p></li>
</ul>
</section>
</section>
<section id="references" class="level2" data-number="6">




</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">6 References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-cameron2005" class="csl-entry" role="listitem">
Cameron, A Colin, and Pravin K Trivedi. 2005. <em>Microeconometrics: Methods and Applications</em>. Cambridge university press.
</div>
<div id="ref-verbeek2017" class="csl-entry" role="listitem">
Verbeek, Marno. 2017. <em>A Guide to Modern Econometrics</em>. John Wiley &amp; Sons.
</div>
<div id="ref-wooldridge2010" class="csl-entry" role="listitem">
Wooldridge, Jeffrey M. 2010. <em>Econometric Analysis of Cross Section and Panel Data</em>. MIT press.
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Machine Learning techniques, such as neural networks, use non-linear operators such as the Softmax function and Rectified Linear Unit (ReLU). Who knows, in a few years, we may no longer think of OLS as the ‘work-horse’ of applied statistics and research.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>A measure of distance must be positive. You can use the (sum of) absolute-value deviations, but the least squares has nice properties including ease of differentiation and overall efficiency (of the estimator).<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>The inner (or dot) product of two <em>equal-length</em> vectors, <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>, is defined as: <span class="math display">\[
\langle a,b\rangle=a\cdot b = \sum_{i=1}^k a_i\times b_i = a'b
\]</span><a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>If <span class="math inline">\(Y\sim N(\mu,\Sigma)\)</span>, <span class="math inline">\(Y\in\mathbf{R}^k\)</span>, then <span class="math inline">\(AY+b\sim N(A\mu+b,A\Sigma A')\)</span> for any non-random <span class="math inline">\(m\times k\)</span> <span class="math inline">\(A\)</span>-matrix and <span class="math inline">\(m\times 1\)</span> <span class="math inline">\(b\)</span>-vector.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>The assumptions are fulfilled by CLRM 2<sup>b</sup> and CLRM 6.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>The finiteness of this matrix requires two assumptions:</p>
<ul>
<li><p><span class="math inline">\(E[X_{i,j}^4]&lt;\infty\)</span> for all <span class="math inline">\(j=1,...k\)</span> (i.e.&nbsp;each regressor has finite fourth moment)</p></li>
<li><p><span class="math inline">\(E[U_j^4]&lt; \infty\)</span></p></li>
</ul>
<p>These assumptions are sufficient for all elements of matrix <span class="math inline">\(E[u_1^2 X_1 X_1']\)</span> to be finite. The proof is an application of the Cauchy-Schwartz Inequality, which we haven’t covered.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>This example is taken from Canova (2007, ch.&nbsp;5, p.&nbsp;167).<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p><span class="math inline">\(\Theta\)</span> is a parameter space and is <em>typically</em> assumed to be compact: a closed and bounded subset of Euclidean space.<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p>You may also see the equivalent notation <span class="math inline">\(L(W_i;\theta)\equiv L_n(\theta)\)</span>. The subscript-<span class="math inline">\(n\)</span> implies that the function depends on the sample.<a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p>This is actually a non-trivial issue and beyond the scope of this module. As noted by <span class="citation" data-cites="wooldridge2010">Wooldridge (<a href="#ref-wooldridge2010" role="doc-biblioref">2010</a>)</span>, we can arrive at the ML estimator by picking the value of <span class="math inline">\(\theta\)</span> to maximize the joint likelihood. However, this approach assumes that the true value of <span class="math inline">\(\theta\in\Theta\)</span>, <span class="math inline">\(\theta_0\)</span>, maximizes the joint likelihood. This is not immediately evident. Once established, we have a more robust basis of the ML estimator.<a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>