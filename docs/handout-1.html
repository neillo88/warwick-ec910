<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Classical Linear Regression Model &amp; Ordinary Least Squares – EC910</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">EC910</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="./index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-lecture-handouts" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Lecture Handouts</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-lecture-handouts">    
        <li>
    <a class="dropdown-item" href="./handout-1.html">
 <span class="dropdown-text">Handout 1</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./handout-2.html">
 <span class="dropdown-text">Handout 2</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./handout-3.html">
 <span class="dropdown-text">Handout 3</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./handout-4.html">
 <span class="dropdown-text">Handout 4</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-problem-sets" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Problem Sets</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-problem-sets">    
        <li>
    <a class="dropdown-item" href="./problem-sets/ps-1/problem-set-1.html">
 <span class="dropdown-text">Problem Set 1</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./problem-sets/ps-1/problem-set-1-solutions.html">
 <span class="dropdown-text">Problem Set 1 (Solutions)</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./problem-set-2.html">
 <span class="dropdown-text">Problem Set 2</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./problem-set-2-solutions.html">
 <span class="dropdown-text">Problem Set 2 (Solutions)</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./problem-set-3.html">
 <span class="dropdown-text">Problem Set 3</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./problem-set-3-solutions.html">
 <span class="dropdown-text">Problem Set 3 (Solutions)</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./problem-set-4.html">
 <span class="dropdown-text">Problem Set 4</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-additional-material" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Additional Material</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-additional-material">    
        <li>
    <a class="dropdown-item" href="./material-cef.html">
 <span class="dropdown-text">Conditional Expectation Function</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./material-interpretation.html">
 <span class="dropdown-text">Interpreting Linear Models</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./material-dummy.html">
 <span class="dropdown-text">Dummy Variables</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./material-linearalgebra.html">
 <span class="dropdown-text">Linear Algebra</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./material-inference.html">
 <span class="dropdown-text">Statistical Inference</span></a>
  </li>  
    </ul>
  </li>
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#overview" id="toc-overview" class="nav-link active" data-scroll-target="#overview"><span class="header-section-number">1</span> Overview</a></li>
  <li><a href="#model-specification" id="toc-model-specification" class="nav-link" data-scroll-target="#model-specification"><span class="header-section-number">2</span> Model Specification</a>
  <ul class="collapse">
  <li><a href="#intercept" id="toc-intercept" class="nav-link" data-scroll-target="#intercept"><span class="header-section-number">2.1</span> Intercept</a></li>
  <li><a href="#matrix-notation" id="toc-matrix-notation" class="nav-link" data-scroll-target="#matrix-notation"><span class="header-section-number">2.2</span> Matrix notation</a></li>
  </ul></li>
  <li><a href="#clrm-assumptions" id="toc-clrm-assumptions" class="nav-link" data-scroll-target="#clrm-assumptions"><span class="header-section-number">3</span> CLRM Assumptions</a>
  <ul class="collapse">
  <li><a href="#non-random-x" id="toc-non-random-x" class="nav-link" data-scroll-target="#non-random-x"><span class="header-section-number">3.1</span> Non-random <span class="math inline">\(X\)</span></a></li>
  <li><a href="#identification" id="toc-identification" class="nav-link" data-scroll-target="#identification"><span class="header-section-number">3.2</span> Identification</a></li>
  </ul></li>
  <li><a href="#interpretation" id="toc-interpretation" class="nav-link" data-scroll-target="#interpretation"><span class="header-section-number">4</span> Interpretation</a></li>
  <li><a href="#ordinary-least-squares" id="toc-ordinary-least-squares" class="nav-link" data-scroll-target="#ordinary-least-squares"><span class="header-section-number">5</span> Ordinary Least Squares</a>
  <ul class="collapse">
  <li><a href="#univariate-case" id="toc-univariate-case" class="nav-link" data-scroll-target="#univariate-case"><span class="header-section-number">5.1</span> Univariate case</a></li>
  <li><a href="#geometry-of-ols" id="toc-geometry-of-ols" class="nav-link" data-scroll-target="#geometry-of-ols"><span class="header-section-number">5.2</span> Geometry of OLS</a></li>
  <li><a href="#partitioned-regression" id="toc-partitioned-regression" class="nav-link" data-scroll-target="#partitioned-regression"><span class="header-section-number">5.3</span> Partitioned regression</a></li>
  </ul></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references"><span class="header-section-number">6</span> References</a></li>
  </ul>
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="handout-1.pdf"><i class="bi bi-file-pdf"></i>PDF</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Classical Linear Regression Model &amp; Ordinary Least Squares</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="overview" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="overview"><span class="header-section-number">1</span> Overview</h2>
<p>In this handout we will revisit the Classical Linear Regression Model (CLRM) <span class="citation" data-cites="wooldridge2010">(see <a href="#ref-wooldridge2010" role="doc-biblioref">Wooldridge 2010, chaps. 1–2</a>)</span>. The goal of this week’s lecture is to:</p>
<ol type="1">
<li><p>understand the model specification;</p></li>
<li><p>it’s underlying assumptions;</p></li>
<li><p>and the appropriate interpretation;</p></li>
<li><p>the OLS estimator, using linear algebra;</p></li>
<li><p>the geometry of OLS and partitioned regression result.</p></li>
</ol>
</section>
<section id="model-specification" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="model-specification"><span class="header-section-number">2</span> Model Specification</h2>
<p>The linear population regression model is given by,</p>
<p><span class="math display">\[
\begin{aligned}
  Y_i =&amp; X_i'\beta+\varepsilon_i \\
  =&amp; \beta_1\mathbf{1}+\beta_2X_{i2}+\beta_3X_{i3}+...+\beta_kX_{ik}+\varepsilon_i
\end{aligned}
\]</span></p>
<p>for <span class="math inline">\(i = 1,2,...,n\)</span>. Where,</p>
<ul>
<li><p><span class="math inline">\(i\)</span>: unit of observation; e.g.&nbsp;individual, firm, union, political party, etc.</p></li>
<li><p><span class="math inline">\(Y_i \in \mathbb{R}\)</span>: scalar random variable.</p></li>
<li><p><span class="math inline">\(X_i \in \mathbb{R}^k\)</span>: <span class="math inline">\(k\)</span>-dimensional (column<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>) vector of regressors, with <span class="math inline">\(k&lt;n\)</span>.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p></li>
<li><p><span class="math inline">\(\beta\)</span>: <span class="math inline">\(k\)</span>-dimensional, non-random vector of unknown population parameters.</p></li>
<li><p><span class="math inline">\(\varepsilon_i\)</span>: <em>unobserved</em>, random error term.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></p></li>
</ul>
<p>The linear population regression equation is <strong>linear in parameters</strong>. This is an important assumption that does NOT restrict the model from being non-linear in regressors. For example, the equation</p>
<p><span class="math display">\[Y_i = \beta_1 + \beta_2X_{i2} + \beta_3X_{i2}^{\color{red}{2}} + \varepsilon_i\]</span> non-linear in <span class="math inline">\(X_{i2}\)</span>, but still linear in parameters. In contrast, the equation</p>
<p><span class="math display">\[Y_i = \beta_1 + \beta_2X_{i2} + ({\color{red}{\beta_2\beta_3}})X_{i3} + \varepsilon_i\]</span> is non-linear in parameters.</p>
<section id="intercept" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="intercept"><span class="header-section-number">2.1</span> Intercept</h3>
<p>The constant (intercept) in the equation serves an important purpose. While there is no <em>a priori</em> reason for the model to have a constant term, it does ensure that the error term is mean zero.</p>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Suppose <span class="math inline">\(E[\varepsilon_i] = \gamma\)</span>.</p>
<p>We can then define a new error term, <span class="math inline">\(\upsilon_i = \varepsilon_i - \gamma\)</span>, such <span class="math inline">\(E[\upsilon_i] = 0\)</span>. The population regression model can be rewritten as, <span class="math display">\[ \begin{aligned} Y_i =&amp; X_ i'\beta + v_i + \gamma  \\
=&amp; \underbrace{(\beta_1+\gamma)}_{\tilde{\beta}_1}\mathbf{1} + \beta_2X_{i2} + \beta_3X_{i3} + ... + \beta_kX_{ik} + v_i
\end{aligned}
\]</span> The model has a new intercept <span class="math inline">\(\tilde{\beta}_1=\beta_1 + \gamma\)</span>, but the other parameters remain unchanged.</p>
</div>
</section>
<section id="matrix-notation" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="matrix-notation"><span class="header-section-number">2.2</span> Matrix notation</h3>
<p>For a sample of <span class="math inline">\(n\)</span> observations, we can stack the unit-level linear regression equation into a vector,</p>
<p><span class="math display">\[
Y =\underbrace{\begin{bmatrix}Y_1 \\ Y_2 \\ \vdots \\ Y_n \end{bmatrix}}_{n\times 1} = \underbrace{\begin{bmatrix}X_1'\beta \\ X_2'\beta \\ \vdots \\ X_n'\beta\end{bmatrix}}_{n\times 1}  + \underbrace{\begin{bmatrix}\varepsilon_1 \\ \varepsilon_2 \\ \vdots \\ \varepsilon_n\end{bmatrix}}_{n\times 1}  
= \underbrace{\begin{bmatrix}X_{11} &amp; X_{12} &amp; \dots &amp; X_{1k}\\ X_{21} &amp; X_{22} &amp;&amp; \\ \vdots &amp; &amp; \ddots &amp; \\ X_{n1} &amp; &amp; &amp; X_{nk} \end{bmatrix}}_{n\times k} \underbrace{\begin{bmatrix}\beta_1 \\ \beta_2 \\ \vdots \\ \beta_k\end{bmatrix}}_{k\times 1} + \begin{bmatrix}\varepsilon_1 \\ \varepsilon_2 \\ \vdots \\ \varepsilon_n\end{bmatrix} = X\beta + \varepsilon
\]</span> Notice, in matrix notation, you lose the transpose from <span class="math inline">\(X_i'\beta\)</span>. Apart from the absence of the <span class="math inline">\(i\)</span> subscript, this is a useful way of knowing the dimension of the equation (in my notes). You MUST always write <span class="math inline">\(X\beta\)</span> and not <span class="math inline">\(\beta X\)</span>. For the scalar case, <span class="math inline">\(X_i'\beta = \beta'X_i\)</span>, but for the vector case <span class="math inline">\(\beta X\)</span> is not defined since <span class="math inline">\(\beta\)</span> is <span class="math inline">\(k\times 1\)</span> and <span class="math inline">\(X\)</span> is <span class="math inline">\(n\times k\)</span>.</p>
</section>
</section>
<section id="clrm-assumptions" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="clrm-assumptions"><span class="header-section-number">3</span> CLRM Assumptions</h2>
<p><strong>Assumption CLRM 1.</strong> Population regression equation is linear in parameters: <span class="math display">\[Y = X\beta+\varepsilon\]</span></p>
<p><strong>Assumption CLRM 2.</strong> Conditional mean independence of the error term: <span class="math display">\[E[\varepsilon|X]=0\]</span></p>
<p>Assumption CLRM 2. is stronger than <span class="math inline">\(E[\varepsilon_i|X_i]\)</span> (mean independence for unit <span class="math inline">\(i\)</span>). If all units were independent, then <span class="math inline">\(E[\varepsilon_i|X_i]\)</span> would imply <span class="math inline">\(E[\varepsilon|X]=0\)</span>. However, since we have not (yet) assumed this, we need this stronger exogeneity assumption. Consider, if <span class="math inline">\(i\)</span> represented units of time (<span class="math inline">\(t\)</span>), as in time-series models, independence across <span class="math inline">\(i\)</span> will not hold.</p>
<p>Together, CLRM 1. and CLRM 2. imply that</p>
<p><span class="math display">\[ E[Y|X] = X\beta  \]</span> This means that the Conditional Expectation Function is known and linear in parameters.</p>
<p>Conditional mean independence implies - by the Law of Iterated Expectations - mean independence of the error term,</p>
<p><span class="math display">\[E[\varepsilon|X]=0 \Rightarrow E\big[E[\varepsilon|X]\big]=E[\varepsilon]=0\]</span></p>
<p>and uncorrelatedness,</p>
<p><span class="math display">\[E[\varepsilon|X]=0 \Rightarrow E[\varepsilon X]=0\]</span> Note, neither of the above statements hold the other way around. Mean independence does not imply conditional mean independence and uncorrelatedness (zero correlation/covariance) does not imply conditional mean independence.</p>
<p>Uncorrelatedness rules out linear relationships between the regressors and error term while conditional mean independence rules out non-linear relationships too.</p>
<p>In general, distributional independence implies mean independence which then implies uncorrelatedness.</p>
<div id="nte-normality">
<p>In the case joint-normally distributed random variables, uncorrelatedness implies independence. That is, if <span class="math display">\[ \begin{bmatrix}Y_1 \\ Y_2\end{bmatrix}\sim N\bigg(\begin{bmatrix}\mu_1 \\ \mu_2\end{bmatrix},\begin{bmatrix}\sigma_1^2 &amp; \sigma_{12}\\ \sigma_{21} &amp; \sigma_2^2\end{bmatrix}\bigg)\]</span> Then <span class="math inline">\(\sigma_{12}=\sigma_{21}=0 \iff f_1*f_2 = f_{12}\)</span>.</p>
</div>
<p>We will later show that uncorrelatedness is sufficient for consistency of the Ordinary Least Squares estimater, while conditional mean independence is required for unbiasedness of OLS.</p>
<p><strong>Assumption CLRM 3.</strong> Homoskedasticity: <span class="math inline">\(Var(\varepsilon|X) = E[\varepsilon\varepsilon'|X] =  \sigma^2I_n\)</span></p>
<p>CLRM 3. states that the variance of the error term is independent of <span class="math inline">\(X\)</span> and constant across units. The diagonal nature of the covariance matrix also implies that the error terms are uncorrelated across units in the data. Note, this does not imply independence of the error terms across units.</p>
<p>Models with heteroskedasticity relax the assumption of constant variance, allowing for a richer variance-covariance matrix that typically depends on <span class="math inline">\(X\)</span>.</p>
<p>This assumption is unlikely to hold in time-series models where units represent repeated observations across time. Such violations are referred to as serial correlation or autocorrelation.</p>
<p>Even in cross-sectional data settings, you can have non-zero correlations across units in the data. A common instance of this is the case of clustering. Clustering can occur when units experience common/correlated ‘shocks’; for example, the data contains groups of students from the same classroom who have a the same teacher. This can also be the result of clustered sampling, a common practice in multi-stage survey design.</p>
<p><strong>Assumption CLRM 4.</strong> Full rank: <span class="math inline">\(rank(X)=k\quad a.s.\)</span> a.s.<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></p>
<p>Since <span class="math inline">\(X\)</span> is a random variable we should add to the assumption: <span class="math inline">\(rank(X) = k\)</span> <em>almost surely</em> (abbreviated a.s.). This means that the set of events in which <span class="math inline">\(X\)</span> is not full rank occur with probability 0. The reason for this addition is that such a set of events (in the sample space) may not be empty.</p>
<p>CLRM 4. is some time referred to as the absence of perfect (or exact) collinearity. Do not confuse this with multicollinearity. Multicollinearity occurs when regressors are highly (linearly) correlated with one another, yielding imprecise estimates.</p>
<p><strong>Assumption CLRM 5.</strong> Normality of the error term: <span class="math inline">\(\varepsilon|X \sim N(0,\sigma^2 I_n)\)</span></p>
<p><strong>Assumption CLRM 6.</strong> Observations <span class="math inline">\(\{(Y_i,X_i): i=1,...,n\}\)</span> are independently and identically distributed (iid).</p>
<p>CLRM 5 &amp; 6 are not part of the Classical assumptions, but do simplify the problem of inference. Note, CLRM 5 implies independence across error terms, not implied by CLRM 3.</p>
<section id="non-random-x" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="non-random-x"><span class="header-section-number">3.1</span> Non-random <span class="math inline">\(X\)</span></h3>
<p>There is an alternative version of the CLRM in which <span class="math inline">\(X\)</span> is a non-random, matrix of regressors/predictors. With <span class="math inline">\(X\)</span> fixed, the error term is the only only random variable in the model. CLRM assumptions 1 and 4 remain the same, while CLRM 2, 3, 5, and 6 become:</p>
<p><strong>Assumption CLRM 2<sup>a</sup>.</strong> Mean independence of the error term: <span class="math display">\[E[\varepsilon]=0\]</span></p>
<p><strong>Assumption CLRM 3<sup>a</sup>.</strong> Homoskedasticity: <span class="math inline">\(Var(\varepsilon) = \sigma^2I_n\)</span></p>
<p><strong>Assumption CLRM 5<sup>a</sup>.</strong> Normality of the error term: <span class="math inline">\(\varepsilon \sim N(0,\sigma^2 I_n)\)</span></p>
<p><strong>Assumption CLRM 6<sup>a</sup>.</strong> Observations <span class="math inline">\(\{\varepsilon_i: i=1,...,n\}\)</span> are independently and identically distributed (iid).</p>
</section>
<section id="identification" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="identification"><span class="header-section-number">3.2</span> Identification</h3>
<p>CLRM 1,2 and 4. are the <em>identifying</em> assumptions of the model. These assumptions allow us to write the parameter of interest as a set of ‘observable’ moments in the data. We can demonstrate this as follows.</p>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Start with CLRM 2.</p>
<p><span class="math display">\[
    E[\varepsilon_i|X_i]=0
\]</span></p>
<p>Pre-multiply by the vector <span class="math inline">\(X_i\)</span>, <span class="math display">\[
        X_iE[\varepsilon_i|X_i]=0
\]</span> Since the expectation is conditional on <span class="math inline">\(X_i\)</span>, we can bring <span class="math inline">\(X_i\)</span> inside the expectation function,</p>
<p><span class="math display">\[
        E[X_i\varepsilon_i|X_i]=0
    \]</span> This conditional expectation is a random-function of <span class="math inline">\(X_i\)</span>. If we take the expectation of this function w.r.t. <span class="math inline">\(X\)</span>, we achieve the aforementioned result that conditional mean independence implies zero covariance, <span class="math display">\[
        E\left[E[X_i\varepsilon_i|X_i]\right]=E[X_i\varepsilon_i]=0
    \]</span></p>
<p>Now substitute in for <span class="math inline">\(\varepsilon_i\)</span> using the linear regression model from CLRM 1 and separate the resulting two terms,</p>
<p><span class="math display">\[
\begin{aligned}
    &amp;E[X_i(Y_i-X_i'\beta)]=0 \\
    \Rightarrow &amp;E[X_iX_i']\beta=E[X_iY_i]
\end{aligned}
\]</span></p>
<p>Since <span class="math inline">\(\beta\)</span> is a non-random vector, we can remove it from the expectation function.</p>
<p>Now we have a system of linear equations (of the form <span class="math inline">\(Av = b\)</span>) with a unique solution if and only if the matrix <span class="math inline">\(E[X_iX_i']\)</span> is invertible. For the inverse of <span class="math inline">\(E[X_iX_i']\)</span> to exist, we require CLRM 4, since <span class="math inline">\(rank(X)=k\quad a.s.\Rightarrow rank(E[X_iX_i'])=k\)</span>.<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a></p>
<p><span class="math display">\[
    \beta = E[X_iX_i']^{-1}E[X_iY_i]
\]</span></p>
</div>
<p>We cannot compute <span class="math inline">\(\beta\)</span> because we do not know the joint distribution of <span class="math inline">\((Y_i,X_i)\)</span> needed to solve for the variance-covariance matrices. However, <span class="math inline">\(\beta\)</span> is (point) identified because both <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span> are observed in the data and the parameters are “pinned down” by a unique set of ‘observable’ moments in the data.</p>
<p><span class="math inline">\(\beta\)</span> is not identified if the above system of linear equations does not have a unique solution. This will occur if two or more of the regressors are perfectly colinear.<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> <span class="math inline">\(\beta\)</span> is also not be identified if the resulting expression for <span class="math inline">\(\beta\)</span> includes ‘objects’ (moments, distribution/scale parameters) that are not ‘observed’ in the data. For example, if the error is not mean independent, the above expression will include a bias term that depends on <span class="math inline">\(E[X_i'\varepsilon_i]\)</span>.</p>
<p>In this instance, the identification of <span class="math inline">\(\beta\)</span> is scale dependent. That is, if we multiply <span class="math inline">\(Y_i\)</span> by a scalar, <span class="math inline">\(\beta\)</span> is multiplied by the same scalar. For example, in cases where a researcher is modelling standardized test-scores.</p>
</section>
</section>
<section id="interpretation" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="interpretation"><span class="header-section-number">4</span> Interpretation</h2>
<p>In this linear regression model each slope coefficient has a partial derivative interpretation,</p>
<p><span class="math display">\[
\beta_j = \frac{\partial E[Y_i|X_i]}{\partial X_{ij}}
\]</span> or, as a vector, <span class="math display">\[
\beta = \frac{\partial E[Y_i|X_i]}{\partial X_{i}} = \begin{bmatrix}\frac{\partial E[Y_i|X_i]}{\partial X_{i1}}\\ \vdots \\ \frac{\partial E[Y_i|X_i]}{\partial X_{ik}}\end{bmatrix} = \begin{bmatrix}\beta_1\\ \vdots \\ \beta_k\end{bmatrix}
\]</span></p>
<p>Note, the derivative is expressed in terms of changes in the <em>expected</em> value of <span class="math inline">\(Y_i\)</span> (conditional on <span class="math inline">\(X_i\)</span>), not <span class="math inline">\(Y_i\)</span> itself. This is because <span class="math inline">\(Y_i\)</span> is a random variable, but under CLRM 1 &amp; 2</p>
<p><span class="math display">\[
E[Y_i|X_i] = X_i'\beta
\]</span></p>
<p>For a given value of <span class="math inline">\(X_i\)</span>, the above expression is non-random.</p>
<p>As <span class="math inline">\(\beta_j\)</span> is a partial derivative, its interpretation is one that “holds fixed” the value of other regressors (i.e.&nbsp;<em>ceteris paribus</em>). Because of this, many researchers apply the experimental language of control variables when interpretting regression coefficients. However, this is dependent on the <em>assumed</em> linearity of the CEF.</p>
</section>
<section id="ordinary-least-squares" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="ordinary-least-squares"><span class="header-section-number">5</span> Ordinary Least Squares</h2>
<p>OLS is <em>an</em> estimator for <span class="math inline">\(\beta\)</span>. As will become evident in Lecture 3, it is not the only estimator for <span class="math inline">\(\beta\)</span>.</p>
<p>The OLS estimator is the solution to,</p>
<p><span class="math display">\[
\min_b\;\sum_{i=1}^n(Y_i-X_i'b)^2
\]</span></p>
<p>Using vector notation, we can rewrite this as</p>
<p><span class="math display">\[
\begin{aligned}
&amp;\min_b\;(Y-Xb)'(Y-Xb)\\
=&amp;\min_b\;Y'Y-Y'Xb-b'X'Y+b'X'Xb \\
=&amp;\min_b\;Y'Y-2b'X'Y+b'X'Xb
\end{aligned}
\]</span> From line 2 to 3 we use the fact that <span class="math inline">\(Y'Xb\)</span> is a scalar and therefore symmetric: <span class="math inline">\(Y'Xb=b'X'Y\)</span>.<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a></p>
<p>Differentiating the above expression w.r.t. the vector <span class="math inline">\(b\)</span> and setting the first-order conditions to <span class="math inline">\(0\)</span>, we find that the following condition must hold for <span class="math inline">\(\hat{\beta}\)</span>, the solution.</p>
<p><span class="math display">\[
  \begin{aligned}
  &amp;0=-2X'Y+2X'X\hat{\beta}
  \\ \Rightarrow&amp; X'X\hat{\beta} = X'Y
  \end{aligned}
\]</span></p>
<hr>
<p><strong>How did we get this result?</strong> Deriving the first order conditions requires knowledge of how to solve for the derivative of a scalar respect to a column vector (in this case <span class="math inline">\(b\in R^k\)</span>). The extra material on Linear Algebra has some notes on vector differentiation.</p>
<p>We can ignore the first term <span class="math inline">\(Y'Y\)</span> as it does not depend on <span class="math inline">\(b\)</span>. The second term is <span class="math inline">\(-2b'X'Y\)</span>. Here we can use the rule that, <span class="math display">\[
  \frac{\partial z'a}{\partial z} = \frac{\partial a'z}{\partial z} = a
\]</span> In this instance, <span class="math inline">\(a = X'Y \in R^k\)</span>. Thus, <span class="math display">\[
  \frac{\partial -2b'X'Y}{\partial b} = -2\frac{\partial b'X'Y}{\partial b} = -2X'Y
\]</span> The third term is <span class="math inline">\(b'X'Xb\)</span>. This is what is commonly referred to as a quadratic form: <span class="math inline">\(z'Az\)</span>. We know that the derivative of this form is, <span class="math display">\[
  \frac{\partial z'Az}{\partial z} = Az + A'z
\]</span> and if <span class="math inline">\(A\)</span> is symmetric, the result simplies to <span class="math inline">\(2Az\)</span>. In this instance, <span class="math inline">\(A = X'X\)</span> is symmetric and the derivative is given by, <span class="math display">\[
  \frac{\partial b'X'Xb}{\partial b} = 2X'X
\]</span></p>
<hr>
<p>In order to solve for <span class="math inline">\(\hat{\beta}\)</span> we need to move the <span class="math inline">\(X'X\)</span> term to the right-hand side. If these were scalars we would simply divide both sides by the same constant. However, as <span class="math inline">\(X'X\)</span> is a matrix, division is not possible. Instead, we need to pre-multiply both sides by the inverse of <span class="math inline">\(X'X\)</span>: <span class="math inline">\((X'X)^{-1}\)</span>. Here’s the issue: the inverse of a matrix need not exist.</p>
<p>Given a <em>square</em> <span class="math inline">\(k\times k\)</span> matrix <span class="math inline">\(A\)</span>, its inverse exists <em>if and only if</em> <span class="math inline">\(A\)</span> is non-singular. For <span class="math inline">\(A\)</span> to be non-singular its rank must have full rank: <span class="math inline">\(r(A)=k\)</span>, the number of rows/columns. This means that all <span class="math inline">\(k\)</span> columns/rows must be linearly independent. (See Material on Linear Algebra for a more detailed discussion of all these terms.)</p>
<p>In our application, <span class="math inline">\(A=X'X\)</span> and</p>
<p><span class="math display">\[ r(X'X) = r(X) = colrank(X)\leq k \]</span></p>
<p>To insure that the inverse of <span class="math inline">\(X'X\)</span> exists, <span class="math inline">\(X\)</span> must have full column rank: all column vectors must be <em>linearly independent</em>. In practice, this means that no regressor can be a <em>perfect</em> linear combination of others. However, we have this from</p>
<p><strong>CLRM 4:</strong> <span class="math inline">\(rank(X)=k\)</span></p>
<p>You may know this assumption by another name: the absence of perfect colinearity between regressors.</p>
<blockquote class="blockquote">
<p>The rank condition is the reason we exclude a base category when working with categorical variables.</p>
<p>Recall, most linear regression models are specified with a constant. Thus, the first column of <span class="math inline">\(X\)</span> is</p>
<p><span class="math display">\[ X_1 = \begin{bmatrix}1 \\ 1 \\ \vdots \\ 1\end{bmatrix} \]</span> a <span class="math inline">\(n\times 1\)</span> vector vector of <span class="math inline">\(1\)</span>’s, denoted here as <span class="math inline">\(\ell\)</span>. Suppose you have a categorical - for example, gender in an individual level dataset - that splits the same in two. The categories are assumed to be exhaustive and mutually exclusive. If you create two dummy variables, one for each category,</p>
<p><span class="math display">\[ X_2 = \begin{bmatrix}1 \\ \vdots \\1\\0\\ \vdots \\ 0\end{bmatrix}\qquad\text{and}\qquad X_3 = \begin{bmatrix}0 \\ \vdots \\0\\1\\ \vdots \\ 1\end{bmatrix} \]</span></p>
<p>it is evident that <span class="math inline">\(X_2+X_3 = \ell\)</span>. (Here I have depicted the sample as sorted along these two categories.) If <span class="math inline">\(X=[X_1\;X_2\;X_3]\)</span>, then it is rank-deficient: <span class="math inline">\(r(X) = 2&lt;3\)</span>, since <span class="math inline">\(X_3=X_1-X_2\)</span>. Thus, we can only include two of these three regressors. We can even exclude the constant and have <span class="math inline">\(X=[X_2\;X_3]\)</span>.</p>
</blockquote>
<p>If <span class="math inline">\(X\)</span> is full rank, then <span class="math inline">\((X'X)^{-1}\)</span> exists and,</p>
<p><span class="math display">\[
\hat{\beta} = (X'X)^{-1}X'Y
\]</span></p>
<p>This relatively simple expression is the solution to least squares minimization problem. Just think, it would take less than three lines of code to programme this. That is the power of knowing a little linear algebra.</p>
<p>We can write the same expression in terms of summations over unit-level observations,</p>
<p><span class="math display">\[
\hat{\beta} = \bigg(\sum_{i=1}^nX_iX_i'\bigg)^{-1}\sum_{i=1}^nX_iY_i
\]</span></p>
<p>Note, the change in position of the transpose: <span class="math inline">\(X_i\)</span> is a column vector <span class="math inline">\(\Rightarrow\)</span> <span class="math inline">\(X_i'X_i\)</span> is a scalar while <span class="math inline">\(X_iX_i'\)</span> is a <span class="math inline">\(k\times k\)</span> matrix. To match the first expression, the term inside the parenthesis must be a <span class="math inline">\(k\times k\)</span> matrix. Similarly, <span class="math inline">\(X'Y\)</span> is a <span class="math inline">\(k\times 1\)</span> vector, as is <span class="math inline">\(X_iY_i\)</span>.</p>
<section id="univariate-case" class="level3" data-number="5.1">
<h3 data-number="5.1" class="anchored" data-anchor-id="univariate-case"><span class="header-section-number">5.1</span> Univariate case</h3>
<p>Undergraduate textbooks all teach a very similar expression for the OLS estimator of a uni-variate regression model (with a constant); typically, something like,<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a></p>
<p><span class="math display">\[
Y_i = \beta_1+\beta_2X_{i2}+\varepsilon_i
\]</span> We know that the OLS estimators are give by,</p>
<p><span class="math display">\[
\begin{aligned}
\tilde{\beta}_2 =&amp; \frac{\sum(Y_i-\bar{Y})(X_{i2}-\bar{X}_2)}{\sum_{i=1}^n(X_{i2}-\bar{X}_2)^2} \\
\text{and}\qquad \tilde{\beta}_1 =&amp; \bar{Y}-\tilde{\beta_2}\bar{X}_2
\end{aligned}
\]</span></p>
<p>I am deliberately using the notation <span class="math inline">\(\tilde{\beta}\)</span> to distinguish these two estimators from the expression below. Let us see if we can replicate this result, using vector notation. The the model is,</p>
<p><span class="math display">\[
\begin{aligned}
Y =&amp; X\beta+\varepsilon \\
=&amp; \begin{bmatrix}1&amp;X_{12} \\ 1 &amp; X_{22} \\ \vdots &amp; \vdots \\ 1 &amp; X_{n2}\end{bmatrix}\begin{bmatrix}\beta_1 \\ \beta_2 \end{bmatrix} + \varepsilon \\
=&amp; \begin{bmatrix}\ell &amp;X_{2} \end{bmatrix}\begin{bmatrix}\beta_1 \\ \beta_2 \end{bmatrix} + \varepsilon
\end{aligned}
\]</span></p>
<p>Therefore,</p>
<p><span class="math display">\[
\begin{aligned}
\hat{\beta} = \begin{bmatrix}\hat{\beta}_1 \\ \hat{\beta}_2\end{bmatrix}=&amp;(X'X)^{-1}X'Y \\
=&amp;\bigg(\begin{bmatrix}\ell' \\ X_{2}' \end{bmatrix}\begin{bmatrix}\ell &amp;X_{2}\end{bmatrix}\bigg)^{-1}\begin{bmatrix}\ell'  \\ X_{2}' \end{bmatrix}Y \\
=&amp;\begin{bmatrix}\ell'\ell &amp; \ell'X_2 \\ X_{2}'\ell &amp; X_2'X_2 \end{bmatrix}^{-1}\begin{bmatrix}\ell'Y  \\ X_{2}'Y \end{bmatrix}
\end{aligned}
\]</span></p>
<p>I went through this rather quickly, using a number of linear algebra rules that you may not be familiar with. Do not worry, the point of the exercise is not become a linear algebra master, but instead to focus on the element of each of each matrix/vector. Each element is a scalar (size <span class="math inline">\(1\times 1\)</span>).</p>
<p>If we right them each down as sums you they might be a little more familiar. First consider the <span class="math inline">\(2\times 2\)</span> matrix:</p>
<ul>
<li><p>element [1,1]: <span class="math inline">\(\ell'\ell = \sum_{i=1}^n 1 = n\)</span></p></li>
<li><p>element [1,2]: <span class="math inline">\(\ell'X_2 = \sum_{i=1}^nX_{i2} = n\bar{X}_2\)</span></p></li>
<li><p>element [2,1]: <span class="math inline">\(X_2'\ell = \sum_{i=1}^nX_{i2} = n\bar{X}_2\)</span> (as above, since scalars are symmetric)</p></li>
<li><p>element [2,2]: <span class="math inline">\(X_2'X_2=\sum_{i=1}^nX_{i2}^2\)</span></p></li>
</ul>
<p>Next, consider the final <span class="math inline">\(2\times 1\)</span> vector,</p>
<ul>
<li><p>element [1,1]: <span class="math inline">\(\ell'Y = \sum_{i=1}^n Y_i = n\bar{Y}\)</span></p></li>
<li><p>element [2,1]: <span class="math inline">\(X_2'Y = \sum_{i=1}^nY_iX_{i2}\)</span></p></li>
</ul>
<p>Our OLS estimator is therefore,</p>
<p><span class="math display">\[
\hat{\beta} = \begin{bmatrix} n &amp; n\bar{X}_2 \\ n\bar{X}_2 &amp; \sum_{i=1}^nX_{i2}^2 \end{bmatrix}^{-1}\begin{bmatrix}n\bar{Y}  \\ \sum_{i=1}^nY_iX_{i2} \end{bmatrix}
\]</span></p>
<p>We now need to solve for the inverse of the <span class="math inline">\(2\times 2\)</span> matrix. You can easily find notes on how to do this online. Here, I will just provide the solution.</p>
<p><span class="math display">\[
\hat{\beta} = \frac{1}{n\sum_{i=1}^nX_{i2}^2-n^2\bar{X}_2^2}\begin{bmatrix} \sum_{i=1}^nX_{i2}^2 &amp; -n\bar{X}_2 \\ -n\bar{X}_2 &amp;  n\end{bmatrix}\begin{bmatrix}n\bar{Y}  \\ \sum_{i=1}^nY_iX_{i2} \end{bmatrix}
\]</span></p>
<p>Remember, this is still a <span class="math inline">\(2\times 1\)</span> vector. We can now solve for the final solution:</p>
<p><span class="math display">\[
\begin{aligned}
\hat{\beta} =&amp; \frac{1}{n\sum_{i=1}^nX_{i2}^2-n^2\bar{X}_2^2}\begin{bmatrix} n\bar{Y}\sum_{i=1}^nX_{i2}^2 -n\bar{X}_2\sum_{i=1}^nY_iX_{i2} \\ n\sum_{i=1}^nY_iX_{i2}-n^2\bar{X}_2\bar{Y}\end{bmatrix} \\
=&amp; \frac{1}{n\sum_{i=1}^n(X_{i2}-\bar{X}_2)^2}\begin{bmatrix} n\bar{Y}\sum_{i=1}^nX_{i2}^2 + n^2\bar{Y}\bar{X}^2 - n^2\bar{Y}\bar{X}^2 -n\bar{X}_2\sum_{i=1}^nY_iX_{i2} \\ n\sum_{i=1}^n(Y_i-\bar{Y})(X_{i2}-\bar{X}_2) \end{bmatrix} \\
=&amp; \frac{1}{n\sum_{i=1}^n(X_{i2}-\bar{X}_2)^2}\begin{bmatrix} n\bar{Y}\sum_{i=1}^n(X_{i2}-\bar{X})^2 -n\bar{X}_2\sum_{i=1}^n(Y_i-\bar{Y})(X_{i2}-\bar{X}_2) \\ n\sum_{i=1}^n(Y_i-\bar{Y})(X_{i2}-\bar{X}_2) \end{bmatrix} \\
=&amp; \begin{bmatrix} \bar{Y}  -\frac{n\sum_{i=1}^n(Y_i-\bar{Y})(X_{i2}-\bar{X}_2)}{n\sum_{i=1}^n(X_{i2}-\bar{X}_2)^2}\bar{X}_2 \\ \frac{n\sum_{i=1}^n(Y_i-\bar{Y})(X_{i2}-\bar{X}_2)}{n\sum_{i=1}^n(X_{i2}-\bar{X}_2)^2} \end{bmatrix} \\
=&amp; \begin{bmatrix} \bar{Y}  -\tilde{\beta}_2\bar{X}_2 \\ \tilde{\beta}_2 \end{bmatrix} \\
=&amp; \begin{bmatrix}\tilde{\beta}_1 \\ \tilde{\beta}_2 \end{bmatrix}
\end{aligned}
\]</span></p>
<p>The math is a little involved, but it shows you these solutions are are the same. Unfortunately, the working gets even more arduous in a multivariate context. However, there are useful tools to help us with that we will discuss next.</p>
</section>
<section id="geometry-of-ols" class="level3" data-number="5.2">
<h3 data-number="5.2" class="anchored" data-anchor-id="geometry-of-ols"><span class="header-section-number">5.2</span> Geometry of OLS</h3>
<p>In the last section we saw how the OLS estimator can, more generally, be described as a linear transformation of the <span class="math inline">\(Y\)</span> vector.</p>
<p><span class="math display">\[
\hat{\beta} = (X'X)^{-1}X'Y
\]</span></p>
<p>We also saw that in order for there to be a (unique) solution to the least squared problem, the <span class="math inline">\(X\)</span> matrix must be full rank. This rules out any perfect colinearity between columns (i.e.&nbsp;regressors) in the <span class="math inline">\(X\)</span> matrix, including the constant.</p>
<p>Given the vector of OLS coefficients, we can also estimate the residual,</p>
<p><span class="math display">\[
\begin{aligned}
\hat{\varepsilon} =&amp; Y - X\hat{\beta} \\
=&amp;Y-X(X'X)^{-1}X'Y \\
=&amp;(I_n-X(X'X)X^{-1})Y
\end{aligned}
\]</span></p>
<p>by plugging the definition of <span class="math inline">\(\hat{\beta}\)</span>. Thus, the OLS estimator separates the vector <span class="math inline">\(Y\)</span> into two components:</p>
<p><span class="math display">\[
\begin{aligned}
Y =&amp; X\hat{\beta} + \hat{\varepsilon} \\
=&amp;\underbrace{X(X'X)^{-1}X'}_{P_X}Y + (\underbrace{I_n-X(X'X)X^{-1}}_{I_n-P_X = M_X})Y \\
=&amp;P_XY + M_XY
\end{aligned}
\]</span></p>
<p>The matrix <span class="math inline">\(P_X = X(X'X)^{-1}X'\)</span> is a <span class="math inline">\(n\times n\)</span> <em>projection</em> matrix. It is a linear transformation that projects any vector into the span of <span class="math inline">\(X\)</span>: <span class="math inline">\(S(X)\subset\mathbb{R}^n\)</span>. (See for more information on these terms.) <span class="math inline">\(S(X)\)</span> is the vector space spanned by the columns of <span class="math inline">\(X\)</span>. The dimensions of this vector space depends on the rank of <span class="math inline">\(P_X\)</span>,</p>
<p><span class="math display">\[
dim(S(X)) = r(P_X) = r(X) = k
\]</span></p>
<p>The matrix <span class="math inline">\(M_X = I_n-X(X'X)^{-1}X'\)</span> is also a <span class="math inline">\(n\times n\)</span> projection matrix. It projects any vector into <span class="math inline">\(X\)</span>’s <em>orthogonal</em> span: <span class="math inline">\(S^{\perp}(X)\)</span>. Any vector <span class="math inline">\(z\in S^{\perp}(X)\)</span> is orthogonal to <span class="math inline">\(X\)</span>. This includes the estimated residual, which is by definition orthogonal to the predicted values and, indeed, any column of <span class="math inline">\(X\)</span> (i.e.&nbsp;any regressor). The dimension of this orthogonal vector space depends on the rank of <span class="math inline">\(M_X\)</span>,</p>
<p><span class="math display">\[
dim(S^{\perp}(X)) = r(M_X) = r(I_n)-r(X) = n-k
\]</span></p>
<p>The orthogonality of these two projections can be easily shown, since projection matrices are idempotent (<span class="math inline">\(P_XP_X = P_X\)</span>) and symmetric (<span class="math inline">\(P_X' = P_X\)</span>). Consider the inner product of these two projections,</p>
<p><span class="math display">\[
P_X'M_X = P_X(I_n-P_X) = P_X-P_XP_X = P_X-P_X = 0
\]</span></p>
<p>The least squares estimator is a projection of Y into two vector spaces: one the span of the columns of <span class="math inline">\(X\)</span> and the other a space orthogonal to <span class="math inline">\(X\)</span>.</p>
<p>Why is this useful? Well, it helps us understand the “mechanics” (technically geometry) of OLS. When working with linear regression models, we typically assume either strict exogeneity - <span class="math inline">\(E[\varepsilon|X]=0\)</span> - or uncorrelatedness - <span class="math inline">\(E[X'\varepsilon]=0\)</span> - where the former implies the latter (but not the other way around).</p>
<p>When we use OLS, we estimate the vector <span class="math inline">\(\hat{\beta}\)</span> such that,</p>
<p><span class="math display">\[
X'(Y-X\hat{\beta})=X'\hat{\varepsilon}=0 \quad always
\]</span></p>
<p>This is true, <em>not just in expectation</em>, but by definition. The relationship is “mechanical”: the regressors and estimated residual are perfectly uncorrelated. This can be easily shown:</p>
<p><span class="math display">\[
\begin{aligned}
X'\hat{\varepsilon} =&amp; X'M_XY \\
=&amp; X'(I_n-P_X)Y \\
=&amp;X'I_nY-X'X(X'X)^{-1}X'Y \\
=&amp;X'Y-X'Y \\
=&amp;0
\end{aligned}
\]</span></p>
<p>You are essentially imposing the assumption of uncorrelatedness between the explained and unexplained components of Y on the data. This means that if the assumption is wrong, so is the projection.</p>
</section>
<section id="partitioned-regression" class="level3" data-number="5.3">
<h3 data-number="5.3" class="anchored" data-anchor-id="partitioned-regression"><span class="header-section-number">5.3</span> Partitioned regression</h3>
<p>The tools of linear algebra can help us better understand partitioned regression. Indeed, I would go as far to to say that it is quite difficult to understand partitioned regression without an understanding of projection matrices. Moreover, we need to understand partitioned regression to really understand multivariate regression. The partitioned regression result is referred to as Frisch-Waugh-Lovell Theorem (FWL).</p>
<div id="thm-fwl" class="theorem" title="Frisch-Waugh-Lovell (FWL)">
<p><span class="theorem-title"><strong>Theorem 1</strong></span> FWL says that if you have two sets of regressors, <span class="math inline">\([X_1,X_2]\)</span>, then <span class="math inline">\(\hat{\beta}_1\)</span>, the OLS estimator for <span class="math inline">\(\beta_1\)</span>, from the regression,</p>
<p><span class="math display">\[
        Y = X_1\beta_1 + X_2\beta_2 + \varepsilon
\]</span></p>
<p>is also given by the regression,</p>
<p><span class="math display">\[
        M_2Y = M_2X_1\beta_1 + \xi
\]</span></p>
</div>
<p>We can demonstrate the FWL theorem result using projection matrices. Two simplify matters, we will divide the set of regressors into two groups: <span class="math inline">\(X_1\)</span> a single regressor and <span class="math inline">\(X_2\)</span> a <span class="math inline">\(n\times (k-1)\)</span> matrix. We can rewrite the linear model as,</p>
<p><span class="math display">\[
Y = X\beta+ \varepsilon =\beta_1X_1+X_2\beta_2+\varepsilon
\]</span></p>
<p>Let us begin by applying our existing knowledge. From above, we know that the residual from the regression of <span class="math inline">\(X_1\)</span> on <span class="math inline">\(X_2\)</span> is,</p>
<p><span class="math display">\[
\hat{\upsilon} = M_2X_1
\]</span></p>
<p>where <span class="math inline">\(M_2 = I_n-X_2(X_2'X_2)^{-1}X_2'\)</span>. It turns out, it does not matter if we residualize <span class="math inline">\(Y\)</span> too. <strong>Can you see why?</strong> Thus, the model we estimate in the second step, is</p>
<p><span class="math display">\[
Y = \gamma_1\underbrace{M_2X_1}_{\hat{\upsilon}}+\xi
\]</span></p>
<p>We know that <span class="math inline">\(\hat{\gamma}_1 = (\hat{\upsilon}'\hat{\upsilon})^{-1}\hat{\upsilon}'Y\)</span>. Replacing the value of the residual, we get,</p>
<p><span class="math display">\[
\begin{aligned}
\hat{\gamma}_1 =&amp; (\hat{\upsilon}'\hat{\upsilon})^{-1}\hat{\upsilon}'Y \\
=&amp;(X_1'M_2M_2X_1)^{-1}X_1'M_2Y \\
=&amp;(X_1'M_2X_1)^{-1}X_1'M_2Y
\end{aligned}
\]</span> Here, we use both the symmetric and idempotent qualities of <span class="math inline">\(M_2\)</span>. Next we want to show that <span class="math inline">\(\hat{\beta}_1\)</span> is given by the same value. This part is more complicated. Let’s start with by reminding ourselves of the following:</p>
<p><span class="math display">\[
\begin{aligned}
X'X\hat{\beta} &amp;= X'Y \\
\begin{bmatrix}X_1 &amp; X_2\end{bmatrix}'\begin{bmatrix}X_1 &amp; X_2\end{bmatrix}\begin{bmatrix}\hat{\beta}_1 \\ \hat{\beta}_2\end{bmatrix} &amp;= \begin{bmatrix}X_1 &amp; X_2\end{bmatrix}'Y \\
\begin{bmatrix}X_1'X_1 &amp; X_1'X_2 \\ X_2'X_1 &amp; X_2'X_2 \end{bmatrix}\begin{bmatrix}\hat{\beta}_1 \\ \hat{\beta}_2\end{bmatrix} &amp;= \begin{bmatrix}X_1'Y \\ X_2'Y\end{bmatrix}
\end{aligned}
\]</span></p>
<p>We could solve for <span class="math inline">\(\hat{\beta}_1\)</span> by solving for the inverse of <span class="math inline">\(X'X\)</span>; however, this will take a long time. An easier approach is to simply verify that <span class="math inline">\(\hat{\beta}_1=(X_1'M_2X_1)^{-1}X_1'M_2Y\)</span>. Recall, <span class="math inline">\(\hat{\beta}\)</span> splits <span class="math inline">\(Y\)</span> into two components:</p>
<p><span class="math display">\[
Y = \hat{\beta}_1X_1+X_2\hat{\beta}_2 + \hat{\varepsilon}
\]</span></p>
<p>If we plug this definition of <span class="math inline">\(Y\)</span> into the above expression we get,</p>
<p><span class="math display">\[
\begin{aligned}
&amp;(X_1'M_2X_1)^{-1}X_1'M_2(\hat{\beta}_1X_1+X_2\hat{\beta}_2 + \hat{\varepsilon}) \\
=&amp;\hat{\beta}_1\underbrace{(X_1'M_2X_1)^{-1}X_1'M_2X_1}_{=I_n} \\
&amp;+\underbrace{(X_1'M_2X_1)^{-1}X_1'M_2X_2\hat{\beta_2}}_{=0} \\
&amp;+\underbrace{(X_1'M_2X_1)^{-1}X_1'M_2\hat{\varepsilon}}_{=0} \\
=&amp;\hat{\beta}_1
\end{aligned}
\]</span></p>
<p>In line 2, I use the fact that <span class="math inline">\(\hat{\beta}_1\)</span> is a scalar and can be moved to the front (since the order of multiplication does not matter with a scalar). In line 3, I use the fact that <span class="math inline">\(M_2X_2=0\)</span> by definition. Line 4 uses the fact that <span class="math inline">\(M_2\hat{\varepsilon}=\hat{\varepsilon}\)</span> which means that <span class="math inline">\(X_1'M_2\hat{\varepsilon}=X_1'\hat{\varepsilon}=0\)</span>.</p>
<p>The OLS estimator solves for <span class="math inline">\(\beta_1\)</span> using the variance in <span class="math inline">\(X_1\)</span> that is orthogonal to <span class="math inline">\(X_2\)</span>. This is the manner in which we “hold <span class="math inline">\(X_2\)</span> constant”: the variation in <span class="math inline">\(M_2X_1\)</span> is orthogonal to <span class="math inline">\(X_2\)</span>. Changes in <span class="math inline">\(M_2X_1\)</span> are <em>uncorrelated</em> with changes in <span class="math inline">\(X_2\)</span>; <em>as if</em> the variation in <span class="math inline">\(M_2X_1\)</span> arose independently of <span class="math inline">\(X_2\)</span>. However, uncorrelatedness does NOT imply independence.</p>
</section>
</section>
<section id="references" class="level2" data-number="6">




</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">6 References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-wooldridge2010" class="csl-entry" role="listitem">
Wooldridge, Jeffrey M. 2010. <em>Econometric Analysis of Cross Section and Panel Data</em>. MIT press.
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>My notation assumes that <span class="math inline">\(X_i\)</span> is a column vector, which makes <span class="math inline">\(X_i'\beta\)</span> a scalar. Wooldridge (2010) uses the notation <span class="math inline">\(X_i\beta\)</span>, implying that <span class="math inline">\(X_i\)</span> is a row vector. This is a matter of preference.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>You might also refer to the vector of regressors or explanatory variables. The terms covariates or control variables are more common in Microeconometrics literature, where regressors are typically included for identification of causal effects. Some texts will use the term independent variables, but this name implies a specific relationship between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span> that need not hold. Note, we will asssume in this term that <span class="math inline">\(n&gt;k\)</span>; i.e.&nbsp;this is “small” data.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>This is <strong>NOT</strong> the residual.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>See extra material on Linear Algebra to read more on rank.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>For <span class="math inline">\(n\)</span> large, <span class="math inline">\(rank(E[X_iX_i'])=k\Rightarrow rank(X)=k\)</span>. This follows from Law of Large Numbers, since <span class="math inline">\(plim(n^{-1}X'X) = E[X_iX_i']\)</span>.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>There are many such failures of (parametric) identification in models that include dummy variables (or fixed effects). Earlier we saw that the intercept is not separately identified from the mean of the error term. Mean independence of the error term, <span class="math inline">\(E[\varepsilon_i]=0\)</span>, is required for us to separately ‘identify’ <span class="math inline">\(\beta_1\)</span>.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>When working with vectors and matrices it is important to keep track of their size. You can only multiply two matrices/vectors if their column and row dimensions match. For example, if <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are both <span class="math inline">\(n\times k\)</span> matrices (<span class="math inline">\(n\neq k\)</span>), then <span class="math inline">\(AB\)</span> is not defined since <span class="math inline">\(A\)</span> has <span class="math inline">\(k\)</span> columns and <span class="math inline">\(B\)</span> <span class="math inline">\(n\)</span> rows. For the same reason <span class="math inline">\(BA\)</span> is also not defined. However, you can pre-multiply <span class="math inline">\(B\)</span> with <span class="math inline">\(A'\)</span> as <span class="math inline">\(A'\)</span> is a <span class="math inline">\(k\times n\)</span> matrix: <span class="math inline">\(A'B\)</span> is therefore a <span class="math inline">\((k\times n)\cdot (n\times k)=k\times k\)</span> matrix. Similarly, <span class="math inline">\(B'A\)</span> is defined, but is a <span class="math inline">\(n\times n\)</span> matrix.</p>
<p>Order matters when working with matrices and vectors. Pre-multiplication and post-multiplication are not the same thing.</p>
<p>Keep track of the size of each term to ensure they correspond to one another. In this instance, each term should be a scalar. For example, <span class="math inline">\(-2b'X'Y\)</span> is the multiplication of a scalar (<span class="math inline">\(-2\)</span>: size <span class="math inline">\(1\times 1\)</span>), row vector (<span class="math inline">\(b'\)</span>: size <span class="math inline">\(1\times k\)</span>), matrix (<span class="math inline">\(X'\)</span>: size <span class="math inline">\(k\times n\)</span>), and column vector (<span class="math inline">\(Y\)</span>: size <span class="math inline">\(n\times 1\)</span>). Thus we have a <span class="math inline">\((1\times 1)\cdot (1\times k)\cdot (k\times n)\cdot (n\times 1)=1\times 1\)</span>.<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>Once you are familiar with vector notation, it is relatively easy to tell whether a model is uni- or multi-variate. This is because the notation <span class="math inline">\(\beta_2 X_{i2}\)</span> is not consistent with <span class="math inline">\(X_{2i}\)</span> being a vector (row or column).</p>
<p>If <span class="math inline">\(X_{i2}\)</span> is a <span class="math inline">\(k\times 1\)</span> vector then so is <span class="math inline">\(\beta_2\)</span>. Thus, <span class="math inline">\(\beta_2 X_{i2}\)</span> is <span class="math inline">\((k\times 1)\cdot (k\times1)\)</span>, which is not defined.</p>
<p>If <span class="math inline">\(X_{i2}\)</span> is a row vector (as in Wooldridge, 2011), <span class="math inline">\(\beta_2 X_{i2}\)</span> will then be <span class="math inline">\((k\times 1)\cdot (1\times k)\)</span>, a <span class="math inline">\(k\times k\)</span> matrix. This cannot be correct since the model is defined at the unit level.</p>
<p>Thus, if you see a model written with the parameter in front of the regressor, you know that this must be a single regressor. This is subtle, yet imporant, distinction that researchers often use to convey the structure of their model. Whenever <span class="math inline">\(X_{i2}\)</span> is a vector, researchers will <em>almost always</em> use the notation <span class="math inline">\(X_{i2}'\beta\)</span> or <span class="math inline">\(X_{i2}\beta\)</span>, depending on whether <span class="math inline">\(X_{i2}\)</span> is assumed to be a column or row vector.<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>