{
  "hash": "875c94bfc0e9317bdc48d5710b63a0f5",
  "result": {
    "engine": "jupyter",
    "markdown": "---\nformat:\n  html: default\n  pdf: default\ntitle: Problem Set 3\n---\n\n\n\n\n\nThe purpose of this problem set is for you to see how the ordinary least squares (OLS) estimator behaves under various assumptions in a linear regression model where you know what the model is â€“ since you are going to be generating the data from a known data generating process (DGP).\n\nThe models estimated are simple bivariate regressions but the properties of the OLS estimator with vary with each case. This is demonstrated by changing the (a) distributional properties of the error term (variance-covariance structure), and (b) inducing correlation between the regressor and the error term. Any resulting bias and/or inconsistency will depend on the DGP.\n\nTo achieve certain results we will have to use a serially-correlated error structure, which is only appropriate in a time-series setting. For this reason, the models will be written with subscript $t$ and not $i$. \n\nThe code has been provided for model 1. You can then modify the code for models 2-4. \n\n## Preamble\n\n\n::: {#6a1f3d13 .cell execution_count=1}\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<style>div.jp-Notebook .datagrid-container {min-height: 448px; }</style>\n```\n:::\n:::\n\n\nYou do not need to load data for this problem set.\n\n::: {#7cfa1042 .cell execution_count=2}\n``` {.stata .cell-code}\nclear \n//or, to remove all stored values (including macros, matrices, scalars, etc.) \n*clear all\n\n* Replace $rootdir with the relevant path to on your local harddrive.\ncd \"$rootdir/problem-sets/ps-3\"\n\ncap log close\nlog using problem-set-3-log.txt, replace\n```\n:::\n\n\nHowever, since we are going to generate random variables, we should set a seed. This ensures replicability of the exercise. The number you choose is arbitrary, it simply ensures that any algorithms used to generate (pseudo) random variables start at the same place. \n\n::: {#9460324c .cell execution_count=3}\n``` {.stata .cell-code}\nset seed 981836\n```\n:::\n\n\n## Model 1: CLRM\n\nThis is your classical linear regression model. OLS estimator is unbiased and consistent.\n\n$$\nY_t = \\beta_1 + \\beta_2X_t + \\upsilon_t \\qquad \\text{with}\\quad \\upsilon_t\\sim N(0,\\sigma^2)\n$$\n\nWe know that the OLS estimator for $\\beta_2$ is given by,\n\n$$\n\\begin{aligned}\n  \\hat{\\beta}_2 =& \\frac{\\sum_t \\big[(X_t-\\bar{X})(Y_t-\\bar{Y})\\big]}{\\sum_t (X_t-\\bar{X})^2} \\\\\n  =& \\beta_2 + \\frac{\\sum_t \\big[(X_t-\\bar{X})(\\upsilon_t-\\bar{\\upsilon})\\big]}{\\sum_t (X_t-\\bar{X})^2} \\\\\n  =& \\beta_2 + \\frac{\\sum_t \\tilde{X}_t\\tilde{\\upsilon}_t}{\\sum_t \\tilde{X}_t^2}\n\\end{aligned}  \n$$\nwhere $\\tilde{X}_t$ and $\\tilde{\\upsilon}_t$ represent the demeaned counterparts of these variables. Alternatively, using linear algebra notation:\n\n$$\n\\begin{aligned}\n  \\hat{\\beta}_2 =& \\frac{X'M_{\\ell}Y}{X'M_{\\ell}X} \\\\\n  =& \\beta_2 + \\frac{X'M_{\\ell}\\upsilon}{X'M_{\\ell}X} \\\\\n  =& \\beta_2 + \\frac{\\tilde{X}'\\tilde{\\upsilon}}{\\tilde{X}'\\tilde{X}}\n\\end{aligned}  \n$$\nwhere $\\tilde{X} = M_{\\ell}X$, $\\tilde{\\upsilon}= M_{\\ell}\\upsilon$, and $M_{\\ell} = I_n-\\ell(\\ell'\\ell)^{-1}\\ell'$ (the orthogonal projection of the constant regressor).\n\nWe know from Handouts 2 & 3, \n\n1.    $E[\\hat{\\beta}_2] = \\beta_2$ (i.e., unbiased)\n\n2.    $p \\lim \\hat{\\beta}_2 = \\beta_2$ (i.e., consistent)\n\n**Can you demonstrate these results?** \n\n### Simulation\n\nBegin by designing a programme that takes the parameters of the model as arguments, generates the data, estimates the model, and then returns the stored values. \n\n::: {#6562510c .cell execution_count=4}\n``` {.stata .cell-code}\ncapture program drop mc1\nprogram define mc1, rclass\n\tsyntax [, obs(integer 1) s(real 1) b1(real 0) b2(real 0)  sigma(real 1)]\n\tdrop _all\n\tset obs `obs'\n\tgen u = rnormal(0,`sigma')\t          // sigma is the std deviation of the error distribution\n\tgen x=uniform()*`s'                   // s is the std devation of the x distribution\n\tgen y=`b1'+`b2'*x + u\t                // this generates the dep variable y\n\treg y x\n\treturn scalar b1=_b[_cons]            // intercept estimate\n\treturn scalar b2=_b[x]\t\t            // coeff on the x variable\n\treturn scalar se2 = _se[x]            // std error\n    return scalar t2 = _b[x]/_se[x]     // t ratio\n\nend\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n```\n:::\n:::\n\n\nUse the the `simulate` command in Stata to estimate the model 100 times:\n\n::: {#648883e1 .cell execution_count=5}\n``` {.stata .cell-code}\nsimulate b1=r(b1) b2=r(b2) se2=r(se2) t2=r(t2), reps(100): mc1, obs(50) s(6) b1(4) b2(2) sigma(3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n      Command: mc1, obs(50) s(6) b1(4) b2(2) sigma(3)\n           b1: r(b1)\n           b2: r(b2)\n          se2: r(se2)\n           t2: r(t2)\n\nSimulations (100): .........10.........20.........30.........40.........50.....\n> ....60.........70.........80.........90.........100 done\n```\n:::\n:::\n\n\nCalculate the bias and plot the distribution of the bias. \n\n::: {#d5c770da .cell execution_count=6}\n``` {.stata .cell-code}\ngen bias2=b2-2\nsu b1 b2 se2 t2\nsu bias2\nhistogram bias2, normal xline(`r(mean)')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n          b1 |        100    3.880226    .9977415   1.080851   6.090028\n          b2 |        100    2.041985     .271704   1.365155   2.673303\n         se2 |        100    .2520885    .0291596   .1814694   .3255497\n          t2 |        100    8.216185      1.4968   5.484826   12.60699\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n       bias2 |        100    .0419852     .271704  -.6348448   .6733029\n(bin=10, start=-.63484478, width=.13081477)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](problem-set-3_files/figure-html/cell-7-output-2.png){}\n:::\n:::\n\n\n## Model 2: Serial Correlation\n\nRelax the assumption of an iid error term and allow for serial correlation. The OLS estimator is unbiased and consistent. However, the std errors are wrong since the software does not know that you have serially correlated errors and you are not taking this into account in the estimation.\n\n$$\nY_t = \\beta_1 + \\beta_2X_t + \\upsilon_t \\qquad \\text{where}\\quad \\upsilon_t = \\rho \\upsilon_{t-1}+\\varepsilon_t \\quad\\text{and}\\quad \\varepsilon_t\\sim N(0,\\sigma^2)\n$$\nWe say that $U_t$ follows an AR(1) process. You can show that $\\hat{\\beta}_2$ remains unbiased and consistant. However, the standard homoskedastic-variance estimator is incorrect:\n\n$$\nVar(\\hat{\\beta}_2) \\neq \\frac{\\sigma^2}{Var(X_i)}\n$$\n\n### Simulation\n\nYou will need to redesign the above programme. The challenging part is the simulation of the error term. This needs to follow an AR(1) process and must therefore be generated in sequence. You can do this as follows:\n\n::: {#c93983c4 .cell execution_count=7}\n``` {.stata .cell-code}\n  gen u=0 \n\tgen time=_n\n\tgen e = rnormal(0,`sigma')\t\n\tforvalues i=2/`obs'  {\n\treplace u=`rho'*u[_n-1] + e if _n==`i'\n\t}\n```\n:::\n\n\n## Model 3: Dynamic model without serial correlation\n\nConsider a version of Model 1, where the regressor is the lag of the dependent variable. \n$$\nY_t = \\beta_1 + \\beta_2Y_{t-1} + \\upsilon_t \\qquad \\text{with}\\quad \\upsilon_t\\sim N(0,\\sigma^2)\n$$\nThe OLS estimator is now, \n$$\n  \\hat{\\beta}_2 = \\beta_2 + \\frac{\\sum_t \\tilde{Y}_{t-1}\\tilde{\\upsilon}_t}{\\sum_t \\tilde{Y}_{t-1}^2}\n$$\nThis model is biased, since\n\n$$\nE\\bigg[\\frac{\\sum_t \\tilde{Y}_{t-1}\\tilde{\\upsilon}_t}{\\sum_t \\tilde{Y}_{t-1}^2}\\bigg] \\neq \\frac{E\\big[\\sum_t \\tilde{Y}_{t-1}\\tilde{\\upsilon}_t\\big]}{E\\big[\\sum_t \\tilde{Y}_{t-1}^2\\big]}\n$$\nWhen the regressor was $X_t$, the above statement was true given the Law of Iterated Expectations. However, you can use Slutsky's theorem and the WLLN to show that $\\hat{\\beta}_2\\rightarrow_p \\beta_2$. This result relies on the fact that $Y_{t-1}$ is realized before $\\upsilon_t$ which is iid. Thus, the bias goes to 0 as $n\\rightarrow \\infty$. \n\n### Simulation\n\nYou can use lag-operators in Stata to regress the outcome against its lagged value. For example:\n\n::: {#6c51b638 .cell execution_count=8}\n``` {.stata .cell-code}\n\tgen time=_n\n\ttsset time\n\treg y  L.y\n```\n:::\n\n\nWhile the error term is serially uncorrelated (as in Model 1), you will need to generate the outcome sequentially (row value by row value). This is because the DGP has a lagged dependent variable structure. \n\n## Model 4: Dynamic model with serial correlation\nConsider a version of Model 2, where the regressor is the lag of the dependent variable. \n$$\nY_t = \\beta_1 + \\beta_2Y_{t-1} + \\upsilon_t \\text{where}\\quad \\upsilon_t = \\rho \\upsilon_{t-1}+\\varepsilon_t \\quad\\text{and}\\quad \\varepsilon_t\\sim N(0,\\sigma^2)\n$$\nAs with model 3, the OLS estimator will be biased. In addition, since $Cov(\\upsilon_t,\\upsilon_{t-1})\\neq0$ and $Cov(Y_t,\\upsilon_{t})\\neq 0$ (for any $t$), \n$$\n\\Rightarrow Cov(Y_{t-1},\\upsilon_{t})\\neq 0\n$$\nAs a result $\\hat{\\beta}_2$ is inconsistent. \n\n### Simulation\n\nYou will need to use tricks from both models 2 and 3 to simulate this model. \n\n## Postamble\n\n::: {#589304aa .cell execution_count=9}\n``` {.stata .cell-code}\nlog close\n```\n:::\n\n\n",
    "supporting": [
      "problem-set-3_files\\figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}