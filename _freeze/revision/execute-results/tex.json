{
  "hash": "20d65ee4a85958f0449dbf7fdb368c59",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Revision Questions\nformat:\n  html:\n    toc-depth: 3           # Set the depth of the table of contents\n    number-sections: true  # Number the sections in the HTML output\n    resources: \n      - \"revision.pdf\" # Make the PDF file available for download\n    code-fold: true   # places code in dropdown\n  pdf:\n    #documentclass: article # LaTeX class used for the PDF document\n    toc: true              # Include a table of contents in the PDF output\n    number-sections: true  # Number the sections in the PDF output\n    keep-md: false          # Optionally keep the intermediate markdown file\n    output-file: \"revision.pdf\" # Specify the PDF output file name\n    echo: false  # hides code\njupyter: nbstata  \n---\n\n\n\n\n\n\n## Basics\n\n1.    Consider two random $k$-dimension vectors $\\{X,Y\\}$ and non-random $k\\times k$ matrices $\\{A,B\\}$. Show,\n\n      1.1. $Var(AX) = A Var(X) A'$\n      \n      1.2. $Var(AX+b) = A Var(X) A'$ for non-random $k$-dimension vector $b$    \n      \n      1.3. $Cov(AX,BY) = A Cov(X,Y) B'$\n      \n2.    Suppose $X\\sim N(\\mu,\\Sigma)$, with $X\\in \\mathbb{R}^k$. Find the distribution, $Y = AX + b$, for non-random $k\\times k$ matrix $A$ and non-random $k$-dimension vector $b$. \n\n## CLRM\n\n1.    Which of the CLRM assumptions is required for identification of $\\beta$? Demonstrate this claim. \n\n2.    Provide an example of a model that is non-linear in regressors, but linear in parameters. Similarly, provide an example of a model that is linear in regressors, but non-linear in parameters. \n\n3.    Suppose, the true data generating process was given by, \n\n      $$\n      Y = X\\beta + \\epsilon\n      $$\n      where $E[\\epsilon|X] = \\alpha$ and $X$ included a constant term with parameter $\\beta_1$. Is $\\beta_1$ identified?\n      \n4.    Given the result $\\beta = E[X_iX_i']^{-1}E[X_iY_i]$. With two regressors, $X_i = [1\\;X_{2i}]'$, show \n\n      4.1.    $\\beta_2 = \\frac{Cov(X_{2i},Y_i)}{Var(X_{2i})}$\n      \n      4.2.    $\\beta_1 = E[Y_i]-\\beta_2E[X_i]$\n      \n## OLS\n\n1.    Consider the projection matrices $P_X = X(X'X)^{-1}X'$ and $M_X = I_n-P_X$. Show,\n\n      1.1.  $P_XX = X$\n    \n      1.2. $M_XX = 0$\n    \n      1.3. $P_XM_X = 0$\n    \n      1.4. $X'P_X=X'$\n    \n2.    Show that $X'X$, where $X$ is a $n\\times k$ random matrix, can be expressed as $\\sum_{i=1}^nX_iX_i'$, where $X_i$ is a $k\\times 1$ vector.\n\n3.    Consider the partitioned regression model, \n      $$\n      Y = X_1\\beta_1 + X_2\\beta_2 + u\n      $$ \n      Show,\n\n         \n      3.1.  $E[\\hat{\\beta}_1|X]=\\beta_1$ where $\\hat{\\beta}_1 = (X_1'M_2X_1)^{-1}X_1'M_2Y$.\n    \n      3.2. Write down the conditional variance of the OLS estimator for $\\beta_1$, assuming homoskedasticity: $E[uu'|X]=\\sigma^2 I_n$.\n    \n      3.3. Write down the conditional variance of the OLS estimator for $\\beta_1$, assuming heteroskedasticity: $E[uu'|X]=\\Omega$.\n\n4.    Demonstrate the BLUE result: the OLS estimator ($\\hat{\\beta}$) is the Best Linear Unbiased Estimator. Consider the alternative unbiased, linear estimator $b=AY$; such that, \n      $$\n      \\begin{aligned}\n      E[b|X] = \\beta\n      \\end{aligned}\n      $$\n    \n      4.1. Show that since $b$ is unbiased, it must be that $AX = I_k$.\n    \n      4.2. Using the above result, show that under CLRM 1-6 (i.e., including homoskedasticity), \n      $$\n      Cov(\\hat{\\beta},b|X) = Var(\\hat{\\beta}|X)\n      $$\n    \n      4.2.   Show that $Var(b|X)-Var(\\hat{\\beta}|X)\\geq 0$ (i.e. a positive semi-definite matrix), by solving for \n      $$\n      Var(\\hat{\\beta}-b|X)\n      $$\n    \n5.    Consider the GLS estimator ($\\tilde{\\beta}$), which solves the problem\n\n      $$\n      \\underset{b}{\\min}\\; (Y-Xb)'\\Omega^{-1}(Y-Xb)\n      $$\n      \n      where $E[\\varepsilon \\varepsilon'|X]=\\Omega$. \n      \n      5.1.    Show that $Var(\\hat{\\beta}|X)-Var(\\tilde{\\beta}|X)\\geq 0$, where $\\hat{\\beta}$ is the OLS estimator. \n      \n      5.2.    Under which assumption are the two estimators equivalent?\n\n## Linear Tests\n\n1.    Under CLRM 1-6, solve for the finite sample distribution of $R\\hat{beta}$, where $R$ is non-random $k\\times k$ matrix. \n\n2.    Consider the multiple, linear hypotheses:\n\n      $$\n      \\begin{aligned}\n      H_0:\\;& \\beta_2 -4\\beta_4 = 1 \\\\\n      & \\beta_3 = 3 \\\\\n      & \\beta_5 = \\beta_6\n      \\end{aligned}\n      $$\n      \n      2.1.    Write the 4 hypotheses in the form $R\\beta = r$. \n      \n      2.2.    Write down the F-statistic for the test (assuming homoskedasticity) as well as it's finite sample distribution. \n      \n      2.3.    What is the asymptotic distribution of this test statistic.\n      \n      2.4.    For a linear model with $k=6$, write the restricted model corresponding to the above hypotheses. \n      \n3.    Consider the model of household food expenditure, \n\n      $$\n      foodexp_i = \\beta_1 + \\beta_2 inc_i + \\beta_3hhsize_i + \\beta_4hhsize^2_i + \\varepsilon_i\n      $$\n      \n      3.1.    Suppose you wish to test the hypothesis of increasing returns to food consumption in the household: each additional household member is marginally cheaper to feed. Which is a more powerful test:\n      \n      $$\n      H_0:\\; \\beta_4=0 \\qquad \\text{against}\\qquad H_1:\\; \\beta_4\\neq0 \n      $$\n      or, \n      $$\n      H_0:\\; \\beta_4\\geq 0 \\qquad \\text{against}\\qquad H_1:\\; \\beta_4<0 \n      $$\n      \n      3.2.    Transform the model to test the hypothesis $H_0: -\\beta_3/\\beta_4 = 5$, using just the coefficient on the variable $hhsize$.\n      \n      3.3.     Transform the model to test the same hypothesis, using just the coefficient on the variable $hhsize^2$.\n      \n\n## Panel Data\n\n1.    Show that $\\tilde{Y}_i = Y_i - \\bar{Y}_i$, where $\\bar{Y}_i = \\frac{1}{T}\\sum_{t=1}^T Y_{it}$, can be written as $Y = M_\\ell Y_i$, where $M_\\ell = \\ell (\\ell'\\ell)^{-1}\\ell'$ and $\\ell$ is a $T\\times 1$ vector of 1's. \n\n2.    Show that $X'X$, where $X$ is a $nT\\times k$ random matrix, can be expressed as $\\sum_{i=1}^nX_i'X_i$, where $X_i$ is a $T\\times k$ matrix. \n\n3.    Show that $I_n\\otimes M_\\ell$ is an idempotent matrix and $\\sum_{i=1}^n\\tilde{X}_i'\\tilde{X}_i=X'(I_n\\otimes M_\\ell)X$.\n\n4.    Show that for T=3, $D'(DD')^{-1}D = M_\\ell$. \n\n5.    Demonstrate that the OLS estimator of the 'within-unit' transformed model is unbiased. Is it efficient?\n\n6.    Conditional variance of random effects model,\n\n      6.1.    Solve for $Var(\\hat{\\beta}^{OLS}|X)$, the variance of the ('pooled') OLS estimator for the random effects model.\n      \n      6.2.    Solve for $Var(\\hat{\\beta}^{GLS}|X)$, the variance of the GLS estimator for the random effects model.\n      \n      6.3.    Verify that $Var(\\hat{\\beta}^{OLS}|X)-Var(\\hat{\\beta}^{GLS}|X)\\geq 0$, is a positive-semidefinite matrix.\n      \n7.    Conditional variance of first-differenced transformation\n\n      7.1.    Solve for $Var(\\hat{\\beta}^{FD-OLS}|X)$, the variance of the OLS estimator for the FD transformation.\n      \n      7.2.    Solve for $Var(\\hat{\\beta}^{FD-GLS}|X)$, the variance of the GLS estimator for the FD transformation.\n      \n      7.3.    Verify that $Var(\\hat{\\beta}^{FD-OLS}|X)-Var(\\hat{\\beta}^{FD-GLS}|X)\\geq 0$, is a positive-semidefinite matrix.\n      \n8.    Why is it important that the assumed model underlying the 'pooled' OLS estimator and 'within' OLS estimator are the same in the Hausman test? That is, why cannot we not compare the LSDV estimator of the fixed-effects model with the 'pooled' OLS estimator?\n\n9.    What is the purpose of the Mundlack correction?\n\n\n## Binary Outcome Models\n\n1.    Consider the Logit model.\n\n      1.1.    Write down the likelihood function. \n  \n      1.2.    State the maximization problem that solves for ML estimator. \n  \n      1.3.    Solve for the F.O.C.s of the ML problem. \n\n      1.4.    Solve for the asymptotic variance-covariance matrix of $\\hat{\\beta}^{ML}$.\n\n2.    Consider the Probit model.\n\n      2.1.    Write down the likelihood function. \n  \n      2.2.    State the maximization problem that solves for ML estimator. \n  \n      2.3.    Solve for the F.O.C.s of the ML problem. \n\n      2.4.    Solve for the asymptotic variance-covariance matrix of $\\hat{\\beta}^{ML}$.\n\n3.    From the proof of the asymptotic normality, show\n\n      $$\n      E\\bigg[\\frac{\\partial^2 f(Y_i|X_i;\\beta_0)/\\partial\\beta\\partial\\beta'}{f(Y_i|X_i;\\beta_0)}\\bigg]=0\n      $$\n\n::: {.cell execution_count=1}\n\n::: {.cell-output .cell-output-display}\n```\n<IPython.core.display.HTML object>\n```\n:::\n:::\n\n\n::: {.cell execution_count=2}\n\n::: {.cell-output .cell-output-stdout}\n```\n\nIteration 0:  Log likelihood = -209.35624  \nIteration 1:  Log likelihood = -205.66439  \nIteration 2:  Log likelihood = -205.27888  \nIteration 3:  Log likelihood = -205.27756  \nIteration 4:  Log likelihood = -205.27756  \n\nLogistic regression                                    Number of obs = 165,038\n                                                       LR chi2(7)    =    8.16\n                                                       Prob > chi2   =  0.3189\nLog likelihood = -205.27756                            Pseudo R2     =  0.0195\n\n------------------------------------------------------------------------------\n    employed | Coefficient  Std. err.      z    P>|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      eduyrs |  -.2139028   .0990457    -2.16   0.031    -.4080287   -.0197768\n       exper |   .0408304   .0861167     0.47   0.635    -.1279552    .2096161\n      exper2 |  -.0012522   .0024549    -0.51   0.610    -.0060637    .0035593\n   1.married |  -.1047635   .5283537    -0.20   0.843    -1.140318    .9307907\n    1.female |   .0208576   .5035107     0.04   0.967    -.9660052    1.007721\n     1.child |   .4950251   .7490084     0.66   0.509    -.9730045    1.963055\n             |\nfemale#child |\n        1 1  |   .6419965   1.046185     0.61   0.539    -1.408489    2.692482\n             |\n       _cons |   11.64254   1.476069     7.89   0.000     8.749502    14.53559\n------------------------------------------------------------------------------\n```\n:::\n:::\n\n\n4.    Consider the logit model output above, estimated using the dataset from Problem Set 4. The sample and variables are defined as in PS 4. \n\n      4.1.    Compute the marginal effect of an additional year of education on the probability of being employed for a childless, unmarried, female with 15 years of education and 5 years of (potential) experience. \n    \n      4.2.    Compute the marginal effect of being married for a male with children, 12 years of education and 10 years of experience.\n    \n    \n## Endogenous Selection Models    \n\n1.    Consider the endogenous the right-censored Tobit model. The observed distribution is,\n\n      $$\n      f(y) = \\begin{cases} f^*(y) \\qquad \\text{for} \\quad y <0 \\\\\n      F^*(0)  \\qquad \\text{for} \\quad y=0 \\\\\n      0 \\qquad \\text{for} \\quad y>0\n      \\end{cases}\n      $$\n      Suppose, $Y^* = X_i'\\beta + \\varepsilon_i$, where the error term has a normally distributed (conditional on $X$).\n      \n      1.1.    Solve for $E[Y_i|Y_i<0]$ \n      \n      1.2.    Write down the likelihood function of the observed data.\n      \n      1.3.    Solve for $\\frac{\\partial E[Y_i|X_i,Y_i<0]}{\\partial X_i}$\n      \n      1.4.    Solve for $\\frac{\\partial E[Y_i|X_i]}{\\partial X_i}$\n\n      1.5.    How do the answers compare to the case where $Y$ is left-censored at 0?\n\n::: {.cell execution_count=3}\n\n::: {.cell-output .cell-output-stdout}\n```\n\nIteration 0:  Log likelihood = -209.35624  \nIteration 1:  Log likelihood = -205.60572  \nIteration 2:  Log likelihood = -205.33806  \nIteration 3:  Log likelihood = -205.33756  \nIteration 4:  Log likelihood = -205.33756  \n\nProbit regression                                      Number of obs = 165,038\n                                                       LR chi2(7)    =    8.04\n                                                       Prob > chi2   =  0.3293\nLog likelihood = -205.33756                            Pseudo R2     =  0.0192\n\n------------------------------------------------------------------------------\n    employed | Coefficient  Std. err.      z    P>|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      eduyrs |  -.0544498   .0255841    -2.13   0.033    -.1045937   -.0043059\n       exper |   .0095296   .0223186     0.43   0.669    -.0342139    .0532732\n      exper2 |  -.0002998   .0006319    -0.47   0.635    -.0015382    .0009387\n   1.married |  -.0259376   .1384233    -0.19   0.851    -.2972423    .2453672\n    1.female |   .0049027   .1321027     0.04   0.970    -.2540139    .2638192\n     1.child |   .1297358   .1937444     0.67   0.503    -.2499962    .5094679\n             |\nfemale#child |\n        1 1  |   .1540261   .2634162     0.58   0.559    -.3622602    .6703123\n             |\n       _cons |   4.341448   .3821216    11.36   0.000     3.592503    5.090392\n------------------------------------------------------------------------------\n```\n:::\n:::\n\n\n2.    Consider an endogenous threshold model, for (log of) employee earnings. Included in the model is a linear term in years of education, quadratic terms in years of (potential) experience, a married dummy-variable, and a female dummy-variable. \n\n      2.1.    Consider the output above from the selection equation, which includes an additional interaction between gender and presence of children (under 18). Compute the Inverse Mills Ratio for a married women, with children, 15 years of education and 8 years of experience. \n      \n      2.2.    Write down the likelihood function of the observed outcomes: $[ln(wage_i),Employed_i]$. Where the latter is a dummy variable indicating employment (positive earnings).\n      \n      2.3.    The Heckit output is given below. What can you conclude regarding selection into employment. And, under what assumptions. \n\n      2.4.    Does the interaction bewteen gender and parenthood belong in the main equation?\n\n::: {.cell execution_count=4}\n\n::: {.cell-output .cell-output-stdout}\n```\nnote: two-step estimate of rho = 19.806689 is being truncated to 1\n\nHeckman selection model -- two-step estimates   Number of obs     =    115,352\n(regression model with sample selection)              Selected    =    115,331\n                                                      Nonselected =         21\n\n                                                Wald chi2(5)      =       4.66\n                                                Prob > chi2       =     0.4590\n\n------------------------------------------------------------------------------\n             | Coefficient  Std. err.      z    P>|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nlnwage       |\n      eduyrs |   .1289768   .0827439     1.56   0.119    -.0331983     .291152\n       exper |   .0559093   .0878259     0.64   0.524    -.1162263    .2280449\n      exper2 |  -.0010644   .0023925    -0.44   0.656    -.0057535    .0036248\n   1.married |   .1015441   .4325466     0.23   0.814    -.7462316    .9493198\n    1.female |  -.3315884   .3953545    -0.84   0.402    -1.106469    .4432921\n       _cons |   4.387386   1.504299     2.92   0.004     1.439014    7.335759\n-------------+----------------------------------------------------------------\nemployed     |\n       exper |   .0079269   .0219371     0.36   0.718    -.0350691    .0509228\n      exper2 |  -.0001872   .0006005    -0.31   0.755     -.001364    .0009897\n   1.married |  -.0687001   .1375189    -0.50   0.617    -.3382323    .2008321\n    1.female |   -.023606   .1337126    -0.18   0.860    -.2856778    .2384659\n     1.child |   .1552287   .1938012     0.80   0.423    -.2246146    .5350721\n             |\nfemale#child |\n        1 1  |   .1308613   .2683362     0.49   0.626    -.3950681    .6567907\n             |\n       _cons |   3.482178   .1598692    21.78   0.000      3.16884    3.795516\n-------------+----------------------------------------------------------------\n/mills       |\n      lambda |   66.28671    929.642     0.07   0.943    -1755.778    1888.352\n-------------+----------------------------------------------------------------\n         rho |    1.00000\n       sigma |  66.286706\n------------------------------------------------------------------------------\n```\n:::\n:::\n\n\n",
    "supporting": [
      "revision_files\\figure-pdf"
    ],
    "filters": []
  }
}