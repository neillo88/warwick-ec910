{
  "hash": "1cfdcad3c3317fb3687bfd29c85a2e0d",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Endogenous Selection Models\nformat:\n  html:\n    toc-depth: 3           # Set the depth of the table of contents\n    number-sections: true  # Number the sections in the HTML output\n    resources: \n      - \"handout-7.pdf\" # Make the PDF file available for download\n    code-fold: true   # places code in dropdown\n  pdf:\n    #documentclass: article # LaTeX class used for the PDF document\n    toc: true              # Include a table of contents in the PDF output\n    number-sections: true  # Number the sections in the PDF output\n    keep-md: false          # Optionally keep the intermediate markdown file\n    output-file: \"handout-7.pdf\" # Specify the PDF output file name\n    echo: false  # hides code\n---\n\n::: {.cell}\n\n:::\n\n\n\n\n## Overview\n\nIn this handout we will review models that allow us to relax two important assumptions\n\n1.    random sampling in the cross-section dimension;\n\n2.    and unrestricted values of the dependent variable. \n\nIn doing so, we will discuss two types of distributions:\n\n1.    truncated distribution\n\n2.    censored distribution\n\nFurther reading can be found in:\n\n-   Chapter 16 of @cameron2005\n-   Section 7.4-7.6 of @verbeek2017\n-   Heckman, J.J. (1979) Sample selction bias as a specification error, Econometrica\n\n## Censored & Truncated Distributions\n\n### Censored data\n\nIn the case of censored data, there is *some* loss of information for the dependent variable but the explanatory variables are still observed. A common-case of censored data is top-coded income data in surveys; i.e. \"Income great than $100,000\". \n\nIn the extreme, you can think of a discrete-choice binary data as censored, since we don't observe the latent variable. \n\nThe following is an example of a left-censored distribution. For individuals with values of the outcome $Y<c$, we only observe the threshold $c$. The distribution is given by, \n\n$$\nf(y) = \\begin{cases} 0 \\qquad \\text{for} \\quad y <c \\\\\nF^*(c)  \\qquad \\text{for} \\quad y=c \\\\\nf^*(y) \\qquad \\text{for} \\quad y>c\n\\end{cases}\n$$\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](handout-7_files/figure-pdf/unnamed-chunk-2-1.pdf)\n:::\n:::\n\n\n\n\n### Truncated data\n\nIn truncated samples, both the dependent and explanatory variables are missing for some observations where he outcome value is one side of a threshold. For example, you might only observe the test score (and characteristics) of students who passed the test. \n\nAnother famous example is the labour market: we only observe the wages of those who are employed. Consider a consider where workers are observed working when they receive a wage offer that is as least as good as their reservation wage. According to such a model, those who are not employed must have received wage offers less than their reservation wage. It suggests, that the distribution of accepted wage offers will be truncated. \n\nThe following, is an example of a distribution that is left-truncated. We only observe values of $Y$ for $Y\\geq c$. The density of a left-truncated random variable is, \n\n$$\nf(y) = f^*(y|y>c) = \\frac{f^*(y)}{1-F^*(c)}  \n$$\nwhere $1-F^*(c) = Pr(y|y>c)$. In this way, truncation reduces the range of values the outcome variable can take. In this case, the mean of the truncated variable is greater than the unconditional mean. \n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](handout-7_files/figure-pdf/unnamed-chunk-3-1.pdf)\n:::\n:::\n\n\n\n\n### Truncated normal distribution\n\nBefore we proceed, it is worth revising some useful traits of (joint) normal distributions. \n\nWe know that if $X\\sim N(\\mu,\\sigma^2)$, then $Z = \\frac{X-\\mu}{\\sigma}\\sim N(0,1)$ (standard normal distribution). Moreover, as discussed in Handout 6, the pdf and cdf of the standard normal distribution are given by,\n\n$$\n\\phi(z) = \\frac{1}{\\sqrt{2\\pi}}exp(-z^2/2)\n$$\nand the CDF by, \n$$\n\\Phi(z) = \\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^z exp(-u^2/2)du\n$$\n\nThe pdf of the $X$ is given by,\n\n$$\nf_X(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}exp\\bigg(-\\frac{1}{2}\\bigg(\\frac{x-\\mu}{\\sigma}\\bigg)^2\\bigg) = \\frac{1}{\\sigma}\\phi(z)\n$$\nand the cdf by,\n$$\nF_X(x) = \\Phi(z) \\qquad \\text{where}\\quad z = \\frac{x-\\mu}{\\sigma}\n$$\nWe can evaluate the truncated mean of $Z\\sim N(0,1)$. First from the left:\n\n$$\n\\begin{aligned}\nE[Z|Z>c] =& \\int_{c}^{+\\infty}z\\frac{\\phi(z)}{1-\\Phi(c)}dz \\\\\n=& \\frac{1}{1-\\Phi(c)}\\int_{c}^{+\\infty}z\\phi(z)dz \\\\\n=& \\frac{1}{1-\\Phi(c)}\\int_{c}^{+\\infty}-\\phi'(z)dz \\\\\n=& \\frac{\\phi(c)}{1-\\Phi(c)}\n\\end{aligned}\n$$\nNext from the left:\n$$\n\\begin{aligned}\nE[Z|Z<c] =& \\int_{-\\infty}^{c}z\\frac{\\phi(z)}{\\Phi(c)}dz \\\\\n=& \\frac{1}{\\Phi(c)}\\int_{-\\infty}^{c}z\\phi(z)dz \\\\\n=& \\frac{1}{\\Phi(c)}\\int_{-\\infty}^{c}-\\phi'(z)dz \\\\\n=& \\frac{-\\phi(c)}{\\Phi(c)} \\\\\n=&-E[Z|Z>-c]\n\\end{aligned}\n$$\nwhere the last line is given by the symmetry of the standard normal distribution: $\\phi(c)=\\phi(-c)$ and $\\Phi(c)=1-\\Phi(-c)$. The function $\\frac{\\phi(c)}{\\Phi(c)} = E[Z|Z>-c]$ is referred to as the inverse mills ratio. \n\n### Conditional normal distribution\n\nLet $\\begin{bmatrix}Y & X\\end{bmatrix}'$ be joint normal:\n\n$$\n\\begin{bmatrix}Y \\\\ X\\end{bmatrix}\\sim N\\bigg(\\begin{bmatrix}\\mu_X \\\\ \\mu_Y\\end{bmatrix},\\begin{bmatrix}\\sigma^2_{Y} & \\sigma_{YX} \\\\ \\sigma_{XY} & \\sigma^2_{X}\\end{bmatrix}\\bigg)\n$$\nwhere $\\sigma_{YX}=\\sigma_{XY}$. Then, \n\n$$\nY|X\\sim N\\bigg(\\mu_Y + \\frac{\\sigma_{YX}}{\\sigma_X^2}(X-\\mu_X),\\sigma^2_{Y}-\\sigma_{YX}^2/\\sigma^2_{X}\\bigg)\n$$\nNote, the conditional mean of $E[Y|X]=\\mu_Y + \\frac{\\sigma_{YX}}{\\sigma_X^2}(X-\\mu_X)$ is a linear function of X. \n\n### Inverse Mills Ratio \nUsing the above result characteristic, we can show that if $\\begin{bmatrix}\\varepsilon_i & \\upsilon_i\\end{bmatrix}'$ are jointly normal, **with mean zero**, then\n\n$$\n\\begin{aligned}\nE[\\varepsilon_i|\\upsilon_i>-c] =& E\\big[E[\\varepsilon_i|\\upsilon_i,\\upsilon_i>-c]\\big|\\upsilon_i>-c\\big] \\\\\n =& E\\bigg[\\frac{\\sigma_{\\varepsilon\\upsilon}}{\\sigma_\\upsilon^2}\\upsilon_i\\bigg|\\upsilon_i>-c\\bigg] \\\\\n=&\\frac{\\sigma_{\\varepsilon\\upsilon}}{\\sigma_\\upsilon^2}E[\\upsilon_i|\\upsilon_i>-c] \\\\\n=&\\frac{\\sigma_{\\varepsilon\\upsilon}}{\\sigma_\\upsilon^2}\\sigma_\\upsilon E\\bigg[\\frac{\\upsilon_i}{\\sigma_\\upsilon}\\bigg|\\frac{\\upsilon_i}{\\sigma_\\upsilon}>-\\frac{c}{\\sigma_\\upsilon}\\bigg] \\\\\n=&\\frac{\\sigma_{\\varepsilon\\upsilon}}{\\sigma_\\upsilon}\\frac{\\phi(c/\\sigma_\\upsilon)}{\\Phi(c/\\sigma_\\upsilon)} \\\\\n=& \\frac{\\sigma_{\\varepsilon\\upsilon}}{\\sigma_\\upsilon}\\lambda(c')\n\\end{aligned}\n$$\nwhere $c' = c/\\sigma_\\upsilon$. In models where $\\upsilon_i$ is the error from the selection equation, $\\sigma_\\upsilon$ is not identified and must be normalized to 1. Thus, $c' = c$.\n\n## Models\n\nIn this section we will review a series of models that build on the latent variable model in Handout 6. \n\n$$\nY_i^* = X_i'\\beta + \\varepsilon_i\n$$\nWe want to learn about $\\beta$, but $Y^*_i$ is only partially observed due to some selection process. We will first review the Tobit model, where the outcome is censored. For example, top-coded earnings data. Next, we will look at selection/threshold models, where the outcome is observed based on a selection decision. For example, non-response in a survey. Finally, will look at endogenous switching models, where you observe the outcome under different potential states, and a selection decision determines the state of observation. For example, wages earned in different sectors of the economy. \n\nIn all cases, the regressors (covariates) will be observed regardless of selection. This is important as we would otherwise not be able to identify the selection decision. Thus, we will not be able to evaluate truncated samples, where covariates are not observed when the outcome is not observed. For example, data on political donations. \n\n\n### Tobit model\n\nIn a Tobit model, the outcome is censored. You observe $Y_i^*$ only above (or below) a given threshold $c$, otherwise you observe the threshold value. However, you do observe the regressors (covariates) for all observations (regardless of censoring). \n\n$$\nY_i = \\begin{cases}Y_i^* \\qquad \\text{if} \\quad D_i=1 \\\\\nc \\qquad \\text{if} \\quad D_i=0 \\end{cases}\n$$\nwhere,\n\n$$\nD_i = \\begin{cases}1 \\qquad \\text{if} \\quad Y^*>c \\\\\n0 \\qquad \\text{if} \\quad Y^*\\leq c \\end{cases}\n$$\nGiven the latent variable model, you observe,\n$$\nY_i = \\begin{cases}X_i'\\beta + \\varepsilon_i \\qquad \\text{if} \\quad X_i'\\beta + \\varepsilon_i>c \\\\\nc \\qquad \\text{if} \\quad X_i'\\beta + \\varepsilon_i\\leq c \\end{cases}\n$$\nThis is an example of left-censoring, but we could equally allow for right-censoring or both left- and right-censoring. \n\nThe conditional mean of the observed outcome (above the threshold) is given by:\n\n$$\n\\begin{aligned}\n&E[Y_{i}|D_i=1,X_i] \\\\\n=& E[Y_{i}^*|Y_i^*>c,X_i] \\\\\n=&X_i'\\beta + E[\\varepsilon_{i}|\\varepsilon_i>c-X_i'\\beta,X_i] \n\\end{aligned}\n$$\nA defining feature of the Tobit model is that observation, or selection, depends on the (latent) outcome $Y^*$ alone. The following two selection models introduce a separate latent variable that determines selection. \n\n### Non-stochastic threshold model\n\nThe outcome $Y^*$ is missing for some units, as in the case of truncated data. However, we do do observe the vector of characteristics $X_i$ and $Z_i$ (which may overlap) for all units. This is referred to as a **selected sample**. Note, some texts refer to the threshold model as the Heckman selection model, or Heckit for short.  \n\nLet $D^*_i$ be a second *continuous* latent outcome, that determines observation/selection, \n\n$$\nD_i^* = Z_i'\\gamma + \\upsilon_i\n$$\nAs in a binary choice model (see Handout 6), the observable dummy-variable, $D_i$, which denotes selection into the observed sample, can be defined as,\n\n$$\nD_i = \\begin{cases}1 \\qquad \\text{if} \\quad D_i^*>0 \\\\\n0 \\qquad \\text{otherwise}\\end{cases}\n$$\nFor example, $D_i$ may identify employment in a wage model where you only observe wages of employed individuals. \n\nThe observed outcome $Y_i$ is then, \n\n$$\nY_i = \\begin{cases}Y_i^* \\qquad \\text{if} \\quad D_i=1 \\\\\n\\text{missing} \\qquad \\text{otherwise} \\end{cases}\n$$\nGiven the latent variable models for $Y_i^*$ and $D_i^*$, this can be written as,\n\n$$\nY_i = \\begin{cases}X_i'\\beta + \\varepsilon_i \\qquad \\text{if} \\quad Z_i'\\gamma +\\upsilon_i>0 \\\\\n\\text{missing} \\qquad \\text{otherwise} \\end{cases}\n$$\n\nThe model has two error terms which means that we need to make an assumption about their **joint** distribution. If the two errors are independent, then we can ignore the missing observations; i.e., there is no selection bias. \n\nConsider the conditional mean of the (observed) outcome: \n\n$$\n\\begin{aligned}\n&E[Y_i|D_i=1,X_i] \\\\\n=& E[Y_i^*|D_i=1,X_i] \\\\\n=&X_i'\\beta + E[\\varepsilon_i|D_i^*>0,X_i] \\\\\n=&X_i'\\beta + E[\\varepsilon_i|\\upsilon_i>-Z_i'\\gamma,X_i] \\\\\n\\end{aligned}\n$$\n\nIn general, $E[\\varepsilon_i|\\upsilon_i>-Z_i'\\gamma,X_i]\\neq 0$. This means that the OLS estimator of, \n\n$$\nY_i = X_i'\\beta + \\varepsilon_i \\qquad \\text{for}\\quad i:D_i = 1\n$$\n\nis biased. However, if we know the (conditional) joint distribution of $\\begin{bmatrix}\\varepsilon_i & \\upsilon_i\\end{bmatrix}'$ then we may be able to compute this bias and explicitly correct for it in the model. We will shortly see that the assumption of joint normality provides a relatively simple solution to the problem. \n\n### Stochastic threshold model\n\nIn the above model, selection (into observation of $Y_i$) was determined by a separate latent variable to the main outcome. As a result, the correlation between $Y_i^*$ and $D_i$ (or $D_i^*$) depended on the unobserved joint error-term distribution. In a stochastic threshold model, the selection depends directly on $Y_i^*$. \n$$\nY_i = \\begin{cases}Y_i^* \\qquad \\text{if} \\quad Y_i^*>S_i^* \\\\\n\\text{missing} \\qquad \\text{otherwise} \\end{cases}\n$$\nwhere, \n$$\nS_i^* = W_i'\\eta + \\nu_i\n$$\nThus, the indicator of observation is given by, \n$$\nD_i = \\begin{cases}1 \\qquad \\text{if} \\quad Y_i^*>S_i^* \\\\\n0 \\qquad \\text{otherwise}\\end{cases}\n$$\nSelection depends on the realization of $\\varepsilon_i$ and not just on $Cov(\\varepsilon_i,\\upsilon_i)$ (where $\\upsilon_i$ is the error term from the selection equation in the non-stochastic model). \n\nThe observed $Y_i$ is given by,\n$$\nY_i = \\begin{cases}X_i'\\beta + \\varepsilon_i \\qquad \\text{if} \\quad X_i'\\beta + \\varepsilon_i\\geq W_i'\\eta + \\nu_i \\\\\n\\text{missing} \\qquad \\text{otherwise} \\end{cases}\n$$\nAs before, consider the conditional mean of the observed outcome:\n$$\n\\begin{aligned}\n&E[Y_i|D_i=1,X_i] \\\\\n=& E[Y_i^*|D_i=1,X_i] \\\\\n=&X_i'\\beta + E[\\varepsilon_i|D_i^*>0,X_i] \\\\\n=&X_i'\\beta + E[\\varepsilon_i|(Y_i^*-S_i^*)>0,X_i] \\\\\n=&X_i'\\beta + E[\\varepsilon_i|\\upsilon_i>-Z_i'\\gamma,X_i] \n\\end{aligned}\n$$\nwhere, \n$$\n\\upsilon_i = \\varepsilon_i-\\nu_i \\qquad \\text{and}\\qquad Z_i'\\gamma = X_i'\\beta - W_i'\\eta\n$$\nThis result shows that the two models - non-stochastic and stochastic threshold models - are equivalent. This equivalence will have implications for the interpretation of the parameters. \n\nConsider, in the non-stochastic model, we have to consider $Cov(\\varepsilon_i,\\upsilon_i)$. In the stochastic model, this is,\n$$\nCov(\\varepsilon_i,\\upsilon_i) = Cov(\\varepsilon_i,\\varepsilon_i-\\nu_i) = Var(\\varepsilon_i)-Cov(\\varepsilon_i,\\nu_i)\n$$\nIf the $Cov(\\varepsilon_i,\\nu_i)=0$ (i.e., no selection in the stochastic model), then $Cov(\\varepsilon_i,\\upsilon_i)>0$. \n\nThe equivalence of these models also implies that $Z_i$ must contain all variables in either $X_i$ or $W_i$. However, some variables in $W_i$ need not appear in $X_i$. There may be some variables in the selection equation that do not appear in the main equation. These are referred to as excluded variables. \n\nIn both threshold models, selection is determined by both observables ($W_i$) and unobservables ($\\upsilon_i$). Other methods, like propensity score matching assume that selection is determined by observables alone. Instrumental variable approaches allow for selection on both, but do require an excluded instrument. \n\n\n### Endogenous Switching Model\n\nConsider a model where there are two latent outcomes $\\{Y_{1i}^*,Y_{2i}^*\\}$:\n\n$$\n\\begin{aligned}\nY_{1i}^* =& X_i'\\beta_1 + \\varepsilon_{1i} \\\\\nY_{2i}^* =& X_i'\\beta_2 + \\varepsilon_{2i}\n\\end{aligned}\n$$\n\nFor example, these could be wage equations from two sectors. Observation within either state is then determined by another latent variable $D_i^*$:\n\n$$\nY_i = \\begin{cases}Y_{1i}^* \\qquad \\text{if} \\quad D_i^*>0 \\\\\nY_{2i}^* \\qquad \\text{otherwise} \\end{cases}\n$$\n\nwhere, \n\n$$\nD_i^* = Z_i\\gamma + \\theta (Y_{1i}^*-Y_{2i}^*) + \\zeta_i\n$$\nYou could extent this to more than 2 states; but that makes modelling selection a little more complex. In this simple set-up, the observed outcome is given by, \n$$\nY_i = \\begin{cases}X_i'\\beta_1 + \\varepsilon_{1i} \\qquad \\text{if} \\quad Z_i\\gamma + \\theta (Y_{1i}^*-Y_{2i}^*) + \\zeta_i>0 \\\\\nX_i'\\beta_2 + \\varepsilon_{2i} \\qquad \\text{otherwise} \\end{cases}\n$$\n\nAn important limitation within this model is the absence of any state-specific covariates in either the $\\{Y_{1i}^*,Y_{2i}^*\\}$ latent models or $D_i^*$ selection model. Selection can only depend on the difference in the latent outcomes. \n\nFor each individual, we only observe one of the outcomes, which is to say there is a \"missing counterfactual\". This is a similar structure then to the potential outcomes framework discussed in Handout 8. Although, with important differences. Here, each covariate has a state-specific vector $\\beta_m$. \n\nThe conditional mean of the observed outcome in each state is given by:\n\n$$\n\\begin{aligned}\n&E[Y_{1i}|D_i=1,X_i] \\\\\n=& E[Y_{1i}^*|D_i=1,X_i] \\\\\n=&X_i'\\beta_1 + E[\\varepsilon_{1i}|D_i^*>0,X_i] \\\\\n=&X_i'\\beta_1 + E[\\varepsilon_{1i}|\\zeta_i>-Z_i'\\gamma-\\theta (Y_{1i}^*-Y_{2i}^*),X_i] \n\\end{aligned}\n$$\nfor state 1, and for state 2:\n$$\n\\begin{aligned}\n&E[Y_{2i}|D_i=0,X_i] \\\\\n=& E[Y_{2i}^*|D_i=0,X_i] \\\\\n=&X_i'\\beta_2 + E[\\varepsilon_{2i}|D_i^*\\leq0,X_i] \\\\\n=&X_i'\\beta_2 + E[\\varepsilon_{2i}|\\zeta_i\\leq -Z_i'\\gamma-\\theta (Y_{1i}^*-Y_{2i}^*),X_i] \n\\end{aligned}\n$$\n\n\n## Estimation\n\nThe goal is to estimate the $\\beta$ parameters from the latent model under various observation mechanisms. \n\n$$\nY_i^* = X_i^*\\beta + \\varepsilon_i\n$$\n\nHowever, we can only ever use the observed outcome $Y_i$ and $D_i$ (indicator of selection/observation).\n\n### Tobit model\n\nRecall, the conditional mean of the observed (non-censored) outcome variable. Applying the definition of the IMR, we have, \n\n$$\n\\begin{aligned}\nE[Y_{i}|Y_i^*>c,X_i] =&X_i'\\beta + E[\\varepsilon_{i}|\\varepsilon_i>c-X_i'\\beta,X_i] \\\\\n=&X_i'\\beta + \\sigma_\\varepsilon E\\bigg[\\frac{\\varepsilon_i}{\\sigma_\\varepsilon}\\bigg|\\frac{\\varepsilon_i}{\\sigma_\\varepsilon}>\\frac{c-X_i'\\beta}{\\sigma_\\varepsilon},X_i\\bigg] \\\\\n=&X_i'\\beta + \\sigma_\\varepsilon \\lambda\\bigg(\\frac{X_i'\\beta-c}{\\sigma_\\varepsilon}\\bigg)\n\\end{aligned}\n$$\nThis equation can be estimated by OLS using a two-step estimator (see of Heckman correction below). However, the more efficient estimator is Maximum Likelihood. The joint likelihood is given by,\n\n$$\n\\begin{aligned}\nL_n(\\theta) =&\\prod_{i:D_i=1}f^*(Y_i|X_i;\\theta)\\prod_{i:D_i=0}F^*(c_i|X_i;\\theta) \\\\\n  =&\\prod_{i=1}^n\\bigg(\\frac{1}{\\sigma_\\varepsilon}\\phi\\bigg(\\frac{Y_i-X_i'\\beta}{\\sigma_\\varepsilon}\\bigg)\\bigg)^{D_i}\\Phi\\bigg(\\frac{c-X_i'\\beta}{\\sigma_\\varepsilon}\\bigg)^{1-D_i}\n  \\end{aligned}\n$$\nwhere $\\theta = [\\beta,\\sigma_\\varepsilon]$ and $F^*(c_i|X_i;\\theta) = Pr(Y_i\\leq c|X_i)$.\n\nNotice, the first part of the likelihood function looks like the likelihood of the CLRM (from Handout 3) while the second part is similar to a probit model likelihood function for $D_i=0$ (from Handout 6). One issue with the likelihood function is that it is not globally concave (see Tobit II model). \n\n### Threshold model\n\nHeckman (1979) put forward a novel solution for the estimation of sample selection models. The approach adds a generated-regressor to the (linear) estimating equation that corrects for the endogenous selection. *Conditional on observation*, the error term in the *observed* model will not be mean zero. The approach is called a **control function** approach and means that the bias corrected model can be estimated using OLS and not ML (which was computationally demanding at the time). \n\nRecall from the endogenous selection models (stochastic or non-stochatic), that the conditional mean of the observed outcome was, \n\n$$\nE[Y_i|D_i=1,X_i] = X_i'\\beta + E[\\varepsilon_i|\\upsilon_i>-Z_i'\\gamma,X_i] \n$$\nSuppose, \n$$\n\\begin{bmatrix}\\varepsilon_i \\\\ \\upsilon_i\\end{bmatrix}|X_i\\sim N\\bigg(\\begin{bmatrix}0 \\\\ 0\\end{bmatrix},\\begin{bmatrix}\\sigma^2_{\\varepsilon} & \\sigma_{\\varepsilon\\upsilon} \\\\ \\sigma_{\\varepsilon\\upsilon} & 1\\end{bmatrix}\\bigg)\n$$\nWe need to normalize the variance of $\\upsilon_i$ as it is not identified. Recall, $\\upsilon_i$ is associated with the discrete outcome $D_i$ that indicates observation. Just as in a probit model, the variance of the latent variable model error term is not identified. \n\nGiven, the joint normality assumption:\n$$\nE[Y_i|D_i=1,X_i] = X_i'\\beta + \\sigma_{\\varepsilon\\upsilon}\\lambda(Z_i'\\gamma)\n$$\n\nThe **Heckman two-step estimator** estimates the adjusted model, \n\n$$\nY_i = X_i'\\beta + \\sigma_{\\varepsilon\\upsilon}\\hat{\\lambda}_i + \\varepsilon_i \\qquad \\text{for}\\quad i:D_i = 1\n$$\nwhere, \n\n1.    Estimate a probit model of $D_i$, using variables $Z$ (those variables in $X_i$ and $W_i$) and construct the IMR:\n\n$$\n\\hat{\\lambda}_i = \\lambda(Z_i'\\hat{\\gamma}) = \\frac{\\phi(Z_i'\\hat{\\gamma})}{\\Phi(Z_i'\\hat{\\gamma})}\n$$\n\n2.    Estimate the linear model with the added IMR using the observed (selected) sample. \n\nRecall, if $\\sigma_{\\varepsilon\\upsilon}=0$ **in the non-stochastic model**, selection into observation is unrelated to the latent outcome. Hence, $H_0:\\sigma_{\\varepsilon\\upsilon}=0$ is a valid test for the selection, *provided the model is non-stochastic*.   \n\nSince the second step includes a generated-regressor, the default vairance estimator will be incorrect. This is standard problem with two-step estimators, like two-stage-least-squares for instrumental variables. \n\nIdentifcation relies on the non-linearity of the IMR. The IMR can be approximately linear for some values which means that the identification depends on there being a significant share of observations in the tail where the IMR is non-linear. In practice, many researchers will include squared terms of the IMR. Identification does NOT rely on an excluded variable: a variable in the selection equation that does not appear in the main equation. However, it will help to have an excluded variable.  \n\nHeckman's control function approach removed the need to use a Maximum Likelihood. However, the ML estimator is more efficient. The joint likelihood is given by,\n\n$$\n\\begin{aligned}\nL_n(\\theta) =&\\prod_{i:D_i=1}f^*(Y_i|D_i=1,Z_i;\\theta)\\cdot Pr(D_i=1|Z_i)\\prod_{i:D_i=0}Pr(D_i=0|Z_i) \\\\\n=& \\prod_{i:D_i=1} f^*(Y_i|Z_i;\\theta)\\cdot Pr(D_i=1|Y_i,Z_i)\\prod_{i:D_i=0}Pr(D_i=0|Z_i)\\\\\n=&\\prod_{i=1}^n\\bigg[\\frac{1}{\\sigma_\\varepsilon}\\phi\\bigg(\\frac{Y_i-X_i'\\beta}{\\sigma_\\varepsilon}\\bigg)\\cdot\\Phi\\bigg(\\frac{Z_i'\\gamma+\\sigma_{\\varepsilon\\upsilon}\\sigma_{\\varepsilon}^{-2} (Y_i-X_i'\\beta)}{\\sqrt{1-\\sigma_{\\varepsilon\\upsilon}^2\\sigma_{\\varepsilon}^{-2}}}\\bigg)\\bigg]^{D_i}(1-\\Phi(Z_i'\\gamma)\\big)^{1-D_i} \n\\end{aligned}\n$$\nwhere $\\theta=[\\beta,\\gamma,\\sigma_\\varepsilon,\\sigma_{\\varepsilon\\upsilon}]$. Line two follows from Bayes' rule. The second term in the third row derives from:\n\n$$\n\\begin{aligned}\nPr(D_i=1|Y_i,Z_i) =& Pr(\\upsilon_i>-Z_i'\\gamma|Y_i,Z_i) \\\\\n=& Pr(\\sigma_{\\varepsilon\\upsilon}/\\sigma_{\\varepsilon}^2\\varepsilon_i+ \\zeta_i >-Z_i'\\gamma|Y_i,Z_i) \\\\\n=& Pr(\\zeta_i >-Z_i'\\gamma-\\sigma_{\\varepsilon\\upsilon}/\\sigma_{\\varepsilon}^2\\varepsilon_i|Y_i,Z_i) \\\\\n=& \\Phi\\bigg(\\frac{Z_i'\\gamma+\\sigma_{\\varepsilon\\upsilon}\\sigma_{\\varepsilon}^{-2} (Y_i-X_i'\\beta)}{\\sqrt{1-\\sigma_{\\varepsilon\\upsilon}^2\\sigma_{\\varepsilon}^{-2}}}\\bigg)\n\\end{aligned}\n$$\nThis is because of the joint normality of the errors, which allows us to write, \n\n$$\n\\upsilon_i = \\sigma_{\\varepsilon\\upsilon}/\\sigma_{\\varepsilon}^2\\varepsilon_i+ \\zeta_i \\qquad \\text{where}\\quad \\zeta_i\\sim N(0,1-\\sigma^2_{\\varepsilon\\upsilon}/\\sigma_{\\varepsilon}^2)\n$$\nConditional on $Y_i^*$, $\\varepsilon_i$ is not random, which is why is moved to the right-handside and is substituted with $\\varepsilon_i=Y_i-X_i'\\beta$.\n\nNotice, if $\\sigma_{\\varepsilon\\upsilon}=0$, then likelihood of selection and observing $Y_i$ are independent of one another.  \n\n### Observational equivalence\n\nIn the non-stochastic threshold model, $Cov(\\varepsilon_i,\\upsilon_i)=\\sigma_{\\varepsilon\\upsilon}=0$ implies there is not selection-bias in the main equation. However, in the stochastic model $\\sigma_{\\varepsilon\\upsilon}>0$ cannot be zero; since $\\upsilon_i = \\varepsilon_i-\\nu_i$\n\n$$\nCov(\\varepsilon_i,\\upsilon_i) = Cov(\\varepsilon_i,\\varepsilon_i-\\nu_i) = Var(\\varepsilon_i)-Cov(\\varepsilon_i,\\nu_i)\n$$\nIn the stochastic model, *when* there is no selection if $Cov(\\varepsilon_i,\\nu_i)=0$. However, this implies that $\\sigma_{\\varepsilon\\upsilon}=\\sigma_\\varepsilon$. As a result, \n\n$$\nE[Y_i|D_i=1,X_i] = X_i'\\beta + \\sigma_{\\varepsilon\\upsilon}\\lambda(Z_i'\\gamma) = X'\\beta + \\sigma_\\varepsilon\\lambda(X_i'\\beta)\n$$\n\nThe stochastic threshold model returns a Tobit model when there is no selection. \n\nWe have a puzzle! You cannot distinguish the stochastic and non-stochastic threshold models. The test for selection - $H_0:\\sigma_{\\varepsilon\\upsilon}=0$ - using the coefficient on the IMR is only valid under the non-stochastic case, since $\\sigma_{\\varepsilon\\upsilon}>0$ in the stochastic case. This means that if the stochastic model is valid, there is no test for selection. And if there is no selection, in the stochastic model, the model is essentially a tobit model. \n\nThis puzzle suggests that all three models are empirically indistinguishable. You need to make an economic argument for why one model is valid. \n\n\n## Interpretation\n\nIn the above models, any regressor affects expected value of the observed outcome through two potential channels: (1) the direct effect; (2) the selection effect. \n\nConsider, there is the effect of $X_i$ on the latent outcome:\n\n$$\n\\frac{\\partial E[Y_i^*|X_i]}{\\partial X_i} = \\beta\n$$\n\nThen, there is the effect of $Z_i$ (which includes $X_i$, but also potential excluded variables) on selection (into observation):\n\n$$\n\\frac{\\partial Pr(D_i=1|Z_i)}{\\partial Z_i} \n$$\n\nThen, there is the effect of $X_i$ conditional on selection (observation),  \n\n$$\n\\frac{\\partial E[Y_i|X_i,D_i=1]}{\\partial X_i}\n$$\nand the total effect of $X_i$ on the observed outcome:\n\n$$\n\\frac{\\partial E[Y_i|X_i]}{\\partial X_i}\n$$\n\n### Tobit model\n\nWe can use the Law of Iterated Expectations to expand the conditional mean of $E[Y_i|X_i]$. Let where $\\omega_i= \\frac{X_i'\\beta-c}{\\sigma_\\varepsilon}$, then\n\n$$\n\\begin{aligned}\nE[Y_i|X_i] =& E[Y_i|X_i,D_i=1]\\cdot Pr(D_i=1|X_i)+E[Y_i|X_i,D_i=0]\\cdot Pr(D_i=0|X_i) \\\\\n=&E[Y_i|X_i,D_i=1]\\cdot Pr(D_i=1|X_i) + c\\cdot Pr(D_i=0|X_i) \\\\\n=&\\big[X_i'\\beta + \\sigma_\\varepsilon \\lambda(\\omega_i)\\big]\\cdot \\Phi(\\omega_i)+c\\cdot \\big[1-\\Phi(\\omega_i)\\big]\n\\end{aligned}\n$$\nRecall, in this case $Pr(D_i=1|X_i)=Pr(Y_i^*>c|X_i)$. Naturally, $c=0$ simplifies the expression. \n\nThe relevant marginal effects are:\n\n-   **conditional on observation/selection:** \n\n$$\n\\begin{aligned}\n\\frac{\\partial E[Y_i|D_i=1,X_i]}{\\partial X_i} =& \\big[1 + \\lambda'(\\omega_i)\\big]\\beta \\\\\n=& \\big[1 -\\omega_i\\cdot\\lambda(\\omega_i)-\\lambda(\\omega_i)^2\\big]\\beta\n\\end{aligned}\n$$\nYou can verify this result using the definition of the IMR. \n\n-   **observation/selection:** \n\n$$\n\\frac{\\partial Pr(Y_i=1|X_i)}{\\partial X_i} = \\phi(\\omega_i)\\beta/\\sigma_\\varepsilon\n$$\n\n### Threshold model\n\nLet us assume that all $X_i$ regressors are included in $Z_i$, which we know should be the case given the observational equivalence of the stochastic and non-stochastic models. The above marginal effects for these selection models is given by,\n\n-   **conditional on selection:** \n$$\n\\begin{aligned}\n\\frac{\\partial E[Y_i|D_i=1,Z_i]}{\\partial X_i} =& \\beta + \\sigma_{\\varepsilon\\upsilon}\\lambda'(Z_i'\\gamma)\\tilde{\\gamma} \\\\\n=&\\beta -\\sigma_{\\varepsilon\\upsilon}\\big[Z_i'\\gamma\\cdot\\lambda(Z_i'\\gamma) + \\lambda(Z_i'\\gamma)^2\\big]\\tilde{\\gamma}\n\\end{aligned}\n$$\nwhere $\\tilde{\\gamma}$ refers to the subset of $\\gamma$-vector corresponding to the $X_i$ regressors that appear in $Z_i$. That is, it excludes the parameters on any excluded variables.  \n\n-   **selection:** \n\n$$\n\\frac{\\partial Pr(D_i=1|Z_i)}{\\partial Z_i} = \\phi(Z_i'\\gamma)\\gamma\n$$\n\nRecall, in the threshold model the variance of the selection equation is not identified. Hence, the selection margin effect resembles that of a probit model. In the Tobit model, selection depends on the outcome itself and the variance can be identified from the part of the data where $Y$ is observed. \n\n## References\n",
    "supporting": [
      "handout-7_files\\figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}